### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.45 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.67 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.47 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.26 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.04 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.29 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.48 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.48 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.06 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.77 sec*proc (28 tests)

Total Test time (real) = 222.78 sec

real	3m42.807s
user	7m43.381s
sys	0m6.327s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.16 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.31 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.41 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.10 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.43 sec*proc (28 tests)

Total Test time (real) =  51.44 sec

real	0m51.454s
user	1m11.795s
sys	0m5.673s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.134 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.206 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.302 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.312 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.314 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.315 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.315 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.322 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.322 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.323 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.324 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.324 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.328 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.329 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.330 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.330 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.331 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.332 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.332 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.632 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.847 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.849 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.850 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.851 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.851 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.029.851 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.852 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.029.852 I llama_model_loader: - type  f32:  124 tensors
0.00.029.853 I llama_model_loader: - type  f16:   73 tensors
0.00.034.315 I llm_load_vocab: special tokens cache size = 5
0.00.036.487 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.036.491 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.036.491 I llm_load_print_meta: arch             = bert
0.00.036.492 I llm_load_print_meta: vocab type       = WPM
0.00.036.492 I llm_load_print_meta: n_vocab          = 30522
0.00.036.492 I llm_load_print_meta: n_merges         = 0
0.00.036.493 I llm_load_print_meta: vocab_only       = 0
0.00.036.493 I llm_load_print_meta: n_ctx_train      = 512
0.00.036.493 I llm_load_print_meta: n_embd           = 384
0.00.036.493 I llm_load_print_meta: n_layer          = 12
0.00.036.504 I llm_load_print_meta: n_head           = 12
0.00.036.505 I llm_load_print_meta: n_head_kv        = 12
0.00.036.505 I llm_load_print_meta: n_rot            = 32
0.00.036.506 I llm_load_print_meta: n_swa            = 0
0.00.036.506 I llm_load_print_meta: n_embd_head_k    = 32
0.00.036.506 I llm_load_print_meta: n_embd_head_v    = 32
0.00.036.507 I llm_load_print_meta: n_gqa            = 1
0.00.036.508 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.036.509 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.036.510 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.036.510 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.036.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.036.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.036.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.036.514 I llm_load_print_meta: n_ff             = 1536
0.00.036.514 I llm_load_print_meta: n_expert         = 0
0.00.036.515 I llm_load_print_meta: n_expert_used    = 0
0.00.036.515 I llm_load_print_meta: causal attn      = 0
0.00.036.515 I llm_load_print_meta: pooling type     = 2
0.00.036.515 I llm_load_print_meta: rope type        = 2
0.00.036.515 I llm_load_print_meta: rope scaling     = linear
0.00.036.516 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.036.516 I llm_load_print_meta: freq_scale_train = 1
0.00.036.517 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.036.523 I llm_load_print_meta: rope_finetuned   = unknown
0.00.036.523 I llm_load_print_meta: ssm_d_conv       = 0
0.00.036.523 I llm_load_print_meta: ssm_d_inner      = 0
0.00.036.523 I llm_load_print_meta: ssm_d_state      = 0
0.00.036.524 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.036.529 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.036.547 I llm_load_print_meta: model type       = 33M
0.00.036.548 I llm_load_print_meta: model ftype      = F16
0.00.036.548 I llm_load_print_meta: model params     = 33.21 M
0.00.036.549 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.036.549 I llm_load_print_meta: general.name     = Bge Small
0.00.036.550 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.036.550 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.036.550 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.036.553 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.036.553 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.036.553 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.036.554 I llm_load_print_meta: max token length = 21
0.00.038.713 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.038.714 I llm_load_tensors: offloading output layer to GPU
0.00.038.715 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.038.736 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.738 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.343 I llama_new_context_with_model: n_seq_max     = 1
0.00.039.344 I llama_new_context_with_model: n_ctx         = 512
0.00.039.345 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.039.345 I llama_new_context_with_model: n_batch       = 2048
0.00.039.345 I llama_new_context_with_model: n_ubatch      = 2048
0.00.039.346 I llama_new_context_with_model: flash_attn    = 0
0.00.039.346 I llama_new_context_with_model: freq_base     = 10000.0
0.00.039.347 I llama_new_context_with_model: freq_scale    = 1
0.00.039.348 I ggml_metal_init: allocating
0.00.039.352 I ggml_metal_init: found device: Apple M4
0.00.039.357 I ggml_metal_init: picking default device: Apple M4
0.00.040.287 I ggml_metal_init: using embedded metal library
0.00.044.656 I ggml_metal_init: GPU name:   Apple M4
0.00.044.659 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.660 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.660 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.660 I ggml_metal_init: simdgroup reduction   = true
0.00.044.661 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.661 I ggml_metal_init: has bfloat            = true
0.00.044.661 I ggml_metal_init: use bfloat            = true
0.00.044.662 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.662 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.056.808 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.057.365 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.057.367 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.057.368 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.144 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.146 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.146 I llama_new_context_with_model: graph nodes  = 429
0.00.058.147 I llama_new_context_with_model: graph splits = 2
0.00.058.167 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.058.168 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.546 I 
0.00.065.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.250 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.305 I llama_perf_context_print:        load time =      46.33 ms
0.00.071.306 I llama_perf_context_print: prompt eval time =       4.89 ms /     9 tokens (    0.54 ms per token,  1838.99 tokens per second)
0.00.071.306 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.307 I llama_perf_context_print:       total time =       5.76 ms /    10 tokens
0.00.071.451 I ggml_metal_free: deallocating

real	0m0.252s
user	0m0.050s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.037 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.106 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.072 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.075 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.076 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.077 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.077 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.078 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.079 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.082 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.082 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.087 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.087 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.088 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.090 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.090 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.090 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.090 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.091 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.091 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.091 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.173 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.175 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.175 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.175 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.176 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.176 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.176 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.177 I llama_model_loader: - type  f32:  124 tensors
0.00.014.177 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.629 I llm_load_vocab: special tokens cache size = 5
0.00.017.912 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.017.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.017.916 I llm_load_print_meta: arch             = bert
0.00.017.917 I llm_load_print_meta: vocab type       = WPM
0.00.017.921 I llm_load_print_meta: n_vocab          = 30522
0.00.017.921 I llm_load_print_meta: n_merges         = 0
0.00.017.922 I llm_load_print_meta: vocab_only       = 0
0.00.017.922 I llm_load_print_meta: n_ctx_train      = 512
0.00.017.922 I llm_load_print_meta: n_embd           = 384
0.00.017.922 I llm_load_print_meta: n_layer          = 12
0.00.017.932 I llm_load_print_meta: n_head           = 12
0.00.017.933 I llm_load_print_meta: n_head_kv        = 12
0.00.017.934 I llm_load_print_meta: n_rot            = 32
0.00.017.934 I llm_load_print_meta: n_swa            = 0
0.00.017.934 I llm_load_print_meta: n_embd_head_k    = 32
0.00.017.934 I llm_load_print_meta: n_embd_head_v    = 32
0.00.017.934 I llm_load_print_meta: n_gqa            = 1
0.00.017.935 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.017.936 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.017.936 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.017.936 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.017.936 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.017.937 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.017.937 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.017.937 I llm_load_print_meta: n_ff             = 1536
0.00.017.938 I llm_load_print_meta: n_expert         = 0
0.00.017.938 I llm_load_print_meta: n_expert_used    = 0
0.00.017.938 I llm_load_print_meta: causal attn      = 0
0.00.017.938 I llm_load_print_meta: pooling type     = 2
0.00.017.938 I llm_load_print_meta: rope type        = 2
0.00.017.938 I llm_load_print_meta: rope scaling     = linear
0.00.017.939 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.017.939 I llm_load_print_meta: freq_scale_train = 1
0.00.017.939 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.017.939 I llm_load_print_meta: rope_finetuned   = unknown
0.00.017.939 I llm_load_print_meta: ssm_d_conv       = 0
0.00.017.939 I llm_load_print_meta: ssm_d_inner      = 0
0.00.017.940 I llm_load_print_meta: ssm_d_state      = 0
0.00.017.940 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.017.940 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.017.945 I llm_load_print_meta: model type       = 33M
0.00.017.945 I llm_load_print_meta: model ftype      = Q8_0
0.00.017.946 I llm_load_print_meta: model params     = 33.21 M
0.00.017.946 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.017.946 I llm_load_print_meta: general.name     = Bge Small
0.00.017.947 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.017.947 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.017.947 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.017.947 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.017.947 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.017.947 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.017.948 I llm_load_print_meta: max token length = 21
0.00.019.253 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.253 I llm_load_tensors: offloading output layer to GPU
0.00.019.254 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.260 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.260 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.623 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.624 I llama_new_context_with_model: n_ctx         = 512
0.00.019.624 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.624 I llama_new_context_with_model: n_batch       = 2048
0.00.019.625 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.625 I llama_new_context_with_model: flash_attn    = 0
0.00.019.625 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.626 I llama_new_context_with_model: freq_scale    = 1
0.00.019.626 I ggml_metal_init: allocating
0.00.019.629 I ggml_metal_init: found device: Apple M4
0.00.019.632 I ggml_metal_init: picking default device: Apple M4
0.00.020.222 I ggml_metal_init: using embedded metal library
0.00.022.571 I ggml_metal_init: GPU name:   Apple M4
0.00.022.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.574 I ggml_metal_init: simdgroup reduction   = true
0.00.022.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.574 I ggml_metal_init: has bfloat            = true
0.00.022.574 I ggml_metal_init: use bfloat            = true
0.00.022.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.942 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.496 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.499 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.503 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.151 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.152 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.152 I llama_new_context_with_model: graph nodes  = 429
0.00.034.152 I llama_new_context_with_model: graph splits = 2
0.00.034.165 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.552 I 
0.00.039.581 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.101 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.676 I llama_perf_context_print:        load time =      30.44 ms
0.00.044.677 I llama_perf_context_print: prompt eval time =       4.46 ms /     9 tokens (    0.50 ms per token,  2018.84 tokens per second)
0.00.044.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.679 I llama_perf_context_print:       total time =       5.13 ms /    10 tokens
0.00.044.850 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.182 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.614 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.086 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.091 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.094 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.096 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.096 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.097 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.099 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.100 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.100 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.101 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.102 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.109 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.110 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.110 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.112 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.045.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.048.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.053.295 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.296 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.053.296 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.053.297 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.053.297 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.053.297 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.053.298 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.053.298 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.053.298 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.053.299 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.053.299 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.053.300 I llama_model_loader: - type  f32:   40 tensors
0.00.053.300 I llama_model_loader: - type  f16:   30 tensors
0.00.071.776 W llm_load_vocab: empty token at index 5
0.00.076.412 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.077.792 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.077.823 I llm_load_vocab: special tokens cache size = 5
0.00.339.345 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.354 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.355 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.355 I llm_load_print_meta: vocab type       = BPE
0.00.339.355 I llm_load_print_meta: n_vocab          = 61056
0.00.339.355 I llm_load_print_meta: n_merges         = 39382
0.00.339.356 I llm_load_print_meta: vocab_only       = 0
0.00.339.356 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.356 I llm_load_print_meta: n_embd           = 384
0.00.339.363 I llm_load_print_meta: n_layer          = 4
0.00.339.395 I llm_load_print_meta: n_head           = 12
0.00.339.396 I llm_load_print_meta: n_head_kv        = 12
0.00.339.396 I llm_load_print_meta: n_rot            = 32
0.00.339.396 I llm_load_print_meta: n_swa            = 0
0.00.339.396 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.396 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.397 I llm_load_print_meta: n_gqa            = 1
0.00.339.401 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.402 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.404 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.405 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.405 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.405 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.407 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.407 I llm_load_print_meta: n_ff             = 1536
0.00.339.408 I llm_load_print_meta: n_expert         = 0
0.00.339.408 I llm_load_print_meta: n_expert_used    = 0
0.00.339.408 I llm_load_print_meta: causal attn      = 0
0.00.339.408 I llm_load_print_meta: pooling type     = -1
0.00.339.408 I llm_load_print_meta: rope type        = -1
0.00.339.408 I llm_load_print_meta: rope scaling     = linear
0.00.339.413 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.413 I llm_load_print_meta: freq_scale_train = 1
0.00.339.413 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.414 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.414 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.414 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.414 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.414 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.415 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.416 I llm_load_print_meta: model type       = 33M
0.00.339.416 I llm_load_print_meta: model ftype      = F16
0.00.339.416 I llm_load_print_meta: model params     = 32.90 M
0.00.339.418 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.418 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.419 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.419 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.419 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.420 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.420 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.420 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.421 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.421 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.421 I llm_load_print_meta: max token length = 45
0.00.340.831 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.340.831 I llm_load_tensors: offloading output layer to GPU
0.00.340.832 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.340.856 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.340.857 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.342.460 I llama_new_context_with_model: n_seq_max     = 1
0.00.342.461 I llama_new_context_with_model: n_ctx         = 8192
0.00.342.462 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.342.462 I llama_new_context_with_model: n_batch       = 2048
0.00.342.462 I llama_new_context_with_model: n_ubatch      = 2048
0.00.342.462 I llama_new_context_with_model: flash_attn    = 0
0.00.342.462 I llama_new_context_with_model: freq_base     = 10000.0
0.00.342.463 I llama_new_context_with_model: freq_scale    = 1
0.00.342.464 I ggml_metal_init: allocating
0.00.342.468 I ggml_metal_init: found device: Apple M4
0.00.342.470 I ggml_metal_init: picking default device: Apple M4
0.00.343.642 I ggml_metal_init: using embedded metal library
0.00.346.514 I ggml_metal_init: GPU name:   Apple M4
0.00.346.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.516 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.516 I ggml_metal_init: simdgroup reduction   = true
0.00.346.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.517 I ggml_metal_init: has bfloat            = true
0.00.346.517 I ggml_metal_init: use bfloat            = true
0.00.346.517 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.355.917 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.358.350 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.358.352 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.358.354 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.358.987 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.358.988 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.358.989 I llama_new_context_with_model: graph nodes  = 154
0.00.358.989 I llama_new_context_with_model: graph splits = 2
0.00.359.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.359.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.447 I 
0.00.371.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.922 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.371.923 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.371.928 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.371.928 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.371.935 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.371.935 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.372.436 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.376.244 I llama_perf_context_print:        load time =     345.83 ms
0.00.376.245 I llama_perf_context_print: prompt eval time =       3.80 ms /    62 tokens (    0.06 ms per token, 16311.50 tokens per second)
0.00.376.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.376.246 I llama_perf_context_print:       total time =       4.80 ms /    63 tokens
0.00.376.507 I ggml_metal_free: deallocating

real	0m1.101s
user	0m0.344s
sys	0m0.049s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.092 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.208 I main: llama backend init
0.00.000.214 I main: load the model and apply lora adapter, if any
0.00.052.119 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.062.937 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.062.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.062.952 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.062.953 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.062.954 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.062.954 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.062.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.062.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.062.958 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.062.958 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.062.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.062.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.062.960 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.062.964 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.062.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.062.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.062.970 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.069.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.072.069 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.246 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.079.255 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.256 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.256 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.258 I llama_model_loader: - type  f32:  194 tensors
0.00.079.259 I llama_model_loader: - type  f16:   98 tensors
0.00.118.501 I llm_load_vocab: special tokens cache size = 25
0.00.126.352 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.126.355 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.126.356 I llm_load_print_meta: arch             = gptneox
0.00.126.356 I llm_load_print_meta: vocab type       = BPE
0.00.126.356 I llm_load_print_meta: n_vocab          = 50304
0.00.126.357 I llm_load_print_meta: n_merges         = 50009
0.00.126.357 I llm_load_print_meta: vocab_only       = 0
0.00.126.357 I llm_load_print_meta: n_ctx_train      = 2048
0.00.126.357 I llm_load_print_meta: n_embd           = 2048
0.00.126.357 I llm_load_print_meta: n_layer          = 24
0.00.126.372 I llm_load_print_meta: n_head           = 16
0.00.126.374 I llm_load_print_meta: n_head_kv        = 16
0.00.126.374 I llm_load_print_meta: n_rot            = 32
0.00.126.374 I llm_load_print_meta: n_swa            = 0
0.00.126.374 I llm_load_print_meta: n_embd_head_k    = 128
0.00.126.374 I llm_load_print_meta: n_embd_head_v    = 128
0.00.126.375 I llm_load_print_meta: n_gqa            = 1
0.00.126.376 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.126.377 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.126.377 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.126.378 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.126.378 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.126.378 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.126.378 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.126.379 I llm_load_print_meta: n_ff             = 8192
0.00.126.379 I llm_load_print_meta: n_expert         = 0
0.00.126.379 I llm_load_print_meta: n_expert_used    = 0
0.00.126.379 I llm_load_print_meta: causal attn      = 1
0.00.126.379 I llm_load_print_meta: pooling type     = 0
0.00.126.380 I llm_load_print_meta: rope type        = 2
0.00.126.380 I llm_load_print_meta: rope scaling     = linear
0.00.126.380 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.126.380 I llm_load_print_meta: freq_scale_train = 1
0.00.126.381 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.126.381 I llm_load_print_meta: rope_finetuned   = unknown
0.00.126.381 I llm_load_print_meta: ssm_d_conv       = 0
0.00.126.381 I llm_load_print_meta: ssm_d_inner      = 0
0.00.126.381 I llm_load_print_meta: ssm_d_state      = 0
0.00.126.381 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.126.382 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.126.382 I llm_load_print_meta: model type       = 1.4B
0.00.126.383 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.126.383 I llm_load_print_meta: model params     = 1.41 B
0.00.126.383 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.126.384 I llm_load_print_meta: general.name     = 1.4B
0.00.126.384 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.126.384 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.126.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.126.385 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.126.385 I llm_load_print_meta: LF token         = 128 ''
0.00.126.385 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.126.385 I llm_load_print_meta: max token length = 1024
0.00.129.132 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.129.132 I llm_load_tensors: offloading output layer to GPU
0.00.129.133 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.129.152 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.129.153 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.130.224 I llama_new_context_with_model: n_seq_max     = 1
0.00.130.225 I llama_new_context_with_model: n_ctx         = 2048
0.00.130.225 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.130.225 I llama_new_context_with_model: n_batch       = 2048
0.00.130.226 I llama_new_context_with_model: n_ubatch      = 512
0.00.130.226 I llama_new_context_with_model: flash_attn    = 0
0.00.130.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.130.227 I llama_new_context_with_model: freq_scale    = 1
0.00.130.227 I ggml_metal_init: allocating
0.00.130.231 I ggml_metal_init: found device: Apple M4
0.00.130.233 I ggml_metal_init: picking default device: Apple M4
0.00.130.922 I ggml_metal_init: using embedded metal library
0.00.140.592 I ggml_metal_init: GPU name:   Apple M4
0.00.140.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.140.594 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.140.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.140.595 I ggml_metal_init: simdgroup reduction   = true
0.00.140.595 I ggml_metal_init: simdgroup matrix mul. = true
0.00.140.595 I ggml_metal_init: has bfloat            = true
0.00.140.595 I ggml_metal_init: use bfloat            = true
0.00.140.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.140.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.165.281 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.186.536 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.186.544 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.186.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.187.620 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.187.623 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.187.623 I llama_new_context_with_model: graph nodes  = 967
0.00.187.623 I llama_new_context_with_model: graph splits = 2
0.00.187.648 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.187.794 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.187.795 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.268.729 I main: llama threadpool init, n_threads = 4
0.00.268.768 I 
0.00.268.811 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.268.812 I 
0.00.268.885 I sampler seed: 1234
0.00.268.889 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.268.913 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.268.915 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.268.915 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.117.749 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55817.61 tokens per second)
0.02.117.750 I llama_perf_context_print:        load time =     216.60 ms
0.02.117.751 I llama_perf_context_print: prompt eval time =      43.78 ms /     7 tokens (    6.25 ms per token,   159.90 tokens per second)
0.02.117.751 I llama_perf_context_print:        eval time =    1802.20 ms /    63 runs   (   28.61 ms per token,    34.96 tokens per second)
0.02.117.752 I llama_perf_context_print:       total time =    1849.02 ms /    70 tokens
0.02.118.024 I ggml_metal_free: deallocating

real	0m2.438s
user	0m0.152s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.549 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.827 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.671 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.678 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.684 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.686 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.688 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.691 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.692 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.117 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.230 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.791 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.794 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.796 I llama_model_loader: - type  f32:  194 tensors
0.00.054.796 I llama_model_loader: - type  f16:   98 tensors
0.00.084.252 I llm_load_vocab: special tokens cache size = 25
0.00.090.815 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.818 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.819 I llm_load_print_meta: arch             = gptneox
0.00.090.819 I llm_load_print_meta: vocab type       = BPE
0.00.090.819 I llm_load_print_meta: n_vocab          = 50304
0.00.090.819 I llm_load_print_meta: n_merges         = 50009
0.00.090.819 I llm_load_print_meta: vocab_only       = 0
0.00.090.819 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.820 I llm_load_print_meta: n_embd           = 2048
0.00.090.820 I llm_load_print_meta: n_layer          = 24
0.00.090.834 I llm_load_print_meta: n_head           = 16
0.00.090.835 I llm_load_print_meta: n_head_kv        = 16
0.00.090.835 I llm_load_print_meta: n_rot            = 32
0.00.090.836 I llm_load_print_meta: n_swa            = 0
0.00.090.836 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.836 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.836 I llm_load_print_meta: n_gqa            = 1
0.00.090.837 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.838 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.838 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.838 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.839 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.839 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.839 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.840 I llm_load_print_meta: n_ff             = 8192
0.00.090.840 I llm_load_print_meta: n_expert         = 0
0.00.090.840 I llm_load_print_meta: n_expert_used    = 0
0.00.090.840 I llm_load_print_meta: causal attn      = 1
0.00.090.841 I llm_load_print_meta: pooling type     = 0
0.00.090.841 I llm_load_print_meta: rope type        = 2
0.00.090.841 I llm_load_print_meta: rope scaling     = linear
0.00.090.841 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.842 I llm_load_print_meta: freq_scale_train = 1
0.00.090.842 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.843 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.843 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.843 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.843 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.843 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.843 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.844 I llm_load_print_meta: model type       = 1.4B
0.00.090.844 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.845 I llm_load_print_meta: model params     = 1.41 B
0.00.090.845 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.845 I llm_load_print_meta: general.name     = 1.4B
0.00.090.846 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.846 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.846 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.847 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.848 I llm_load_print_meta: LF token         = 128 ''
0.00.090.848 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.848 I llm_load_print_meta: max token length = 1024
0.00.093.313 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.313 I llm_load_tensors: offloading output layer to GPU
0.00.093.313 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.324 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.325 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.225 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.226 I llama_new_context_with_model: n_ctx         = 128
0.00.094.226 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.226 I llama_new_context_with_model: n_batch       = 128
0.00.094.226 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.226 I llama_new_context_with_model: flash_attn    = 0
0.00.094.227 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.227 I llama_new_context_with_model: freq_scale    = 1
0.00.094.227 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.228 I ggml_metal_init: allocating
0.00.094.231 I ggml_metal_init: found device: Apple M4
0.00.094.233 I ggml_metal_init: picking default device: Apple M4
0.00.094.830 I ggml_metal_init: using embedded metal library
0.00.097.338 I ggml_metal_init: GPU name:   Apple M4
0.00.097.339 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.339 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.340 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.340 I ggml_metal_init: simdgroup reduction   = true
0.00.097.340 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.340 I ggml_metal_init: has bfloat            = true
0.00.097.341 I ggml_metal_init: use bfloat            = true
0.00.097.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.600 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.952 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.955 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.971 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.840 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.840 I llama_new_context_with_model: graph nodes  = 967
0.00.111.840 I llama_new_context_with_model: graph splits = 2
0.00.111.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.111.853 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.941.831 I 
0.00.941.872 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.941.910 I perplexity: tokenizing the input ..
0.00.955.268 I perplexity: tokenization took 13.352 ms
0.00.955.274 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.076.132 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.077.840 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.077.868 I llama_perf_context_print:        load time =     917.99 ms
0.01.077.870 I llama_perf_context_print: prompt eval time =     119.94 ms /   128 tokens (    0.94 ms per token,  1067.20 tokens per second)
0.01.077.871 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.077.872 I llama_perf_context_print:       total time =     136.04 ms /   129 tokens
0.01.078.738 I ggml_metal_free: deallocating

real	0m1.271s
user	0m0.125s
sys	0m0.189s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.919 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.391 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.396 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.398 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.399 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.399 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.399 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.400 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.401 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.401 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.402 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.402 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.402 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.403 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.404 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.406 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.406 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.406 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.326 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.525 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.527 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.528 I llama_model_loader: - type  f32:  194 tensors
0.00.028.529 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.693 I llm_load_vocab: special tokens cache size = 25
0.00.056.497 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.501 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.501 I llm_load_print_meta: arch             = gptneox
0.00.056.502 I llm_load_print_meta: vocab type       = BPE
0.00.056.502 I llm_load_print_meta: n_vocab          = 50304
0.00.056.502 I llm_load_print_meta: n_merges         = 50009
0.00.056.502 I llm_load_print_meta: vocab_only       = 0
0.00.056.503 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.503 I llm_load_print_meta: n_embd           = 2048
0.00.056.505 I llm_load_print_meta: n_layer          = 24
0.00.056.524 I llm_load_print_meta: n_head           = 16
0.00.056.525 I llm_load_print_meta: n_head_kv        = 16
0.00.056.526 I llm_load_print_meta: n_rot            = 32
0.00.056.526 I llm_load_print_meta: n_swa            = 0
0.00.056.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.527 I llm_load_print_meta: n_gqa            = 1
0.00.056.527 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.528 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.528 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.528 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.529 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.529 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.529 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.530 I llm_load_print_meta: n_ff             = 8192
0.00.056.530 I llm_load_print_meta: n_expert         = 0
0.00.056.530 I llm_load_print_meta: n_expert_used    = 0
0.00.056.530 I llm_load_print_meta: causal attn      = 1
0.00.056.530 I llm_load_print_meta: pooling type     = 0
0.00.056.530 I llm_load_print_meta: rope type        = 2
0.00.056.531 I llm_load_print_meta: rope scaling     = linear
0.00.056.531 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.532 I llm_load_print_meta: freq_scale_train = 1
0.00.056.532 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.532 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.532 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.533 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.533 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.533 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.533 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.534 I llm_load_print_meta: model type       = 1.4B
0.00.056.534 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.534 I llm_load_print_meta: model params     = 1.41 B
0.00.056.535 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.535 I llm_load_print_meta: general.name     = 1.4B
0.00.056.535 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.535 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.535 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.536 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.536 I llm_load_print_meta: LF token         = 128 ''
0.00.056.536 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.536 I llm_load_print_meta: max token length = 1024
0.00.059.084 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.084 I llm_load_tensors: offloading output layer to GPU
0.00.059.085 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.096 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.097 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.098 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.099 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.099 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.099 I llama_new_context_with_model: n_batch       = 2048
0.00.060.099 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.100 I llama_new_context_with_model: flash_attn    = 0
0.00.060.100 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.100 I llama_new_context_with_model: freq_scale    = 1
0.00.060.101 I ggml_metal_init: allocating
0.00.060.104 I ggml_metal_init: found device: Apple M4
0.00.060.107 I ggml_metal_init: picking default device: Apple M4
0.00.060.871 I ggml_metal_init: using embedded metal library
0.00.063.467 I ggml_metal_init: GPU name:   Apple M4
0.00.063.469 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.469 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.470 I ggml_metal_init: simdgroup reduction   = true
0.00.063.470 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.471 I ggml_metal_init: has bfloat            = true
0.00.063.471 I ggml_metal_init: use bfloat            = true
0.00.063.471 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.472 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.960 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.156 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.164 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.190 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.380 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.382 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.383 I llama_new_context_with_model: graph nodes  = 967
0.00.099.383 I llama_new_context_with_model: graph splits = 2
0.00.099.402 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.542 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.142.515 I main: llama threadpool init, n_threads = 4
0.01.142.550 I 
0.01.142.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.142.582 I 
0.01.142.803 I sampler seed: 1234
0.01.142.808 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.142.854 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.142.858 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.142.858 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.228.247 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.02.228.247 I llama_perf_context_print:        load time =    1132.59 ms
0.02.228.248 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.63 tokens per second)
0.02.228.249 I llama_perf_context_print:        eval time =    1038.61 ms /    63 runs   (   16.49 ms per token,    60.66 tokens per second)
0.02.228.249 I llama_perf_context_print:       total time =    1085.73 ms /    70 tokens
0.02.228.464 I ggml_metal_free: deallocating

real	0m2.247s
user	0m0.113s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.157 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.794 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.795 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.568 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.923 I llama_model_loader: - type  f32:  194 tensors
0.00.033.924 I llama_model_loader: - type q8_0:   98 tensors
0.00.059.702 I llm_load_vocab: special tokens cache size = 25
0.00.066.003 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.006 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.006 I llm_load_print_meta: arch             = gptneox
0.00.066.007 I llm_load_print_meta: vocab type       = BPE
0.00.066.007 I llm_load_print_meta: n_vocab          = 50304
0.00.066.007 I llm_load_print_meta: n_merges         = 50009
0.00.066.007 I llm_load_print_meta: vocab_only       = 0
0.00.066.007 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.008 I llm_load_print_meta: n_embd           = 2048
0.00.066.008 I llm_load_print_meta: n_layer          = 24
0.00.066.024 I llm_load_print_meta: n_head           = 16
0.00.066.025 I llm_load_print_meta: n_head_kv        = 16
0.00.066.025 I llm_load_print_meta: n_rot            = 32
0.00.066.025 I llm_load_print_meta: n_swa            = 0
0.00.066.026 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.026 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.026 I llm_load_print_meta: n_gqa            = 1
0.00.066.027 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.028 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.029 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.029 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.029 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.029 I llm_load_print_meta: n_ff             = 8192
0.00.066.030 I llm_load_print_meta: n_expert         = 0
0.00.066.030 I llm_load_print_meta: n_expert_used    = 0
0.00.066.032 I llm_load_print_meta: causal attn      = 1
0.00.066.032 I llm_load_print_meta: pooling type     = 0
0.00.066.033 I llm_load_print_meta: rope type        = 2
0.00.066.033 I llm_load_print_meta: rope scaling     = linear
0.00.066.033 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.033 I llm_load_print_meta: freq_scale_train = 1
0.00.066.034 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.034 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.034 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.034 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.034 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.034 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.034 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.035 I llm_load_print_meta: model type       = 1.4B
0.00.066.035 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.036 I llm_load_print_meta: model params     = 1.41 B
0.00.066.036 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.036 I llm_load_print_meta: general.name     = 1.4B
0.00.066.036 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.037 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.037 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.037 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.037 I llm_load_print_meta: LF token         = 128 ''
0.00.066.038 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.038 I llm_load_print_meta: max token length = 1024
0.00.068.384 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.384 I llm_load_tensors: offloading output layer to GPU
0.00.068.385 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.395 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.068.396 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.069.339 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.340 I llama_new_context_with_model: n_ctx         = 128
0.00.069.340 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.340 I llama_new_context_with_model: n_batch       = 128
0.00.069.340 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.340 I llama_new_context_with_model: flash_attn    = 0
0.00.069.341 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.341 I llama_new_context_with_model: freq_scale    = 1
0.00.069.341 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.342 I ggml_metal_init: allocating
0.00.069.345 I ggml_metal_init: found device: Apple M4
0.00.069.348 I ggml_metal_init: picking default device: Apple M4
0.00.070.018 I ggml_metal_init: using embedded metal library
0.00.072.654 I ggml_metal_init: GPU name:   Apple M4
0.00.072.655 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.656 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.656 I ggml_metal_init: simdgroup reduction   = true
0.00.072.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.657 I ggml_metal_init: has bfloat            = true
0.00.072.657 I ggml_metal_init: use bfloat            = true
0.00.072.657 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.502 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.929 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.934 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.950 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.906 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.907 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.907 I llama_new_context_with_model: graph nodes  = 967
0.00.085.908 I llama_new_context_with_model: graph splits = 2
0.00.085.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.921 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.941.811 I 
0.00.941.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.941.876 I perplexity: tokenizing the input ..
0.00.949.591 I perplexity: tokenization took 7.713 ms
0.00.949.594 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.073.965 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.075.252 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.075.279 I llama_perf_context_print:        load time =     930.02 ms
0.01.075.281 I llama_perf_context_print: prompt eval time =     124.14 ms /   128 tokens (    0.97 ms per token,  1031.08 tokens per second)
0.01.075.282 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.075.282 I llama_perf_context_print:       total time =     133.47 ms /   129 tokens
0.01.075.765 I ggml_metal_free: deallocating

real	0m1.097s
user	0m0.095s
sys	0m0.155s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.860 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.504 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.506 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.507 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.508 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.510 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.510 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.482 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.766 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.823 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.824 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.825 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.825 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.826 I llama_model_loader: - type  f32:  194 tensors
0.00.026.826 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.826 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.508 I llm_load_vocab: special tokens cache size = 25
0.00.054.586 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.589 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.589 I llm_load_print_meta: arch             = gptneox
0.00.054.590 I llm_load_print_meta: vocab type       = BPE
0.00.054.590 I llm_load_print_meta: n_vocab          = 50304
0.00.054.590 I llm_load_print_meta: n_merges         = 50009
0.00.054.590 I llm_load_print_meta: vocab_only       = 0
0.00.054.590 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.590 I llm_load_print_meta: n_embd           = 2048
0.00.054.591 I llm_load_print_meta: n_layer          = 24
0.00.054.608 I llm_load_print_meta: n_head           = 16
0.00.054.610 I llm_load_print_meta: n_head_kv        = 16
0.00.054.610 I llm_load_print_meta: n_rot            = 32
0.00.054.610 I llm_load_print_meta: n_swa            = 0
0.00.054.610 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.610 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.611 I llm_load_print_meta: n_gqa            = 1
0.00.054.612 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.612 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.613 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.613 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.613 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.614 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.615 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.615 I llm_load_print_meta: n_ff             = 8192
0.00.054.615 I llm_load_print_meta: n_expert         = 0
0.00.054.615 I llm_load_print_meta: n_expert_used    = 0
0.00.054.616 I llm_load_print_meta: causal attn      = 1
0.00.054.616 I llm_load_print_meta: pooling type     = 0
0.00.054.616 I llm_load_print_meta: rope type        = 2
0.00.054.616 I llm_load_print_meta: rope scaling     = linear
0.00.054.616 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.616 I llm_load_print_meta: freq_scale_train = 1
0.00.054.617 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.617 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.617 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.617 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.617 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.617 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.618 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.618 I llm_load_print_meta: model type       = 1.4B
0.00.054.619 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.619 I llm_load_print_meta: model params     = 1.41 B
0.00.054.619 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.619 I llm_load_print_meta: general.name     = 1.4B
0.00.054.620 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.620 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.620 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.620 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.621 I llm_load_print_meta: LF token         = 128 ''
0.00.054.621 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.621 I llm_load_print_meta: max token length = 1024
0.00.056.962 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.962 I llm_load_tensors: offloading output layer to GPU
0.00.056.963 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.974 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.975 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.989 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.990 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.990 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.990 I llama_new_context_with_model: n_batch       = 2048
0.00.057.991 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.991 I llama_new_context_with_model: flash_attn    = 0
0.00.057.991 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.992 I llama_new_context_with_model: freq_scale    = 1
0.00.057.992 I ggml_metal_init: allocating
0.00.057.999 I ggml_metal_init: found device: Apple M4
0.00.058.002 I ggml_metal_init: picking default device: Apple M4
0.00.058.724 I ggml_metal_init: using embedded metal library
0.00.061.300 I ggml_metal_init: GPU name:   Apple M4
0.00.061.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.303 I ggml_metal_init: simdgroup reduction   = true
0.00.061.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.304 I ggml_metal_init: has bfloat            = true
0.00.061.304 I ggml_metal_init: use bfloat            = true
0.00.061.304 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.088 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.653 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.665 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.692 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.924 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.926 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.927 I llama_new_context_with_model: graph nodes  = 967
0.00.095.927 I llama_new_context_with_model: graph splits = 2
0.00.095.946 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.073 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.399 I main: llama threadpool init, n_threads = 4
0.00.661.439 I 
0.00.661.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.489 I 
0.00.661.731 I sampler seed: 1234
0.00.661.739 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.661.749 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.661.750 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.661.750 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.344.573 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52631.58 tokens per second)
0.01.344.574 I llama_perf_context_print:        load time =     650.53 ms
0.01.344.576 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.57 tokens per second)
0.01.344.577 I llama_perf_context_print:        eval time =     637.62 ms /    63 runs   (   10.12 ms per token,    98.80 tokens per second)
0.01.344.577 I llama_perf_context_print:       total time =     683.18 ms /    70 tokens
0.01.344.809 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.112s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.611 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.488 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.489 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.489 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.491 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.491 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.494 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.610 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.561 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.562 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.563 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.564 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.564 I llama_model_loader: - type  f32:  194 tensors
0.00.025.565 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.565 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.978 I llm_load_vocab: special tokens cache size = 25
0.00.051.810 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.813 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.813 I llm_load_print_meta: arch             = gptneox
0.00.051.813 I llm_load_print_meta: vocab type       = BPE
0.00.051.814 I llm_load_print_meta: n_vocab          = 50304
0.00.051.814 I llm_load_print_meta: n_merges         = 50009
0.00.051.814 I llm_load_print_meta: vocab_only       = 0
0.00.051.814 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.814 I llm_load_print_meta: n_embd           = 2048
0.00.051.815 I llm_load_print_meta: n_layer          = 24
0.00.051.829 I llm_load_print_meta: n_head           = 16
0.00.051.830 I llm_load_print_meta: n_head_kv        = 16
0.00.051.830 I llm_load_print_meta: n_rot            = 32
0.00.051.830 I llm_load_print_meta: n_swa            = 0
0.00.051.830 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.830 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.831 I llm_load_print_meta: n_gqa            = 1
0.00.051.832 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.832 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.833 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.833 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.833 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.834 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.834 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.834 I llm_load_print_meta: n_ff             = 8192
0.00.051.835 I llm_load_print_meta: n_expert         = 0
0.00.051.835 I llm_load_print_meta: n_expert_used    = 0
0.00.051.835 I llm_load_print_meta: causal attn      = 1
0.00.051.835 I llm_load_print_meta: pooling type     = 0
0.00.051.835 I llm_load_print_meta: rope type        = 2
0.00.051.835 I llm_load_print_meta: rope scaling     = linear
0.00.051.836 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.836 I llm_load_print_meta: freq_scale_train = 1
0.00.051.838 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.838 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.838 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.838 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.838 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.839 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.839 I llm_load_print_meta: model type       = 1.4B
0.00.051.839 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.840 I llm_load_print_meta: model params     = 1.41 B
0.00.051.840 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.840 I llm_load_print_meta: general.name     = 1.4B
0.00.051.840 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.841 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.841 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.841 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.841 I llm_load_print_meta: LF token         = 128 ''
0.00.051.841 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.841 I llm_load_print_meta: max token length = 1024
0.00.053.773 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.774 I llm_load_tensors: offloading output layer to GPU
0.00.053.774 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.785 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.786 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.693 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.694 I llama_new_context_with_model: n_ctx         = 128
0.00.054.694 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.694 I llama_new_context_with_model: n_batch       = 128
0.00.054.695 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.695 I llama_new_context_with_model: flash_attn    = 0
0.00.054.695 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.695 I llama_new_context_with_model: freq_scale    = 1
0.00.054.696 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.696 I ggml_metal_init: allocating
0.00.054.699 I ggml_metal_init: found device: Apple M4
0.00.054.701 I ggml_metal_init: picking default device: Apple M4
0.00.055.282 I ggml_metal_init: using embedded metal library
0.00.057.592 I ggml_metal_init: GPU name:   Apple M4
0.00.057.594 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.594 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.595 I ggml_metal_init: simdgroup reduction   = true
0.00.057.595 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.595 I ggml_metal_init: has bfloat            = true
0.00.057.595 I ggml_metal_init: use bfloat            = true
0.00.057.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.604 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.898 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.915 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.789 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.790 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.790 I llama_new_context_with_model: graph nodes  = 967
0.00.069.790 I llama_new_context_with_model: graph splits = 2
0.00.069.803 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.803 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.402 I 
0.00.583.457 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.468 I perplexity: tokenizing the input ..
0.00.591.680 I perplexity: tokenization took 8.208 ms
0.00.591.683 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.340 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.715.499 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.715.516 I llama_perf_context_print:        load time =     572.78 ms
0.00.715.517 I llama_perf_context_print: prompt eval time =     122.43 ms /   128 tokens (    0.96 ms per token,  1045.49 tokens per second)
0.00.715.518 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.715.519 I llama_perf_context_print:       total time =     132.12 ms /   129 tokens
0.00.715.975 I ggml_metal_free: deallocating

real	0m0.732s
user	0m0.078s
sys	0m0.098s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.747 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.539 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.546 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.547 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.558 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.559 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.843 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.845 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.845 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.845 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.846 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.847 I llama_model_loader: - type  f32:  194 tensors
0.00.026.847 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.743 I llm_load_vocab: special tokens cache size = 25
0.00.054.732 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.736 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.737 I llm_load_print_meta: arch             = gptneox
0.00.054.737 I llm_load_print_meta: vocab type       = BPE
0.00.054.737 I llm_load_print_meta: n_vocab          = 50304
0.00.054.738 I llm_load_print_meta: n_merges         = 50009
0.00.054.738 I llm_load_print_meta: vocab_only       = 0
0.00.054.738 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.738 I llm_load_print_meta: n_embd           = 2048
0.00.054.738 I llm_load_print_meta: n_layer          = 24
0.00.054.757 I llm_load_print_meta: n_head           = 16
0.00.054.759 I llm_load_print_meta: n_head_kv        = 16
0.00.054.759 I llm_load_print_meta: n_rot            = 32
0.00.054.759 I llm_load_print_meta: n_swa            = 0
0.00.054.759 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.760 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.760 I llm_load_print_meta: n_gqa            = 1
0.00.054.761 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.761 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.762 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.764 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.764 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.765 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.765 I llm_load_print_meta: n_ff             = 8192
0.00.054.765 I llm_load_print_meta: n_expert         = 0
0.00.054.765 I llm_load_print_meta: n_expert_used    = 0
0.00.054.766 I llm_load_print_meta: causal attn      = 1
0.00.054.766 I llm_load_print_meta: pooling type     = 0
0.00.054.766 I llm_load_print_meta: rope type        = 2
0.00.054.766 I llm_load_print_meta: rope scaling     = linear
0.00.054.766 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.767 I llm_load_print_meta: freq_scale_train = 1
0.00.054.767 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.767 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.767 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.767 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.767 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.767 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.767 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.768 I llm_load_print_meta: model type       = 1.4B
0.00.054.768 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.769 I llm_load_print_meta: model params     = 1.41 B
0.00.054.769 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.769 I llm_load_print_meta: general.name     = 1.4B
0.00.054.769 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.770 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.770 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.770 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.770 I llm_load_print_meta: LF token         = 128 ''
0.00.054.770 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.770 I llm_load_print_meta: max token length = 1024
0.00.056.856 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.857 I llm_load_tensors: offloading output layer to GPU
0.00.056.857 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.874 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.875 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.057.858 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.859 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.859 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.859 I llama_new_context_with_model: n_batch       = 2048
0.00.057.860 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.860 I llama_new_context_with_model: flash_attn    = 0
0.00.057.860 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.861 I llama_new_context_with_model: freq_scale    = 1
0.00.057.861 I ggml_metal_init: allocating
0.00.057.865 I ggml_metal_init: found device: Apple M4
0.00.057.867 I ggml_metal_init: picking default device: Apple M4
0.00.058.511 I ggml_metal_init: using embedded metal library
0.00.061.011 I ggml_metal_init: GPU name:   Apple M4
0.00.061.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.015 I ggml_metal_init: simdgroup reduction   = true
0.00.061.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.015 I ggml_metal_init: has bfloat            = true
0.00.061.015 I ggml_metal_init: use bfloat            = true
0.00.061.016 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.691 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.187 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.216 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.254 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.256 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.256 I llama_new_context_with_model: graph nodes  = 967
0.00.091.256 I llama_new_context_with_model: graph splits = 2
0.00.091.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.726.050 I main: llama threadpool init, n_threads = 4
0.00.726.081 I 
0.00.726.116 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.726.117 I 
0.00.726.342 I sampler seed: 1234
0.00.726.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.726.358 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.726.360 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.726.360 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.460.555 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.01.460.556 I llama_perf_context_print:        load time =     715.30 ms
0.01.460.557 I llama_perf_context_print: prompt eval time =      43.05 ms /     7 tokens (    6.15 ms per token,   162.58 tokens per second)
0.01.460.558 I llama_perf_context_print:        eval time =     688.40 ms /    63 runs   (   10.93 ms per token,    91.52 tokens per second)
0.01.460.558 I llama_perf_context_print:       total time =     734.51 ms /    70 tokens
0.01.460.798 I ggml_metal_free: deallocating

real	0m1.480s
user	0m0.111s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.761 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.605 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.607 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.608 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.609 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.609 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.610 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.611 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.611 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.611 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.612 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.612 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.612 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.614 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.400 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.434 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.243 I llama_model_loader: - type  f32:  194 tensors
0.00.023.244 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.244 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.504 I llm_load_vocab: special tokens cache size = 25
0.00.049.482 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.485 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.485 I llm_load_print_meta: arch             = gptneox
0.00.049.485 I llm_load_print_meta: vocab type       = BPE
0.00.049.486 I llm_load_print_meta: n_vocab          = 50304
0.00.049.486 I llm_load_print_meta: n_merges         = 50009
0.00.049.486 I llm_load_print_meta: vocab_only       = 0
0.00.049.486 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.486 I llm_load_print_meta: n_embd           = 2048
0.00.049.487 I llm_load_print_meta: n_layer          = 24
0.00.049.501 I llm_load_print_meta: n_head           = 16
0.00.049.501 I llm_load_print_meta: n_head_kv        = 16
0.00.049.502 I llm_load_print_meta: n_rot            = 32
0.00.049.502 I llm_load_print_meta: n_swa            = 0
0.00.049.502 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.502 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.503 I llm_load_print_meta: n_gqa            = 1
0.00.049.504 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.504 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.505 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.505 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.506 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.507 I llm_load_print_meta: n_ff             = 8192
0.00.049.508 I llm_load_print_meta: n_expert         = 0
0.00.049.508 I llm_load_print_meta: n_expert_used    = 0
0.00.049.508 I llm_load_print_meta: causal attn      = 1
0.00.049.509 I llm_load_print_meta: pooling type     = 0
0.00.049.509 I llm_load_print_meta: rope type        = 2
0.00.049.509 I llm_load_print_meta: rope scaling     = linear
0.00.049.510 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.510 I llm_load_print_meta: freq_scale_train = 1
0.00.049.510 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.510 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.511 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.511 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.511 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.511 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.511 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.512 I llm_load_print_meta: model type       = 1.4B
0.00.049.512 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.512 I llm_load_print_meta: model params     = 1.41 B
0.00.049.513 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.513 I llm_load_print_meta: general.name     = 1.4B
0.00.049.513 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.514 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.514 I llm_load_print_meta: LF token         = 128 ''
0.00.049.514 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.515 I llm_load_print_meta: max token length = 1024
0.00.051.478 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.478 I llm_load_tensors: offloading output layer to GPU
0.00.051.479 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.489 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.490 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.381 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.381 I llama_new_context_with_model: n_ctx         = 128
0.00.052.381 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.382 I llama_new_context_with_model: n_batch       = 128
0.00.052.382 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.382 I llama_new_context_with_model: flash_attn    = 0
0.00.052.382 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.382 I llama_new_context_with_model: freq_scale    = 1
0.00.052.383 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.383 I ggml_metal_init: allocating
0.00.052.386 I ggml_metal_init: found device: Apple M4
0.00.052.389 I ggml_metal_init: picking default device: Apple M4
0.00.052.952 I ggml_metal_init: using embedded metal library
0.00.055.226 I ggml_metal_init: GPU name:   Apple M4
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.227 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.228 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.228 I ggml_metal_init: simdgroup reduction   = true
0.00.055.228 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.228 I ggml_metal_init: has bfloat            = true
0.00.055.228 I ggml_metal_init: use bfloat            = true
0.00.055.229 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.760 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.030 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.035 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.050 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.921 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.922 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.923 I llama_new_context_with_model: graph nodes  = 967
0.00.066.923 I llama_new_context_with_model: graph splits = 2
0.00.066.935 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.492 I 
0.00.667.546 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.559 I perplexity: tokenizing the input ..
0.00.675.776 I perplexity: tokenization took 8.216 ms
0.00.675.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.767 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.799.964 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.799.983 I llama_perf_context_print:        load time =     658.72 ms
0.00.799.984 I llama_perf_context_print: prompt eval time =     122.74 ms /   128 tokens (    0.96 ms per token,  1042.84 tokens per second)
0.00.799.985 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.987 I llama_perf_context_print:       total time =     132.50 ms /   129 tokens
0.00.800.483 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.078s
sys	0m0.113s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.071 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.474 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.475 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.475 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.476 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.476 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.476 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.480 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.554 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.555 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.555 I llama_model_loader: - type  f32:  194 tensors
0.00.025.556 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.556 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.495 I llm_load_vocab: special tokens cache size = 25
0.00.052.418 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.421 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.421 I llm_load_print_meta: arch             = gptneox
0.00.052.422 I llm_load_print_meta: vocab type       = BPE
0.00.052.422 I llm_load_print_meta: n_vocab          = 50304
0.00.052.422 I llm_load_print_meta: n_merges         = 50009
0.00.052.422 I llm_load_print_meta: vocab_only       = 0
0.00.052.422 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.423 I llm_load_print_meta: n_embd           = 2048
0.00.052.423 I llm_load_print_meta: n_layer          = 24
0.00.052.438 I llm_load_print_meta: n_head           = 16
0.00.052.439 I llm_load_print_meta: n_head_kv        = 16
0.00.052.439 I llm_load_print_meta: n_rot            = 32
0.00.052.439 I llm_load_print_meta: n_swa            = 0
0.00.052.440 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.440 I llm_load_print_meta: n_gqa            = 1
0.00.052.441 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.443 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.444 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.444 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.444 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.445 I llm_load_print_meta: n_ff             = 8192
0.00.052.445 I llm_load_print_meta: n_expert         = 0
0.00.052.445 I llm_load_print_meta: n_expert_used    = 0
0.00.052.447 I llm_load_print_meta: causal attn      = 1
0.00.052.448 I llm_load_print_meta: pooling type     = 0
0.00.052.448 I llm_load_print_meta: rope type        = 2
0.00.052.448 I llm_load_print_meta: rope scaling     = linear
0.00.052.448 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.449 I llm_load_print_meta: freq_scale_train = 1
0.00.052.449 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.450 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.450 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.450 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.450 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.450 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.451 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.451 I llm_load_print_meta: model type       = 1.4B
0.00.052.451 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.452 I llm_load_print_meta: model params     = 1.41 B
0.00.052.452 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.453 I llm_load_print_meta: general.name     = 1.4B
0.00.052.453 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.453 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.453 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.453 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.453 I llm_load_print_meta: LF token         = 128 ''
0.00.052.454 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.454 I llm_load_print_meta: max token length = 1024
0.00.054.123 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.124 I llm_load_tensors: offloading output layer to GPU
0.00.054.124 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.134 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.136 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.015 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.015 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.016 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.016 I llama_new_context_with_model: n_batch       = 2048
0.00.055.016 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.016 I llama_new_context_with_model: flash_attn    = 0
0.00.055.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.017 I llama_new_context_with_model: freq_scale    = 1
0.00.055.017 I ggml_metal_init: allocating
0.00.055.024 I ggml_metal_init: found device: Apple M4
0.00.055.027 I ggml_metal_init: picking default device: Apple M4
0.00.055.654 I ggml_metal_init: using embedded metal library
0.00.057.988 I ggml_metal_init: GPU name:   Apple M4
0.00.057.990 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.990 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.991 I ggml_metal_init: simdgroup reduction   = true
0.00.057.991 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.991 I ggml_metal_init: has bfloat            = true
0.00.057.991 I ggml_metal_init: use bfloat            = true
0.00.057.992 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.993 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.726 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.653 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.659 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.680 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.770 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.772 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.772 I llama_new_context_with_model: graph nodes  = 967
0.00.088.773 I llama_new_context_with_model: graph splits = 2
0.00.088.787 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.942 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.492 I main: llama threadpool init, n_threads = 4
0.00.699.541 I 
0.00.699.578 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.579 I 
0.00.699.745 I sampler seed: 1234
0.00.699.750 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.760 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.762 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.762 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.532.569 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.532.570 I llama_perf_context_print:        load time =     690.42 ms
0.01.532.571 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.19 tokens per second)
0.01.532.571 I llama_perf_context_print:        eval time =     786.52 ms /    63 runs   (   12.48 ms per token,    80.10 tokens per second)
0.01.532.572 I llama_perf_context_print:       total time =     833.08 ms /    70 tokens
0.01.532.810 I ggml_metal_free: deallocating

real	0m1.552s
user	0m0.112s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.013 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.549 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.550 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.550 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.550 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.551 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.552 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.553 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.555 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.555 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.555 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.507 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.657 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.659 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.659 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.659 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.660 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.660 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.660 I llama_model_loader: - type  f32:  194 tensors
0.00.024.661 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.661 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.547 I llm_load_vocab: special tokens cache size = 25
0.00.051.628 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.630 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.630 I llm_load_print_meta: arch             = gptneox
0.00.051.631 I llm_load_print_meta: vocab type       = BPE
0.00.051.631 I llm_load_print_meta: n_vocab          = 50304
0.00.051.631 I llm_load_print_meta: n_merges         = 50009
0.00.051.631 I llm_load_print_meta: vocab_only       = 0
0.00.051.631 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.632 I llm_load_print_meta: n_embd           = 2048
0.00.051.632 I llm_load_print_meta: n_layer          = 24
0.00.051.646 I llm_load_print_meta: n_head           = 16
0.00.051.647 I llm_load_print_meta: n_head_kv        = 16
0.00.051.647 I llm_load_print_meta: n_rot            = 32
0.00.051.647 I llm_load_print_meta: n_swa            = 0
0.00.051.648 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.648 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.650 I llm_load_print_meta: n_gqa            = 1
0.00.051.651 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.651 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.652 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.652 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.652 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.653 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.653 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.653 I llm_load_print_meta: n_ff             = 8192
0.00.051.654 I llm_load_print_meta: n_expert         = 0
0.00.051.654 I llm_load_print_meta: n_expert_used    = 0
0.00.051.654 I llm_load_print_meta: causal attn      = 1
0.00.051.654 I llm_load_print_meta: pooling type     = 0
0.00.051.654 I llm_load_print_meta: rope type        = 2
0.00.051.654 I llm_load_print_meta: rope scaling     = linear
0.00.051.655 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.655 I llm_load_print_meta: freq_scale_train = 1
0.00.051.657 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.657 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.657 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.657 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.657 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.657 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.657 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.658 I llm_load_print_meta: model type       = 1.4B
0.00.051.658 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.659 I llm_load_print_meta: model params     = 1.41 B
0.00.051.659 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.659 I llm_load_print_meta: general.name     = 1.4B
0.00.051.659 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.660 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.660 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.660 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.660 I llm_load_print_meta: LF token         = 128 ''
0.00.051.660 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.661 I llm_load_print_meta: max token length = 1024
0.00.053.705 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.705 I llm_load_tensors: offloading output layer to GPU
0.00.053.706 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.716 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.717 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.656 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.657 I llama_new_context_with_model: n_ctx         = 128
0.00.054.657 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.657 I llama_new_context_with_model: n_batch       = 128
0.00.054.657 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.657 I llama_new_context_with_model: flash_attn    = 0
0.00.054.658 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.658 I llama_new_context_with_model: freq_scale    = 1
0.00.054.658 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.659 I ggml_metal_init: allocating
0.00.054.662 I ggml_metal_init: found device: Apple M4
0.00.054.664 I ggml_metal_init: picking default device: Apple M4
0.00.055.218 I ggml_metal_init: using embedded metal library
0.00.057.519 I ggml_metal_init: GPU name:   Apple M4
0.00.057.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.521 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.522 I ggml_metal_init: simdgroup reduction   = true
0.00.057.522 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.522 I ggml_metal_init: has bfloat            = true
0.00.057.522 I ggml_metal_init: use bfloat            = true
0.00.057.523 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.523 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.312 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.528 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.531 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.545 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.409 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.410 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.411 I llama_new_context_with_model: graph nodes  = 967
0.00.069.411 I llama_new_context_with_model: graph splits = 2
0.00.069.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.941 I 
0.00.565.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.566.023 I perplexity: tokenizing the input ..
0.00.573.972 I perplexity: tokenization took 7.948 ms
0.00.573.975 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.708.615 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.709.786 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.709.801 I llama_perf_context_print:        load time =     555.92 ms
0.00.709.802 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.28 tokens per second)
0.00.709.803 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.709.803 I llama_perf_context_print:       total time =     143.86 ms /   129 tokens
0.00.710.226 I ggml_metal_free: deallocating

real	0m0.726s
user	0m0.079s
sys	0m0.096s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.197 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.496 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.506 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.507 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.507 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.509 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.511 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.511 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.519 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.519 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.520 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.447 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.449 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.450 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.450 I llama_model_loader: - type  f32:  194 tensors
0.00.026.450 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.451 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.659 I llm_load_vocab: special tokens cache size = 25
0.00.052.564 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.566 I llm_load_print_meta: arch             = gptneox
0.00.052.567 I llm_load_print_meta: vocab type       = BPE
0.00.052.567 I llm_load_print_meta: n_vocab          = 50304
0.00.052.567 I llm_load_print_meta: n_merges         = 50009
0.00.052.567 I llm_load_print_meta: vocab_only       = 0
0.00.052.567 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.568 I llm_load_print_meta: n_embd           = 2048
0.00.052.568 I llm_load_print_meta: n_layer          = 24
0.00.052.582 I llm_load_print_meta: n_head           = 16
0.00.052.583 I llm_load_print_meta: n_head_kv        = 16
0.00.052.583 I llm_load_print_meta: n_rot            = 32
0.00.052.585 I llm_load_print_meta: n_swa            = 0
0.00.052.585 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.585 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.586 I llm_load_print_meta: n_gqa            = 1
0.00.052.587 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.587 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.588 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.588 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.588 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.589 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.590 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.591 I llm_load_print_meta: n_ff             = 8192
0.00.052.591 I llm_load_print_meta: n_expert         = 0
0.00.052.591 I llm_load_print_meta: n_expert_used    = 0
0.00.052.591 I llm_load_print_meta: causal attn      = 1
0.00.052.591 I llm_load_print_meta: pooling type     = 0
0.00.052.591 I llm_load_print_meta: rope type        = 2
0.00.052.593 I llm_load_print_meta: rope scaling     = linear
0.00.052.593 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.593 I llm_load_print_meta: freq_scale_train = 1
0.00.052.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.594 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.594 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.594 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.595 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.595 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.596 I llm_load_print_meta: model type       = 1.4B
0.00.052.600 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.600 I llm_load_print_meta: model params     = 1.41 B
0.00.052.600 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.601 I llm_load_print_meta: general.name     = 1.4B
0.00.052.601 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.601 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.601 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.601 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.602 I llm_load_print_meta: LF token         = 128 ''
0.00.052.602 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.602 I llm_load_print_meta: max token length = 1024
0.00.054.230 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.231 I llm_load_tensors: offloading output layer to GPU
0.00.054.231 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.241 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.242 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.175 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.176 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.176 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.176 I llama_new_context_with_model: n_batch       = 2048
0.00.055.176 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.176 I llama_new_context_with_model: flash_attn    = 0
0.00.055.177 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.177 I llama_new_context_with_model: freq_scale    = 1
0.00.055.177 I ggml_metal_init: allocating
0.00.055.181 I ggml_metal_init: found device: Apple M4
0.00.055.183 I ggml_metal_init: picking default device: Apple M4
0.00.055.782 I ggml_metal_init: using embedded metal library
0.00.058.107 I ggml_metal_init: GPU name:   Apple M4
0.00.058.108 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.109 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.109 I ggml_metal_init: simdgroup reduction   = true
0.00.058.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.110 I ggml_metal_init: has bfloat            = true
0.00.058.110 I ggml_metal_init: use bfloat            = true
0.00.058.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.111 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.053 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.626 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.642 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.662 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.775 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.777 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.777 I llama_new_context_with_model: graph nodes  = 967
0.00.088.778 I llama_new_context_with_model: graph splits = 2
0.00.088.794 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.940 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.280 I main: llama threadpool init, n_threads = 4
0.00.747.317 I 
0.00.747.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.346 I 
0.00.747.565 I sampler seed: 1234
0.00.747.570 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.747.580 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.747.581 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.747.581 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.588.692 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.588.693 I llama_perf_context_print:        load time =     737.08 ms
0.01.588.694 I llama_perf_context_print: prompt eval time =      46.19 ms /     7 tokens (    6.60 ms per token,   151.54 tokens per second)
0.01.588.694 I llama_perf_context_print:        eval time =     791.89 ms /    63 runs   (   12.57 ms per token,    79.56 tokens per second)
0.01.588.695 I llama_perf_context_print:       total time =     841.41 ms /    70 tokens
0.01.588.912 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.110s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.019 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.710 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.716 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.716 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.717 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.717 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.720 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.611 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.792 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.793 I llama_model_loader: - type  f32:  194 tensors
0.00.023.793 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.794 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.909 I llm_load_vocab: special tokens cache size = 25
0.00.049.935 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.938 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.938 I llm_load_print_meta: arch             = gptneox
0.00.049.938 I llm_load_print_meta: vocab type       = BPE
0.00.049.939 I llm_load_print_meta: n_vocab          = 50304
0.00.049.939 I llm_load_print_meta: n_merges         = 50009
0.00.049.939 I llm_load_print_meta: vocab_only       = 0
0.00.049.939 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.939 I llm_load_print_meta: n_embd           = 2048
0.00.049.939 I llm_load_print_meta: n_layer          = 24
0.00.049.948 I llm_load_print_meta: n_head           = 16
0.00.049.949 I llm_load_print_meta: n_head_kv        = 16
0.00.049.949 I llm_load_print_meta: n_rot            = 32
0.00.049.949 I llm_load_print_meta: n_swa            = 0
0.00.049.950 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.950 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.951 I llm_load_print_meta: n_gqa            = 1
0.00.049.951 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.952 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.953 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.953 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.953 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.953 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.953 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.954 I llm_load_print_meta: n_ff             = 8192
0.00.049.954 I llm_load_print_meta: n_expert         = 0
0.00.049.954 I llm_load_print_meta: n_expert_used    = 0
0.00.049.955 I llm_load_print_meta: causal attn      = 1
0.00.049.955 I llm_load_print_meta: pooling type     = 0
0.00.049.955 I llm_load_print_meta: rope type        = 2
0.00.049.955 I llm_load_print_meta: rope scaling     = linear
0.00.049.955 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.956 I llm_load_print_meta: freq_scale_train = 1
0.00.049.956 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.956 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.958 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.958 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.959 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.959 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.959 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.959 I llm_load_print_meta: model type       = 1.4B
0.00.049.960 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.960 I llm_load_print_meta: model params     = 1.41 B
0.00.049.961 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.961 I llm_load_print_meta: general.name     = 1.4B
0.00.049.961 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.961 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.961 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.962 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.962 I llm_load_print_meta: LF token         = 128 ''
0.00.049.962 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.962 I llm_load_print_meta: max token length = 1024
0.00.051.705 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.706 I llm_load_tensors: offloading output layer to GPU
0.00.051.706 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.711 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.711 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.568 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.568 I llama_new_context_with_model: n_ctx         = 128
0.00.052.568 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.569 I llama_new_context_with_model: n_batch       = 128
0.00.052.569 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.569 I llama_new_context_with_model: flash_attn    = 0
0.00.052.569 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.570 I llama_new_context_with_model: freq_scale    = 1
0.00.052.570 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.570 I ggml_metal_init: allocating
0.00.052.573 I ggml_metal_init: found device: Apple M4
0.00.052.575 I ggml_metal_init: picking default device: Apple M4
0.00.053.131 I ggml_metal_init: using embedded metal library
0.00.055.435 I ggml_metal_init: GPU name:   Apple M4
0.00.055.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.437 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.437 I ggml_metal_init: simdgroup reduction   = true
0.00.055.438 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.438 I ggml_metal_init: has bfloat            = true
0.00.055.438 I ggml_metal_init: use bfloat            = true
0.00.055.438 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.870 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.147 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.166 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.072 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.073 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.073 I llama_new_context_with_model: graph nodes  = 967
0.00.067.073 I llama_new_context_with_model: graph splits = 2
0.00.067.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.083 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.074 I 
0.00.680.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.125 I perplexity: tokenizing the input ..
0.00.687.548 I perplexity: tokenization took 7.421 ms
0.00.687.556 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.821.594 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.823.002 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.823.019 I llama_perf_context_print:        load time =     671.05 ms
0.00.823.020 I llama_perf_context_print: prompt eval time =     133.80 ms /   128 tokens (    1.05 ms per token,   956.64 tokens per second)
0.00.823.021 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.823.021 I llama_perf_context_print:       total time =     142.95 ms /   129 tokens
0.00.823.384 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.078s
sys	0m0.117s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.871 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.294 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.295 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.300 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.303 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.360 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.363 I llama_model_loader: - type  f32:  194 tensors
0.00.024.363 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.363 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.364 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.768 I llm_load_vocab: special tokens cache size = 25
0.00.050.751 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.753 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.754 I llm_load_print_meta: arch             = gptneox
0.00.050.754 I llm_load_print_meta: vocab type       = BPE
0.00.050.754 I llm_load_print_meta: n_vocab          = 50304
0.00.050.755 I llm_load_print_meta: n_merges         = 50009
0.00.050.755 I llm_load_print_meta: vocab_only       = 0
0.00.050.755 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.755 I llm_load_print_meta: n_embd           = 2048
0.00.050.755 I llm_load_print_meta: n_layer          = 24
0.00.050.769 I llm_load_print_meta: n_head           = 16
0.00.050.770 I llm_load_print_meta: n_head_kv        = 16
0.00.050.770 I llm_load_print_meta: n_rot            = 32
0.00.050.771 I llm_load_print_meta: n_swa            = 0
0.00.050.771 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.771 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.772 I llm_load_print_meta: n_gqa            = 1
0.00.050.772 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.773 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.774 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.774 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.774 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.774 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.774 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.777 I llm_load_print_meta: n_ff             = 8192
0.00.050.777 I llm_load_print_meta: n_expert         = 0
0.00.050.777 I llm_load_print_meta: n_expert_used    = 0
0.00.050.777 I llm_load_print_meta: causal attn      = 1
0.00.050.777 I llm_load_print_meta: pooling type     = 0
0.00.050.777 I llm_load_print_meta: rope type        = 2
0.00.050.778 I llm_load_print_meta: rope scaling     = linear
0.00.050.778 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.778 I llm_load_print_meta: freq_scale_train = 1
0.00.050.778 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.779 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.779 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.780 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.780 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.780 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.780 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.781 I llm_load_print_meta: model type       = 1.4B
0.00.050.781 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.781 I llm_load_print_meta: model params     = 1.41 B
0.00.050.782 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.782 I llm_load_print_meta: general.name     = 1.4B
0.00.050.783 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.783 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: LF token         = 128 ''
0.00.050.785 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.785 I llm_load_print_meta: max token length = 1024
0.00.052.704 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.705 I llm_load_tensors: offloading output layer to GPU
0.00.052.705 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.715 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.716 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.632 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.633 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.633 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.633 I llama_new_context_with_model: n_batch       = 2048
0.00.053.633 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.633 I llama_new_context_with_model: flash_attn    = 0
0.00.053.634 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.634 I llama_new_context_with_model: freq_scale    = 1
0.00.053.635 I ggml_metal_init: allocating
0.00.053.638 I ggml_metal_init: found device: Apple M4
0.00.053.640 I ggml_metal_init: picking default device: Apple M4
0.00.054.230 I ggml_metal_init: using embedded metal library
0.00.056.533 I ggml_metal_init: GPU name:   Apple M4
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.535 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.535 I ggml_metal_init: simdgroup reduction   = true
0.00.056.536 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.536 I ggml_metal_init: has bfloat            = true
0.00.056.536 I ggml_metal_init: use bfloat            = true
0.00.056.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.357 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.268 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.278 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.296 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.459 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.460 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.460 I llama_new_context_with_model: graph nodes  = 967
0.00.087.461 I llama_new_context_with_model: graph splits = 2
0.00.087.478 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.618 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.294 I main: llama threadpool init, n_threads = 4
0.00.444.330 I 
0.00.444.364 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.364 I 
0.00.444.590 I sampler seed: 1234
0.00.444.595 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.634 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.636 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.636 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.124.305 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65257.35 tokens per second)
0.01.124.305 I llama_perf_context_print:        load time =     434.42 ms
0.01.124.306 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.63 tokens per second)
0.01.124.307 I llama_perf_context_print:        eval time =     641.07 ms /    63 runs   (   10.18 ms per token,    98.27 tokens per second)
0.01.124.307 I llama_perf_context_print:       total time =     680.01 ms /    70 tokens
0.01.124.512 I ggml_metal_free: deallocating

real	0m1.142s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.827 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.424 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.428 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.430 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.431 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.431 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.431 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.432 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.433 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.433 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.433 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.437 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.484 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.730 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.772 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.773 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.774 I llama_model_loader: - type  f32:  194 tensors
0.00.026.774 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.774 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.775 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.708 I llm_load_vocab: special tokens cache size = 25
0.00.054.740 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.744 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.745 I llm_load_print_meta: arch             = gptneox
0.00.054.745 I llm_load_print_meta: vocab type       = BPE
0.00.054.745 I llm_load_print_meta: n_vocab          = 50304
0.00.054.745 I llm_load_print_meta: n_merges         = 50009
0.00.054.746 I llm_load_print_meta: vocab_only       = 0
0.00.054.746 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.746 I llm_load_print_meta: n_embd           = 2048
0.00.054.746 I llm_load_print_meta: n_layer          = 24
0.00.054.762 I llm_load_print_meta: n_head           = 16
0.00.054.764 I llm_load_print_meta: n_head_kv        = 16
0.00.054.764 I llm_load_print_meta: n_rot            = 32
0.00.054.764 I llm_load_print_meta: n_swa            = 0
0.00.054.764 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.765 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.765 I llm_load_print_meta: n_gqa            = 1
0.00.054.766 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.766 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.767 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.767 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.767 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.767 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.768 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.768 I llm_load_print_meta: n_ff             = 8192
0.00.054.768 I llm_load_print_meta: n_expert         = 0
0.00.054.768 I llm_load_print_meta: n_expert_used    = 0
0.00.054.769 I llm_load_print_meta: causal attn      = 1
0.00.054.769 I llm_load_print_meta: pooling type     = 0
0.00.054.769 I llm_load_print_meta: rope type        = 2
0.00.054.769 I llm_load_print_meta: rope scaling     = linear
0.00.054.769 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.770 I llm_load_print_meta: freq_scale_train = 1
0.00.054.770 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.770 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.770 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.773 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.773 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.773 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.773 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.774 I llm_load_print_meta: model type       = 1.4B
0.00.054.774 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.054.774 I llm_load_print_meta: model params     = 1.41 B
0.00.054.775 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.054.775 I llm_load_print_meta: general.name     = 1.4B
0.00.054.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.775 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.776 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.776 I llm_load_print_meta: LF token         = 128 ''
0.00.054.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.776 I llm_load_print_meta: max token length = 1024
0.00.056.704 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.705 I llm_load_tensors: offloading output layer to GPU
0.00.056.705 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.716 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.056.718 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.057.599 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.600 I llama_new_context_with_model: n_ctx         = 128
0.00.057.600 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.057.601 I llama_new_context_with_model: n_batch       = 128
0.00.057.601 I llama_new_context_with_model: n_ubatch      = 128
0.00.057.601 I llama_new_context_with_model: flash_attn    = 0
0.00.057.601 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.602 I llama_new_context_with_model: freq_scale    = 1
0.00.057.602 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.057.603 I ggml_metal_init: allocating
0.00.057.607 I ggml_metal_init: found device: Apple M4
0.00.057.609 I ggml_metal_init: picking default device: Apple M4
0.00.058.235 I ggml_metal_init: using embedded metal library
0.00.060.689 I ggml_metal_init: GPU name:   Apple M4
0.00.060.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.692 I ggml_metal_init: simdgroup reduction   = true
0.00.060.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.692 I ggml_metal_init: has bfloat            = true
0.00.060.692 I ggml_metal_init: use bfloat            = true
0.00.060.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.694 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.940 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.072.322 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.072.325 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.072.337 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.073.350 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.073.351 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.073.351 I llama_new_context_with_model: graph nodes  = 967
0.00.073.351 I llama_new_context_with_model: graph splits = 2
0.00.073.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.073.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.866 I 
0.00.381.909 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.922 I perplexity: tokenizing the input ..
0.00.389.626 I perplexity: tokenization took 7.702 ms
0.00.389.632 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.521.266 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.522.776 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.522.792 I llama_perf_context_print:        load time =     372.03 ms
0.00.522.793 I llama_perf_context_print: prompt eval time =     131.38 ms /   128 tokens (    1.03 ms per token,   974.30 tokens per second)
0.00.522.794 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.522.794 I llama_perf_context_print:       total time =     140.93 ms /   129 tokens
0.00.523.142 I ggml_metal_free: deallocating

real	0m0.541s
user	0m0.081s
sys	0m0.055s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.596 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.601 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.616 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.617 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.617 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.618 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.620 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.816 I llama_model_loader: - type  f32:  194 tensors
0.00.026.816 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.816 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.816 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.817 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.045 I llm_load_vocab: special tokens cache size = 25
0.00.054.006 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.009 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.009 I llm_load_print_meta: arch             = gptneox
0.00.054.009 I llm_load_print_meta: vocab type       = BPE
0.00.054.009 I llm_load_print_meta: n_vocab          = 50304
0.00.054.010 I llm_load_print_meta: n_merges         = 50009
0.00.054.010 I llm_load_print_meta: vocab_only       = 0
0.00.054.010 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.010 I llm_load_print_meta: n_embd           = 2048
0.00.054.010 I llm_load_print_meta: n_layer          = 24
0.00.054.026 I llm_load_print_meta: n_head           = 16
0.00.054.028 I llm_load_print_meta: n_head_kv        = 16
0.00.054.028 I llm_load_print_meta: n_rot            = 32
0.00.054.028 I llm_load_print_meta: n_swa            = 0
0.00.054.029 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.030 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.030 I llm_load_print_meta: n_gqa            = 1
0.00.054.031 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.031 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.032 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.032 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.032 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.032 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.033 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.033 I llm_load_print_meta: n_ff             = 8192
0.00.054.035 I llm_load_print_meta: n_expert         = 0
0.00.054.036 I llm_load_print_meta: n_expert_used    = 0
0.00.054.036 I llm_load_print_meta: causal attn      = 1
0.00.054.036 I llm_load_print_meta: pooling type     = 0
0.00.054.037 I llm_load_print_meta: rope type        = 2
0.00.054.037 I llm_load_print_meta: rope scaling     = linear
0.00.054.037 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.037 I llm_load_print_meta: freq_scale_train = 1
0.00.054.037 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.038 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.038 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.038 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.038 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.038 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.038 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.040 I llm_load_print_meta: model type       = 1.4B
0.00.054.040 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.054.040 I llm_load_print_meta: model params     = 1.41 B
0.00.054.041 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.054.041 I llm_load_print_meta: general.name     = 1.4B
0.00.054.041 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.041 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.042 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.042 I llm_load_print_meta: LF token         = 128 ''
0.00.054.042 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.042 I llm_load_print_meta: max token length = 1024
0.00.055.989 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.989 I llm_load_tensors: offloading output layer to GPU
0.00.055.990 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.000 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.056.001 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.930 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.931 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.931 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.931 I llama_new_context_with_model: n_batch       = 2048
0.00.056.932 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.932 I llama_new_context_with_model: flash_attn    = 0
0.00.056.932 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.932 I llama_new_context_with_model: freq_scale    = 1
0.00.056.933 I ggml_metal_init: allocating
0.00.056.940 I ggml_metal_init: found device: Apple M4
0.00.056.943 I ggml_metal_init: picking default device: Apple M4
0.00.057.539 I ggml_metal_init: using embedded metal library
0.00.059.879 I ggml_metal_init: GPU name:   Apple M4
0.00.059.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.882 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.882 I ggml_metal_init: simdgroup reduction   = true
0.00.059.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.883 I ggml_metal_init: has bfloat            = true
0.00.059.883 I ggml_metal_init: use bfloat            = true
0.00.059.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.884 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.345 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.162 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.167 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.184 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.253 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.255 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.255 I llama_new_context_with_model: graph nodes  = 967
0.00.089.256 I llama_new_context_with_model: graph splits = 2
0.00.089.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.408 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.548.190 I main: llama threadpool init, n_threads = 4
0.00.548.229 I 
0.00.548.281 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.548.282 I 
0.00.548.509 I sampler seed: 1234
0.00.548.513 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.548.534 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.548.534 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.548.535 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.299.025 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54911.06 tokens per second)
0.01.299.025 I llama_perf_context_print:        load time =     538.59 ms
0.01.299.026 I llama_perf_context_print: prompt eval time =      44.49 ms /     7 tokens (    6.36 ms per token,   157.34 tokens per second)
0.01.299.027 I llama_perf_context_print:        eval time =     702.90 ms /    63 runs   (   11.16 ms per token,    89.63 tokens per second)
0.01.299.027 I llama_perf_context_print:       total time =     750.84 ms /    70 tokens
0.01.299.257 I ggml_metal_free: deallocating

real	0m1.315s
user	0m0.110s
sys	0m0.133s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.794 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.789 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.794 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.795 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.801 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.801 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.801 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.802 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.804 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.804 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.804 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.684 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.919 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.954 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.956 I llama_model_loader: - type  f32:  194 tensors
0.00.023.956 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.956 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.956 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.957 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.113 I llm_load_vocab: special tokens cache size = 25
0.00.051.170 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.172 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.173 I llm_load_print_meta: arch             = gptneox
0.00.051.173 I llm_load_print_meta: vocab type       = BPE
0.00.051.174 I llm_load_print_meta: n_vocab          = 50304
0.00.051.174 I llm_load_print_meta: n_merges         = 50009
0.00.051.174 I llm_load_print_meta: vocab_only       = 0
0.00.051.174 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.174 I llm_load_print_meta: n_embd           = 2048
0.00.051.175 I llm_load_print_meta: n_layer          = 24
0.00.051.184 I llm_load_print_meta: n_head           = 16
0.00.051.185 I llm_load_print_meta: n_head_kv        = 16
0.00.051.185 I llm_load_print_meta: n_rot            = 32
0.00.051.185 I llm_load_print_meta: n_swa            = 0
0.00.051.187 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.187 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.188 I llm_load_print_meta: n_gqa            = 1
0.00.051.189 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.190 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.190 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.195 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.195 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.195 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.196 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.197 I llm_load_print_meta: n_ff             = 8192
0.00.051.197 I llm_load_print_meta: n_expert         = 0
0.00.051.197 I llm_load_print_meta: n_expert_used    = 0
0.00.051.198 I llm_load_print_meta: causal attn      = 1
0.00.051.198 I llm_load_print_meta: pooling type     = 0
0.00.051.198 I llm_load_print_meta: rope type        = 2
0.00.051.198 I llm_load_print_meta: rope scaling     = linear
0.00.051.198 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.199 I llm_load_print_meta: freq_scale_train = 1
0.00.051.199 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.204 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.205 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.206 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.206 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.206 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.206 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.206 I llm_load_print_meta: model type       = 1.4B
0.00.051.207 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.207 I llm_load_print_meta: model params     = 1.41 B
0.00.051.208 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.208 I llm_load_print_meta: general.name     = 1.4B
0.00.051.208 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.208 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.209 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.209 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.209 I llm_load_print_meta: LF token         = 128 ''
0.00.051.209 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.210 I llm_load_print_meta: max token length = 1024
0.00.052.993 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.994 I llm_load_tensors: offloading output layer to GPU
0.00.052.994 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.000 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.001 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.937 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.938 I llama_new_context_with_model: n_ctx         = 128
0.00.053.938 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.938 I llama_new_context_with_model: n_batch       = 128
0.00.053.938 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.939 I llama_new_context_with_model: flash_attn    = 0
0.00.053.939 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.939 I llama_new_context_with_model: freq_scale    = 1
0.00.053.940 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.940 I ggml_metal_init: allocating
0.00.053.943 I ggml_metal_init: found device: Apple M4
0.00.053.945 I ggml_metal_init: picking default device: Apple M4
0.00.054.546 I ggml_metal_init: using embedded metal library
0.00.056.925 I ggml_metal_init: GPU name:   Apple M4
0.00.056.926 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.927 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.927 I ggml_metal_init: simdgroup reduction   = true
0.00.056.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.928 I ggml_metal_init: has bfloat            = true
0.00.056.928 I ggml_metal_init: use bfloat            = true
0.00.056.928 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.929 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.133 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.396 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.398 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.413 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.313 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.314 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.315 I llama_new_context_with_model: graph nodes  = 967
0.00.069.315 I llama_new_context_with_model: graph splits = 2
0.00.069.328 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.482.007 I 
0.00.482.044 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.482.059 I perplexity: tokenizing the input ..
0.00.489.916 I perplexity: tokenization took 7.855 ms
0.00.489.923 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.621.811 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.623.090 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.623.105 I llama_perf_context_print:        load time =     473.21 ms
0.00.623.106 I llama_perf_context_print: prompt eval time =     131.66 ms /   128 tokens (    1.03 ms per token,   972.23 tokens per second)
0.00.623.107 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.112 I llama_perf_context_print:       total time =     141.10 ms /   129 tokens
0.00.623.524 I ggml_metal_free: deallocating

real	0m0.636s
user	0m0.080s
sys	0m0.086s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.610 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.102 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.113 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.113 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.114 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.115 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.115 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.115 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.116 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.118 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.119 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.119 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.106 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.364 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.365 I llama_model_loader: - type  f32:  194 tensors
0.00.025.365 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.365 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.365 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.656 I llm_load_vocab: special tokens cache size = 25
0.00.051.585 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.587 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.588 I llm_load_print_meta: arch             = gptneox
0.00.051.588 I llm_load_print_meta: vocab type       = BPE
0.00.051.588 I llm_load_print_meta: n_vocab          = 50304
0.00.051.589 I llm_load_print_meta: n_merges         = 50009
0.00.051.589 I llm_load_print_meta: vocab_only       = 0
0.00.051.589 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.589 I llm_load_print_meta: n_embd           = 2048
0.00.051.589 I llm_load_print_meta: n_layer          = 24
0.00.051.603 I llm_load_print_meta: n_head           = 16
0.00.051.604 I llm_load_print_meta: n_head_kv        = 16
0.00.051.604 I llm_load_print_meta: n_rot            = 32
0.00.051.604 I llm_load_print_meta: n_swa            = 0
0.00.051.604 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.605 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.605 I llm_load_print_meta: n_gqa            = 1
0.00.051.606 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.607 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.608 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.608 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.609 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.609 I llm_load_print_meta: n_ff             = 8192
0.00.051.610 I llm_load_print_meta: n_expert         = 0
0.00.051.611 I llm_load_print_meta: n_expert_used    = 0
0.00.051.612 I llm_load_print_meta: causal attn      = 1
0.00.051.612 I llm_load_print_meta: pooling type     = 0
0.00.051.613 I llm_load_print_meta: rope type        = 2
0.00.051.613 I llm_load_print_meta: rope scaling     = linear
0.00.051.613 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.613 I llm_load_print_meta: freq_scale_train = 1
0.00.051.614 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.614 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.614 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.614 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.614 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.614 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.614 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.616 I llm_load_print_meta: model type       = 1.4B
0.00.051.616 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.616 I llm_load_print_meta: model params     = 1.41 B
0.00.051.617 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.617 I llm_load_print_meta: general.name     = 1.4B
0.00.051.617 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.617 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.618 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.619 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.619 I llm_load_print_meta: LF token         = 128 ''
0.00.051.619 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.619 I llm_load_print_meta: max token length = 1024
0.00.053.594 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.595 I llm_load_tensors: offloading output layer to GPU
0.00.053.595 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.605 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.606 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.513 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.514 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.514 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.514 I llama_new_context_with_model: n_batch       = 2048
0.00.054.514 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.514 I llama_new_context_with_model: flash_attn    = 0
0.00.054.515 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.515 I llama_new_context_with_model: freq_scale    = 1
0.00.054.516 I ggml_metal_init: allocating
0.00.054.519 I ggml_metal_init: found device: Apple M4
0.00.054.521 I ggml_metal_init: picking default device: Apple M4
0.00.055.141 I ggml_metal_init: using embedded metal library
0.00.057.439 I ggml_metal_init: GPU name:   Apple M4
0.00.057.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.443 I ggml_metal_init: simdgroup reduction   = true
0.00.057.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.443 I ggml_metal_init: has bfloat            = true
0.00.057.443 I ggml_metal_init: use bfloat            = true
0.00.057.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.444 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.078 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.053 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.062 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.105 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.107 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.107 I llama_new_context_with_model: graph nodes  = 967
0.00.088.107 I llama_new_context_with_model: graph splits = 2
0.00.088.121 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.263 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.329 I main: llama threadpool init, n_threads = 4
0.00.642.368 I 
0.00.642.398 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.399 I 
0.00.642.635 I sampler seed: 1234
0.00.642.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.642.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.642.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.642.691 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.404.612 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.404.613 I llama_perf_context_print:        load time =     632.72 ms
0.01.404.614 I llama_perf_context_print: prompt eval time =      50.99 ms /     7 tokens (    7.28 ms per token,   137.29 tokens per second)
0.01.404.614 I llama_perf_context_print:        eval time =     707.93 ms /    63 runs   (   11.24 ms per token,    88.99 tokens per second)
0.01.404.615 I llama_perf_context_print:       total time =     762.29 ms /    70 tokens
0.01.404.808 I ggml_metal_free: deallocating

real	0m1.425s
user	0m0.111s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.237 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.067 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.072 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.074 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.074 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.075 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.075 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.076 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.076 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.063 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.296 I llama_model_loader: - type  f32:  194 tensors
0.00.024.296 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.296 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.297 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.455 I llm_load_vocab: special tokens cache size = 25
0.00.051.447 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.450 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.450 I llm_load_print_meta: arch             = gptneox
0.00.051.450 I llm_load_print_meta: vocab type       = BPE
0.00.051.451 I llm_load_print_meta: n_vocab          = 50304
0.00.051.451 I llm_load_print_meta: n_merges         = 50009
0.00.051.451 I llm_load_print_meta: vocab_only       = 0
0.00.051.451 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.452 I llm_load_print_meta: n_embd           = 2048
0.00.051.452 I llm_load_print_meta: n_layer          = 24
0.00.051.465 I llm_load_print_meta: n_head           = 16
0.00.051.466 I llm_load_print_meta: n_head_kv        = 16
0.00.051.466 I llm_load_print_meta: n_rot            = 32
0.00.051.467 I llm_load_print_meta: n_swa            = 0
0.00.051.467 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.467 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.468 I llm_load_print_meta: n_gqa            = 1
0.00.051.469 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.469 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.470 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.470 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.470 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.471 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.471 I llm_load_print_meta: n_ff             = 8192
0.00.051.472 I llm_load_print_meta: n_expert         = 0
0.00.051.472 I llm_load_print_meta: n_expert_used    = 0
0.00.051.472 I llm_load_print_meta: causal attn      = 1
0.00.051.472 I llm_load_print_meta: pooling type     = 0
0.00.051.472 I llm_load_print_meta: rope type        = 2
0.00.051.472 I llm_load_print_meta: rope scaling     = linear
0.00.051.473 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.473 I llm_load_print_meta: freq_scale_train = 1
0.00.051.473 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.473 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.473 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.475 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.475 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.476 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.476 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.477 I llm_load_print_meta: model type       = 1.4B
0.00.051.477 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.477 I llm_load_print_meta: model params     = 1.41 B
0.00.051.478 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.478 I llm_load_print_meta: general.name     = 1.4B
0.00.051.478 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.479 I llm_load_print_meta: LF token         = 128 ''
0.00.051.479 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.480 I llm_load_print_meta: max token length = 1024
0.00.053.505 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.506 I llm_load_tensors: offloading output layer to GPU
0.00.053.506 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.517 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.518 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.388 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.389 I llama_new_context_with_model: n_ctx         = 128
0.00.054.389 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.390 I llama_new_context_with_model: n_batch       = 128
0.00.054.390 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.390 I llama_new_context_with_model: flash_attn    = 0
0.00.054.390 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.391 I llama_new_context_with_model: freq_scale    = 1
0.00.054.391 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.391 I ggml_metal_init: allocating
0.00.054.397 I ggml_metal_init: found device: Apple M4
0.00.054.399 I ggml_metal_init: picking default device: Apple M4
0.00.054.980 I ggml_metal_init: using embedded metal library
0.00.057.321 I ggml_metal_init: GPU name:   Apple M4
0.00.057.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.324 I ggml_metal_init: simdgroup reduction   = true
0.00.057.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.324 I ggml_metal_init: has bfloat            = true
0.00.057.324 I ggml_metal_init: use bfloat            = true
0.00.057.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.765 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.010 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.014 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.028 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.884 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.885 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.886 I llama_new_context_with_model: graph nodes  = 967
0.00.068.886 I llama_new_context_with_model: graph splits = 2
0.00.068.894 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.895 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.470 I 
0.00.582.501 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.582.514 I perplexity: tokenizing the input ..
0.00.590.496 I perplexity: tokenization took 7.98 ms
0.00.590.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.724.761 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.725.950 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.725.981 I llama_perf_context_print:        load time =     573.23 ms
0.00.725.982 I llama_perf_context_print: prompt eval time =     134.04 ms /   128 tokens (    1.05 ms per token,   954.97 tokens per second)
0.00.725.983 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.725.983 I llama_perf_context_print:       total time =     143.51 ms /   129 tokens
0.00.726.570 I ggml_metal_free: deallocating

real	0m0.744s
user	0m0.079s
sys	0m0.105s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.014.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.941 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.020.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.952 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.954 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.954 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.955 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.955 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.955 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.061 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.425 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.426 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.426 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.427 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.427 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.427 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.030.428 I llama_model_loader: - type  f32:  194 tensors
0.00.030.428 I llama_model_loader: - type q5_K:   61 tensors
0.00.030.428 I llama_model_loader: - type q6_K:   37 tensors
0.00.051.573 I llm_load_vocab: special tokens cache size = 25
0.00.057.682 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.057.685 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.057.685 I llm_load_print_meta: arch             = gptneox
0.00.057.685 I llm_load_print_meta: vocab type       = BPE
0.00.057.686 I llm_load_print_meta: n_vocab          = 50304
0.00.057.686 I llm_load_print_meta: n_merges         = 50009
0.00.057.686 I llm_load_print_meta: vocab_only       = 0
0.00.057.686 I llm_load_print_meta: n_ctx_train      = 2048
0.00.057.687 I llm_load_print_meta: n_embd           = 2048
0.00.057.687 I llm_load_print_meta: n_layer          = 24
0.00.057.701 I llm_load_print_meta: n_head           = 16
0.00.057.702 I llm_load_print_meta: n_head_kv        = 16
0.00.057.703 I llm_load_print_meta: n_rot            = 32
0.00.057.703 I llm_load_print_meta: n_swa            = 0
0.00.057.703 I llm_load_print_meta: n_embd_head_k    = 128
0.00.057.703 I llm_load_print_meta: n_embd_head_v    = 128
0.00.057.704 I llm_load_print_meta: n_gqa            = 1
0.00.057.704 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.057.705 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.057.706 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.057.706 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.057.706 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.057.707 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.057.707 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.057.707 I llm_load_print_meta: n_ff             = 8192
0.00.057.707 I llm_load_print_meta: n_expert         = 0
0.00.057.708 I llm_load_print_meta: n_expert_used    = 0
0.00.057.708 I llm_load_print_meta: causal attn      = 1
0.00.057.708 I llm_load_print_meta: pooling type     = 0
0.00.057.709 I llm_load_print_meta: rope type        = 2
0.00.057.709 I llm_load_print_meta: rope scaling     = linear
0.00.057.709 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.057.709 I llm_load_print_meta: freq_scale_train = 1
0.00.057.709 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.057.710 I llm_load_print_meta: rope_finetuned   = unknown
0.00.057.710 I llm_load_print_meta: ssm_d_conv       = 0
0.00.057.710 I llm_load_print_meta: ssm_d_inner      = 0
0.00.057.710 I llm_load_print_meta: ssm_d_state      = 0
0.00.057.710 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.057.710 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.057.711 I llm_load_print_meta: model type       = 1.4B
0.00.057.711 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.057.711 I llm_load_print_meta: model params     = 1.41 B
0.00.057.712 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.057.712 I llm_load_print_meta: general.name     = 1.4B
0.00.057.712 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.057.712 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.057.714 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.057.714 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.057.714 I llm_load_print_meta: LF token         = 128 ''
0.00.057.714 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.057.715 I llm_load_print_meta: max token length = 1024
0.00.059.794 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.795 I llm_load_tensors: offloading output layer to GPU
0.00.059.795 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.805 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.059.806 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.060.744 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.745 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.745 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.745 I llama_new_context_with_model: n_batch       = 2048
0.00.060.745 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.745 I llama_new_context_with_model: flash_attn    = 0
0.00.060.746 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.746 I llama_new_context_with_model: freq_scale    = 1
0.00.060.747 I ggml_metal_init: allocating
0.00.060.752 I ggml_metal_init: found device: Apple M4
0.00.060.755 I ggml_metal_init: picking default device: Apple M4
0.00.061.369 I ggml_metal_init: using embedded metal library
0.00.063.697 I ggml_metal_init: GPU name:   Apple M4
0.00.063.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.699 I ggml_metal_init: simdgroup reduction   = true
0.00.063.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.700 I ggml_metal_init: has bfloat            = true
0.00.063.700 I ggml_metal_init: use bfloat            = true
0.00.063.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.186 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.721 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.726 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.744 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.681 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.683 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.683 I llama_new_context_with_model: graph nodes  = 967
0.00.094.684 I llama_new_context_with_model: graph splits = 2
0.00.094.691 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.840 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.841 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.210 I main: llama threadpool init, n_threads = 4
0.00.706.271 I 
0.00.706.299 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.300 I 
0.00.706.528 I sampler seed: 1234
0.00.706.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.586 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.589 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.589 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.568.573 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51189.62 tokens per second)
0.01.568.574 I llama_perf_context_print:        load time =     691.43 ms
0.01.568.576 I llama_perf_context_print: prompt eval time =      57.84 ms /     7 tokens (    8.26 ms per token,   121.03 tokens per second)
0.01.568.577 I llama_perf_context_print:        eval time =     801.51 ms /    63 runs   (   12.72 ms per token,    78.60 tokens per second)
0.01.568.578 I llama_perf_context_print:       total time =     862.37 ms /    70 tokens
0.01.568.807 I ggml_metal_free: deallocating

real	0m1.588s
user	0m0.111s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.800 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.714 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.719 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.721 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.721 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.722 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.722 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.722 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.724 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.724 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.724 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.725 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.726 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.726 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.727 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.728 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.728 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.728 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.733 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.994 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.947 I llama_model_loader: - type  f32:  194 tensors
0.00.023.947 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.947 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.018 I llm_load_vocab: special tokens cache size = 25
0.00.050.795 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.798 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.798 I llm_load_print_meta: arch             = gptneox
0.00.050.799 I llm_load_print_meta: vocab type       = BPE
0.00.050.799 I llm_load_print_meta: n_vocab          = 50304
0.00.050.799 I llm_load_print_meta: n_merges         = 50009
0.00.050.799 I llm_load_print_meta: vocab_only       = 0
0.00.050.800 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.800 I llm_load_print_meta: n_embd           = 2048
0.00.050.800 I llm_load_print_meta: n_layer          = 24
0.00.050.814 I llm_load_print_meta: n_head           = 16
0.00.050.815 I llm_load_print_meta: n_head_kv        = 16
0.00.050.815 I llm_load_print_meta: n_rot            = 32
0.00.050.815 I llm_load_print_meta: n_swa            = 0
0.00.050.815 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.815 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.816 I llm_load_print_meta: n_gqa            = 1
0.00.050.817 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.817 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.818 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.818 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.819 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.819 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.819 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.820 I llm_load_print_meta: n_ff             = 8192
0.00.050.820 I llm_load_print_meta: n_expert         = 0
0.00.050.820 I llm_load_print_meta: n_expert_used    = 0
0.00.050.820 I llm_load_print_meta: causal attn      = 1
0.00.050.820 I llm_load_print_meta: pooling type     = 0
0.00.050.820 I llm_load_print_meta: rope type        = 2
0.00.050.821 I llm_load_print_meta: rope scaling     = linear
0.00.050.821 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.821 I llm_load_print_meta: freq_scale_train = 1
0.00.050.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.822 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.822 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.822 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.823 I llm_load_print_meta: model type       = 1.4B
0.00.050.824 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.825 I llm_load_print_meta: model params     = 1.41 B
0.00.050.825 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.825 I llm_load_print_meta: general.name     = 1.4B
0.00.050.826 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.826 I llm_load_print_meta: LF token         = 128 ''
0.00.050.827 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.830 I llm_load_print_meta: max token length = 1024
0.00.052.841 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.842 I llm_load_tensors: offloading output layer to GPU
0.00.052.842 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.853 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.854 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.766 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.767 I llama_new_context_with_model: n_ctx         = 128
0.00.053.767 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.767 I llama_new_context_with_model: n_batch       = 128
0.00.053.767 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.767 I llama_new_context_with_model: flash_attn    = 0
0.00.053.768 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.768 I llama_new_context_with_model: freq_scale    = 1
0.00.053.768 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.769 I ggml_metal_init: allocating
0.00.053.772 I ggml_metal_init: found device: Apple M4
0.00.053.774 I ggml_metal_init: picking default device: Apple M4
0.00.054.338 I ggml_metal_init: using embedded metal library
0.00.056.676 I ggml_metal_init: GPU name:   Apple M4
0.00.056.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.678 I ggml_metal_init: simdgroup reduction   = true
0.00.056.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.679 I ggml_metal_init: has bfloat            = true
0.00.056.679 I ggml_metal_init: use bfloat            = true
0.00.056.679 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.435 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.904 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.908 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.924 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.814 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.815 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.815 I llama_new_context_with_model: graph nodes  = 967
0.00.068.815 I llama_new_context_with_model: graph splits = 2
0.00.068.827 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.368 I 
0.00.637.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.413 I perplexity: tokenizing the input ..
0.00.645.574 I perplexity: tokenization took 8.16 ms
0.00.645.577 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.637 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.787.910 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.787.944 I llama_perf_context_print:        load time =     628.56 ms
0.00.787.945 I llama_perf_context_print: prompt eval time =     140.81 ms /   128 tokens (    1.10 ms per token,   909.02 tokens per second)
0.00.787.946 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.946 I llama_perf_context_print:       total time =     150.58 ms /   129 tokens
0.00.788.454 I ggml_metal_free: deallocating

real	0m0.801s
user	0m0.079s
sys	0m0.115s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.370 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.374 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.382 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.383 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.384 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.385 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.385 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.386 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.386 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.388 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.388 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.488 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.490 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.490 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.491 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.492 I llama_model_loader: - type  f32:  194 tensors
0.00.025.492 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.626 I llm_load_vocab: special tokens cache size = 25
0.00.052.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.604 I llm_load_print_meta: arch             = gptneox
0.00.052.605 I llm_load_print_meta: vocab type       = BPE
0.00.052.605 I llm_load_print_meta: n_vocab          = 50304
0.00.052.605 I llm_load_print_meta: n_merges         = 50009
0.00.052.606 I llm_load_print_meta: vocab_only       = 0
0.00.052.606 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.606 I llm_load_print_meta: n_embd           = 2048
0.00.052.606 I llm_load_print_meta: n_layer          = 24
0.00.052.620 I llm_load_print_meta: n_head           = 16
0.00.052.621 I llm_load_print_meta: n_head_kv        = 16
0.00.052.621 I llm_load_print_meta: n_rot            = 32
0.00.052.621 I llm_load_print_meta: n_swa            = 0
0.00.052.621 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.622 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.622 I llm_load_print_meta: n_gqa            = 1
0.00.052.623 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.627 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.627 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.628 I llm_load_print_meta: n_ff             = 8192
0.00.052.628 I llm_load_print_meta: n_expert         = 0
0.00.052.628 I llm_load_print_meta: n_expert_used    = 0
0.00.052.628 I llm_load_print_meta: causal attn      = 1
0.00.052.629 I llm_load_print_meta: pooling type     = 0
0.00.052.629 I llm_load_print_meta: rope type        = 2
0.00.052.629 I llm_load_print_meta: rope scaling     = linear
0.00.052.629 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.629 I llm_load_print_meta: freq_scale_train = 1
0.00.052.630 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.630 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.630 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.630 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.632 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.632 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.632 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.632 I llm_load_print_meta: model type       = 1.4B
0.00.052.633 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.633 I llm_load_print_meta: model params     = 1.41 B
0.00.052.634 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.634 I llm_load_print_meta: general.name     = 1.4B
0.00.052.635 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.635 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.635 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.635 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.636 I llm_load_print_meta: LF token         = 128 ''
0.00.052.636 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.639 I llm_load_print_meta: max token length = 1024
0.00.054.688 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.689 I llm_load_tensors: offloading output layer to GPU
0.00.054.689 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.699 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.700 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.658 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.659 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.659 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.659 I llama_new_context_with_model: n_batch       = 2048
0.00.055.659 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.659 I llama_new_context_with_model: flash_attn    = 0
0.00.055.660 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.660 I llama_new_context_with_model: freq_scale    = 1
0.00.055.660 I ggml_metal_init: allocating
0.00.055.664 I ggml_metal_init: found device: Apple M4
0.00.055.666 I ggml_metal_init: picking default device: Apple M4
0.00.056.274 I ggml_metal_init: using embedded metal library
0.00.058.657 I ggml_metal_init: GPU name:   Apple M4
0.00.058.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.659 I ggml_metal_init: simdgroup reduction   = true
0.00.058.659 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.659 I ggml_metal_init: has bfloat            = true
0.00.058.660 I ggml_metal_init: use bfloat            = true
0.00.058.660 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.852 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.089.758 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.772 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.794 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.843 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.845 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.845 I llama_new_context_with_model: graph nodes  = 967
0.00.090.845 I llama_new_context_with_model: graph splits = 2
0.00.090.861 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.003 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.133 I main: llama threadpool init, n_threads = 4
0.00.784.166 I 
0.00.784.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.209 I 
0.00.784.436 I sampler seed: 1234
0.00.784.440 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.465 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.469 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.469 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.672.066 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.672.066 I llama_perf_context_print:        load time =     774.44 ms
0.01.672.067 I llama_perf_context_print: prompt eval time =      58.36 ms /     7 tokens (    8.34 ms per token,   119.94 tokens per second)
0.01.672.068 I llama_perf_context_print:        eval time =     826.06 ms /    63 runs   (   13.11 ms per token,    76.27 tokens per second)
0.01.672.068 I llama_perf_context_print:       total time =     887.93 ms /    70 tokens
0.01.672.275 I ggml_metal_free: deallocating

real	0m1.690s
user	0m0.112s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4397 (48160087) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.015 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.692 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.696 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.698 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.698 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.699 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.699 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.700 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.700 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.701 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.701 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.702 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.704 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.704 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.678 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.870 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.872 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.872 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.873 I llama_model_loader: - type  f32:  194 tensors
0.00.024.873 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.058 I llm_load_vocab: special tokens cache size = 25
0.00.052.097 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.100 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.100 I llm_load_print_meta: arch             = gptneox
0.00.052.100 I llm_load_print_meta: vocab type       = BPE
0.00.052.101 I llm_load_print_meta: n_vocab          = 50304
0.00.052.101 I llm_load_print_meta: n_merges         = 50009
0.00.052.101 I llm_load_print_meta: vocab_only       = 0
0.00.052.101 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.102 I llm_load_print_meta: n_embd           = 2048
0.00.052.102 I llm_load_print_meta: n_layer          = 24
0.00.052.116 I llm_load_print_meta: n_head           = 16
0.00.052.116 I llm_load_print_meta: n_head_kv        = 16
0.00.052.117 I llm_load_print_meta: n_rot            = 32
0.00.052.117 I llm_load_print_meta: n_swa            = 0
0.00.052.117 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.117 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.118 I llm_load_print_meta: n_gqa            = 1
0.00.052.119 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.119 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.120 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.120 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.122 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.122 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.122 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.122 I llm_load_print_meta: n_ff             = 8192
0.00.052.123 I llm_load_print_meta: n_expert         = 0
0.00.052.123 I llm_load_print_meta: n_expert_used    = 0
0.00.052.123 I llm_load_print_meta: causal attn      = 1
0.00.052.123 I llm_load_print_meta: pooling type     = 0
0.00.052.123 I llm_load_print_meta: rope type        = 2
0.00.052.123 I llm_load_print_meta: rope scaling     = linear
0.00.052.124 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.124 I llm_load_print_meta: freq_scale_train = 1
0.00.052.126 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.126 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.126 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.126 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.126 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.127 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.127 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.127 I llm_load_print_meta: model type       = 1.4B
0.00.052.128 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.128 I llm_load_print_meta: model params     = 1.41 B
0.00.052.128 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.128 I llm_load_print_meta: general.name     = 1.4B
0.00.052.129 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.130 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.130 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.130 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.130 I llm_load_print_meta: LF token         = 128 ''
0.00.052.131 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.131 I llm_load_print_meta: max token length = 1024
0.00.054.241 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.241 I llm_load_tensors: offloading output layer to GPU
0.00.054.242 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.252 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.253 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.128 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.128 I llama_new_context_with_model: n_ctx         = 128
0.00.055.128 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.129 I llama_new_context_with_model: n_batch       = 128
0.00.055.129 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.129 I llama_new_context_with_model: flash_attn    = 0
0.00.055.129 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.130 I llama_new_context_with_model: freq_scale    = 1
0.00.055.130 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.130 I ggml_metal_init: allocating
0.00.055.134 I ggml_metal_init: found device: Apple M4
0.00.055.136 I ggml_metal_init: picking default device: Apple M4
0.00.055.715 I ggml_metal_init: using embedded metal library
0.00.058.050 I ggml_metal_init: GPU name:   Apple M4
0.00.058.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.052 I ggml_metal_init: simdgroup reduction   = true
0.00.058.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.052 I ggml_metal_init: has bfloat            = true
0.00.058.052 I ggml_metal_init: use bfloat            = true
0.00.058.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.884 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.320 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.324 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.340 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.227 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.228 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.229 I llama_new_context_with_model: graph nodes  = 967
0.00.070.229 I llama_new_context_with_model: graph splits = 2
0.00.070.241 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.182.653 I 
0.00.182.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.182.718 I perplexity: tokenizing the input ..
0.00.190.292 I perplexity: tokenization took 7.572 ms
0.00.190.296 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.330.639 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.331.814 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.331.835 I llama_perf_context_print:        load time =     172.63 ms
0.00.331.839 I llama_perf_context_print: prompt eval time =     140.03 ms /   128 tokens (    1.09 ms per token,   914.08 tokens per second)
0.00.331.842 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.331.842 I llama_perf_context_print:       total time =     149.18 ms /   129 tokens
0.00.332.306 I ggml_metal_free: deallocating

real	0m0.349s
user	0m0.079s
sys	0m0.045s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4397 (48160087)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106e0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106e0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106e0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106e0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106e0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106e0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106e0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106e0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106e0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106e0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106e0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106e0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106e0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106e0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106e0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106e101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106e10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106e11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106e11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106e11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106e12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106e12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106e13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106e13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106e14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106e14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106e14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106e15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106e15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106e16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106e16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106e168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106e17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106e176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106e17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106e17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106e182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106e18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106e18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106e19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106e19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106e199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106e19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106e1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106e1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106e1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106e1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106e1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106e1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106e1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106e1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106e1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106e1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106e1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106e1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106e1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106e1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106e1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106e20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106e20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106e208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106e20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106e21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106e216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106e21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106e21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106e22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106e22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106e22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106e23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106e23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106e23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106e240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106e24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106e24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106e250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106e25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106e25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106e260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106e26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106e26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106e270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106e27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106e27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106e280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106e28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106e28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106e290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106e295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106e29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106e2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106e2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106e2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106e2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106e2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106e2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106e1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106e2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106e2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106e2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106e2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106e2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106e2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106e2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106e2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106e2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106e2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106e2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106e2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106e301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106e30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106e30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106e310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106e31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106e31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106e31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106e32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106e32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106e32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106e33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106e335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106e33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106e33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106e343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106e34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106e351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106e35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106e35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106e35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106e368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106e36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106e37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106e376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106e37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106e37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106e38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106e38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106e38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106e39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106e39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106e3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106e3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106e3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106e3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106e3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106e3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106e3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106e3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106e3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106e3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106e3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106e3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106e3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106e3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106e3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106e3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106e3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106e3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106e3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106e3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106e40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106e40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106e413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106e41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106e41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106e421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106e42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106e42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106e42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106e43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106e438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106e43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106e44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106e446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106e44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106e45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106e454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106e45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106e45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106e46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106e46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106e46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106e47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106e47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106e479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106e47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106e483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106e488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106e48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106e49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106e49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106e49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106e4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106e4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106e4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106e4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106e4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106e4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106e4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106e4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106e4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106e4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106e4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106e4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106e4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106e4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106e4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106e4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106e50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106e506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106e50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106e51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106e51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106e51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106e52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106e52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106e52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106e53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106e53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106e53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106e54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106e54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106e54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106e55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106e55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106e55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106e560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106e56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106e56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106e570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106e57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106e57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106e580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106e58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106e58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106e590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106e59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106e59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106e5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106e5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106e5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106e5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106e5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106e5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106e5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106e5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106e5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106e5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106e5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106e5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106e5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106e5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106e5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106e5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106e5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106e5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106e60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106e605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106e60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106e60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106e61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106e618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106e61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106e62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106e626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106e62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106e62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106e63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106e63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106e64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106e64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106e64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106e65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106e655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106e65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106e663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106e66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106e67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106e674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106e67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106e67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106e685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.146.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.146.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106e68250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106e49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x106e49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106e4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106e1d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106e1d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106e1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x106e4c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106e149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x106e1b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106e1bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106e1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106e1a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x106e1c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106e139c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x106e1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x106e2c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x106e677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x106e16ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x106e16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x106e4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x106e4ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x106e14fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x106e15290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x106e15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x106e68a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x106e68cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x106e68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x106e69240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x106e69500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x106e697c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106e69a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106e69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106e6a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106e6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106e6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106e6a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106e6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x106e6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106e6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106e6b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106e6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x106e6b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x106e6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106e6be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106e6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106e6c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106e6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106e6c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x106e6cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106e6cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106e6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106e6d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106e6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106e6d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106e6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106e6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x106e6e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106e6e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x106e6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106e6ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106e6ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106e6efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106e6f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106e6f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106e6f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x106e6fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x106e6fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x106e70040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x106e70300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x106e705c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x106e70880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x106e70b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x106e70e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x106e710c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x106e71380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x106e71640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x106e71900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x106e71bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x106e71e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x106e72140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x106e72400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x106e726c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x106e72980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x106e72c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x106e72f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x106e731c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x106e73480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x106e73740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x106e73a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x106e73cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x106e73f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x106e74240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x106e74500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x106e747c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x106e74a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x106e74d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x106e75000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x106e752c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x106e75580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x106e75840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x106e75b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x106e75dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x106e76080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x106e76340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x106e76600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x106e768c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x106e76b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x106e76e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x106e77100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x106e773c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x106e77680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x106e77940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x106e77c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x106e77ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x106e78180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x106e78440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x106e78700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x106e789c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x106e78c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x106e78f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x106e79200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x106e794c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x106e79780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x106e79a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106e79d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x106e79fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106e7a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x106e7a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x106e7a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106e7aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x106e7ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106e7b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106e7b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106e7b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106e7b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106e7bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106e7be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106e7c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106e7c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106e7c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106e7c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106e7cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106e7ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x106e7d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x106e7d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106e7d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x106e7d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106e7dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106e7df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106e7e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106e7e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106e7e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106e7ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106e7ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106e7ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x106e7f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x106e7f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x106e7f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x106e7fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x106e7fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x106e80000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x106e802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x106e80580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x106e80840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x106e80b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x106e80dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x106e81080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x106e81340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x106e81600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x106e818c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x106e81b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x106e81e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x106e82100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x106e823c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106e82680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x106e82940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106e82c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x106e82ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x106e83180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106e83440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106e83700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106e839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106e83c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106e83f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106e84200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106e844c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106e84780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106e84a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106e84d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106e84fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106e85280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x106e85540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106e85800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106e85ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106e85d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106e86040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106e86300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106e865c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106e86880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x106e86b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x106e86e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x106e870c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106e87380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x106e87640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x106e87900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x106e87bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106e87e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x106e88140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x106e885e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x106e88d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x106e89050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x106e89310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x106e89780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x106e89bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x106e8a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x106e8a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x106e8a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x106e8adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x106e8b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x106e8b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x106e8bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x106e8bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x106e8c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x106e8c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x106e8ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106e8d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106e8d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106e8da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106e8de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106e8e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106e8e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106e8ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106e8f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106e8f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106e8f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106e8fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106e90200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106e90670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106e90ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106e90f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x106e913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106e91830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106e91ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x106e92110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106e92580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106e929f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x106e92e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106e932d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106e93740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x106e93bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106e94020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106e94490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x106e94900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106e94d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106e951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x106e95650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x106e95ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x106e95f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x106e963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x106e96810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x106e96c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x106e970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x106e97560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x106e979d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x106e97e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x106e982b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x106e98720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x106e98b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x106e99000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x106e99470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x106e998e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x106e99d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x106e9a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x106e9a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x106e9aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x106e9af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106e9b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106e9b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106e9bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106e9c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x106e9c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106e9c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106e9d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106e9db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106e9e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x106e9e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106e9ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x106e9f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106e9f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106e9fd00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1256044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1256056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1256063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x125606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125606cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125607140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125607860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125608380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125608b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125609340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x125609a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12560a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12560a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12560afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12560b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12560be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12560c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12560cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12560d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12560da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12560dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12560e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12560e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12560e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12560ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12560f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12560f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12560fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12560fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1256102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125610710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125610b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125610ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125611460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1256118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125611d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1256121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125612620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125612a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125612f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125613370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1256137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125613c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1256140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125614530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1256149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125614e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125615280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1256156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125615b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x125615fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125616a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x125616eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125617320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125617790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125617c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125618070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1256184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x125618dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125619230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1256196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x125619b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125619f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12561a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12561a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12561acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12561b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12561b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12561ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12561be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12561c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12561c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12561cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12561d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12561d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12561d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12561dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12561e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12561e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12561eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12561ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12561f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12561f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12561fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125620120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125620590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125620a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125620e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1256212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125621750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125622030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1256224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125622910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125622d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1256231f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125623a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x125623d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1256241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125624620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125624a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x125624f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125625370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1256257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x125625c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1256260c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125626530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1256269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x125626e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125627280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1256276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125627b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125627fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x125628440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1256288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125628d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125629190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125629600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125629a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125629ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12562a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12562a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12562ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12562b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12562b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12562b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12562bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12562c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12562c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12562cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12562cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12562d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12562d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12562dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12562e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12562e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12562ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12562eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12562f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12562f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12562fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125630080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1256304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125630960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125630dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x125631240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1256316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x125631b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x125632400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125632870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125632ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x125633150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1256335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125633a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125633ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x125634310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125634780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125634bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125635060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1256354d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125635940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125635db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x125636220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125636690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x125636b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125636f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1256373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125637850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125637cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125638130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1256385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125638a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125638e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1256392f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125639760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12563a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12563a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12563a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12563ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12563b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12563b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12563bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12563bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12563c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12563c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12563cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12563d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12563d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12563d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12563de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12563e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12563e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12563ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12563f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12563f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12563f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12563fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1256401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125640650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125640ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125640f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125641d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125642030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1256424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125642910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125642d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1256431f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125643660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125643ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x125643f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1256443b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125644820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125644c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125645100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125645570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1256459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125645e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1256462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125646730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125646ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125647010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125647480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1256478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125647d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1256481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125648640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125648ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125648f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125649390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125649800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125649c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12564a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12564a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12564a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12564ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12564b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12564b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12564bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12564bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12564c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12564c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12564cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12564d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12564d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12564da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12564df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12564e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12564e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12564ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12564f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12564f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12564f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12564fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125650280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1256506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x125650b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125650fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125651440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1256518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x125651d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125652190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125652600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125652a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125652ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125653350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1256537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125653c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1256540a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125654510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125654980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125654df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x125655260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1256556d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125656140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125656860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125656f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1256576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125657960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125657dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1256583d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1256589e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.788s
user	0m0.297s
sys	0m0.318s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4397 (48160087)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12380a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12380a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12380ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12380b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12380b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12380bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12380c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12380cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12380d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12380d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12380dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12380dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12380ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12380f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12380fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1238101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1238108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123810ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123811710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123812d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123813440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123813ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123814400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1238146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123814cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123815e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1238165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1238168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123818710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123818bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123819050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1238194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123819990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123819e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12381a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12381a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12381aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12381b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12381bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12381c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12381c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12381cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12381d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12381d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12381df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12381e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12381ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12381f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12381f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12381f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1238203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123820d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1238211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123821b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123822440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1238228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123822d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1238236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123823b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1238240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123824b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1238250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1238255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123825b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123826090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1238265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123827080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1238275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123828070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1238285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123828b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123829060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1238295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123829b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12382a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12382a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12382aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12382b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12382b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12382bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12381b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12382bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12382c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12382cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12382d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12382d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12382dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12382e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12382e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12382ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12382f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12382f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12382fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1238306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123830c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1238310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123831550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1238319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1238327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123833110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1238335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123833a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123834390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123834830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123834cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123835170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123835610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123835f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1238363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123836d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1238371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123837670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123837b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123837fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123838450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1238388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123838d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123839230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1238396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123839b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12383a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12383a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12383a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12383adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12383b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12383b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12383bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12383c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12383c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12383c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12383ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12383d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12383d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12383dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12383e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12383e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12383ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12383eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12383f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12383f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12383fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123840130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1238405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123840a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123840f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1238413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123841cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123842f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1238438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123843d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1238441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123844690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123845470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123845910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1238466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123847030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1238474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123847970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123847e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1238488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123848e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123849610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123849c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12384a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12384a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12384b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12384b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12384b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12384bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12384c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12384cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12384d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12384d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12384d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12384e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12384e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12384ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12384f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12384f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12384fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123850110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123850660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123851100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123851650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1238520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123852640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123852b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1238530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123853630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123853b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1238540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123854620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123854b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1238550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123855610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1238560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123856600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123856b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1238570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1238575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123857b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123858090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1238585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123858b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123859080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1238595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123859b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12385a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12385a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12385ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12385b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12385b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12385bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12385c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12385c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12385caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12385d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12385d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12385dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12385e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12385e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12385ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12385f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12385f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12385fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123860010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123860560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123860ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123860f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1238613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123861890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123861d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1238621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123862670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123862b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123862fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1238638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123863d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123864230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1238646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123865010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123865560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123865c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1238663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123866ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1238671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1238674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123867c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123867f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123868560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.974 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x122705900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x122705d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1227061e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x122706650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x122706ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x122706f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1227073a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x122707810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x122707c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1227080f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x122708560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x122708c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x122709770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x122709f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12270a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12270ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12270b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12270bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12270c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12270cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12270d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12270d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12270e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12270e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12270ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12270f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12270f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12270f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12270fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122710150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1227105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122710af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122710f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122711220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x122711690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122711b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122711f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1227123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122712850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122712cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122713130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1227135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122713a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122713e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1227142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122714760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122714bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122715040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1227154b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x122715920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122715d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x122716200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122716670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x122716ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x122716f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1227173c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x122717930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122717e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1227182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x122718710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x122718b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x122718ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x122719460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1227198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x122719d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12271a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12271a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12271aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12271af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12271b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12271b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12271bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12271c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12271c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12271c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12271ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12271d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12271d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12271db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12271dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12271e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12271e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12271ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12271f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12271f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12271fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12271fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122720350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1227207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122720c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1227210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122721510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x122721980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122721df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122722260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1227226d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122722b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122722fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122723420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x122723890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122723d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x122724170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1227245e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122724a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x122724ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122725330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1227257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x122725c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x122726080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1227264f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122726960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x122726dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x122727240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1227276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x122727b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x122727f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x122728400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x122728870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x122728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x122729150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1227295c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122729a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x122729ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12272a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12272a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12272abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12272b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12272b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12272b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12272bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12272c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12272c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12272cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12272cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12272d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12272d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12272dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12272e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12272e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12272ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12272ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12272f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12272f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12272fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x122730040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1227304b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122730920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122730d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x122731200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122731670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122731ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x122731f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1227323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122732830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122732ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122733110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122733580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1227339f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122733e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1227342d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x122734740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122734bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x122735020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x122735490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x122735900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x122735d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1227369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x122736c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x122736f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x122737390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122737800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x122737c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1227380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x122738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1227389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x122738e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1227392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x122739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x122739b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x122739ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12273a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12273a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12273ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12273b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12273b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12273ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12273bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12273c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12273c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12273cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12273d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12273d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12273d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12273de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12273e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12273e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12273eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12273efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12273f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12273f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12273fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122740190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1227406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x122740c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122741070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1227414e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122741950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122741dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1227422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1227427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122743360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122743620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122743be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1227441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122744760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x122744d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1227452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1227458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122745e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x122746420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1227469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x122746fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x122747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x122747b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1227480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1227486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x122748c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x122749220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1227497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x122749da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12274a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12274a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12274aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12274b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12274ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12274c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12274c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12274cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12274d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12274d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12274dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12274e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12274e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12274ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12274f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12274f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12274ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122750520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122750ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1227510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122751660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122751c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1227521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1227527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122752d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122753320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1227538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122753ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122754460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x122754a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122754fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1227555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122755b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x122756120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1227566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122756ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122757260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122757820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122757d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122758220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122758720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122758c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122759120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122759620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122759b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12275a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12275a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12275aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12275af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12275b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12275b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12275be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12275c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12275cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12275d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12275db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12275e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12275e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12275ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12275f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12275f610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12260a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12260ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12260b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12260b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12260b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12260be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12260c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12260c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12260cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12260cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12260d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12260dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12260e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12260edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12260f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12260fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x122610400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x122610b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x122611240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x122611a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x122612130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x122612850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x122612f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x122613690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x122613db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x122614070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x122614330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1226147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x122614c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x122615080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1226154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x122615a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x122615e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x122616150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1226165c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x122616a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122616ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122617310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x122617780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x122617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122618060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1226184d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x122618940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x122618db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122619220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x122619690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x122619b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x122619f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12261a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12261a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12261acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12261b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12261b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12261ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12261be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12261c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12261c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12261cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12261d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12261d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12261dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12261df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12261e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12261e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12261ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12261f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12261f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12261f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12261fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1226202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x122620710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122620b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x122620ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x122621460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1226218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x122621d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1226221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x122622620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x122622a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x122622f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x122623370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1226237e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x122623c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1226240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x122624530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1226249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122624e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x122625280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1226256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x122625b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x122625fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x122626440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1226268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x122626d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x122627190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x122627600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x122627a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x122627ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x122628350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1226287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x122628c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1226290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x122629510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x122629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12262a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12262a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12262a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12262adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12262b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12262b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12262bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12262bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12262c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12262c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12262ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12262d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12262d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12262da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12262de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12262e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12262e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12262ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12262f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12262f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12262f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12262fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122630200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x122630670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122630ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x122630f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1226313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122631830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x122631ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122632110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x122632580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1226329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x122632e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1226332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x122633740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x122633bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x122634020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122634490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x122634900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x122634d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1226351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x122635650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x122635ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x122635f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1226363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x122636810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x122636c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1226370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x122637560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1226379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x122637e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1226382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x122638720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122638b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x122639000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x122639470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1226398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x122639d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12263a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12263a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12263aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12263af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12263b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12263b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12263bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12263c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12263c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12263c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12263ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12263d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12263d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12263db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12263dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12263e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12263e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12263ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12263f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12263f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12263fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12263fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x122640360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1226407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x122640c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1226410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x122641520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x122641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x122641e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x122642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1226426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x122642b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x122642fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x122643430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1226438a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x122643d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x122644180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1226445f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122644a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122644ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x122645340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1226457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x122645c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122646090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x122646500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x122646970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122646de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x122647250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x122647dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x122648090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x122648350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1226487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x122648c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1226490a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x122649510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x122649980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x122649df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12264a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12264a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12264ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12264afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12264b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12264b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12264bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12264c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12264c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12264ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12264cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12264d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12264d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12264dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12264e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12264e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12264e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12264edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12264f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12264f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12264fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12264ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x122650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122650870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x122650ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x122651150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1226515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x122651a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x122651ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x122652310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x122652780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x122652bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x122653060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1226534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122653940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x122653db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x122654220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x122654690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x122654b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x122654f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1226553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x122655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x122655cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122656130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1226565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x122656a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x122656e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1226572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x122657760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x122657bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122658040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1226584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x122658920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122658d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122659200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x122659670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x122659ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x122659f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12265a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12265a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12265aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12265b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12265b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12265b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12265c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12265cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12265d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12265d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12265dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12265e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12265e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12265ed00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.915s
user	0m0.243s
sys	0m0.133s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.55 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
