+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/48/1600876708c697c875ad5a18684b595dfbadc3/ggml-0-x86-cpu-low-perf/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/48/1600876708c697c875ad5a18684b595dfbadc3/ggml-0-x86-cpu-low-perf/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Configuring done (0.6s)
-- Generating done (0.1s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m0.724s
user	0m0.535s
sys	0m0.192s
+ tee -a /home/ggml/results/llama.cpp/48/1600876708c697c875ad5a18684b595dfbadc3/ggml-0-x86-cpu-low-perf/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Built target sha256
[  2%] Built target xxhash
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Built target sha1
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  6%] Linking CXX shared library libggml-base.so
[  7%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  7%] Built target build_info
[  7%] Built target ggml-base
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 13%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 13%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 16%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Built target llama-gguf-hash
[ 16%] Built target llama-gguf
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
/home/ggml/work/llama.cpp/src/llama-adapter.cpp: In function ‘void llama_lora_adapter_init_impl(llama_model&, const char*, llama_lora_adapter&)’:
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:162:20: error: ‘runtime_error’ is not a member of ‘std’
  162 |         throw std::runtime_error("failed to load lora adapter file from " + std::string(path_lora));
      |                    ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:181:24: error: ‘runtime_error’ is not a member of ‘std’
  181 |             throw std::runtime_error("expect general.type to be 'adapter', but got: " + general_type);
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:187:24: error: ‘runtime_error’ is not a member of ‘std’
  187 |             throw std::runtime_error("model arch and LoRA arch mismatch");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:192:24: error: ‘runtime_error’ is not a member of ‘std’
  192 |             throw std::runtime_error("expect adapter.type to be 'lora', but got: " + adapter_type);
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:245:24: error: ‘runtime_error’ is not a member of ‘std’
  245 |             throw std::runtime_error("LoRA tensor '" + name + "' has unexpected suffix");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:255:24: error: ‘runtime_error’ is not a member of ‘std’
  255 |             throw std::runtime_error("LoRA tensor pair for '" + name + "' is missing one component");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:261:24: error: ‘runtime_error’ is not a member of ‘std’
  261 |             throw std::runtime_error("LoRA tensor '" + name + "' does not exist in base model");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:267:24: error: ‘runtime_error’ is not a member of ‘std’
  267 |             throw std::runtime_error("tensor '" + name + "' has incorrect shape");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:270:24: error: ‘runtime_error’ is not a member of ‘std’
  270 |             throw std::runtime_error("lora_a tensor is not transposed (hint: adapter from \"finetune\" example is no longer supported)");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:290:28: error: ‘runtime_error’ is not a member of ‘std’
  290 |                 throw std::runtime_error("failed to allocate buffer for lora adapter\n");
      |                            ^~~~~~~~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:90: src/CMakeFiles/llama.dir/llama-adapter.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_model_info(const llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:270:24: error: ‘runtime_error’ is not a member of ‘std’
  270 |             throw std::runtime_error(format("wrong model arch: '%s' instead of '%s'", arch_str.c_str(), cur_arch_str.c_str()));
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_output_ids(llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:294:24: error: ‘runtime_error’ is not a member of ‘std’
  294 |             throw std::runtime_error("could not reserve outputs");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp:304:32: error: ‘runtime_error’ is not a member of ‘std’
  304 |                     throw std::runtime_error(format("invalid output id, %d does not fit in batch size of %u", id, ctx->cparams.n_batch));
      |                                ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_logits(llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:318:24: error: ‘runtime_error’ is not a member of ‘std’
  318 |             throw std::runtime_error("logits buffer too small");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_embeddings(llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:331:24: error: ‘runtime_error’ is not a member of ‘std’
  331 |             throw std::runtime_error("embeddings buffer too small");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_kv_cache(llama_context*, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:569:24: error: ‘runtime_error’ is not a member of ‘std’
  569 |             throw std::runtime_error("failed to restore kv cache");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_data_write_buffer::write(const void*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:601:24: error: ‘runtime_error’ is not a member of ‘std’
  601 |             throw std::runtime_error("unexpectedly reached end of buffer");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_data_write_buffer::write_tensor_data(const ggml_tensor*, size_t, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:611:24: error: ‘runtime_error’ is not a member of ‘std’
  611 |             throw std::runtime_error("unexpectedly reached end of buffer");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual const uint8_t* llama_data_read_buffer::read(size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:634:24: error: ‘runtime_error’ is not a member of ‘std’
  634 |             throw std::runtime_error("unexpectedly reached end of buffer");
      |                        ^~~~~~~~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m7.051s
user	0m9.945s
sys	0m0.963s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/48/1600876708c697c875ad5a18684b595dfbadc3/ggml-0-x86-cpu-low-perf/ctest_debug-ctest.log: No such file or directory
