+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=ares+crypto+noprofile+dotprod+noi8mm+nosve 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m2.049s
user	0m1.351s
sys	0m0.497s
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Built target sha256
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Built target xxhash
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha1
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Linking CXX shared library libggml-base.so
[  7%] Built target build_info
[  7%] Built target ggml-base
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 13%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 16%] Built target llama-gguf
[ 16%] Built target llama-gguf-hash
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
/home/ggml/work/llama.cpp/src/llama-adapter.cpp: In function ‘void llama_lora_adapter_init_impl(llama_model&, const char*, llama_lora_adapter&)’:
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:162:20: error: ‘runtime_error’ is not a member of ‘std’
  162 |         throw std::runtime_error("failed to load lora adapter file from " + std::string(path_lora));
      |                    ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:181:24: error: ‘runtime_error’ is not a member of ‘std’
  181 |             throw std::runtime_error("expect general.type to be 'adapter', but got: " + general_type);
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:187:24: error: ‘runtime_error’ is not a member of ‘std’
  187 |             throw std::runtime_error("model arch and LoRA arch mismatch");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:192:24: error: ‘runtime_error’ is not a member of ‘std’
  192 |             throw std::runtime_error("expect adapter.type to be 'lora', but got: " + adapter_type);
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:245:24: error: ‘runtime_error’ is not a member of ‘std’
  245 |             throw std::runtime_error("LoRA tensor '" + name + "' has unexpected suffix");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:255:24: error: ‘runtime_error’ is not a member of ‘std’
  255 |             throw std::runtime_error("LoRA tensor pair for '" + name + "' is missing one component");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:261:24: error: ‘runtime_error’ is not a member of ‘std’
  261 |             throw std::runtime_error("LoRA tensor '" + name + "' does not exist in base model");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:267:24: error: ‘runtime_error’ is not a member of ‘std’
  267 |             throw std::runtime_error("tensor '" + name + "' has incorrect shape");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:270:24: error: ‘runtime_error’ is not a member of ‘std’
  270 |             throw std::runtime_error("lora_a tensor is not transposed (hint: adapter from \"finetune\" example is no longer supported)");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-adapter.cpp:290:28: error: ‘runtime_error’ is not a member of ‘std’
  290 |                 throw std::runtime_error("failed to allocate buffer for lora adapter\n");
      |                            ^~~~~~~~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:90: src/CMakeFiles/llama.dir/llama-adapter.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_model_info(const llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:270:24: error: ‘runtime_error’ is not a member of ‘std’
  270 |             throw std::runtime_error(format("wrong model arch: '%s' instead of '%s'", arch_str.c_str(), cur_arch_str.c_str()));
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_output_ids(llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:294:24: error: ‘runtime_error’ is not a member of ‘std’
  294 |             throw std::runtime_error("could not reserve outputs");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp:304:32: error: ‘runtime_error’ is not a member of ‘std’
  304 |                     throw std::runtime_error(format("invalid output id, %d does not fit in batch size of %u", id, ctx->cparams.n_batch));
      |                                ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_logits(llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:318:24: error: ‘runtime_error’ is not a member of ‘std’
  318 |             throw std::runtime_error("logits buffer too small");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_embeddings(llama_context*)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:331:24: error: ‘runtime_error’ is not a member of ‘std’
  331 |             throw std::runtime_error("embeddings buffer too small");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘void llama_data_read::read_kv_cache(llama_context*, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:569:24: error: ‘runtime_error’ is not a member of ‘std’
  569 |             throw std::runtime_error("failed to restore kv cache");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_data_write_buffer::write(const void*, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:601:24: error: ‘runtime_error’ is not a member of ‘std’
  601 |             throw std::runtime_error("unexpectedly reached end of buffer");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual void llama_data_write_buffer::write_tensor_data(const ggml_tensor*, size_t, size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:611:24: error: ‘runtime_error’ is not a member of ‘std’
  611 |             throw std::runtime_error("unexpectedly reached end of buffer");
      |                        ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual const uint8_t* llama_data_read_buffer::read(size_t)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:634:24: error: ‘runtime_error’ is not a member of ‘std’
  634 |             throw std::runtime_error("unexpectedly reached end of buffer");
      |                        ^~~~~~~~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m9.789s
user	0m12.322s
sys	0m1.453s
