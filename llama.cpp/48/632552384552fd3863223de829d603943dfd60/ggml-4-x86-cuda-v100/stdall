Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.45.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.16.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.2.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2023.12.25)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (23.2)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.2)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.66.2)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.13.1)
Requirement already satisfied: tokenizers<0.21,>=0.20 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.20.0)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.23.5)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (11.4.5.107)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (11.0.2.54)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.0.106)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: triton==2.2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.0)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.3)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.2.1)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (10.3.2.106)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.12)
Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.19.3)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (8.9.2.26)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.2.0)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.1.3.1)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (12.3.101)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.1.5)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.3.2)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.2.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.6)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.1)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (1.26.4)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (4.66.2)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (6.0.1)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.16.0) (0.2.0)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.16.0-py3-none-any.whl size=3462 sha256=21a785bbda7f70574e9e564518770340ba410d02065476143fe78d1ebce56fe3
  Stored in directory: /tmp/pip-ephem-wheel-cache-av2l662z/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.16.0
    Uninstalling gguf-0.16.0:
      Successfully uninstalled gguf-0.16.0
Successfully installed gguf-0.16.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ tee /home/ggml/results/llama.cpp/48/632552384552fd3863223de829d603943dfd60/ggml-4-x86-cuda-v100/ctest_debug.log
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/48/632552384552fd3863223de829d603943dfd60/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=native ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- CUDA Toolkit found
-- Using CUDA architectures: native
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- CUDA host compiler is GNU 11.4.0

-- Including CUDA backend
-- Configuring done (9.0s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m9.275s
user	0m7.397s
sys	0m1.840s
+ tee -a /home/ggml/results/llama.cpp/48/632552384552fd3863223de829d603943dfd60/ggml-4-x86-cuda-v100/ctest_debug-make.log
++ nproc
+ make -j6
[  0%] Generating build details from Git
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  2%] Built target sha256
[  2%] Built target sha1
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Built target xxhash
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Linking CXX shared library ../../bin/libggml-base.so
[  4%] Built target build_info
[  4%] Built target ggml-base
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[  7%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o
[  8%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o
[  8%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o
[  8%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o
[  9%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.so
[ 10%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o
[ 10%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o
[ 11%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o
[ 11%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o
[ 11%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o
[ 12%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o
[ 12%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o
[ 12%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o
[ 13%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o
[ 13%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o
[ 14%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o
[ 14%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o
[ 14%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o
[ 15%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o
[ 15%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o
[ 15%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o
[ 16%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o
[ 16%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o
[ 16%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o
[ 17%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o
[ 17%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o
[ 17%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o
[ 18%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o
[ 18%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o
[ 18%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o
[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o
[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o
[ 19%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o
[ 20%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o
[ 20%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o
[ 20%] Built target ggml-cpu
[ 20%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o
[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o
[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv6.cu.o
[ 21%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o
[ 22%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o
[ 22%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o
[ 22%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o
[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o
[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o
[ 23%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o
[ 24%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o
[ 24%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o
[ 24%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o
[ 25%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o
[ 25%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o
[ 25%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o
[ 26%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o
[ 26%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o
[ 26%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o
[ 27%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o
[ 27%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o
[ 27%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o
[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o
[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o
[ 28%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o
[ 29%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o
[ 29%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o
[ 29%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o
[ 30%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o
[ 30%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o
[ 31%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o
[ 31%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o
[ 31%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o
[ 32%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o
[ 32%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o
[ 32%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o
[ 33%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o
[ 33%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o
[ 33%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o
[ 34%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o
[ 34%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o
[ 34%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o
[ 35%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o
[ 35%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o
[ 35%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o
[ 36%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o
[ 36%] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o
[ 36%] Linking CUDA shared library ../../../bin/libggml-cuda.so
[ 36%] Built target ggml-cuda
[ 36%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 37%] Linking CXX shared library ../../bin/libggml.so
[ 37%] Built target ggml
[ 37%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 38%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 39%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 39%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 39%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 40%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 41%] Linking CXX executable ../../bin/llama-gguf
[ 41%] Linking CXX executable ../../bin/llama-gguf-hash
[ 41%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 41%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 42%] Built target llama-gguf-hash
[ 42%] Built target llama-gguf
[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 43%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 43%] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o
[ 43%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 44%] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o
[ 44%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 44%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 45%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-adapter.cpp:5:
/home/ggml/work/llama.cpp/src/llama-graph.h:347:27: error: ‘function’ in namespace ‘std’ does not name a template type
  347 | using llm_graph_cb = std::function<void(ggml_tensor * cur, const char * name, int il)>;
      |                           ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:11:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
   10 | #include <set>
  +++ |+#include <functional>
   11 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:368:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  368 |     const llm_graph_cb & cb;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-graph.h:419:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  419 |     const llm_graph_cb & cb_func;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
make[2]: *** [src/CMakeFiles/llama.dir/build.make:90: src/CMakeFiles/llama.dir/llama-adapter.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama.cpp:7:
/home/ggml/work/llama.cpp/src/llama-graph.h:347:27: error: ‘function’ in namespace ‘std’ does not name a template type
  347 | using llm_graph_cb = std::function<void(ggml_tensor * cur, const char * name, int il)>;
      |                           ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:11:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
   10 | #include <set>
  +++ |+#include <functional>
   11 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:368:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  368 |     const llm_graph_cb & cb;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-graph.h:419:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  419 |     const llm_graph_cb & cb_func;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
make[2]: *** [src/CMakeFiles/llama.dir/build.make:76: src/CMakeFiles/llama.dir/llama.cpp.o] Error 1
In file included from /home/ggml/work/llama.cpp/src/llama-graph.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:347:27: error: ‘function’ in namespace ‘std’ does not name a template type
  347 | using llm_graph_cb = std::function<void(ggml_tensor * cur, const char * name, int il)>;
      |                           ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:11:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
   10 | #include <set>
  +++ |+#include <functional>
   11 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:368:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  368 |     const llm_graph_cb & cb;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-graph.h:419:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  419 |     const llm_graph_cb & cb_func;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In constructor ‘llm_graph_context::llm_graph_context(const llm_graph_params&)’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:618:5: error: class ‘llm_graph_context’ does not have any field named ‘cb_func’
  618 |     cb_func          (params.cb),
      |     ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.cpp:618:30: error: ‘const struct llm_graph_params’ has no member named ‘cb’
  618 |     cb_func          (params.cb),
      |                              ^~
/home/ggml/work/llama.cpp/src/llama-graph.cpp: In member function ‘void llm_graph_context::cb(ggml_tensor*, const char*, int) const’:
/home/ggml/work/llama.cpp/src/llama-graph.cpp:627:9: error: ‘cb_func’ was not declared in this scope
  627 |     if (cb_func) {
      |         ^~~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:174: src/CMakeFiles/llama.dir/llama-graph.cpp.o] Error 1
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:347:27: error: ‘function’ in namespace ‘std’ does not name a template type
  347 | using llm_graph_cb = std::function<void(ggml_tensor * cur, const char * name, int il)>;
      |                           ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:11:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
   10 | #include <set>
  +++ |+#include <functional>
   11 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:368:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  368 |     const llm_graph_cb & cb;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-graph.h:419:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  419 |     const llm_graph_cb & cb_func;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual llm_graph_result_ptr llama_context_base::graph_build(ggml_context*, ggml_cgraph*, const llama_ubatch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1131:29: error: cannot convert ‘<brace-enclosed initializer list>’ to ‘const llm_graph_params&’
 1131 |     return model.build_graph(
      |            ~~~~~~~~~~~~~~~~~^
 1132 |             {
      |             ~                
 1133 |                 /*.ctx         =*/ ctx,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~
 1134 |                 /*.arch        =*/ model.arch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1135 |                 /*.hparams     =*/ model.hparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1136 |                 /*.cparams     =*/ cparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1137 |                 /*.ubatch      =*/ ubatch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 1138 |                 /*.sched       =*/ sched.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1139 |                 /*.backend_cpu =*/ backend_cpu,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1140 |                 /*.cvec        =*/ &cvec,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~
 1141 |                 /*.loras       =*/ &loras,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 1142 |                 /*.memory      =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1143 |                 /*.cross       =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1144 |                 /*.n_outputs   =*/ n_outputs,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1145 |                 /*.cb          =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 1146 |             }, gf, gtype);
      |             ~~~~~~~~~~~~~    
In file included from /home/ggml/work/llama.cpp/src/llama-context.cpp:6:
/home/ggml/work/llama.cpp/src/llama-model.h:373:38: note:   initializing argument 1 of ‘llm_graph_result_ptr llama_model::build_graph(const llm_graph_params&, ggml_cgraph*, llm_graph_type) const’
  373 |             const llm_graph_params & params,
      |             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual llm_graph_result_ptr llama_context_kv_self::graph_build(ggml_context*, ggml_cgraph*, const llama_ubatch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:2568:29: error: cannot convert ‘<brace-enclosed initializer list>’ to ‘const llm_graph_params&’
 2568 |     return model.build_graph(
      |            ~~~~~~~~~~~~~~~~~^
 2569 |             {
      |             ~                
 2570 |                 /*.ctx         =*/ ctx,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~
 2571 |                 /*.arch        =*/ model.arch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2572 |                 /*.hparams     =*/ model.hparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2573 |                 /*.cparams     =*/ cparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2574 |                 /*.ubatch      =*/ ubatch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 2575 |                 /*.sched       =*/ sched.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2576 |                 /*.backend_cpu =*/ backend_cpu,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2577 |                 /*.cvec        =*/ &cvec,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~
 2578 |                 /*.loras       =*/ &loras,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 2579 |                 /*.memory      =*/ kv_self.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2580 |                 /*.cross       =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2581 |                 /*.n_outputs   =*/ n_outputs,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2582 |                 /*.cb          =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 2583 |             }, gf, gtype);
      |             ~~~~~~~~~~~~~    
In file included from /home/ggml/work/llama.cpp/src/llama-context.cpp:6:
/home/ggml/work/llama.cpp/src/llama-model.h:373:38: note:   initializing argument 1 of ‘llm_graph_result_ptr llama_model::build_graph(const llm_graph_params&, ggml_cgraph*, llm_graph_type) const’
  373 |             const llm_graph_params & params,
      |             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual llm_graph_result_ptr llama_context_recurrent::graph_build(ggml_context*, ggml_cgraph*, const llama_ubatch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:3012:29: error: cannot convert ‘<brace-enclosed initializer list>’ to ‘const llm_graph_params&’
 3012 |     return model.build_graph(
      |            ~~~~~~~~~~~~~~~~~^
 3013 |             {
      |             ~                
 3014 |                 /*.ctx         =*/ ctx,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~
 3015 |                 /*.arch        =*/ model.arch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3016 |                 /*.hparams     =*/ model.hparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3017 |                 /*.cparams     =*/ cparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3018 |                 /*.ubatch      =*/ ubatch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 3019 |                 /*.sched       =*/ sched.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3020 |                 /*.backend_cpu =*/ backend_cpu,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3021 |                 /*.cvec        =*/ &cvec,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~
 3022 |                 /*.loras       =*/ &loras,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 3023 |                 /*.memory      =*/ kv_self.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3024 |                 /*.cross       =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3025 |                 /*.n_outputs   =*/ n_outputs,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3026 |                 /*.cb          =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3027 |             }, gf, gtype);
      |             ~~~~~~~~~~~~~    
In file included from /home/ggml/work/llama.cpp/src/llama-context.cpp:6:
/home/ggml/work/llama.cpp/src/llama-model.h:373:38: note:   initializing argument 1 of ‘llm_graph_result_ptr llama_model::build_graph(const llm_graph_params&, ggml_cgraph*, llm_graph_type) const’
  373 |             const llm_graph_params & params,
      |             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In member function ‘virtual llm_graph_result_ptr llama_context_dec::graph_build(ggml_context*, ggml_cgraph*, const llama_ubatch&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:3230:29: error: cannot convert ‘<brace-enclosed initializer list>’ to ‘const llm_graph_params&’
 3230 |     return model.build_graph(
      |            ~~~~~~~~~~~~~~~~~^
 3231 |             {
      |             ~                
 3232 |                 /*.ctx         =*/ ctx,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~
 3233 |                 /*.arch        =*/ model.arch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3234 |                 /*.hparams     =*/ model.hparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3235 |                 /*.cparams     =*/ cparams,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3236 |                 /*.ubatch      =*/ ubatch,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 3237 |                 /*.sched       =*/ sched.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3238 |                 /*.backend_cpu =*/ backend_cpu,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3239 |                 /*.cvec        =*/ &cvec,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~
 3240 |                 /*.loras       =*/ &loras,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
 3241 |                 /*.memory      =*/ kv_self.get(),
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3242 |                 /*.cross       =*/ cross,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~
 3243 |                 /*.n_outputs   =*/ n_outputs,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3244 |                 /*.cb          =*/ nullptr,
      |                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~
 3245 |             }, gf, gtype);
      |             ~~~~~~~~~~~~~    
In file included from /home/ggml/work/llama.cpp/src/llama-context.cpp:6:
/home/ggml/work/llama.cpp/src/llama-model.h:373:38: note:   initializing argument 1 of ‘llm_graph_result_ptr llama_model::build_graph(const llm_graph_params&, ggml_cgraph*, llm_graph_type) const’
  373 |             const llm_graph_params & params,
      |             ~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
In file included from /home/ggml/work/llama.cpp/src/llama-model.h:6,
                 from /home/ggml/work/llama.cpp/src/llama-model.cpp:1:
/home/ggml/work/llama.cpp/src/llama-graph.h:347:27: error: ‘function’ in namespace ‘std’ does not name a template type
  347 | using llm_graph_cb = std::function<void(ggml_tensor * cur, const char * name, int il)>;
      |                           ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-graph.h:11:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
   10 | #include <set>
  +++ |+#include <functional>
   11 | 
/home/ggml/work/llama.cpp/src/llama-graph.h:368:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  368 |     const llm_graph_cb & cb;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
/home/ggml/work/llama.cpp/src/llama-graph.h:419:11: error: ‘llm_graph_cb’ does not name a type; did you mean ‘llm_graph_type’?
  419 |     const llm_graph_cb & cb_func;
      |           ^~~~~~~~~~~~
      |           llm_graph_type
make[2]: *** [src/CMakeFiles/llama.dir/build.make:286: src/CMakeFiles/llama.dir/llama-model.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1817: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m3.988s
user	0m9.329s
sys	0m1.992s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/48/632552384552fd3863223de829d603943dfd60/ggml-4-x86-cuda-v100/ctest_debug-ctest.log: No such file or directory
