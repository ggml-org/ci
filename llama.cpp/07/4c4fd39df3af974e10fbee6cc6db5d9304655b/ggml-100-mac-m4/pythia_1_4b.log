Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.106s
user	0m1.041s
sys	0m1.508s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-cpu
[ 12%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 34%] Built target test-c
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target llama-simple
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-sampling
[ 45%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-chat
[ 48%] Built target test-sampling
[ 48%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-log
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-chat-template
[ 59%] Linking CXX executable ../bin/test-gguf
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-gguf
[ 63%] Built target test-arg-parser
[ 63%] Built target test-chat-template
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-embedding
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-infill
[ 73%] Built target llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-cli
[ 81%] Built target llama-passkey
[ 81%] Built target llama-parallel
[ 81%] Built target llama-quantize
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-lookup-stats
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-gen-docs
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-speculative
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-tts
[ 92%] Built target llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-q8dot
[ 99%] Built target llama-vdot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.287s
user	0m6.522s
sys	0m10.477s

main: quantize time =  3492.92 ms
main:    total time =  3492.92 ms

main: quantize time =  1940.33 ms
main:    total time =  1940.33 ms

main: quantize time =  2394.38 ms
main:    total time =  2394.38 ms

main: quantize time =  2089.04 ms
main:    total time =  2089.04 ms

main: quantize time =  2991.51 ms
main:    total time =  2991.51 ms

main: quantize time =  5226.13 ms
main:    total time =  5226.13 ms

main: quantize time =  5713.14 ms
main:    total time =  5713.14 ms

main: quantize time =  7027.28 ms
main:    total time =  7027.28 ms

main: quantize time =  6065.92 ms
main:    total time =  6065.92 ms

main: quantize time =  4825.57 ms
main:    total time =  4825.57 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.152 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.328 I main: llama backend init
0.00.000.335 I main: load the model and apply lora adapter, if any
0.00.082.780 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.095.418 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.095.433 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.095.437 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.095.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.095.439 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.095.439 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.095.440 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.095.442 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.095.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.095.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.095.450 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.095.451 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.095.451 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.095.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.095.457 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.095.457 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.095.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.102.310 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.104.445 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.111.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.111.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.111.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.111.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.111.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.111.190 I llama_model_loader: - type  f32:  194 tensors
0.00.111.190 I llama_model_loader: - type  f16:   98 tensors
0.00.111.192 I print_info: file format = GGUF V3 (latest)
0.00.111.193 I print_info: file type   = all F32 (guessed)
0.00.111.197 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.129.471 I load: special tokens cache size = 25
0.00.140.527 I load: token to piece cache size = 0.2984 MB
0.00.140.560 I print_info: arch             = gptneox
0.00.140.561 I print_info: vocab_only       = 0
0.00.140.561 I print_info: n_ctx_train      = 2048
0.00.140.561 I print_info: n_embd           = 2048
0.00.140.562 I print_info: n_layer          = 24
0.00.140.572 I print_info: n_head           = 16
0.00.140.573 I print_info: n_head_kv        = 16
0.00.140.574 I print_info: n_rot            = 32
0.00.140.574 I print_info: n_swa            = 0
0.00.140.574 I print_info: n_embd_head_k    = 128
0.00.140.577 I print_info: n_embd_head_v    = 128
0.00.140.578 I print_info: n_gqa            = 1
0.00.140.579 I print_info: n_embd_k_gqa     = 2048
0.00.140.582 I print_info: n_embd_v_gqa     = 2048
0.00.140.583 I print_info: f_norm_eps       = 1.0e-05
0.00.140.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.140.586 I print_info: f_clamp_kqv      = 0.0e+00
0.00.140.586 I print_info: f_max_alibi_bias = 0.0e+00
0.00.140.586 I print_info: f_logit_scale    = 0.0e+00
0.00.140.587 I print_info: n_ff             = 8192
0.00.140.587 I print_info: n_expert         = 0
0.00.140.587 I print_info: n_expert_used    = 0
0.00.140.588 I print_info: causal attn      = 1
0.00.140.588 I print_info: pooling type     = 0
0.00.140.588 I print_info: rope type        = 2
0.00.140.588 I print_info: rope scaling     = linear
0.00.140.589 I print_info: freq_base_train  = 10000.0
0.00.140.594 I print_info: freq_scale_train = 1
0.00.140.594 I print_info: n_ctx_orig_yarn  = 2048
0.00.140.595 I print_info: rope_finetuned   = unknown
0.00.140.595 I print_info: ssm_d_conv       = 0
0.00.140.595 I print_info: ssm_d_inner      = 0
0.00.140.595 I print_info: ssm_d_state      = 0
0.00.140.595 I print_info: ssm_dt_rank      = 0
0.00.140.596 I print_info: ssm_dt_b_c_rms   = 0
0.00.140.597 I print_info: model type       = 1.4B
0.00.140.597 I print_info: model params     = 1.41 B
0.00.140.598 I print_info: general.name     = 1.4B
0.00.140.598 I print_info: vocab type       = BPE
0.00.140.599 I print_info: n_vocab          = 50304
0.00.140.599 I print_info: n_merges         = 50009
0.00.140.599 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.140.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.140.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.140.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.140.600 I print_info: LF token         = 187 'Ċ'
0.00.140.601 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.140.602 I print_info: max token length = 1024
0.00.140.603 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.210.313 I load_tensors: offloading 24 repeating layers to GPU
0.00.210.316 I load_tensors: offloading output layer to GPU
0.00.210.316 I load_tensors: offloaded 25/25 layers to GPU
0.00.210.343 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.210.344 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.211.010 I llama_init_from_model: n_seq_max     = 1
0.00.211.012 I llama_init_from_model: n_ctx         = 2048
0.00.211.012 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.211.012 I llama_init_from_model: n_batch       = 2048
0.00.211.012 I llama_init_from_model: n_ubatch      = 512
0.00.211.012 I llama_init_from_model: flash_attn    = 0
0.00.211.013 I llama_init_from_model: freq_base     = 10000.0
0.00.211.013 I llama_init_from_model: freq_scale    = 1
0.00.211.014 I ggml_metal_init: allocating
0.00.211.057 I ggml_metal_init: found device: Apple M4
0.00.211.063 I ggml_metal_init: picking default device: Apple M4
0.00.211.754 I ggml_metal_init: using embedded metal library
0.00.233.130 I ggml_metal_init: GPU name:   Apple M4
0.00.233.132 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.233.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.233.133 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.233.133 I ggml_metal_init: simdgroup reduction   = true
0.00.233.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.233.134 I ggml_metal_init: has residency sets    = true
0.00.233.134 I ggml_metal_init: has bfloat            = true
0.00.233.134 I ggml_metal_init: use bfloat            = true
0.00.233.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.233.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.300.391 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.330.756 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.330.762 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.330.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.335.598 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.335.600 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.335.600 I llama_init_from_model: graph nodes  = 967
0.00.335.601 I llama_init_from_model: graph splits = 2
0.00.335.607 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.335.736 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.335.737 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.401.459 I main: llama threadpool init, n_threads = 4
0.00.401.504 I 
0.00.401.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.401.536 I 
0.00.401.720 I sampler seed: 1234
0.00.401.725 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.401.759 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.401.760 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.401.760 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.250.577 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.02.250.578 I llama_perf_context_print:        load time =     317.73 ms
0.02.250.578 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.43 tokens per second)
0.02.250.579 I llama_perf_context_print:        eval time =    1802.31 ms /    63 runs   (   28.61 ms per token,    34.96 tokens per second)
0.02.250.580 I llama_perf_context_print:       total time =    1850.06 ms /    70 tokens
0.02.250.808 I ggml_metal_free: deallocating

real	0m2.543s
user	0m0.136s
sys	0m0.158s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.742 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.877 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.877 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.877 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.878 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.878 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.879 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.880 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.682 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.678 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.540 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.541 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.542 I llama_model_loader: - type  f32:  194 tensors
0.00.027.542 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.543 I print_info: file format = GGUF V3 (latest)
0.00.027.543 I print_info: file type   = Q8_0
0.00.027.544 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.731 I load: special tokens cache size = 25
0.00.042.190 I load: token to piece cache size = 0.2984 MB
0.00.042.209 I print_info: arch             = gptneox
0.00.042.210 I print_info: vocab_only       = 0
0.00.042.210 I print_info: n_ctx_train      = 2048
0.00.042.210 I print_info: n_embd           = 2048
0.00.042.210 I print_info: n_layer          = 24
0.00.042.215 I print_info: n_head           = 16
0.00.042.216 I print_info: n_head_kv        = 16
0.00.042.216 I print_info: n_rot            = 32
0.00.042.216 I print_info: n_swa            = 0
0.00.042.217 I print_info: n_embd_head_k    = 128
0.00.042.217 I print_info: n_embd_head_v    = 128
0.00.042.217 I print_info: n_gqa            = 1
0.00.042.220 I print_info: n_embd_k_gqa     = 2048
0.00.042.221 I print_info: n_embd_v_gqa     = 2048
0.00.042.222 I print_info: f_norm_eps       = 1.0e-05
0.00.042.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.223 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.223 I print_info: f_logit_scale    = 0.0e+00
0.00.042.224 I print_info: n_ff             = 8192
0.00.042.224 I print_info: n_expert         = 0
0.00.042.224 I print_info: n_expert_used    = 0
0.00.042.224 I print_info: causal attn      = 1
0.00.042.224 I print_info: pooling type     = 0
0.00.042.225 I print_info: rope type        = 2
0.00.042.225 I print_info: rope scaling     = linear
0.00.042.225 I print_info: freq_base_train  = 10000.0
0.00.042.226 I print_info: freq_scale_train = 1
0.00.042.226 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.229 I print_info: rope_finetuned   = unknown
0.00.042.229 I print_info: ssm_d_conv       = 0
0.00.042.229 I print_info: ssm_d_inner      = 0
0.00.042.229 I print_info: ssm_d_state      = 0
0.00.042.229 I print_info: ssm_dt_rank      = 0
0.00.042.229 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.230 I print_info: model type       = 1.4B
0.00.042.230 I print_info: model params     = 1.41 B
0.00.042.230 I print_info: general.name     = 1.4B
0.00.042.231 I print_info: vocab type       = BPE
0.00.042.231 I print_info: n_vocab          = 50304
0.00.042.231 I print_info: n_merges         = 50009
0.00.042.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.231 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.232 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.232 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.232 I print_info: LF token         = 187 'Ċ'
0.00.042.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.232 I print_info: max token length = 1024
0.00.042.233 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.019.102 I load_tensors: offloading 24 repeating layers to GPU
0.01.019.108 I load_tensors: offloading output layer to GPU
0.01.019.109 I load_tensors: offloaded 25/25 layers to GPU
0.01.019.133 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.019.134 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.019.954 I llama_init_from_model: n_seq_max     = 1
0.01.019.956 I llama_init_from_model: n_ctx         = 2048
0.01.019.956 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.019.957 I llama_init_from_model: n_batch       = 2048
0.01.019.957 I llama_init_from_model: n_ubatch      = 512
0.01.019.957 I llama_init_from_model: flash_attn    = 0
0.01.019.958 I llama_init_from_model: freq_base     = 10000.0
0.01.019.959 I llama_init_from_model: freq_scale    = 1
0.01.019.959 I ggml_metal_init: allocating
0.01.019.973 I ggml_metal_init: found device: Apple M4
0.01.019.981 I ggml_metal_init: picking default device: Apple M4
0.01.021.241 I ggml_metal_init: using embedded metal library
0.01.026.806 I ggml_metal_init: GPU name:   Apple M4
0.01.026.809 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.026.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.026.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.026.810 I ggml_metal_init: simdgroup reduction   = true
0.01.026.810 I ggml_metal_init: simdgroup matrix mul. = true
0.01.026.811 I ggml_metal_init: has residency sets    = true
0.01.026.811 I ggml_metal_init: has bfloat            = true
0.01.026.811 I ggml_metal_init: use bfloat            = true
0.01.026.812 I ggml_metal_init: hasUnifiedMemory      = true
0.01.026.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.045.692 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.097.890 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.097.898 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.097.920 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.102.279 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.102.281 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.102.281 I llama_init_from_model: graph nodes  = 967
0.01.102.281 I llama_init_from_model: graph splits = 2
0.01.102.287 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.102.416 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.102.417 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.158.527 I main: llama threadpool init, n_threads = 4
0.01.158.569 I 
0.01.158.591 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.158.591 I 
0.01.158.745 I sampler seed: 1234
0.01.158.750 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.158.794 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.158.797 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.158.798 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.252.933 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.02.252.934 I llama_perf_context_print:        load time =    1148.07 ms
0.02.252.935 I llama_perf_context_print: prompt eval time =      49.11 ms /     7 tokens (    7.02 ms per token,   142.55 tokens per second)
0.02.252.936 I llama_perf_context_print:        eval time =    1042.07 ms /    63 runs   (   16.54 ms per token,    60.46 tokens per second)
0.02.252.936 I llama_perf_context_print:       total time =    1095.12 ms /    70 tokens
0.02.253.166 I ggml_metal_free: deallocating

real	0m2.271s
user	0m0.109s
sys	0m0.270s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.013.896 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.812 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.818 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.825 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.826 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.827 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.827 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.828 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.830 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.830 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.832 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.832 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.732 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.735 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.530 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.531 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.532 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.532 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.533 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.533 I llama_model_loader: - type  f32:  194 tensors
0.00.032.534 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.534 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.535 I print_info: file format = GGUF V3 (latest)
0.00.032.535 I print_info: file type   = Q4_0
0.00.032.536 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.040.664 I load: special tokens cache size = 25
0.00.047.234 I load: token to piece cache size = 0.2984 MB
0.00.047.244 I print_info: arch             = gptneox
0.00.047.245 I print_info: vocab_only       = 0
0.00.047.245 I print_info: n_ctx_train      = 2048
0.00.047.245 I print_info: n_embd           = 2048
0.00.047.245 I print_info: n_layer          = 24
0.00.047.250 I print_info: n_head           = 16
0.00.047.251 I print_info: n_head_kv        = 16
0.00.047.251 I print_info: n_rot            = 32
0.00.047.253 I print_info: n_swa            = 0
0.00.047.253 I print_info: n_embd_head_k    = 128
0.00.047.253 I print_info: n_embd_head_v    = 128
0.00.047.254 I print_info: n_gqa            = 1
0.00.047.255 I print_info: n_embd_k_gqa     = 2048
0.00.047.255 I print_info: n_embd_v_gqa     = 2048
0.00.047.256 I print_info: f_norm_eps       = 1.0e-05
0.00.047.257 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.257 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.257 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.259 I print_info: f_logit_scale    = 0.0e+00
0.00.047.259 I print_info: n_ff             = 8192
0.00.047.259 I print_info: n_expert         = 0
0.00.047.260 I print_info: n_expert_used    = 0
0.00.047.260 I print_info: causal attn      = 1
0.00.047.260 I print_info: pooling type     = 0
0.00.047.260 I print_info: rope type        = 2
0.00.047.260 I print_info: rope scaling     = linear
0.00.047.260 I print_info: freq_base_train  = 10000.0
0.00.047.261 I print_info: freq_scale_train = 1
0.00.047.261 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.262 I print_info: rope_finetuned   = unknown
0.00.047.262 I print_info: ssm_d_conv       = 0
0.00.047.262 I print_info: ssm_d_inner      = 0
0.00.047.262 I print_info: ssm_d_state      = 0
0.00.047.262 I print_info: ssm_dt_rank      = 0
0.00.047.263 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.263 I print_info: model type       = 1.4B
0.00.047.264 I print_info: model params     = 1.41 B
0.00.047.264 I print_info: general.name     = 1.4B
0.00.047.265 I print_info: vocab type       = BPE
0.00.047.265 I print_info: n_vocab          = 50304
0.00.047.266 I print_info: n_merges         = 50009
0.00.047.266 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.266 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.267 I print_info: LF token         = 187 'Ċ'
0.00.047.268 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.268 I print_info: max token length = 1024
0.00.047.268 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.498.304 I load_tensors: offloading 24 repeating layers to GPU
0.00.498.320 I load_tensors: offloading output layer to GPU
0.00.498.321 I load_tensors: offloaded 25/25 layers to GPU
0.00.498.352 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.498.354 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.500.033 I llama_init_from_model: n_seq_max     = 1
0.00.500.035 I llama_init_from_model: n_ctx         = 2048
0.00.500.036 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.500.037 I llama_init_from_model: n_batch       = 2048
0.00.500.037 I llama_init_from_model: n_ubatch      = 512
0.00.500.037 I llama_init_from_model: flash_attn    = 0
0.00.500.040 I llama_init_from_model: freq_base     = 10000.0
0.00.500.040 I llama_init_from_model: freq_scale    = 1
0.00.500.043 I ggml_metal_init: allocating
0.00.500.116 I ggml_metal_init: found device: Apple M4
0.00.500.128 I ggml_metal_init: picking default device: Apple M4
0.00.502.012 I ggml_metal_init: using embedded metal library
0.00.508.729 I ggml_metal_init: GPU name:   Apple M4
0.00.508.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.508.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.508.737 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.508.737 I ggml_metal_init: simdgroup reduction   = true
0.00.508.738 I ggml_metal_init: simdgroup matrix mul. = true
0.00.508.738 I ggml_metal_init: has residency sets    = true
0.00.508.738 I ggml_metal_init: has bfloat            = true
0.00.508.739 I ggml_metal_init: use bfloat            = true
0.00.508.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.508.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.527.550 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.582.066 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.582.072 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.582.096 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.586.871 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.586.874 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.586.874 I llama_init_from_model: graph nodes  = 967
0.00.586.874 I llama_init_from_model: graph splits = 2
0.00.586.880 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.587.009 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.587.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.143 I main: llama threadpool init, n_threads = 4
0.00.645.187 I 
0.00.645.211 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.214 I 
0.00.645.368 I sampler seed: 1234
0.00.645.373 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.645.388 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.645.390 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.645.390 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.326.036 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48135.59 tokens per second)
0.01.326.037 I llama_perf_context_print:        load time =     630.49 ms
0.01.326.038 I llama_perf_context_print: prompt eval time =      48.76 ms /     7 tokens (    6.97 ms per token,   143.56 tokens per second)
0.01.326.038 I llama_perf_context_print:        eval time =     629.21 ms /    63 runs   (    9.99 ms per token,   100.13 tokens per second)
0.01.326.039 I llama_perf_context_print:       total time =     681.65 ms /    70 tokens
0.01.326.314 I ggml_metal_free: deallocating

real	0m1.346s
user	0m0.110s
sys	0m0.200s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.789 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.834 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.834 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.837 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.838 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.838 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.838 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.841 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.843 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.844 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.731 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.480 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.482 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.483 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.483 I llama_model_loader: - type  f32:  194 tensors
0.00.025.484 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.484 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.485 I print_info: file format = GGUF V3 (latest)
0.00.025.485 I print_info: file type   = Q4_1
0.00.025.486 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.657 I load: special tokens cache size = 25
0.00.040.075 I load: token to piece cache size = 0.2984 MB
0.00.040.089 I print_info: arch             = gptneox
0.00.040.090 I print_info: vocab_only       = 0
0.00.040.090 I print_info: n_ctx_train      = 2048
0.00.040.090 I print_info: n_embd           = 2048
0.00.040.091 I print_info: n_layer          = 24
0.00.040.093 I print_info: n_head           = 16
0.00.040.094 I print_info: n_head_kv        = 16
0.00.040.094 I print_info: n_rot            = 32
0.00.040.094 I print_info: n_swa            = 0
0.00.040.094 I print_info: n_embd_head_k    = 128
0.00.040.095 I print_info: n_embd_head_v    = 128
0.00.040.095 I print_info: n_gqa            = 1
0.00.040.096 I print_info: n_embd_k_gqa     = 2048
0.00.040.097 I print_info: n_embd_v_gqa     = 2048
0.00.040.097 I print_info: f_norm_eps       = 1.0e-05
0.00.040.098 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.098 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.098 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.098 I print_info: f_logit_scale    = 0.0e+00
0.00.040.099 I print_info: n_ff             = 8192
0.00.040.099 I print_info: n_expert         = 0
0.00.040.099 I print_info: n_expert_used    = 0
0.00.040.099 I print_info: causal attn      = 1
0.00.040.100 I print_info: pooling type     = 0
0.00.040.101 I print_info: rope type        = 2
0.00.040.101 I print_info: rope scaling     = linear
0.00.040.101 I print_info: freq_base_train  = 10000.0
0.00.040.102 I print_info: freq_scale_train = 1
0.00.040.102 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.102 I print_info: rope_finetuned   = unknown
0.00.040.102 I print_info: ssm_d_conv       = 0
0.00.040.102 I print_info: ssm_d_inner      = 0
0.00.040.102 I print_info: ssm_d_state      = 0
0.00.040.102 I print_info: ssm_dt_rank      = 0
0.00.040.102 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.103 I print_info: model type       = 1.4B
0.00.040.103 I print_info: model params     = 1.41 B
0.00.040.103 I print_info: general.name     = 1.4B
0.00.040.104 I print_info: vocab type       = BPE
0.00.040.104 I print_info: n_vocab          = 50304
0.00.040.104 I print_info: n_merges         = 50009
0.00.040.104 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.104 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.104 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.105 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.105 I print_info: LF token         = 187 'Ċ'
0.00.040.107 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.107 I print_info: max token length = 1024
0.00.040.107 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.560.894 I load_tensors: offloading 24 repeating layers to GPU
0.00.560.906 I load_tensors: offloading output layer to GPU
0.00.560.907 I load_tensors: offloaded 25/25 layers to GPU
0.00.560.936 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.560.937 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.562.514 I llama_init_from_model: n_seq_max     = 1
0.00.562.520 I llama_init_from_model: n_ctx         = 2048
0.00.562.520 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.562.521 I llama_init_from_model: n_batch       = 2048
0.00.562.521 I llama_init_from_model: n_ubatch      = 512
0.00.562.521 I llama_init_from_model: flash_attn    = 0
0.00.562.523 I llama_init_from_model: freq_base     = 10000.0
0.00.562.523 I llama_init_from_model: freq_scale    = 1
0.00.562.526 I ggml_metal_init: allocating
0.00.562.578 I ggml_metal_init: found device: Apple M4
0.00.562.591 I ggml_metal_init: picking default device: Apple M4
0.00.564.391 I ggml_metal_init: using embedded metal library
0.00.570.198 I ggml_metal_init: GPU name:   Apple M4
0.00.570.203 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.570.204 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.570.204 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.570.205 I ggml_metal_init: simdgroup reduction   = true
0.00.570.205 I ggml_metal_init: simdgroup matrix mul. = true
0.00.570.206 I ggml_metal_init: has residency sets    = true
0.00.570.206 I ggml_metal_init: has bfloat            = true
0.00.570.206 I ggml_metal_init: use bfloat            = true
0.00.570.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.570.209 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.590.356 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.076 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.647.083 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.647.149 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.263 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.651.265 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.651.265 I llama_init_from_model: graph nodes  = 967
0.00.651.266 I llama_init_from_model: graph splits = 2
0.00.651.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.651.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.651.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.544 I main: llama threadpool init, n_threads = 4
0.00.707.592 I 
0.00.707.614 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.616 I 
0.00.707.766 I sampler seed: 1234
0.00.707.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.812 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.813 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.813 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.427.763 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.01.427.764 I llama_perf_context_print:        load time =     697.99 ms
0.01.427.764 I llama_perf_context_print: prompt eval time =      44.23 ms /     7 tokens (    6.32 ms per token,   158.25 tokens per second)
0.01.427.765 I llama_perf_context_print:        eval time =     673.02 ms /    63 runs   (   10.68 ms per token,    93.61 tokens per second)
0.01.427.765 I llama_perf_context_print:       total time =     720.99 ms /    70 tokens
0.01.428.003 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.111s
sys	0m0.204s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.973 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.568 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.572 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.574 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.575 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.578 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.578 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.579 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.579 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.588 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.589 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.457 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.216 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.217 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.217 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.218 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.218 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.218 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.219 I llama_model_loader: - type  f32:  194 tensors
0.00.026.219 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.219 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.220 I print_info: file format = GGUF V3 (latest)
0.00.026.220 I print_info: file type   = Q5_0
0.00.026.221 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.152 I load: special tokens cache size = 25
0.00.040.339 I load: token to piece cache size = 0.2984 MB
0.00.040.353 I print_info: arch             = gptneox
0.00.040.354 I print_info: vocab_only       = 0
0.00.040.354 I print_info: n_ctx_train      = 2048
0.00.040.355 I print_info: n_embd           = 2048
0.00.040.355 I print_info: n_layer          = 24
0.00.040.358 I print_info: n_head           = 16
0.00.040.358 I print_info: n_head_kv        = 16
0.00.040.359 I print_info: n_rot            = 32
0.00.040.359 I print_info: n_swa            = 0
0.00.040.359 I print_info: n_embd_head_k    = 128
0.00.040.359 I print_info: n_embd_head_v    = 128
0.00.040.360 I print_info: n_gqa            = 1
0.00.040.361 I print_info: n_embd_k_gqa     = 2048
0.00.040.361 I print_info: n_embd_v_gqa     = 2048
0.00.040.362 I print_info: f_norm_eps       = 1.0e-05
0.00.040.369 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.371 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.371 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.371 I print_info: f_logit_scale    = 0.0e+00
0.00.040.373 I print_info: n_ff             = 8192
0.00.040.373 I print_info: n_expert         = 0
0.00.040.374 I print_info: n_expert_used    = 0
0.00.040.374 I print_info: causal attn      = 1
0.00.040.374 I print_info: pooling type     = 0
0.00.040.374 I print_info: rope type        = 2
0.00.040.374 I print_info: rope scaling     = linear
0.00.040.375 I print_info: freq_base_train  = 10000.0
0.00.040.375 I print_info: freq_scale_train = 1
0.00.040.375 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.375 I print_info: rope_finetuned   = unknown
0.00.040.376 I print_info: ssm_d_conv       = 0
0.00.040.376 I print_info: ssm_d_inner      = 0
0.00.040.376 I print_info: ssm_d_state      = 0
0.00.040.376 I print_info: ssm_dt_rank      = 0
0.00.040.376 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.376 I print_info: model type       = 1.4B
0.00.040.377 I print_info: model params     = 1.41 B
0.00.040.377 I print_info: general.name     = 1.4B
0.00.040.378 I print_info: vocab type       = BPE
0.00.040.378 I print_info: n_vocab          = 50304
0.00.040.379 I print_info: n_merges         = 50009
0.00.040.380 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.380 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.380 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.380 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.381 I print_info: LF token         = 187 'Ċ'
0.00.040.381 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.381 I print_info: max token length = 1024
0.00.040.382 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.213 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.230 I load_tensors: offloading output layer to GPU
0.00.589.231 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.265 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.589.266 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.590.710 I llama_init_from_model: n_seq_max     = 1
0.00.590.713 I llama_init_from_model: n_ctx         = 2048
0.00.590.713 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.590.714 I llama_init_from_model: n_batch       = 2048
0.00.590.714 I llama_init_from_model: n_ubatch      = 512
0.00.590.715 I llama_init_from_model: flash_attn    = 0
0.00.590.716 I llama_init_from_model: freq_base     = 10000.0
0.00.590.717 I llama_init_from_model: freq_scale    = 1
0.00.590.721 I ggml_metal_init: allocating
0.00.590.791 I ggml_metal_init: found device: Apple M4
0.00.590.805 I ggml_metal_init: picking default device: Apple M4
0.00.592.685 I ggml_metal_init: using embedded metal library
0.00.599.380 I ggml_metal_init: GPU name:   Apple M4
0.00.599.385 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.386 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.386 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.387 I ggml_metal_init: simdgroup reduction   = true
0.00.599.387 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.387 I ggml_metal_init: has residency sets    = true
0.00.599.388 I ggml_metal_init: has bfloat            = true
0.00.599.388 I ggml_metal_init: use bfloat            = true
0.00.599.389 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.331 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.667.972 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.667.979 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.668.000 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.722 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.672.724 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.672.724 I llama_init_from_model: graph nodes  = 967
0.00.672.724 I llama_init_from_model: graph splits = 2
0.00.672.730 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.672.858 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.672.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.964 I main: llama threadpool init, n_threads = 4
0.00.729.019 I 
0.00.729.042 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.043 I 
0.00.729.198 I sampler seed: 1234
0.00.729.203 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.729.218 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.729.220 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.729.220 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.521.377 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.521.378 I llama_perf_context_print:        load time =     718.22 ms
0.01.521.379 I llama_perf_context_print: prompt eval time =      42.83 ms /     7 tokens (    6.12 ms per token,   163.43 tokens per second)
0.01.521.379 I llama_perf_context_print:        eval time =     746.50 ms /    63 runs   (   11.85 ms per token,    84.39 tokens per second)
0.01.521.382 I llama_perf_context_print:       total time =     793.18 ms /    70 tokens
0.01.521.610 I ggml_metal_free: deallocating

real	0m1.542s
user	0m0.112s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.121 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.125 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.125 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.126 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.126 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.129 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.129 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.129 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.814 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.814 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.815 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.815 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.816 I llama_model_loader: - type  f32:  194 tensors
0.00.025.816 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.816 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.817 I print_info: file format = GGUF V3 (latest)
0.00.025.817 I print_info: file type   = Q5_1
0.00.025.818 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.972 I load: special tokens cache size = 25
0.00.039.960 I load: token to piece cache size = 0.2984 MB
0.00.039.975 I print_info: arch             = gptneox
0.00.039.976 I print_info: vocab_only       = 0
0.00.039.976 I print_info: n_ctx_train      = 2048
0.00.039.976 I print_info: n_embd           = 2048
0.00.039.976 I print_info: n_layer          = 24
0.00.039.979 I print_info: n_head           = 16
0.00.039.980 I print_info: n_head_kv        = 16
0.00.039.980 I print_info: n_rot            = 32
0.00.039.981 I print_info: n_swa            = 0
0.00.039.981 I print_info: n_embd_head_k    = 128
0.00.039.981 I print_info: n_embd_head_v    = 128
0.00.039.982 I print_info: n_gqa            = 1
0.00.039.982 I print_info: n_embd_k_gqa     = 2048
0.00.039.984 I print_info: n_embd_v_gqa     = 2048
0.00.039.985 I print_info: f_norm_eps       = 1.0e-05
0.00.039.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.985 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.986 I print_info: f_logit_scale    = 0.0e+00
0.00.039.986 I print_info: n_ff             = 8192
0.00.039.986 I print_info: n_expert         = 0
0.00.039.986 I print_info: n_expert_used    = 0
0.00.039.986 I print_info: causal attn      = 1
0.00.039.987 I print_info: pooling type     = 0
0.00.039.988 I print_info: rope type        = 2
0.00.039.988 I print_info: rope scaling     = linear
0.00.039.989 I print_info: freq_base_train  = 10000.0
0.00.039.989 I print_info: freq_scale_train = 1
0.00.039.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.989 I print_info: rope_finetuned   = unknown
0.00.039.989 I print_info: ssm_d_conv       = 0
0.00.039.989 I print_info: ssm_d_inner      = 0
0.00.039.989 I print_info: ssm_d_state      = 0
0.00.039.990 I print_info: ssm_dt_rank      = 0
0.00.039.990 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.990 I print_info: model type       = 1.4B
0.00.039.990 I print_info: model params     = 1.41 B
0.00.039.994 I print_info: general.name     = 1.4B
0.00.039.994 I print_info: vocab type       = BPE
0.00.039.994 I print_info: n_vocab          = 50304
0.00.039.996 I print_info: n_merges         = 50009
0.00.039.996 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.996 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.996 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.997 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.997 I print_info: LF token         = 187 'Ċ'
0.00.039.997 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.997 I print_info: max token length = 1024
0.00.039.998 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.620.472 I load_tensors: offloading 24 repeating layers to GPU
0.00.620.477 I load_tensors: offloading output layer to GPU
0.00.620.479 I load_tensors: offloaded 25/25 layers to GPU
0.00.620.502 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.620.505 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.622.008 I llama_init_from_model: n_seq_max     = 1
0.00.622.010 I llama_init_from_model: n_ctx         = 2048
0.00.622.010 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.622.010 I llama_init_from_model: n_batch       = 2048
0.00.622.011 I llama_init_from_model: n_ubatch      = 512
0.00.622.011 I llama_init_from_model: flash_attn    = 0
0.00.622.012 I llama_init_from_model: freq_base     = 10000.0
0.00.622.013 I llama_init_from_model: freq_scale    = 1
0.00.622.014 I ggml_metal_init: allocating
0.00.622.033 I ggml_metal_init: found device: Apple M4
0.00.622.043 I ggml_metal_init: picking default device: Apple M4
0.00.623.495 I ggml_metal_init: using embedded metal library
0.00.629.629 I ggml_metal_init: GPU name:   Apple M4
0.00.629.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.629.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.629.634 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.629.634 I ggml_metal_init: simdgroup reduction   = true
0.00.629.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.629.635 I ggml_metal_init: has residency sets    = true
0.00.629.635 I ggml_metal_init: has bfloat            = true
0.00.629.635 I ggml_metal_init: use bfloat            = true
0.00.629.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.629.637 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.502 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.757 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.697.763 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.697.786 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.701.940 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.701.943 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.701.943 I llama_init_from_model: graph nodes  = 967
0.00.701.944 I llama_init_from_model: graph splits = 2
0.00.701.953 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.702.082 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.702.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.418 I main: llama threadpool init, n_threads = 4
0.00.758.460 I 
0.00.758.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.483 I 
0.00.758.633 I sampler seed: 1234
0.00.758.637 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.652 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.653 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.655 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.600.624 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.600.625 I llama_perf_context_print:        load time =     748.89 ms
0.01.600.627 I llama_perf_context_print: prompt eval time =      42.00 ms /     7 tokens (    6.00 ms per token,   166.68 tokens per second)
0.01.600.628 I llama_perf_context_print:        eval time =     797.03 ms /    63 runs   (   12.65 ms per token,    79.04 tokens per second)
0.01.600.630 I llama_perf_context_print:       total time =     843.00 ms /    70 tokens
0.01.600.871 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.550 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.763 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.774 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.775 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.775 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.777 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.780 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.781 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.782 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.783 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.783 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.660 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.439 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.440 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.441 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.441 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.441 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.442 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.442 I llama_model_loader: - type  f32:  194 tensors
0.00.026.442 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.443 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.443 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.443 I print_info: file format = GGUF V3 (latest)
0.00.026.444 I print_info: file type   = Q2_K - Medium
0.00.026.445 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.349 I load: special tokens cache size = 25
0.00.040.737 I load: token to piece cache size = 0.2984 MB
0.00.040.746 I print_info: arch             = gptneox
0.00.040.746 I print_info: vocab_only       = 0
0.00.040.747 I print_info: n_ctx_train      = 2048
0.00.040.747 I print_info: n_embd           = 2048
0.00.040.747 I print_info: n_layer          = 24
0.00.040.749 I print_info: n_head           = 16
0.00.040.750 I print_info: n_head_kv        = 16
0.00.040.750 I print_info: n_rot            = 32
0.00.040.751 I print_info: n_swa            = 0
0.00.040.751 I print_info: n_embd_head_k    = 128
0.00.040.753 I print_info: n_embd_head_v    = 128
0.00.040.754 I print_info: n_gqa            = 1
0.00.040.755 I print_info: n_embd_k_gqa     = 2048
0.00.040.755 I print_info: n_embd_v_gqa     = 2048
0.00.040.756 I print_info: f_norm_eps       = 1.0e-05
0.00.040.756 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.756 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.757 I print_info: f_logit_scale    = 0.0e+00
0.00.040.758 I print_info: n_ff             = 8192
0.00.040.758 I print_info: n_expert         = 0
0.00.040.758 I print_info: n_expert_used    = 0
0.00.040.758 I print_info: causal attn      = 1
0.00.040.758 I print_info: pooling type     = 0
0.00.040.758 I print_info: rope type        = 2
0.00.040.759 I print_info: rope scaling     = linear
0.00.040.759 I print_info: freq_base_train  = 10000.0
0.00.040.759 I print_info: freq_scale_train = 1
0.00.040.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.760 I print_info: rope_finetuned   = unknown
0.00.040.761 I print_info: ssm_d_conv       = 0
0.00.040.761 I print_info: ssm_d_inner      = 0
0.00.040.761 I print_info: ssm_d_state      = 0
0.00.040.761 I print_info: ssm_dt_rank      = 0
0.00.040.762 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.762 I print_info: model type       = 1.4B
0.00.040.762 I print_info: model params     = 1.41 B
0.00.040.762 I print_info: general.name     = 1.4B
0.00.040.763 I print_info: vocab type       = BPE
0.00.040.763 I print_info: n_vocab          = 50304
0.00.040.763 I print_info: n_merges         = 50009
0.00.040.766 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.766 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.767 I print_info: LF token         = 187 'Ċ'
0.00.040.767 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.767 I print_info: max token length = 1024
0.00.040.768 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.415.920 I load_tensors: offloading 24 repeating layers to GPU
0.00.415.924 I load_tensors: offloading output layer to GPU
0.00.415.925 I load_tensors: offloaded 25/25 layers to GPU
0.00.415.942 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.415.943 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.416.875 I llama_init_from_model: n_seq_max     = 1
0.00.416.877 I llama_init_from_model: n_ctx         = 2048
0.00.416.878 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.416.878 I llama_init_from_model: n_batch       = 2048
0.00.416.878 I llama_init_from_model: n_ubatch      = 512
0.00.416.879 I llama_init_from_model: flash_attn    = 0
0.00.416.880 I llama_init_from_model: freq_base     = 10000.0
0.00.416.880 I llama_init_from_model: freq_scale    = 1
0.00.416.882 I ggml_metal_init: allocating
0.00.416.916 I ggml_metal_init: found device: Apple M4
0.00.416.927 I ggml_metal_init: picking default device: Apple M4
0.00.417.985 I ggml_metal_init: using embedded metal library
0.00.422.311 I ggml_metal_init: GPU name:   Apple M4
0.00.422.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.422.320 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.422.321 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.422.321 I ggml_metal_init: simdgroup reduction   = true
0.00.422.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.422.322 I ggml_metal_init: has residency sets    = true
0.00.422.322 I ggml_metal_init: has bfloat            = true
0.00.422.323 I ggml_metal_init: use bfloat            = true
0.00.422.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.422.327 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.439.293 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.472.817 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.472.822 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.472.845 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.477.763 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.477.764 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.477.765 I llama_init_from_model: graph nodes  = 967
0.00.477.765 I llama_init_from_model: graph splits = 2
0.00.477.770 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.477.895 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.477.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.828 I main: llama threadpool init, n_threads = 4
0.00.536.867 I 
0.00.536.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.536.892 I 
0.00.537.060 I sampler seed: 1234
0.00.537.065 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.537.102 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.537.104 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.537.104 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.208.324 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.208.325 I llama_perf_context_print:        load time =     525.51 ms
0.01.208.326 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.42 tokens per second)
0.01.208.327 I llama_perf_context_print:        eval time =     632.63 ms /    63 runs   (   10.04 ms per token,    99.58 tokens per second)
0.01.208.327 I llama_perf_context_print:       total time =     672.26 ms /    70 tokens
0.01.208.571 I ggml_metal_free: deallocating

real	0m1.226s
user	0m0.106s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.272 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.188 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.028.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.195 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.196 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.196 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.196 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.196 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.197 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.198 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.198 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.201 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.201 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.203 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.204 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.204 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.135 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.166 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.061 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.063 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.063 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.063 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.063 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.064 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.037.064 I llama_model_loader: - type  f32:  194 tensors
0.00.037.065 I llama_model_loader: - type q3_K:   25 tensors
0.00.037.065 I llama_model_loader: - type q4_K:   71 tensors
0.00.037.065 I llama_model_loader: - type q5_K:    1 tensors
0.00.037.065 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.066 I print_info: file format = GGUF V3 (latest)
0.00.037.066 I print_info: file type   = Q3_K - Medium
0.00.037.067 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.748 I load: special tokens cache size = 25
0.00.053.129 I load: token to piece cache size = 0.2984 MB
0.00.053.143 I print_info: arch             = gptneox
0.00.053.144 I print_info: vocab_only       = 0
0.00.053.144 I print_info: n_ctx_train      = 2048
0.00.053.144 I print_info: n_embd           = 2048
0.00.053.144 I print_info: n_layer          = 24
0.00.053.147 I print_info: n_head           = 16
0.00.053.148 I print_info: n_head_kv        = 16
0.00.053.148 I print_info: n_rot            = 32
0.00.053.148 I print_info: n_swa            = 0
0.00.053.148 I print_info: n_embd_head_k    = 128
0.00.053.148 I print_info: n_embd_head_v    = 128
0.00.053.149 I print_info: n_gqa            = 1
0.00.053.150 I print_info: n_embd_k_gqa     = 2048
0.00.053.150 I print_info: n_embd_v_gqa     = 2048
0.00.053.151 I print_info: f_norm_eps       = 1.0e-05
0.00.053.151 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.152 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.152 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.152 I print_info: f_logit_scale    = 0.0e+00
0.00.053.152 I print_info: n_ff             = 8192
0.00.053.153 I print_info: n_expert         = 0
0.00.053.153 I print_info: n_expert_used    = 0
0.00.053.154 I print_info: causal attn      = 1
0.00.053.155 I print_info: pooling type     = 0
0.00.053.155 I print_info: rope type        = 2
0.00.053.155 I print_info: rope scaling     = linear
0.00.053.156 I print_info: freq_base_train  = 10000.0
0.00.053.156 I print_info: freq_scale_train = 1
0.00.053.156 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.156 I print_info: rope_finetuned   = unknown
0.00.053.157 I print_info: ssm_d_conv       = 0
0.00.053.158 I print_info: ssm_d_inner      = 0
0.00.053.158 I print_info: ssm_d_state      = 0
0.00.053.158 I print_info: ssm_dt_rank      = 0
0.00.053.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.158 I print_info: model type       = 1.4B
0.00.053.163 I print_info: model params     = 1.41 B
0.00.053.163 I print_info: general.name     = 1.4B
0.00.053.163 I print_info: vocab type       = BPE
0.00.053.164 I print_info: n_vocab          = 50304
0.00.053.164 I print_info: n_merges         = 50009
0.00.053.165 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.165 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: LF token         = 187 'Ċ'
0.00.053.166 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.166 I print_info: max token length = 1024
0.00.053.167 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.507.562 I load_tensors: offloading 24 repeating layers to GPU
0.00.507.583 I load_tensors: offloading output layer to GPU
0.00.507.584 I load_tensors: offloaded 25/25 layers to GPU
0.00.507.620 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.507.621 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.508.998 I llama_init_from_model: n_seq_max     = 1
0.00.509.001 I llama_init_from_model: n_ctx         = 2048
0.00.509.001 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.509.002 I llama_init_from_model: n_batch       = 2048
0.00.509.002 I llama_init_from_model: n_ubatch      = 512
0.00.509.003 I llama_init_from_model: flash_attn    = 0
0.00.509.006 I llama_init_from_model: freq_base     = 10000.0
0.00.509.006 I llama_init_from_model: freq_scale    = 1
0.00.509.008 I ggml_metal_init: allocating
0.00.509.085 I ggml_metal_init: found device: Apple M4
0.00.509.098 I ggml_metal_init: picking default device: Apple M4
0.00.511.030 I ggml_metal_init: using embedded metal library
0.00.516.850 I ggml_metal_init: GPU name:   Apple M4
0.00.516.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.516.869 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.516.870 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.516.870 I ggml_metal_init: simdgroup reduction   = true
0.00.516.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.516.871 I ggml_metal_init: has residency sets    = true
0.00.516.871 I ggml_metal_init: has bfloat            = true
0.00.516.872 I ggml_metal_init: use bfloat            = true
0.00.516.875 I ggml_metal_init: hasUnifiedMemory      = true
0.00.516.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.538.943 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.595.207 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.595.214 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.595.242 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.600.377 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.600.379 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.600.379 I llama_init_from_model: graph nodes  = 967
0.00.600.379 I llama_init_from_model: graph splits = 2
0.00.600.385 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.600.509 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.600.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.051 I main: llama threadpool init, n_threads = 4
0.00.650.097 I 
0.00.650.120 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.121 I 
0.00.650.252 I sampler seed: 1234
0.00.650.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.650.271 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.650.273 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.650.273 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.433.908 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.433.908 I llama_perf_context_print:        load time =     638.04 ms
0.01.433.909 I llama_perf_context_print: prompt eval time =      50.23 ms /     7 tokens (    7.18 ms per token,   139.35 tokens per second)
0.01.433.910 I llama_perf_context_print:        eval time =     730.43 ms /    63 runs   (   11.59 ms per token,    86.25 tokens per second)
0.01.433.914 I llama_perf_context_print:       total time =     784.60 ms /    70 tokens
0.01.434.130 I ggml_metal_free: deallocating

real	0m1.449s
user	0m0.115s
sys	0m0.190s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.323 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.943 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.949 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.952 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.952 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.952 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.953 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.954 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.956 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.956 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.957 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.960 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.960 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.782 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.554 I llama_model_loader: - type  f32:  194 tensors
0.00.026.555 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.555 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.555 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.556 I print_info: file format = GGUF V3 (latest)
0.00.026.556 I print_info: file type   = Q4_K - Medium
0.00.026.557 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.711 I load: special tokens cache size = 25
0.00.041.026 I load: token to piece cache size = 0.2984 MB
0.00.041.040 I print_info: arch             = gptneox
0.00.041.041 I print_info: vocab_only       = 0
0.00.041.041 I print_info: n_ctx_train      = 2048
0.00.041.041 I print_info: n_embd           = 2048
0.00.041.041 I print_info: n_layer          = 24
0.00.041.045 I print_info: n_head           = 16
0.00.041.045 I print_info: n_head_kv        = 16
0.00.041.045 I print_info: n_rot            = 32
0.00.041.045 I print_info: n_swa            = 0
0.00.041.046 I print_info: n_embd_head_k    = 128
0.00.041.046 I print_info: n_embd_head_v    = 128
0.00.041.047 I print_info: n_gqa            = 1
0.00.041.047 I print_info: n_embd_k_gqa     = 2048
0.00.041.050 I print_info: n_embd_v_gqa     = 2048
0.00.041.051 I print_info: f_norm_eps       = 1.0e-05
0.00.041.051 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.051 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.051 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.051 I print_info: f_logit_scale    = 0.0e+00
0.00.041.052 I print_info: n_ff             = 8192
0.00.041.052 I print_info: n_expert         = 0
0.00.041.052 I print_info: n_expert_used    = 0
0.00.041.053 I print_info: causal attn      = 1
0.00.041.055 I print_info: pooling type     = 0
0.00.041.055 I print_info: rope type        = 2
0.00.041.055 I print_info: rope scaling     = linear
0.00.041.056 I print_info: freq_base_train  = 10000.0
0.00.041.056 I print_info: freq_scale_train = 1
0.00.041.056 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.056 I print_info: rope_finetuned   = unknown
0.00.041.056 I print_info: ssm_d_conv       = 0
0.00.041.057 I print_info: ssm_d_inner      = 0
0.00.041.058 I print_info: ssm_d_state      = 0
0.00.041.058 I print_info: ssm_dt_rank      = 0
0.00.041.058 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.058 I print_info: model type       = 1.4B
0.00.041.059 I print_info: model params     = 1.41 B
0.00.041.059 I print_info: general.name     = 1.4B
0.00.041.059 I print_info: vocab type       = BPE
0.00.041.060 I print_info: n_vocab          = 50304
0.00.041.060 I print_info: n_merges         = 50009
0.00.041.060 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.060 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.060 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.060 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.061 I print_info: LF token         = 187 'Ċ'
0.00.041.061 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.062 I print_info: max token length = 1024
0.00.041.063 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.525.233 I load_tensors: offloading 24 repeating layers to GPU
0.00.525.254 I load_tensors: offloading output layer to GPU
0.00.525.254 I load_tensors: offloaded 25/25 layers to GPU
0.00.525.285 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.525.286 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.526.620 I llama_init_from_model: n_seq_max     = 1
0.00.526.624 I llama_init_from_model: n_ctx         = 2048
0.00.526.625 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.526.625 I llama_init_from_model: n_batch       = 2048
0.00.526.625 I llama_init_from_model: n_ubatch      = 512
0.00.526.626 I llama_init_from_model: flash_attn    = 0
0.00.526.628 I llama_init_from_model: freq_base     = 10000.0
0.00.526.629 I llama_init_from_model: freq_scale    = 1
0.00.526.631 I ggml_metal_init: allocating
0.00.526.712 I ggml_metal_init: found device: Apple M4
0.00.526.727 I ggml_metal_init: picking default device: Apple M4
0.00.528.644 I ggml_metal_init: using embedded metal library
0.00.534.570 I ggml_metal_init: GPU name:   Apple M4
0.00.534.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.534.576 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.534.577 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.534.578 I ggml_metal_init: simdgroup reduction   = true
0.00.534.578 I ggml_metal_init: simdgroup matrix mul. = true
0.00.534.578 I ggml_metal_init: has residency sets    = true
0.00.534.579 I ggml_metal_init: has bfloat            = true
0.00.534.579 I ggml_metal_init: use bfloat            = true
0.00.534.580 I ggml_metal_init: hasUnifiedMemory      = true
0.00.534.582 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.561 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.609.662 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.609.672 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.609.694 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.812 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.613.815 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.613.815 I llama_init_from_model: graph nodes  = 967
0.00.613.815 I llama_init_from_model: graph splits = 2
0.00.613.822 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.613.952 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.613.953 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.145 I main: llama threadpool init, n_threads = 4
0.00.664.189 I 
0.00.664.213 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.213 I 
0.00.664.338 I sampler seed: 1234
0.00.664.343 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.388 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.388 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.471.693 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49615.65 tokens per second)
0.01.471.694 I llama_perf_context_print:        load time =     653.09 ms
0.01.471.695 I llama_perf_context_print: prompt eval time =      58.69 ms /     7 tokens (    8.38 ms per token,   119.27 tokens per second)
0.01.471.696 I llama_perf_context_print:        eval time =     745.64 ms /    63 runs   (   11.84 ms per token,    84.49 tokens per second)
0.01.471.696 I llama_perf_context_print:       total time =     808.28 ms /    70 tokens
0.01.471.965 I ggml_metal_free: deallocating

real	0m1.491s
user	0m0.111s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.651 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.656 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.662 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.662 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.663 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.663 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.663 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.664 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.665 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.668 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.410 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.097 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.098 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.099 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.099 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.100 I llama_model_loader: - type  f32:  194 tensors
0.00.026.100 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.100 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.101 I print_info: file format = GGUF V3 (latest)
0.00.026.101 I print_info: file type   = Q5_K - Medium
0.00.026.105 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.949 I load: special tokens cache size = 25
0.00.040.270 I load: token to piece cache size = 0.2984 MB
0.00.040.284 I print_info: arch             = gptneox
0.00.040.286 I print_info: vocab_only       = 0
0.00.040.286 I print_info: n_ctx_train      = 2048
0.00.040.286 I print_info: n_embd           = 2048
0.00.040.286 I print_info: n_layer          = 24
0.00.040.289 I print_info: n_head           = 16
0.00.040.290 I print_info: n_head_kv        = 16
0.00.040.290 I print_info: n_rot            = 32
0.00.040.290 I print_info: n_swa            = 0
0.00.040.291 I print_info: n_embd_head_k    = 128
0.00.040.291 I print_info: n_embd_head_v    = 128
0.00.040.293 I print_info: n_gqa            = 1
0.00.040.294 I print_info: n_embd_k_gqa     = 2048
0.00.040.295 I print_info: n_embd_v_gqa     = 2048
0.00.040.295 I print_info: f_norm_eps       = 1.0e-05
0.00.040.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.296 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.296 I print_info: f_logit_scale    = 0.0e+00
0.00.040.297 I print_info: n_ff             = 8192
0.00.040.297 I print_info: n_expert         = 0
0.00.040.297 I print_info: n_expert_used    = 0
0.00.040.297 I print_info: causal attn      = 1
0.00.040.298 I print_info: pooling type     = 0
0.00.040.298 I print_info: rope type        = 2
0.00.040.298 I print_info: rope scaling     = linear
0.00.040.299 I print_info: freq_base_train  = 10000.0
0.00.040.299 I print_info: freq_scale_train = 1
0.00.040.299 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.299 I print_info: rope_finetuned   = unknown
0.00.040.300 I print_info: ssm_d_conv       = 0
0.00.040.301 I print_info: ssm_d_inner      = 0
0.00.040.301 I print_info: ssm_d_state      = 0
0.00.040.301 I print_info: ssm_dt_rank      = 0
0.00.040.301 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.301 I print_info: model type       = 1.4B
0.00.040.301 I print_info: model params     = 1.41 B
0.00.040.302 I print_info: general.name     = 1.4B
0.00.040.302 I print_info: vocab type       = BPE
0.00.040.302 I print_info: n_vocab          = 50304
0.00.040.302 I print_info: n_merges         = 50009
0.00.040.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: LF token         = 187 'Ċ'
0.00.040.303 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.303 I print_info: max token length = 1024
0.00.040.304 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.601.237 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.254 I load_tensors: offloading output layer to GPU
0.00.601.255 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.286 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.288 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.602.586 I llama_init_from_model: n_seq_max     = 1
0.00.602.589 I llama_init_from_model: n_ctx         = 2048
0.00.602.590 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.602.590 I llama_init_from_model: n_batch       = 2048
0.00.602.591 I llama_init_from_model: n_ubatch      = 512
0.00.602.591 I llama_init_from_model: flash_attn    = 0
0.00.602.594 I llama_init_from_model: freq_base     = 10000.0
0.00.602.595 I llama_init_from_model: freq_scale    = 1
0.00.602.597 I ggml_metal_init: allocating
0.00.602.674 I ggml_metal_init: found device: Apple M4
0.00.602.689 I ggml_metal_init: picking default device: Apple M4
0.00.604.590 I ggml_metal_init: using embedded metal library
0.00.611.661 I ggml_metal_init: GPU name:   Apple M4
0.00.611.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.667 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.668 I ggml_metal_init: simdgroup reduction   = true
0.00.611.668 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.669 I ggml_metal_init: has residency sets    = true
0.00.611.669 I ggml_metal_init: has bfloat            = true
0.00.611.669 I ggml_metal_init: use bfloat            = true
0.00.611.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.672 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.548 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.902 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.677.908 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.929 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.635 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.683.637 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.683.638 I llama_init_from_model: graph nodes  = 967
0.00.683.638 I llama_init_from_model: graph splits = 2
0.00.683.645 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.683.775 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.683.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.184 I main: llama threadpool init, n_threads = 4
0.00.736.231 I 
0.00.736.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.254 I 
0.00.736.377 I sampler seed: 1234
0.00.736.382 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.405 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.407 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.407 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.610.216 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.610.217 I llama_perf_context_print:        load time =     726.34 ms
0.01.610.219 I llama_perf_context_print: prompt eval time =      52.79 ms /     7 tokens (    7.54 ms per token,   132.61 tokens per second)
0.01.610.219 I llama_perf_context_print:        eval time =     818.25 ms /    63 runs   (   12.99 ms per token,    76.99 tokens per second)
0.01.610.220 I llama_perf_context_print:       total time =     874.78 ms /    70 tokens
0.01.610.488 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.111s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.655 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.505 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.517 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.519 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.519 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.520 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.520 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.521 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.523 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.523 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.372 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.414 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.173 I llama_model_loader: - type  f32:  194 tensors
0.00.027.173 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.174 I print_info: file format = GGUF V3 (latest)
0.00.027.174 I print_info: file type   = Q6_K
0.00.027.175 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.299 I load: special tokens cache size = 25
0.00.041.293 I load: token to piece cache size = 0.2984 MB
0.00.041.302 I print_info: arch             = gptneox
0.00.041.303 I print_info: vocab_only       = 0
0.00.041.304 I print_info: n_ctx_train      = 2048
0.00.041.304 I print_info: n_embd           = 2048
0.00.041.304 I print_info: n_layer          = 24
0.00.041.307 I print_info: n_head           = 16
0.00.041.308 I print_info: n_head_kv        = 16
0.00.041.308 I print_info: n_rot            = 32
0.00.041.308 I print_info: n_swa            = 0
0.00.041.309 I print_info: n_embd_head_k    = 128
0.00.041.309 I print_info: n_embd_head_v    = 128
0.00.041.310 I print_info: n_gqa            = 1
0.00.041.310 I print_info: n_embd_k_gqa     = 2048
0.00.041.311 I print_info: n_embd_v_gqa     = 2048
0.00.041.312 I print_info: f_norm_eps       = 1.0e-05
0.00.041.312 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.312 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.312 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.312 I print_info: f_logit_scale    = 0.0e+00
0.00.041.313 I print_info: n_ff             = 8192
0.00.041.314 I print_info: n_expert         = 0
0.00.041.314 I print_info: n_expert_used    = 0
0.00.041.314 I print_info: causal attn      = 1
0.00.041.314 I print_info: pooling type     = 0
0.00.041.314 I print_info: rope type        = 2
0.00.041.314 I print_info: rope scaling     = linear
0.00.041.315 I print_info: freq_base_train  = 10000.0
0.00.041.315 I print_info: freq_scale_train = 1
0.00.041.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.315 I print_info: rope_finetuned   = unknown
0.00.041.316 I print_info: ssm_d_conv       = 0
0.00.041.316 I print_info: ssm_d_inner      = 0
0.00.041.316 I print_info: ssm_d_state      = 0
0.00.041.316 I print_info: ssm_dt_rank      = 0
0.00.041.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.317 I print_info: model type       = 1.4B
0.00.041.317 I print_info: model params     = 1.41 B
0.00.041.317 I print_info: general.name     = 1.4B
0.00.041.318 I print_info: vocab type       = BPE
0.00.041.318 I print_info: n_vocab          = 50304
0.00.041.318 I print_info: n_merges         = 50009
0.00.041.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.320 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.321 I print_info: LF token         = 187 'Ċ'
0.00.041.321 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.321 I print_info: max token length = 1024
0.00.041.321 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.709 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.714 I load_tensors: offloading output layer to GPU
0.00.637.716 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.740 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.637.745 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.205 I llama_init_from_model: n_seq_max     = 1
0.00.639.207 I llama_init_from_model: n_ctx         = 2048
0.00.639.207 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.208 I llama_init_from_model: n_batch       = 2048
0.00.639.208 I llama_init_from_model: n_ubatch      = 512
0.00.639.209 I llama_init_from_model: flash_attn    = 0
0.00.639.209 I llama_init_from_model: freq_base     = 10000.0
0.00.639.210 I llama_init_from_model: freq_scale    = 1
0.00.639.211 I ggml_metal_init: allocating
0.00.639.224 I ggml_metal_init: found device: Apple M4
0.00.639.232 I ggml_metal_init: picking default device: Apple M4
0.00.640.504 I ggml_metal_init: using embedded metal library
0.00.646.524 I ggml_metal_init: GPU name:   Apple M4
0.00.646.528 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.529 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.530 I ggml_metal_init: simdgroup reduction   = true
0.00.646.531 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.531 I ggml_metal_init: has residency sets    = true
0.00.646.531 I ggml_metal_init: has bfloat            = true
0.00.646.531 I ggml_metal_init: use bfloat            = true
0.00.646.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.534 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.291 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.587 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.594 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.627 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.384 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.386 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.387 I llama_init_from_model: graph nodes  = 967
0.00.717.387 I llama_init_from_model: graph splits = 2
0.00.717.397 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.530 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.697 I main: llama threadpool init, n_threads = 4
0.00.781.743 I 
0.00.781.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.770 I 
0.00.781.920 I sampler seed: 1234
0.00.781.924 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.781.940 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.781.941 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.781.941 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.656.970 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.656.971 I llama_perf_context_print:        load time =     770.27 ms
0.01.656.972 I llama_perf_context_print: prompt eval time =      57.45 ms /     7 tokens (    8.21 ms per token,   121.83 tokens per second)
0.01.656.973 I llama_perf_context_print:        eval time =     814.77 ms /    63 runs   (   12.93 ms per token,    77.32 tokens per second)
0.01.656.975 I llama_perf_context_print:       total time =     876.05 ms /    70 tokens
0.01.657.242 I ggml_metal_free: deallocating

real	0m1.676s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.714 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.136 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.501 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.511 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.512 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.512 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.513 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.516 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.517 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.517 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.518 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.518 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.519 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.521 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.841 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.697 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.701 I llama_model_loader: - type  f32:  194 tensors
0.00.053.701 I llama_model_loader: - type  f16:   98 tensors
0.00.053.702 I print_info: file format = GGUF V3 (latest)
0.00.053.702 I print_info: file type   = all F32 (guessed)
0.00.053.703 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.335 I load: special tokens cache size = 25
0.00.072.054 I load: token to piece cache size = 0.2984 MB
0.00.072.063 I print_info: arch             = gptneox
0.00.072.064 I print_info: vocab_only       = 0
0.00.072.064 I print_info: n_ctx_train      = 2048
0.00.072.065 I print_info: n_embd           = 2048
0.00.072.065 I print_info: n_layer          = 24
0.00.072.068 I print_info: n_head           = 16
0.00.072.068 I print_info: n_head_kv        = 16
0.00.072.068 I print_info: n_rot            = 32
0.00.072.069 I print_info: n_swa            = 0
0.00.072.069 I print_info: n_embd_head_k    = 128
0.00.072.069 I print_info: n_embd_head_v    = 128
0.00.072.070 I print_info: n_gqa            = 1
0.00.072.070 I print_info: n_embd_k_gqa     = 2048
0.00.072.071 I print_info: n_embd_v_gqa     = 2048
0.00.072.072 I print_info: f_norm_eps       = 1.0e-05
0.00.072.072 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.072 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.072 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.072 I print_info: f_logit_scale    = 0.0e+00
0.00.072.073 I print_info: n_ff             = 8192
0.00.072.073 I print_info: n_expert         = 0
0.00.072.073 I print_info: n_expert_used    = 0
0.00.072.073 I print_info: causal attn      = 1
0.00.072.074 I print_info: pooling type     = 0
0.00.072.074 I print_info: rope type        = 2
0.00.072.074 I print_info: rope scaling     = linear
0.00.072.074 I print_info: freq_base_train  = 10000.0
0.00.072.075 I print_info: freq_scale_train = 1
0.00.072.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.076 I print_info: rope_finetuned   = unknown
0.00.072.076 I print_info: ssm_d_conv       = 0
0.00.072.076 I print_info: ssm_d_inner      = 0
0.00.072.077 I print_info: ssm_d_state      = 0
0.00.072.077 I print_info: ssm_dt_rank      = 0
0.00.072.077 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.078 I print_info: model type       = 1.4B
0.00.072.078 I print_info: model params     = 1.41 B
0.00.072.079 I print_info: general.name     = 1.4B
0.00.072.079 I print_info: vocab type       = BPE
0.00.072.079 I print_info: n_vocab          = 50304
0.00.072.081 I print_info: n_merges         = 50009
0.00.072.081 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.081 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.081 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.082 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.082 I print_info: LF token         = 187 'Ċ'
0.00.072.082 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.082 I print_info: max token length = 1024
0.00.072.083 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.405.351 I load_tensors: offloading 24 repeating layers to GPU
0.01.405.356 I load_tensors: offloading output layer to GPU
0.01.405.356 I load_tensors: offloaded 25/25 layers to GPU
0.01.405.380 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.405.381 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.406.501 I llama_init_from_model: n_seq_max     = 1
0.01.406.502 I llama_init_from_model: n_ctx         = 128
0.01.406.503 I llama_init_from_model: n_ctx_per_seq = 128
0.01.406.503 I llama_init_from_model: n_batch       = 128
0.01.406.503 I llama_init_from_model: n_ubatch      = 128
0.01.406.503 I llama_init_from_model: flash_attn    = 0
0.01.406.504 I llama_init_from_model: freq_base     = 10000.0
0.01.406.504 I llama_init_from_model: freq_scale    = 1
0.01.406.504 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.406.505 I ggml_metal_init: allocating
0.01.406.548 I ggml_metal_init: found device: Apple M4
0.01.406.554 I ggml_metal_init: picking default device: Apple M4
0.01.407.635 I ggml_metal_init: using embedded metal library
0.01.411.564 I ggml_metal_init: GPU name:   Apple M4
0.01.411.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.411.567 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.411.567 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.411.568 I ggml_metal_init: simdgroup reduction   = true
0.01.411.568 I ggml_metal_init: simdgroup matrix mul. = true
0.01.411.568 I ggml_metal_init: has residency sets    = true
0.01.411.568 I ggml_metal_init: has bfloat            = true
0.01.411.568 I ggml_metal_init: use bfloat            = true
0.01.411.569 I ggml_metal_init: hasUnifiedMemory      = true
0.01.411.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.423.398 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.425.140 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.425.142 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.425.156 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.426.842 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.426.843 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.426.844 I llama_init_from_model: graph nodes  = 967
0.01.426.844 I llama_init_from_model: graph splits = 2
0.01.426.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.426.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.462.577 I 
0.01.462.617 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.462.649 I perplexity: tokenizing the input ..
0.01.467.896 I perplexity: tokenization took 5.245 ms
0.01.467.910 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.587.172 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.589.486 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.589.503 I llama_perf_context_print:        load time =    1440.43 ms
0.01.589.504 I llama_perf_context_print: prompt eval time =     118.95 ms /   128 tokens (    0.93 ms per token,  1076.04 tokens per second)
0.01.589.504 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.589.505 I llama_perf_context_print:       total time =     126.93 ms /   129 tokens
0.01.589.909 I ggml_metal_free: deallocating

real	0m1.778s
user	0m0.101s
sys	0m0.250s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.283 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.351 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.642 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.646 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.650 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.651 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.651 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.652 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.654 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.429 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.431 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.431 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.431 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.432 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.432 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.433 I llama_model_loader: - type  f32:  194 tensors
0.00.026.433 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.434 I print_info: file format = GGUF V3 (latest)
0.00.026.434 I print_info: file type   = Q8_0
0.00.026.435 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.898 I load: special tokens cache size = 25
0.00.041.288 I load: token to piece cache size = 0.2984 MB
0.00.041.306 I print_info: arch             = gptneox
0.00.041.306 I print_info: vocab_only       = 0
0.00.041.307 I print_info: n_ctx_train      = 2048
0.00.041.307 I print_info: n_embd           = 2048
0.00.041.307 I print_info: n_layer          = 24
0.00.041.311 I print_info: n_head           = 16
0.00.041.312 I print_info: n_head_kv        = 16
0.00.041.312 I print_info: n_rot            = 32
0.00.041.313 I print_info: n_swa            = 0
0.00.041.313 I print_info: n_embd_head_k    = 128
0.00.041.313 I print_info: n_embd_head_v    = 128
0.00.041.313 I print_info: n_gqa            = 1
0.00.041.314 I print_info: n_embd_k_gqa     = 2048
0.00.041.315 I print_info: n_embd_v_gqa     = 2048
0.00.041.315 I print_info: f_norm_eps       = 1.0e-05
0.00.041.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.316 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.316 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.316 I print_info: f_logit_scale    = 0.0e+00
0.00.041.317 I print_info: n_ff             = 8192
0.00.041.317 I print_info: n_expert         = 0
0.00.041.317 I print_info: n_expert_used    = 0
0.00.041.317 I print_info: causal attn      = 1
0.00.041.317 I print_info: pooling type     = 0
0.00.041.317 I print_info: rope type        = 2
0.00.041.318 I print_info: rope scaling     = linear
0.00.041.318 I print_info: freq_base_train  = 10000.0
0.00.041.318 I print_info: freq_scale_train = 1
0.00.041.318 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.319 I print_info: rope_finetuned   = unknown
0.00.041.319 I print_info: ssm_d_conv       = 0
0.00.041.319 I print_info: ssm_d_inner      = 0
0.00.041.319 I print_info: ssm_d_state      = 0
0.00.041.319 I print_info: ssm_dt_rank      = 0
0.00.041.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.322 I print_info: model type       = 1.4B
0.00.041.322 I print_info: model params     = 1.41 B
0.00.041.322 I print_info: general.name     = 1.4B
0.00.041.323 I print_info: vocab type       = BPE
0.00.041.323 I print_info: n_vocab          = 50304
0.00.041.323 I print_info: n_merges         = 50009
0.00.041.323 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.324 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.324 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.324 I print_info: LF token         = 187 'Ċ'
0.00.041.324 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.325 I print_info: max token length = 1024
0.00.041.325 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.948.832 I load_tensors: offloading 24 repeating layers to GPU
0.00.948.839 I load_tensors: offloading output layer to GPU
0.00.948.839 I load_tensors: offloaded 25/25 layers to GPU
0.00.948.867 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.948.868 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.950.478 I llama_init_from_model: n_seq_max     = 1
0.00.950.481 I llama_init_from_model: n_ctx         = 128
0.00.950.481 I llama_init_from_model: n_ctx_per_seq = 128
0.00.950.481 I llama_init_from_model: n_batch       = 128
0.00.950.482 I llama_init_from_model: n_ubatch      = 128
0.00.950.482 I llama_init_from_model: flash_attn    = 0
0.00.950.483 I llama_init_from_model: freq_base     = 10000.0
0.00.950.483 I llama_init_from_model: freq_scale    = 1
0.00.950.484 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.950.485 I ggml_metal_init: allocating
0.00.950.566 I ggml_metal_init: found device: Apple M4
0.00.950.575 I ggml_metal_init: picking default device: Apple M4
0.00.952.002 I ggml_metal_init: using embedded metal library
0.00.957.287 I ggml_metal_init: GPU name:   Apple M4
0.00.957.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.957.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.957.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.957.292 I ggml_metal_init: simdgroup reduction   = true
0.00.957.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.957.292 I ggml_metal_init: has residency sets    = true
0.00.957.292 I ggml_metal_init: has bfloat            = true
0.00.957.292 I ggml_metal_init: use bfloat            = true
0.00.957.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.957.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.972.576 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.975.890 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.975.893 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.975.922 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.978.779 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.978.780 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.978.781 I llama_init_from_model: graph nodes  = 967
0.00.978.781 I llama_init_from_model: graph splits = 2
0.00.978.784 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.978.784 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.002.620 I 
0.01.002.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.002.686 I perplexity: tokenizing the input ..
0.01.008.635 I perplexity: tokenization took 5.947 ms
0.01.008.646 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.145.067 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.146.632 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.146.644 I llama_perf_context_print:        load time =     992.26 ms
0.01.146.645 I llama_perf_context_print: prompt eval time =     135.88 ms /   128 tokens (    1.06 ms per token,   941.99 tokens per second)
0.01.146.646 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.146.646 I llama_perf_context_print:       total time =     144.03 ms /   129 tokens
0.01.147.036 I ggml_metal_free: deallocating

real	0m1.163s
user	0m0.075s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.277 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.442 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.969 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.977 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.977 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.978 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.979 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.979 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.979 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.980 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.980 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.980 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.982 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.752 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.508 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.509 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.510 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.510 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.511 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.511 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.512 I llama_model_loader: - type  f32:  194 tensors
0.00.026.512 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.512 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.513 I print_info: file format = GGUF V3 (latest)
0.00.026.518 I print_info: file type   = Q4_0
0.00.026.519 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.986 I load: special tokens cache size = 25
0.00.041.372 I load: token to piece cache size = 0.2984 MB
0.00.041.390 I print_info: arch             = gptneox
0.00.041.391 I print_info: vocab_only       = 0
0.00.041.391 I print_info: n_ctx_train      = 2048
0.00.041.392 I print_info: n_embd           = 2048
0.00.041.392 I print_info: n_layer          = 24
0.00.041.395 I print_info: n_head           = 16
0.00.041.396 I print_info: n_head_kv        = 16
0.00.041.396 I print_info: n_rot            = 32
0.00.041.396 I print_info: n_swa            = 0
0.00.041.396 I print_info: n_embd_head_k    = 128
0.00.041.397 I print_info: n_embd_head_v    = 128
0.00.041.397 I print_info: n_gqa            = 1
0.00.041.398 I print_info: n_embd_k_gqa     = 2048
0.00.041.398 I print_info: n_embd_v_gqa     = 2048
0.00.041.399 I print_info: f_norm_eps       = 1.0e-05
0.00.041.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.399 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.400 I print_info: f_logit_scale    = 0.0e+00
0.00.041.400 I print_info: n_ff             = 8192
0.00.041.400 I print_info: n_expert         = 0
0.00.041.400 I print_info: n_expert_used    = 0
0.00.041.401 I print_info: causal attn      = 1
0.00.041.401 I print_info: pooling type     = 0
0.00.041.401 I print_info: rope type        = 2
0.00.041.401 I print_info: rope scaling     = linear
0.00.041.401 I print_info: freq_base_train  = 10000.0
0.00.041.402 I print_info: freq_scale_train = 1
0.00.041.402 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.402 I print_info: rope_finetuned   = unknown
0.00.041.402 I print_info: ssm_d_conv       = 0
0.00.041.402 I print_info: ssm_d_inner      = 0
0.00.041.402 I print_info: ssm_d_state      = 0
0.00.041.402 I print_info: ssm_dt_rank      = 0
0.00.041.403 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.403 I print_info: model type       = 1.4B
0.00.041.403 I print_info: model params     = 1.41 B
0.00.041.403 I print_info: general.name     = 1.4B
0.00.041.404 I print_info: vocab type       = BPE
0.00.041.404 I print_info: n_vocab          = 50304
0.00.041.404 I print_info: n_merges         = 50009
0.00.041.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.405 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.405 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.405 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.405 I print_info: LF token         = 187 'Ċ'
0.00.041.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.407 I print_info: max token length = 1024
0.00.041.407 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.493.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.493.516 I load_tensors: offloading output layer to GPU
0.00.493.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.493.550 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.493.552 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.494.663 I llama_init_from_model: n_seq_max     = 1
0.00.494.666 I llama_init_from_model: n_ctx         = 128
0.00.494.667 I llama_init_from_model: n_ctx_per_seq = 128
0.00.494.667 I llama_init_from_model: n_batch       = 128
0.00.494.667 I llama_init_from_model: n_ubatch      = 128
0.00.494.668 I llama_init_from_model: flash_attn    = 0
0.00.494.670 I llama_init_from_model: freq_base     = 10000.0
0.00.494.670 I llama_init_from_model: freq_scale    = 1
0.00.494.671 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.494.674 I ggml_metal_init: allocating
0.00.494.764 I ggml_metal_init: found device: Apple M4
0.00.494.776 I ggml_metal_init: picking default device: Apple M4
0.00.496.606 I ggml_metal_init: using embedded metal library
0.00.502.092 I ggml_metal_init: GPU name:   Apple M4
0.00.502.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.502.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.502.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.502.107 I ggml_metal_init: simdgroup reduction   = true
0.00.502.108 I ggml_metal_init: simdgroup matrix mul. = true
0.00.502.108 I ggml_metal_init: has residency sets    = true
0.00.502.108 I ggml_metal_init: has bfloat            = true
0.00.502.109 I ggml_metal_init: use bfloat            = true
0.00.502.111 I ggml_metal_init: hasUnifiedMemory      = true
0.00.502.116 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.522.309 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.951 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.525.955 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.525.992 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.529.526 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.529.528 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.529.528 I llama_init_from_model: graph nodes  = 967
0.00.529.528 I llama_init_from_model: graph splits = 2
0.00.529.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.529.532 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.802 I 
0.00.554.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.903 I perplexity: tokenizing the input ..
0.00.561.547 I perplexity: tokenization took 6.641 ms
0.00.561.560 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.589 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.694.917 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.694.931 I llama_perf_context_print:        load time =     544.35 ms
0.00.694.932 I llama_perf_context_print: prompt eval time =     131.37 ms /   128 tokens (    1.03 ms per token,   974.35 tokens per second)
0.00.694.933 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.694.933 I llama_perf_context_print:       total time =     140.13 ms /   129 tokens
0.00.695.310 I ggml_metal_free: deallocating

real	0m0.712s
user	0m0.080s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.002 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.629 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.631 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.632 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.634 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.635 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.635 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.635 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.636 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.638 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.638 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.639 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.487 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.236 I llama_model_loader: - type  f32:  194 tensors
0.00.025.236 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.237 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.237 I print_info: file format = GGUF V3 (latest)
0.00.025.238 I print_info: file type   = Q4_1
0.00.025.239 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.942 I load: special tokens cache size = 25
0.00.040.340 I load: token to piece cache size = 0.2984 MB
0.00.040.358 I print_info: arch             = gptneox
0.00.040.358 I print_info: vocab_only       = 0
0.00.040.359 I print_info: n_ctx_train      = 2048
0.00.040.359 I print_info: n_embd           = 2048
0.00.040.359 I print_info: n_layer          = 24
0.00.040.363 I print_info: n_head           = 16
0.00.040.364 I print_info: n_head_kv        = 16
0.00.040.364 I print_info: n_rot            = 32
0.00.040.364 I print_info: n_swa            = 0
0.00.040.364 I print_info: n_embd_head_k    = 128
0.00.040.364 I print_info: n_embd_head_v    = 128
0.00.040.365 I print_info: n_gqa            = 1
0.00.040.365 I print_info: n_embd_k_gqa     = 2048
0.00.040.366 I print_info: n_embd_v_gqa     = 2048
0.00.040.367 I print_info: f_norm_eps       = 1.0e-05
0.00.040.367 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.367 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.367 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.368 I print_info: f_logit_scale    = 0.0e+00
0.00.040.368 I print_info: n_ff             = 8192
0.00.040.368 I print_info: n_expert         = 0
0.00.040.369 I print_info: n_expert_used    = 0
0.00.040.369 I print_info: causal attn      = 1
0.00.040.369 I print_info: pooling type     = 0
0.00.040.369 I print_info: rope type        = 2
0.00.040.369 I print_info: rope scaling     = linear
0.00.040.370 I print_info: freq_base_train  = 10000.0
0.00.040.370 I print_info: freq_scale_train = 1
0.00.040.370 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.370 I print_info: rope_finetuned   = unknown
0.00.040.370 I print_info: ssm_d_conv       = 0
0.00.040.370 I print_info: ssm_d_inner      = 0
0.00.040.370 I print_info: ssm_d_state      = 0
0.00.040.371 I print_info: ssm_dt_rank      = 0
0.00.040.371 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.371 I print_info: model type       = 1.4B
0.00.040.371 I print_info: model params     = 1.41 B
0.00.040.371 I print_info: general.name     = 1.4B
0.00.040.372 I print_info: vocab type       = BPE
0.00.040.372 I print_info: n_vocab          = 50304
0.00.040.372 I print_info: n_merges         = 50009
0.00.040.372 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: LF token         = 187 'Ċ'
0.00.040.373 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.373 I print_info: max token length = 1024
0.00.040.374 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.220 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.239 I load_tensors: offloading output layer to GPU
0.00.538.240 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.274 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.538.281 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.540.033 I llama_init_from_model: n_seq_max     = 1
0.00.540.036 I llama_init_from_model: n_ctx         = 128
0.00.540.037 I llama_init_from_model: n_ctx_per_seq = 128
0.00.540.037 I llama_init_from_model: n_batch       = 128
0.00.540.037 I llama_init_from_model: n_ubatch      = 128
0.00.540.038 I llama_init_from_model: flash_attn    = 0
0.00.540.040 I llama_init_from_model: freq_base     = 10000.0
0.00.540.040 I llama_init_from_model: freq_scale    = 1
0.00.540.041 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.540.043 I ggml_metal_init: allocating
0.00.540.122 I ggml_metal_init: found device: Apple M4
0.00.540.135 I ggml_metal_init: picking default device: Apple M4
0.00.541.947 I ggml_metal_init: using embedded metal library
0.00.548.719 I ggml_metal_init: GPU name:   Apple M4
0.00.548.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.548.727 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.548.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.548.728 I ggml_metal_init: simdgroup reduction   = true
0.00.548.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.548.729 I ggml_metal_init: has residency sets    = true
0.00.548.729 I ggml_metal_init: has bfloat            = true
0.00.548.729 I ggml_metal_init: use bfloat            = true
0.00.548.730 I ggml_metal_init: hasUnifiedMemory      = true
0.00.548.739 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.451 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.569.839 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.569.845 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.569.872 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.572.996 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.572.998 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.572.999 I llama_init_from_model: graph nodes  = 967
0.00.572.999 I llama_init_from_model: graph splits = 2
0.00.573.002 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.573.003 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.802 I 
0.00.596.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.887 I perplexity: tokenizing the input ..
0.00.604.102 I perplexity: tokenization took 7.212 ms
0.00.604.109 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.788 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.729.115 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.729.130 I llama_perf_context_print:        load time =     587.79 ms
0.00.729.131 I llama_perf_context_print: prompt eval time =     122.77 ms /   128 tokens (    0.96 ms per token,  1042.58 tokens per second)
0.00.729.131 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.729.132 I llama_perf_context_print:       total time =     132.33 ms /   129 tokens
0.00.729.546 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.081s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.964 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.474 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.478 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.482 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.482 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.483 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.484 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.484 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.484 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.485 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.338 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.284 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.286 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.286 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.287 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.287 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.288 I llama_model_loader: - type  f32:  194 tensors
0.00.026.288 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.288 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.289 I print_info: file format = GGUF V3 (latest)
0.00.026.290 I print_info: file type   = Q5_0
0.00.026.291 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.803 I load: special tokens cache size = 25
0.00.041.379 I load: token to piece cache size = 0.2984 MB
0.00.041.397 I print_info: arch             = gptneox
0.00.041.398 I print_info: vocab_only       = 0
0.00.041.398 I print_info: n_ctx_train      = 2048
0.00.041.398 I print_info: n_embd           = 2048
0.00.041.398 I print_info: n_layer          = 24
0.00.041.402 I print_info: n_head           = 16
0.00.041.402 I print_info: n_head_kv        = 16
0.00.041.402 I print_info: n_rot            = 32
0.00.041.403 I print_info: n_swa            = 0
0.00.041.403 I print_info: n_embd_head_k    = 128
0.00.041.403 I print_info: n_embd_head_v    = 128
0.00.041.403 I print_info: n_gqa            = 1
0.00.041.404 I print_info: n_embd_k_gqa     = 2048
0.00.041.407 I print_info: n_embd_v_gqa     = 2048
0.00.041.408 I print_info: f_norm_eps       = 1.0e-05
0.00.041.408 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.409 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.409 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.410 I print_info: f_logit_scale    = 0.0e+00
0.00.041.411 I print_info: n_ff             = 8192
0.00.041.411 I print_info: n_expert         = 0
0.00.041.411 I print_info: n_expert_used    = 0
0.00.041.411 I print_info: causal attn      = 1
0.00.041.411 I print_info: pooling type     = 0
0.00.041.411 I print_info: rope type        = 2
0.00.041.412 I print_info: rope scaling     = linear
0.00.041.413 I print_info: freq_base_train  = 10000.0
0.00.041.413 I print_info: freq_scale_train = 1
0.00.041.414 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.414 I print_info: rope_finetuned   = unknown
0.00.041.414 I print_info: ssm_d_conv       = 0
0.00.041.414 I print_info: ssm_d_inner      = 0
0.00.041.414 I print_info: ssm_d_state      = 0
0.00.041.414 I print_info: ssm_dt_rank      = 0
0.00.041.414 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.416 I print_info: model type       = 1.4B
0.00.041.416 I print_info: model params     = 1.41 B
0.00.041.417 I print_info: general.name     = 1.4B
0.00.041.417 I print_info: vocab type       = BPE
0.00.041.417 I print_info: n_vocab          = 50304
0.00.041.418 I print_info: n_merges         = 50009
0.00.041.418 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.418 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.418 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.418 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.418 I print_info: LF token         = 187 'Ċ'
0.00.041.419 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.419 I print_info: max token length = 1024
0.00.041.419 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.581.008 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.024 I load_tensors: offloading output layer to GPU
0.00.581.024 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.056 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.581.057 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.582.195 I llama_init_from_model: n_seq_max     = 1
0.00.582.198 I llama_init_from_model: n_ctx         = 128
0.00.582.198 I llama_init_from_model: n_ctx_per_seq = 128
0.00.582.199 I llama_init_from_model: n_batch       = 128
0.00.582.199 I llama_init_from_model: n_ubatch      = 128
0.00.582.200 I llama_init_from_model: flash_attn    = 0
0.00.582.202 I llama_init_from_model: freq_base     = 10000.0
0.00.582.202 I llama_init_from_model: freq_scale    = 1
0.00.582.203 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.582.205 I ggml_metal_init: allocating
0.00.582.312 I ggml_metal_init: found device: Apple M4
0.00.582.326 I ggml_metal_init: picking default device: Apple M4
0.00.584.352 I ggml_metal_init: using embedded metal library
0.00.591.010 I ggml_metal_init: GPU name:   Apple M4
0.00.591.016 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.591.017 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.591.018 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.591.018 I ggml_metal_init: simdgroup reduction   = true
0.00.591.019 I ggml_metal_init: simdgroup matrix mul. = true
0.00.591.019 I ggml_metal_init: has residency sets    = true
0.00.591.019 I ggml_metal_init: has bfloat            = true
0.00.591.019 I ggml_metal_init: use bfloat            = true
0.00.591.021 I ggml_metal_init: hasUnifiedMemory      = true
0.00.591.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.408 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.923 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.611.927 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.611.957 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.615.295 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.615.297 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.615.298 I llama_init_from_model: graph nodes  = 967
0.00.615.298 I llama_init_from_model: graph splits = 2
0.00.615.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.615.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.148 I 
0.00.646.229 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.258 I perplexity: tokenizing the input ..
0.00.652.818 I perplexity: tokenization took 6.557 ms
0.00.652.824 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.177 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.799.596 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.799.615 I llama_perf_context_print:        load time =     636.17 ms
0.00.799.616 I llama_perf_context_print: prompt eval time =     144.80 ms /   128 tokens (    1.13 ms per token,   883.96 tokens per second)
0.00.799.616 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.617 I llama_perf_context_print:       total time =     153.47 ms /   129 tokens
0.00.799.997 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.079s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.154 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.562 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.568 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.407 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.193 I llama_model_loader: - type  f32:  194 tensors
0.00.025.193 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.193 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.194 I print_info: file format = GGUF V3 (latest)
0.00.025.194 I print_info: file type   = Q5_1
0.00.025.195 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.412 I load: special tokens cache size = 25
0.00.039.946 I load: token to piece cache size = 0.2984 MB
0.00.039.964 I print_info: arch             = gptneox
0.00.039.965 I print_info: vocab_only       = 0
0.00.039.966 I print_info: n_ctx_train      = 2048
0.00.039.966 I print_info: n_embd           = 2048
0.00.039.966 I print_info: n_layer          = 24
0.00.039.970 I print_info: n_head           = 16
0.00.039.971 I print_info: n_head_kv        = 16
0.00.039.971 I print_info: n_rot            = 32
0.00.039.971 I print_info: n_swa            = 0
0.00.039.971 I print_info: n_embd_head_k    = 128
0.00.039.971 I print_info: n_embd_head_v    = 128
0.00.039.972 I print_info: n_gqa            = 1
0.00.039.972 I print_info: n_embd_k_gqa     = 2048
0.00.039.973 I print_info: n_embd_v_gqa     = 2048
0.00.039.974 I print_info: f_norm_eps       = 1.0e-05
0.00.039.975 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.976 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.980 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.980 I print_info: f_logit_scale    = 0.0e+00
0.00.039.981 I print_info: n_ff             = 8192
0.00.039.981 I print_info: n_expert         = 0
0.00.039.981 I print_info: n_expert_used    = 0
0.00.039.981 I print_info: causal attn      = 1
0.00.039.981 I print_info: pooling type     = 0
0.00.039.981 I print_info: rope type        = 2
0.00.039.982 I print_info: rope scaling     = linear
0.00.039.982 I print_info: freq_base_train  = 10000.0
0.00.039.982 I print_info: freq_scale_train = 1
0.00.039.982 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.983 I print_info: rope_finetuned   = unknown
0.00.039.983 I print_info: ssm_d_conv       = 0
0.00.039.983 I print_info: ssm_d_inner      = 0
0.00.039.983 I print_info: ssm_d_state      = 0
0.00.039.983 I print_info: ssm_dt_rank      = 0
0.00.039.983 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.983 I print_info: model type       = 1.4B
0.00.039.984 I print_info: model params     = 1.41 B
0.00.039.984 I print_info: general.name     = 1.4B
0.00.039.985 I print_info: vocab type       = BPE
0.00.039.985 I print_info: n_vocab          = 50304
0.00.039.987 I print_info: n_merges         = 50009
0.00.039.987 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.987 I print_info: LF token         = 187 'Ċ'
0.00.039.988 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.988 I print_info: max token length = 1024
0.00.039.988 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.702 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.713 I load_tensors: offloading output layer to GPU
0.00.628.714 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.753 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.628.757 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.630.495 I llama_init_from_model: n_seq_max     = 1
0.00.630.500 I llama_init_from_model: n_ctx         = 128
0.00.630.500 I llama_init_from_model: n_ctx_per_seq = 128
0.00.630.501 I llama_init_from_model: n_batch       = 128
0.00.630.501 I llama_init_from_model: n_ubatch      = 128
0.00.630.502 I llama_init_from_model: flash_attn    = 0
0.00.630.504 I llama_init_from_model: freq_base     = 10000.0
0.00.630.505 I llama_init_from_model: freq_scale    = 1
0.00.630.506 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.508 I ggml_metal_init: allocating
0.00.630.580 I ggml_metal_init: found device: Apple M4
0.00.630.594 I ggml_metal_init: picking default device: Apple M4
0.00.632.179 I ggml_metal_init: using embedded metal library
0.00.638.533 I ggml_metal_init: GPU name:   Apple M4
0.00.638.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.538 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.539 I ggml_metal_init: simdgroup reduction   = true
0.00.638.540 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.540 I ggml_metal_init: has residency sets    = true
0.00.638.540 I ggml_metal_init: has bfloat            = true
0.00.638.540 I ggml_metal_init: use bfloat            = true
0.00.638.541 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.494 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.659.498 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.659.524 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.662.695 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.662.697 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.662.697 I llama_init_from_model: graph nodes  = 967
0.00.662.698 I llama_init_from_model: graph splits = 2
0.00.662.700 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.662.700 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.696.017 I 
0.00.696.117 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.696.144 I perplexity: tokenizing the input ..
0.00.703.019 I perplexity: tokenization took 6.872 ms
0.00.703.027 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.826 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.847.161 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.847.179 I llama_perf_context_print:        load time =     686.86 ms
0.00.847.180 I llama_perf_context_print: prompt eval time =     141.93 ms /   128 tokens (    1.11 ms per token,   901.86 tokens per second)
0.00.847.181 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.181 I llama_perf_context_print:       total time =     151.17 ms /   129 tokens
0.00.847.615 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.080s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.155 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.130 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.130 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.132 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.135 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.137 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.139 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.140 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.881 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.597 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.598 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.599 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.600 I llama_model_loader: - type  f32:  194 tensors
0.00.025.600 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.600 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.600 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.601 I print_info: file format = GGUF V3 (latest)
0.00.025.606 I print_info: file type   = Q2_K - Medium
0.00.025.608 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.123 I load: special tokens cache size = 25
0.00.040.650 I load: token to piece cache size = 0.2984 MB
0.00.040.668 I print_info: arch             = gptneox
0.00.040.669 I print_info: vocab_only       = 0
0.00.040.669 I print_info: n_ctx_train      = 2048
0.00.040.670 I print_info: n_embd           = 2048
0.00.040.670 I print_info: n_layer          = 24
0.00.040.673 I print_info: n_head           = 16
0.00.040.674 I print_info: n_head_kv        = 16
0.00.040.674 I print_info: n_rot            = 32
0.00.040.676 I print_info: n_swa            = 0
0.00.040.676 I print_info: n_embd_head_k    = 128
0.00.040.676 I print_info: n_embd_head_v    = 128
0.00.040.679 I print_info: n_gqa            = 1
0.00.040.679 I print_info: n_embd_k_gqa     = 2048
0.00.040.680 I print_info: n_embd_v_gqa     = 2048
0.00.040.681 I print_info: f_norm_eps       = 1.0e-05
0.00.040.681 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.681 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.681 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.681 I print_info: f_logit_scale    = 0.0e+00
0.00.040.682 I print_info: n_ff             = 8192
0.00.040.682 I print_info: n_expert         = 0
0.00.040.682 I print_info: n_expert_used    = 0
0.00.040.682 I print_info: causal attn      = 1
0.00.040.683 I print_info: pooling type     = 0
0.00.040.683 I print_info: rope type        = 2
0.00.040.683 I print_info: rope scaling     = linear
0.00.040.683 I print_info: freq_base_train  = 10000.0
0.00.040.683 I print_info: freq_scale_train = 1
0.00.040.684 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.684 I print_info: rope_finetuned   = unknown
0.00.040.684 I print_info: ssm_d_conv       = 0
0.00.040.684 I print_info: ssm_d_inner      = 0
0.00.040.684 I print_info: ssm_d_state      = 0
0.00.040.684 I print_info: ssm_dt_rank      = 0
0.00.040.684 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.685 I print_info: model type       = 1.4B
0.00.040.685 I print_info: model params     = 1.41 B
0.00.040.685 I print_info: general.name     = 1.4B
0.00.040.686 I print_info: vocab type       = BPE
0.00.040.686 I print_info: n_vocab          = 50304
0.00.040.686 I print_info: n_merges         = 50009
0.00.040.686 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.686 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.686 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.687 I print_info: LF token         = 187 'Ċ'
0.00.040.687 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.687 I print_info: max token length = 1024
0.00.040.688 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.354.676 I load_tensors: offloading 24 repeating layers to GPU
0.00.354.689 I load_tensors: offloading output layer to GPU
0.00.354.689 I load_tensors: offloaded 25/25 layers to GPU
0.00.354.724 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.354.725 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.356.427 I llama_init_from_model: n_seq_max     = 1
0.00.356.430 I llama_init_from_model: n_ctx         = 128
0.00.356.431 I llama_init_from_model: n_ctx_per_seq = 128
0.00.356.431 I llama_init_from_model: n_batch       = 128
0.00.356.432 I llama_init_from_model: n_ubatch      = 128
0.00.356.432 I llama_init_from_model: flash_attn    = 0
0.00.356.434 I llama_init_from_model: freq_base     = 10000.0
0.00.356.434 I llama_init_from_model: freq_scale    = 1
0.00.356.435 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.356.438 I ggml_metal_init: allocating
0.00.356.553 I ggml_metal_init: found device: Apple M4
0.00.356.567 I ggml_metal_init: picking default device: Apple M4
0.00.358.519 I ggml_metal_init: using embedded metal library
0.00.363.966 I ggml_metal_init: GPU name:   Apple M4
0.00.363.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.979 I ggml_metal_init: simdgroup reduction   = true
0.00.363.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.980 I ggml_metal_init: has residency sets    = true
0.00.363.980 I ggml_metal_init: has bfloat            = true
0.00.363.981 I ggml_metal_init: use bfloat            = true
0.00.363.982 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.280 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.389.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.389.019 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.389.056 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.392.416 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.392.418 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.392.418 I llama_init_from_model: graph nodes  = 967
0.00.392.419 I llama_init_from_model: graph splits = 2
0.00.392.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.392.422 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.421.736 I 
0.00.421.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.421.839 I perplexity: tokenizing the input ..
0.00.428.613 I perplexity: tokenization took 6.772 ms
0.00.428.619 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.935 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.231 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.246 I llama_perf_context_print:        load time =     411.57 ms
0.00.562.247 I llama_perf_context_print: prompt eval time =     131.44 ms /   128 tokens (    1.03 ms per token,   973.84 tokens per second)
0.00.562.248 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.248 I llama_perf_context_print:       total time =     140.51 ms /   129 tokens
0.00.562.633 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.082s
sys	0m0.086s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.887 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.308 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.315 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.317 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.317 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.317 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.318 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.319 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.320 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.321 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.321 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.321 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.322 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.324 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.325 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.325 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.127 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.113 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.907 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.909 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.909 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.910 I llama_model_loader: - type  f32:  194 tensors
0.00.024.910 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.911 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.911 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.911 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.912 I print_info: file format = GGUF V3 (latest)
0.00.024.913 I print_info: file type   = Q3_K - Medium
0.00.024.914 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.427 I load: special tokens cache size = 25
0.00.039.917 I load: token to piece cache size = 0.2984 MB
0.00.039.934 I print_info: arch             = gptneox
0.00.039.935 I print_info: vocab_only       = 0
0.00.039.936 I print_info: n_ctx_train      = 2048
0.00.039.936 I print_info: n_embd           = 2048
0.00.039.936 I print_info: n_layer          = 24
0.00.039.940 I print_info: n_head           = 16
0.00.039.942 I print_info: n_head_kv        = 16
0.00.039.942 I print_info: n_rot            = 32
0.00.039.942 I print_info: n_swa            = 0
0.00.039.943 I print_info: n_embd_head_k    = 128
0.00.039.943 I print_info: n_embd_head_v    = 128
0.00.039.943 I print_info: n_gqa            = 1
0.00.039.944 I print_info: n_embd_k_gqa     = 2048
0.00.039.945 I print_info: n_embd_v_gqa     = 2048
0.00.039.950 I print_info: f_norm_eps       = 1.0e-05
0.00.039.950 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.950 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.950 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.951 I print_info: f_logit_scale    = 0.0e+00
0.00.039.951 I print_info: n_ff             = 8192
0.00.039.951 I print_info: n_expert         = 0
0.00.039.955 I print_info: n_expert_used    = 0
0.00.039.956 I print_info: causal attn      = 1
0.00.039.956 I print_info: pooling type     = 0
0.00.039.956 I print_info: rope type        = 2
0.00.039.956 I print_info: rope scaling     = linear
0.00.039.957 I print_info: freq_base_train  = 10000.0
0.00.039.957 I print_info: freq_scale_train = 1
0.00.039.957 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.957 I print_info: rope_finetuned   = unknown
0.00.039.957 I print_info: ssm_d_conv       = 0
0.00.039.957 I print_info: ssm_d_inner      = 0
0.00.039.957 I print_info: ssm_d_state      = 0
0.00.039.957 I print_info: ssm_dt_rank      = 0
0.00.039.958 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.958 I print_info: model type       = 1.4B
0.00.039.958 I print_info: model params     = 1.41 B
0.00.039.958 I print_info: general.name     = 1.4B
0.00.039.959 I print_info: vocab type       = BPE
0.00.039.959 I print_info: n_vocab          = 50304
0.00.039.959 I print_info: n_merges         = 50009
0.00.039.959 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.959 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.960 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.960 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.960 I print_info: LF token         = 187 'Ċ'
0.00.039.960 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.960 I print_info: max token length = 1024
0.00.039.961 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.440.859 I load_tensors: offloading 24 repeating layers to GPU
0.00.440.875 I load_tensors: offloading output layer to GPU
0.00.440.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.440.907 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.440.909 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.568 I llama_init_from_model: n_seq_max     = 1
0.00.442.572 I llama_init_from_model: n_ctx         = 128
0.00.442.572 I llama_init_from_model: n_ctx_per_seq = 128
0.00.442.573 I llama_init_from_model: n_batch       = 128
0.00.442.573 I llama_init_from_model: n_ubatch      = 128
0.00.442.574 I llama_init_from_model: flash_attn    = 0
0.00.442.576 I llama_init_from_model: freq_base     = 10000.0
0.00.442.576 I llama_init_from_model: freq_scale    = 1
0.00.442.577 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.442.580 I ggml_metal_init: allocating
0.00.442.684 I ggml_metal_init: found device: Apple M4
0.00.442.699 I ggml_metal_init: picking default device: Apple M4
0.00.444.508 I ggml_metal_init: using embedded metal library
0.00.450.142 I ggml_metal_init: GPU name:   Apple M4
0.00.450.158 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.159 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.160 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.160 I ggml_metal_init: simdgroup reduction   = true
0.00.450.161 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.161 I ggml_metal_init: has residency sets    = true
0.00.450.161 I ggml_metal_init: has bfloat            = true
0.00.450.162 I ggml_metal_init: use bfloat            = true
0.00.450.164 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.296 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.474.851 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.474.858 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.474.890 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.478.298 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.478.300 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.478.300 I llama_init_from_model: graph nodes  = 967
0.00.478.301 I llama_init_from_model: graph splits = 2
0.00.478.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.478.304 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.017 I 
0.00.507.094 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.122 I perplexity: tokenizing the input ..
0.00.514.412 I perplexity: tokenization took 7.287 ms
0.00.514.420 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.887 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.657.222 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.657.235 I llama_perf_context_print:        load time =     498.12 ms
0.00.657.236 I llama_perf_context_print: prompt eval time =     140.55 ms /   128 tokens (    1.10 ms per token,   910.73 tokens per second)
0.00.657.236 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.657.236 I llama_perf_context_print:       total time =     150.22 ms /   129 tokens
0.00.657.620 I ggml_metal_free: deallocating

real	0m0.671s
user	0m0.082s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.266 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.269 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.272 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.273 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.275 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.275 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.276 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.165 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.088 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.090 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.090 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.091 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.091 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.092 I llama_model_loader: - type  f32:  194 tensors
0.00.025.092 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.092 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.093 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.093 I print_info: file format = GGUF V3 (latest)
0.00.025.100 I print_info: file type   = Q4_K - Medium
0.00.025.101 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.596 I load: special tokens cache size = 25
0.00.040.008 I load: token to piece cache size = 0.2984 MB
0.00.040.026 I print_info: arch             = gptneox
0.00.040.027 I print_info: vocab_only       = 0
0.00.040.027 I print_info: n_ctx_train      = 2048
0.00.040.027 I print_info: n_embd           = 2048
0.00.040.027 I print_info: n_layer          = 24
0.00.040.030 I print_info: n_head           = 16
0.00.040.031 I print_info: n_head_kv        = 16
0.00.040.031 I print_info: n_rot            = 32
0.00.040.031 I print_info: n_swa            = 0
0.00.040.031 I print_info: n_embd_head_k    = 128
0.00.040.031 I print_info: n_embd_head_v    = 128
0.00.040.032 I print_info: n_gqa            = 1
0.00.040.033 I print_info: n_embd_k_gqa     = 2048
0.00.040.033 I print_info: n_embd_v_gqa     = 2048
0.00.040.034 I print_info: f_norm_eps       = 1.0e-05
0.00.040.034 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.034 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.034 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.035 I print_info: f_logit_scale    = 0.0e+00
0.00.040.035 I print_info: n_ff             = 8192
0.00.040.036 I print_info: n_expert         = 0
0.00.040.037 I print_info: n_expert_used    = 0
0.00.040.037 I print_info: causal attn      = 1
0.00.040.037 I print_info: pooling type     = 0
0.00.040.037 I print_info: rope type        = 2
0.00.040.037 I print_info: rope scaling     = linear
0.00.040.038 I print_info: freq_base_train  = 10000.0
0.00.040.038 I print_info: freq_scale_train = 1
0.00.040.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.038 I print_info: rope_finetuned   = unknown
0.00.040.038 I print_info: ssm_d_conv       = 0
0.00.040.038 I print_info: ssm_d_inner      = 0
0.00.040.041 I print_info: ssm_d_state      = 0
0.00.040.041 I print_info: ssm_dt_rank      = 0
0.00.040.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.041 I print_info: model type       = 1.4B
0.00.040.041 I print_info: model params     = 1.41 B
0.00.040.041 I print_info: general.name     = 1.4B
0.00.040.042 I print_info: vocab type       = BPE
0.00.040.042 I print_info: n_vocab          = 50304
0.00.040.042 I print_info: n_merges         = 50009
0.00.040.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: LF token         = 187 'Ċ'
0.00.040.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.045 I print_info: max token length = 1024
0.00.040.045 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.544.824 I load_tensors: offloading 24 repeating layers to GPU
0.00.544.832 I load_tensors: offloading output layer to GPU
0.00.544.833 I load_tensors: offloaded 25/25 layers to GPU
0.00.544.867 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.544.868 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.546.266 I llama_init_from_model: n_seq_max     = 1
0.00.546.268 I llama_init_from_model: n_ctx         = 128
0.00.546.269 I llama_init_from_model: n_ctx_per_seq = 128
0.00.546.269 I llama_init_from_model: n_batch       = 128
0.00.546.270 I llama_init_from_model: n_ubatch      = 128
0.00.546.270 I llama_init_from_model: flash_attn    = 0
0.00.546.272 I llama_init_from_model: freq_base     = 10000.0
0.00.546.273 I llama_init_from_model: freq_scale    = 1
0.00.546.273 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.546.277 I ggml_metal_init: allocating
0.00.546.361 I ggml_metal_init: found device: Apple M4
0.00.546.374 I ggml_metal_init: picking default device: Apple M4
0.00.548.145 I ggml_metal_init: using embedded metal library
0.00.554.999 I ggml_metal_init: GPU name:   Apple M4
0.00.555.007 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.555.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.555.008 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.555.009 I ggml_metal_init: simdgroup reduction   = true
0.00.555.009 I ggml_metal_init: simdgroup matrix mul. = true
0.00.555.009 I ggml_metal_init: has residency sets    = true
0.00.555.010 I ggml_metal_init: has bfloat            = true
0.00.555.010 I ggml_metal_init: use bfloat            = true
0.00.555.011 I ggml_metal_init: hasUnifiedMemory      = true
0.00.555.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.573.558 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.577.168 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.577.175 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.577.219 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.580.400 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.580.402 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.580.403 I llama_init_from_model: graph nodes  = 967
0.00.580.403 I llama_init_from_model: graph splits = 2
0.00.580.406 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.580.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.607.632 I 
0.00.607.713 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.607.742 I perplexity: tokenizing the input ..
0.00.614.673 I perplexity: tokenization took 6.928 ms
0.00.614.690 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.016 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.748.328 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.748.340 I llama_perf_context_print:        load time =     598.61 ms
0.00.748.341 I llama_perf_context_print: prompt eval time =     131.61 ms /   128 tokens (    1.03 ms per token,   972.58 tokens per second)
0.00.748.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.748.342 I llama_perf_context_print:       total time =     140.71 ms /   129 tokens
0.00.748.681 I ggml_metal_free: deallocating

real	0m0.762s
user	0m0.081s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.698 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.711 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.724 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.553 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.554 I llama_model_loader: - type  f32:  194 tensors
0.00.026.554 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.555 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.555 I print_info: file format = GGUF V3 (latest)
0.00.026.556 I print_info: file type   = Q5_K - Medium
0.00.026.557 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.006 I load: special tokens cache size = 25
0.00.041.532 I load: token to piece cache size = 0.2984 MB
0.00.041.549 I print_info: arch             = gptneox
0.00.041.550 I print_info: vocab_only       = 0
0.00.041.550 I print_info: n_ctx_train      = 2048
0.00.041.550 I print_info: n_embd           = 2048
0.00.041.550 I print_info: n_layer          = 24
0.00.041.554 I print_info: n_head           = 16
0.00.041.554 I print_info: n_head_kv        = 16
0.00.041.555 I print_info: n_rot            = 32
0.00.041.555 I print_info: n_swa            = 0
0.00.041.555 I print_info: n_embd_head_k    = 128
0.00.041.555 I print_info: n_embd_head_v    = 128
0.00.041.556 I print_info: n_gqa            = 1
0.00.041.556 I print_info: n_embd_k_gqa     = 2048
0.00.041.560 I print_info: n_embd_v_gqa     = 2048
0.00.041.560 I print_info: f_norm_eps       = 1.0e-05
0.00.041.561 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.561 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.563 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.563 I print_info: f_logit_scale    = 0.0e+00
0.00.041.563 I print_info: n_ff             = 8192
0.00.041.564 I print_info: n_expert         = 0
0.00.041.564 I print_info: n_expert_used    = 0
0.00.041.564 I print_info: causal attn      = 1
0.00.041.564 I print_info: pooling type     = 0
0.00.041.564 I print_info: rope type        = 2
0.00.041.565 I print_info: rope scaling     = linear
0.00.041.565 I print_info: freq_base_train  = 10000.0
0.00.041.566 I print_info: freq_scale_train = 1
0.00.041.566 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.566 I print_info: rope_finetuned   = unknown
0.00.041.566 I print_info: ssm_d_conv       = 0
0.00.041.567 I print_info: ssm_d_inner      = 0
0.00.041.568 I print_info: ssm_d_state      = 0
0.00.041.568 I print_info: ssm_dt_rank      = 0
0.00.041.568 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.568 I print_info: model type       = 1.4B
0.00.041.568 I print_info: model params     = 1.41 B
0.00.041.568 I print_info: general.name     = 1.4B
0.00.041.569 I print_info: vocab type       = BPE
0.00.041.569 I print_info: n_vocab          = 50304
0.00.041.569 I print_info: n_merges         = 50009
0.00.041.570 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.570 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.570 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.571 I print_info: LF token         = 187 'Ċ'
0.00.041.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.572 I print_info: max token length = 1024
0.00.041.572 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.479.535 I load_tensors: offloading 24 repeating layers to GPU
0.00.479.546 I load_tensors: offloading output layer to GPU
0.00.479.546 I load_tensors: offloaded 25/25 layers to GPU
0.00.479.574 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.479.576 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.481.375 I llama_init_from_model: n_seq_max     = 1
0.00.481.379 I llama_init_from_model: n_ctx         = 128
0.00.481.380 I llama_init_from_model: n_ctx_per_seq = 128
0.00.481.380 I llama_init_from_model: n_batch       = 128
0.00.481.381 I llama_init_from_model: n_ubatch      = 128
0.00.481.381 I llama_init_from_model: flash_attn    = 0
0.00.481.383 I llama_init_from_model: freq_base     = 10000.0
0.00.481.384 I llama_init_from_model: freq_scale    = 1
0.00.481.385 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.481.388 I ggml_metal_init: allocating
0.00.481.459 I ggml_metal_init: found device: Apple M4
0.00.481.472 I ggml_metal_init: picking default device: Apple M4
0.00.482.909 I ggml_metal_init: using embedded metal library
0.00.489.344 I ggml_metal_init: GPU name:   Apple M4
0.00.489.348 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.489.349 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.489.350 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.489.354 I ggml_metal_init: simdgroup reduction   = true
0.00.489.354 I ggml_metal_init: simdgroup matrix mul. = true
0.00.489.354 I ggml_metal_init: has residency sets    = true
0.00.489.355 I ggml_metal_init: has bfloat            = true
0.00.489.355 I ggml_metal_init: use bfloat            = true
0.00.489.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.489.367 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.507.057 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.510.489 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.510.492 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.510.526 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.513.725 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.513.727 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.513.728 I llama_init_from_model: graph nodes  = 967
0.00.513.728 I llama_init_from_model: graph splits = 2
0.00.513.730 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.513.731 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.709 I 
0.00.544.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.816 I perplexity: tokenizing the input ..
0.00.551.946 I perplexity: tokenization took 7.129 ms
0.00.551.955 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.688.318 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.689.652 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.689.666 I llama_perf_context_print:        load time =     535.00 ms
0.00.689.667 I llama_perf_context_print: prompt eval time =     136.13 ms /   128 tokens (    1.06 ms per token,   940.24 tokens per second)
0.00.689.668 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.689.668 I llama_perf_context_print:       total time =     144.96 ms /   129 tokens
0.00.690.020 I ggml_metal_free: deallocating

real	0m0.706s
user	0m0.080s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.991 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.309 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.314 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.316 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.322 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.322 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.325 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.326 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.326 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.326 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.327 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.017 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.019 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.019 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.020 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.021 I llama_model_loader: - type  f32:  194 tensors
0.00.025.021 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.022 I print_info: file format = GGUF V3 (latest)
0.00.025.023 I print_info: file type   = Q6_K
0.00.025.024 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.235 I load: special tokens cache size = 25
0.00.039.861 I load: token to piece cache size = 0.2984 MB
0.00.039.878 I print_info: arch             = gptneox
0.00.039.879 I print_info: vocab_only       = 0
0.00.039.879 I print_info: n_ctx_train      = 2048
0.00.039.879 I print_info: n_embd           = 2048
0.00.039.879 I print_info: n_layer          = 24
0.00.039.884 I print_info: n_head           = 16
0.00.039.884 I print_info: n_head_kv        = 16
0.00.039.885 I print_info: n_rot            = 32
0.00.039.885 I print_info: n_swa            = 0
0.00.039.885 I print_info: n_embd_head_k    = 128
0.00.039.885 I print_info: n_embd_head_v    = 128
0.00.039.886 I print_info: n_gqa            = 1
0.00.039.886 I print_info: n_embd_k_gqa     = 2048
0.00.039.887 I print_info: n_embd_v_gqa     = 2048
0.00.039.887 I print_info: f_norm_eps       = 1.0e-05
0.00.039.890 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.890 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.890 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.891 I print_info: f_logit_scale    = 0.0e+00
0.00.039.891 I print_info: n_ff             = 8192
0.00.039.891 I print_info: n_expert         = 0
0.00.039.891 I print_info: n_expert_used    = 0
0.00.039.892 I print_info: causal attn      = 1
0.00.039.892 I print_info: pooling type     = 0
0.00.039.892 I print_info: rope type        = 2
0.00.039.893 I print_info: rope scaling     = linear
0.00.039.893 I print_info: freq_base_train  = 10000.0
0.00.039.893 I print_info: freq_scale_train = 1
0.00.039.893 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.894 I print_info: rope_finetuned   = unknown
0.00.039.894 I print_info: ssm_d_conv       = 0
0.00.039.894 I print_info: ssm_d_inner      = 0
0.00.039.894 I print_info: ssm_d_state      = 0
0.00.039.894 I print_info: ssm_dt_rank      = 0
0.00.039.894 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.894 I print_info: model type       = 1.4B
0.00.039.894 I print_info: model params     = 1.41 B
0.00.039.895 I print_info: general.name     = 1.4B
0.00.039.895 I print_info: vocab type       = BPE
0.00.039.895 I print_info: n_vocab          = 50304
0.00.039.895 I print_info: n_merges         = 50009
0.00.039.896 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.896 I print_info: LF token         = 187 'Ċ'
0.00.039.897 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.897 I print_info: max token length = 1024
0.00.039.897 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.818 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.827 I load_tensors: offloading output layer to GPU
0.00.632.828 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.861 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.632.864 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.634.303 I llama_init_from_model: n_seq_max     = 1
0.00.634.306 I llama_init_from_model: n_ctx         = 128
0.00.634.306 I llama_init_from_model: n_ctx_per_seq = 128
0.00.634.307 I llama_init_from_model: n_batch       = 128
0.00.634.307 I llama_init_from_model: n_ubatch      = 128
0.00.634.307 I llama_init_from_model: flash_attn    = 0
0.00.634.308 I llama_init_from_model: freq_base     = 10000.0
0.00.634.309 I llama_init_from_model: freq_scale    = 1
0.00.634.310 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.634.311 I ggml_metal_init: allocating
0.00.634.374 I ggml_metal_init: found device: Apple M4
0.00.634.387 I ggml_metal_init: picking default device: Apple M4
0.00.635.885 I ggml_metal_init: using embedded metal library
0.00.641.677 I ggml_metal_init: GPU name:   Apple M4
0.00.641.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.641.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.641.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.641.682 I ggml_metal_init: simdgroup reduction   = true
0.00.641.682 I ggml_metal_init: simdgroup matrix mul. = true
0.00.641.682 I ggml_metal_init: has residency sets    = true
0.00.641.683 I ggml_metal_init: has bfloat            = true
0.00.641.683 I ggml_metal_init: use bfloat            = true
0.00.641.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.641.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.201 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.679 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.661.687 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.723 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.877 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.664.879 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.664.879 I llama_init_from_model: graph nodes  = 967
0.00.664.879 I llama_init_from_model: graph splits = 2
0.00.664.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.664.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.285 I 
0.00.702.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.394 I perplexity: tokenizing the input ..
0.00.708.985 I perplexity: tokenization took 6.588 ms
0.00.708.991 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.840.323 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.841.665 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.841.676 I llama_perf_context_print:        load time =     693.28 ms
0.00.841.677 I llama_perf_context_print: prompt eval time =     130.38 ms /   128 tokens (    1.02 ms per token,   981.75 tokens per second)
0.00.841.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.678 I llama_perf_context_print:       total time =     139.40 ms /   129 tokens
0.00.842.017 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.078s
sys	0m0.135s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.304 I build: 4829 (074c4fd3) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.981 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.903 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.909 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.911 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.912 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.919 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.924 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.925 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.926 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.929 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.934 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.934 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.129 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.218 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.056.220 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.222 I llama_model_loader: - type  f32:  194 tensors
0.00.056.223 I llama_model_loader: - type  f16:   98 tensors
0.00.056.224 I print_info: file format = GGUF V3 (latest)
0.00.056.225 I print_info: file type   = all F32 (guessed)
0.00.056.226 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.069.024 I load: special tokens cache size = 25
0.00.077.418 I load: token to piece cache size = 0.2984 MB
0.00.077.433 I print_info: arch             = gptneox
0.00.077.434 I print_info: vocab_only       = 0
0.00.077.434 I print_info: n_ctx_train      = 2048
0.00.077.435 I print_info: n_embd           = 2048
0.00.077.435 I print_info: n_layer          = 24
0.00.077.438 I print_info: n_head           = 16
0.00.077.439 I print_info: n_head_kv        = 16
0.00.077.439 I print_info: n_rot            = 32
0.00.077.439 I print_info: n_swa            = 0
0.00.077.440 I print_info: n_embd_head_k    = 128
0.00.077.440 I print_info: n_embd_head_v    = 128
0.00.077.440 I print_info: n_gqa            = 1
0.00.077.443 I print_info: n_embd_k_gqa     = 2048
0.00.077.444 I print_info: n_embd_v_gqa     = 2048
0.00.077.444 I print_info: f_norm_eps       = 1.0e-05
0.00.077.445 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.445 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.445 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.445 I print_info: f_logit_scale    = 0.0e+00
0.00.077.446 I print_info: n_ff             = 8192
0.00.077.446 I print_info: n_expert         = 0
0.00.077.446 I print_info: n_expert_used    = 0
0.00.077.446 I print_info: causal attn      = 1
0.00.077.447 I print_info: pooling type     = 0
0.00.077.447 I print_info: rope type        = 2
0.00.077.447 I print_info: rope scaling     = linear
0.00.077.447 I print_info: freq_base_train  = 10000.0
0.00.077.448 I print_info: freq_scale_train = 1
0.00.077.448 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.448 I print_info: rope_finetuned   = unknown
0.00.077.448 I print_info: ssm_d_conv       = 0
0.00.077.449 I print_info: ssm_d_inner      = 0
0.00.077.449 I print_info: ssm_d_state      = 0
0.00.077.449 I print_info: ssm_dt_rank      = 0
0.00.077.449 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.449 I print_info: model type       = 1.4B
0.00.077.450 I print_info: model params     = 1.41 B
0.00.077.450 I print_info: general.name     = 1.4B
0.00.077.450 I print_info: vocab type       = BPE
0.00.077.451 I print_info: n_vocab          = 50304
0.00.077.451 I print_info: n_merges         = 50009
0.00.077.451 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.451 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.451 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.452 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.452 I print_info: LF token         = 187 'Ċ'
0.00.077.453 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.453 I print_info: max token length = 1024
0.00.077.454 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.420.285 I load_tensors: offloading 24 repeating layers to GPU
0.01.420.291 I load_tensors: offloading output layer to GPU
0.01.420.292 I load_tensors: offloaded 25/25 layers to GPU
0.01.420.316 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.420.317 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.421.420 I llama_init_from_model: n_seq_max     = 1
0.01.421.422 I llama_init_from_model: n_ctx         = 128
0.01.421.422 I llama_init_from_model: n_ctx_per_seq = 128
0.01.421.422 I llama_init_from_model: n_batch       = 128
0.01.421.422 I llama_init_from_model: n_ubatch      = 128
0.01.421.423 I llama_init_from_model: flash_attn    = 0
0.01.421.423 I llama_init_from_model: freq_base     = 10000.0
0.01.421.424 I llama_init_from_model: freq_scale    = 1
0.01.421.424 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.421.426 I ggml_metal_init: allocating
0.01.421.531 I ggml_metal_init: found device: Apple M4
0.01.421.546 I ggml_metal_init: picking default device: Apple M4
0.01.422.887 I ggml_metal_init: using embedded metal library
0.01.427.183 I ggml_metal_init: GPU name:   Apple M4
0.01.427.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.427.186 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.427.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.427.187 I ggml_metal_init: simdgroup reduction   = true
0.01.427.188 I ggml_metal_init: simdgroup matrix mul. = true
0.01.427.188 I ggml_metal_init: has residency sets    = true
0.01.427.188 I ggml_metal_init: has bfloat            = true
0.01.427.188 I ggml_metal_init: use bfloat            = true
0.01.427.189 I ggml_metal_init: hasUnifiedMemory      = true
0.01.427.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.441.024 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.442.922 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.442.925 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.442.940 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.444.704 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.444.705 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.444.706 I llama_init_from_model: graph nodes  = 967
0.01.444.706 I llama_init_from_model: graph splits = 2
0.01.444.707 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.444.708 I 
0.01.444.751 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.444.753 I compute_imatrix: tokenizing the input ..
0.01.449.328 I compute_imatrix: tokenization took 4.574 ms
0.01.449.330 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.713.051 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.715.938 I llama_perf_context_print:        load time =    1687.06 ms
0.01.715.939 I llama_perf_context_print: prompt eval time =     261.84 ms /   128 tokens (    2.05 ms per token,   488.85 tokens per second)
0.01.715.940 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.715.940 I llama_perf_context_print:       total time =    1689.94 ms /   129 tokens
0.01.716.531 I ggml_metal_free: deallocating

real	0m1.905s
user	0m0.127s
sys	0m0.253s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4829 (074c4fd3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105d04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105d04a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105d04e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105d052e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105d05750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105d05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105d06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105d064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105d06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105d06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105d071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105d07890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105d083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105d08b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105d09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105d09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105d0a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105d0a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105d0aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105d0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105d0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105d0c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105d0cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105d0d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105d0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105d0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105d0e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105d0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105d0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105d0f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105d0f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105d0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105d10060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105d10320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105d10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105d11040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105d11300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105d11770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105d11be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105d12050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105d124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105d12930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105d12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105d13210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105d13680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105d13af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105d13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105d14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105d14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105d150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105d15530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105d159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105d15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105d16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105d166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105d16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105d17240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105d17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105d17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105d18040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105d18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105d18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105d18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105d19100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105d19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105d19b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105d1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105d1a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105d1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105d1af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105d1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105d1b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105d1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x105d1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x105d1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x105d1ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x105d1d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x105d1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x105d1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x105d1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x105d1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x105d1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x105d1f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x105d1fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x105d20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x105d20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x105d20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x105d212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x105d21850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x105d21e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x105d223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x105d22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x105d22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x105d234c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x105d23a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x105d24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x105d245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x105d14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x105d24d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x105d251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x105d25610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x105d25bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x105d26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x105d26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x105d26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x105d27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x105d27830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x105d27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x105d28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x105d28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x105d28ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x105d294a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x105d29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105d2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105d2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105d2aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105d2af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105d2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105d2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105d2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105d2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105d2c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105d2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105d2d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105d2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105d2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105d2e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105d2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105d2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105d2f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105d2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105d2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105d2ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105d30400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105d30900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105d30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105d31300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105d31800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105d31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105d32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105d32700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105d32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105d33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105d33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105d33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105d34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105d34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105d34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105d34f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105d35400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105d35900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105d35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105d36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105d36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105d36d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105d37200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105d37700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105d37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105d38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105d38600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105d38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105d39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105d39500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105d39a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105d39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105d3a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105d3a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105d3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105d3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105d3b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105d3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105d3c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105d3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105d3cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105d3d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105d3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105d3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105d3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105d3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105d3ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105d3ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105d3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105d3f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105d3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105d40300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105d40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105d40d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105d41200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105d41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105d41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105d42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105d42600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105d42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105d43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105d435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105d43b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105d44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105d446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105d44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105d452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105d458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x105d460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x105d46580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105d46840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105d46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x105d47460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105d47c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105d480f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105d48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105d48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105d491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105d49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105d49c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105d4a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105d4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105d4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105d4b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105d4b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105d4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105d4c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105d4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105d4cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105d4d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105d4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105d4dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105d4e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105d4e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105d4ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105d4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105d4f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105d4fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105d50170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105d506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105d50c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105d51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105d516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105d51c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105d52150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105d526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105d52bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105d53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105d53690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105d53be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105d54130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105d54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105d54bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105d55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105d55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105d55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105d56110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105d56660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105d56bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105d57100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105d57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105d57ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105d580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105d58640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105d58b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105d590e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105d59630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105d59b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105d5a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105d5a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105d5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105d5b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105d5b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105d5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x105d5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x105d5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105d5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105d5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105d5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105d5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105d5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105d5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105d5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105d5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105d5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105d5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105d5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105d5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105d600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x105d60560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x105d60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x105d60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x105d61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x105d617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x105d61c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x105d62120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x105d625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x105d62a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x105d62f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105d63450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105d63b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105d64290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105d649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105d650d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105d65390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x105d65b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105d65e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105d66450 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.622.466 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.622.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10da04ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10da05150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10da055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10da05a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10da05ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10da06310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10da06780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10da06bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10da07060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10da074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10da07940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10da08000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10da08b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10da092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10da09ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10da0a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10da0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10da0b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10da0b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10da0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10da0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10da0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10da0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10da0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10da0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10da0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10da0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10da0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10da0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10da0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10da0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10da0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10da103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10da10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10da10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10da10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10da113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10da11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10da11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10da12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10da12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10da129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10da12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10da132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10da13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10da13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10da14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10da14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10da14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10da14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10da151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10da15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10da15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10da15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10da163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10da16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10da16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10da17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10da176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10da17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10da17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10da18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10da188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10da18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10da19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10da19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10da19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10da19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10da1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10da1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10da1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10da1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10da1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10da1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10da1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10da1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10da1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10da1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10da1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10da1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10da1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10da1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10da1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10da1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10da1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10da1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10da1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10da1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10da1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10da20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10da204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10da20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10da20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10da21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10da216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10da21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10da21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10da22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10da22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10da22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10da23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10da235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10da23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10da23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10da24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10da24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10da24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10da25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10da254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10da25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10da25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10da26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10da26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10da26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10da26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10da273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10da27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10da27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10da28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10da285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10da28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10da28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10da292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10da29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10da29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10da2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10da2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10da2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10da2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10da2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10da2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10da2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10da2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10da2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10da2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10da2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10da2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10da2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10da2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10da2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10da2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10da2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10da2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10da2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10da2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10da2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10da2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10da301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10da30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10da30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10da30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10da313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10da31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10da31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10da320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10da32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10da329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10da32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10da332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10da33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10da33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10da34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10da34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10da348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10da34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10da351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10da35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10da360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10da36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10da367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10da36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10da370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10da37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10da379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10da37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10da38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10da386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10da38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10da38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10da39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10da398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10da39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10da3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10da3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10da3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10da3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10da3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10da3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10da3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10da3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10da3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10da3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10da3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10da3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10da3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10da3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10da3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10da3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10da3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10da3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10da3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10da3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10da3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10da40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10da404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10da40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10da40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10da41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10da41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10da41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10da427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10da42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10da43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10da435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10da43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10da44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10da44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10da44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10da452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10da45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10da45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10da463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10da469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10da46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10da47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10da47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10da480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10da48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10da48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10da491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10da497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10da49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10da4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10da4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10da4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10da4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10da4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10da4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10da4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10da4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10da4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10da4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10da4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10da4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10da4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10da4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10da4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10da4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10da4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10da504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10da50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10da51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10da51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10da51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10da521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10da52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10da52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10da532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10da538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10da53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10da54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10da549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10da54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10da55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10da55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10da560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10da566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10da56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10da57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10da57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10da57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10da58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10da58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10da58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10da58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10da59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10da59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10da59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10da5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10da5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10da5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10da5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10da5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x10da5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x10da5c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x10da5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10da5cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x10da5d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x10da5d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x10da5da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x10da5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x10da5e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10da5e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10da5f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10da5faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10da601c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10da608e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10da60ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10da61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10da61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10da61c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1297044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1297056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1297063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129706cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129707140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129707860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129708380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129708b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129709340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129709a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12970a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12970a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12970afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12970b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12970be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12970c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12970cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12970d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12970da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12970dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12970e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12970e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12970e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12970ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12970f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12970f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12970fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12970fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1297102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129710710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129710b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129710ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129711460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1297118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129711d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1297121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129712620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129712a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129712f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129713370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1297137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129713c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1297140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129714530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1297149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129714e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1297156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129715b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129715fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129716540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129716a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129716eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129717320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129717790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129717c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129718070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1297184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129718950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129718dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129719230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1297196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129719b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12971a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12971a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12971acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12971b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12971b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12971ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12971be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12971c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12971c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12971cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12971d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12971d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12971d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12971dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12971e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12971e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12971eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12971ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12971f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12971f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12971fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129720120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129720590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129720a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129720e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1297212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129721750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129722420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129722940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1297234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129723a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129724000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1297245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129724b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129725110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1297256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129725c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1297267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129726d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129727330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1297278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129727de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1297282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1297287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129728ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1297291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1297296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129729be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12972a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12972a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12972aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12972afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12972b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12972b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12972bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12972c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12972c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12972cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12972d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12972d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12972dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12972e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12972e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12972ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12972f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12972f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12972fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12972ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1297304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1297309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129730ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1297313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1297318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129731de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1297322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1297327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129732ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1297331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1297336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129733be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1297340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1297345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129734ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1297354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1297359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129735ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1297363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1297368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129736de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1297372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1297377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129737ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1297381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1297386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129738be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1297390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1297395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129739ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12973a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12973a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12973aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12973b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12973b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12973bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12973c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12973c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12973cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12973d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12973d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12973dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12973e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12973e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12973eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12973efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12973f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12973f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12973fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1297403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1297408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129740e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129741440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1297419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129741fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1297425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1297431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1297439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129744120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129744730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129744d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129745530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1297459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129745e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129746ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129747010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129747ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129748550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129748aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129748ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129749540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129749a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129749fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12974a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12974aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12974afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12974b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12974ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12974bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12974c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12974ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12974cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12974d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12974da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12974dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12974e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12974ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12974ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12974f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12974fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12974ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1297504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129750a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129750f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1297514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129751a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129751f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1297524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129752a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1297534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1297539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129753f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129754490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1297549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129754f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129755480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1297559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129755f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129756470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1297569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129756f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129757460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1297579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129757f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129758450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1297589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129758ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129759440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1297598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129759d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12975a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12975a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12975ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12975b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12975b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12975b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12975bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12975c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12975c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12975cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12975d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12975d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12975d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12975de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12975e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12975e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12975ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12975f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12975f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12975fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12975fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x129760340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1297607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129760d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129761450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129761b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129762290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1297629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129762c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129763460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129763720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129763d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.676s
user	0m0.263s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4829 (074c4fd3)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12900b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12900b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12900bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12900c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12900c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12900ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12900d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12900d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12900dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12900e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12900e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12900eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12900f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129010170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129010980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1290110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1290117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129011ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129012600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129012dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1290134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129013c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129014330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129014bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1290152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1290155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129015bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129016830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129016d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129017030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1290174d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129017790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129018020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129018560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129018820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129018cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129019160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129019600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129019aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129019f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12901a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12901a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12901ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12901b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12901b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12901ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12901c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12901c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12901cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12901d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12901dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12901e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12901e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12901ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12901f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12901fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12901ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129020210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129020820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129021010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1290212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129021770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1290220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129022550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1290229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129022e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129023330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1290237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129023c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129024110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1290245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129024a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129024fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1290254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129025a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129025f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1290264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129026a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129026f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1290274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129027a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129027f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1290284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129028a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129028f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1290294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129029a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129029f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12902a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12902a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12902af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12902b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12902b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12902bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12902c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12902c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12901c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12902ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12902d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12902db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12902e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12902e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12902eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12902f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12902f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12902fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129030070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1290305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129030b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129031060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1290315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129031b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129031fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129032440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1290328e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129032d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129033220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1290336c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129033b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129034000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1290344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129034940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129034de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129035280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129035720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129035bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129036060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129036500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1290369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129036e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1290372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129037780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129037c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1290380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129038560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129038a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129038ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1290397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129039c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12903a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12903a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12903aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12903af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12903b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12903b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12903bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12903c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12903c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12903cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12903cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12903d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12903d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12903dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12903e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12903e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12903eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12903efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12903f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12903f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12903fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129040240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1290406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129040b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129041020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1290414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129041960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129041e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1290422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129042740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129042be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129043080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129043520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1290439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129043e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129044300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1290447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129044c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1290450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129045580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129045a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129045ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129046360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129046800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129046ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129047140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1290475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129047a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129047f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1290483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129048860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129048d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129049250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1290497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129049cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12904a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12904a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12904ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12904b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12904b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12904bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12904c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12904c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12904cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12904d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12904da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12904df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12904e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12904e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12904f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12904f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12904fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129050010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129050560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129050ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129051000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129051550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129051aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129051ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129052540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129052a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129052fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129053530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129053a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129053fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129054520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129054a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129054fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129055510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129055a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129055fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129056500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129056a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129056fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1290574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129057a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129057f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1290584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129058a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129058f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1290594d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129059a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129059f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12905a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12905aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12905af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12905b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12905ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12905bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12905c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12905c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12905cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12905d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12905d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12905df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12905e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12905e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12905ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12905f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12905f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12905ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129060460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1290609b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129060f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129061450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1290619a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129061e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1290622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129062780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129062c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1290630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129063560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129063a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129063ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129064340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1290647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129064c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129065120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1290655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129065a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129065f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1290663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x129066840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x129066ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x129067180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x129067620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x129067ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x129067f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x129068400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1290688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x129068d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129069290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1290699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12906a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12906a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12906af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12906b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12906b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12906bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12906c290 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.884 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x128704fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x128705440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1287058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x128705d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x128706190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x128706600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x128706a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x128706ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x128707350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1287077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x128707c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x128708340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x128708e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x128709610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x128709e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12870a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12870ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12870b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12870baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12870c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12870c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12870d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12870d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12870de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12870e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12870e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12870eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12870ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12870f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12870f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12870fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1287101e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x128710650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x128710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x128710d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1287111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x128711660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x128711ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x128711f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1287123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x128712820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x128712c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x128713100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x128713570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1287139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x128713e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1287142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x128714730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x128714ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x128715010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x128715480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1287158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x128715d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1287161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x128716640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x128716ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x128717020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x128717520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x128717990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x128717e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x128718270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1287186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x128718b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x128718fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x128719430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1287198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x128719d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12871a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12871a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12871aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12871aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12871b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12871b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12871bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12871c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12871c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12871c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12871cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12871d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12871d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12871db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12871dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12871e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12871e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12871ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12871f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12871f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12871fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12871feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x128720320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x128720790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x128720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x128721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1287214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x128721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x128721dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x128722230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1287226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x128722b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x128722f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1287233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x128723860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x128723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x128724140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1287245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x128724a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x128724e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x128725300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x128725770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x128725be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x128726050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1287264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x128726930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x128726da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x128727210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x128727680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x128727af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x128727f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1287283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x128728840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x128728cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x128729120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x128729590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x128729a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x128729e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12872a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12872a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12872abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12872b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12872b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12872b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12872bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12872c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12872c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12872cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12872cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12872d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12872d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12872dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12872e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12872e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12872e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12872ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12872f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12872f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12872fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x128730010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x128730480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1287308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x128730d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1287311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x128731640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x128731ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x128731f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x128732390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x128732800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x128732c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1287330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x128733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1287339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x128733e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1287342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x128734710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x128734b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x128734ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x128735460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x128736090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x128736350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x128736610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x128736a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x128736ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x128737360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1287377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x128737c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1287380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x128738520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x128738990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x128738e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x128739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1287396e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x128739b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x128739fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12873a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12873a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12873ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12873b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12873b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12873ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12873bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12873c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12873c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12873cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12873d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12873d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12873d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12873dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12873e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12873e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12873eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12873efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12873f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12873f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12873fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1287402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x128740760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x128740bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x128741040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1287414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1287419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x128741ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x128742a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x128742d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1287432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x128743890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x128743e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x128744410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1287449d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x128744f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x128745550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x128745b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1287460d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x128746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x128746c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x128747210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1287477d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x128747d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x128748350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x128748910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x128748ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x128749490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x128749a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12874a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12874a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12874ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12874b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12874b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12874bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12874c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12874c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12874ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12874d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12874d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12874df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12874e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12874ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12874f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12874f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12874fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1287501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x128750790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x128750d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x128751310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1287518d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x128751e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x128752450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x128752a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x128752fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x128753590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x128753b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x128754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1287546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x128754c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x128755250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x128755810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x128755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x128756390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x128756950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x128756f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x128757410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x128757910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x128757e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x128758310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x128758810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x128758d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x128759210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x128759710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x128759c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12875a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12875a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12875ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12875b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12875b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12875ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12875bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12875c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12875c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12875ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12875d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12875d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12875dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12875e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12875e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12875ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12875f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12875fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x128760460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x128760b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x128760e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x128761630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1287618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x128761f00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12904add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12904c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12906bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12904a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12904b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12901e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12901deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1290204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12904cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129015870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12901c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12901cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12901bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12901ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12901d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129014870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12902d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12906b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129017a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129017d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12904d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12904b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129015e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129016140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129016400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12906c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12906c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12906cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12906cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12906d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12906d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12906d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12906da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12906dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12906dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12906e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12906e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12906e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12906eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12906ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12906f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12906f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12906f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12906f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12906fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12906fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1290700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129070370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129070630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1290708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129070bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129070e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129071130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1290713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1290716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129071970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129071c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129071ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1290721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129072470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129072730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1290729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129072cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129072f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129073230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1290734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1290737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129073a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129073d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129073ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1290742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129074570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129074830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129074af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129074db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129075070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129075330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1290755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1290758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129075b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129075e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1290760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1290763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129076670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129076930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129076bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129076eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129077170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129077430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1290776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1290779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129077c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129077f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1290781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1290784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129078770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129078a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129078cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129078fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129079270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129079530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1290797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129079ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129079d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12907a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12907a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12907a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12907a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12907ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12907adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12907b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12907b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12907b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12907b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12907bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12907be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12907c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12907c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12907c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12907c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12907cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12907cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12907d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12907d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12907d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12907d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12907dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12907df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12907e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12907e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12907e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12907ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12907ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12907eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12907f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12907f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12907f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12907faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12907fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129080070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129080330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1290805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1290808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129080b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129080e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1290810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1290813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129081670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129081930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129081bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129081eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129082170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129082430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1290826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1290829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129082c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129082f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1290831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1290834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129083770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129083a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129083cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129083fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129084270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129084530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1290847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129084ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129084d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129085030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1290852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1290855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129085870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129085b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129085df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1290860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129086370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129086630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1290868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129086bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129086e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129087130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1290873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1290876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129087970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129087c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129087ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1290881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129088470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129088730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1290889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129088cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129088f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129089230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1290894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1290897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129089a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129089d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129089ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12908a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12908a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12908a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12908aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12908adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12908b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12908b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12908b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12908b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12908bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12908be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12908c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12908c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12908c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12908cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12908cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12908d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12908d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12908d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12908da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12908dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12908df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12908e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12908ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12908ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12908f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12908fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12908ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1290904b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129090a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129090f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1290914a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1290919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129091f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129092490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1290929e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129092f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129093480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1290939d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129093f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129094470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1290949c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129094f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129095460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1290959b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129095f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129096450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1290969a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129096ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129097440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129097990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129097ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129098430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129098980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129098ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129099420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129099970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129099ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12909a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12909a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12909aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12909b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12909b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12909bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12909c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12909c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12909ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12909d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12909d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12909dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12909deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12909e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12909e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12909ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12909eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12909f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12909f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12909fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1290a0080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1290a04f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1290a0960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1290a0dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1290a1240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1290a16b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1290a1b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1290a1f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1290a2400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1290a2870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1290a2ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1290a3150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1290a35c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1290a3a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1290a3ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1290a4310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1290a4780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1290a51e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1290a5900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1290a6020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1290a6740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1290a6a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1290a6e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1290a7470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1290a7a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.964s
user	0m0.233s
sys	0m0.192s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
