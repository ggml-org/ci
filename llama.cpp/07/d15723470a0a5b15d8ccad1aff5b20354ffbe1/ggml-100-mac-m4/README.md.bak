### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.12 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.20 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.92 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.30 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.35 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.01 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.92 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  104.09 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.82 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.74 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.32 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 165.55 sec*proc (29 tests)

Total Test time (real) = 165.56 sec

real	2m45.709s
user	4m38.680s
sys	0m5.681s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.15 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.15 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.85 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.16 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.23 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.42 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.36 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.25 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.01 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.19 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.09 sec*proc (29 tests)

Total Test time (real) =  48.11 sec

real	0m48.118s
user	0m54.548s
sys	0m5.159s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.122 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.086 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.014.461 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.014.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.469 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.014.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.470 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.014.470 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.014.470 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.014.471 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.014.472 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.014.472 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.014.473 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.014.473 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.014.475 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.014.475 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.014.476 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.014.476 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.014.476 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.014.477 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.014.477 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.017.182 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.017.183 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.017.183 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.017.183 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.017.184 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.017.184 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.017.185 I llama_model_loader: - type  f32:  124 tensors
0.00.017.185 I llama_model_loader: - type  f16:   73 tensors
0.00.017.185 I print_info: file format = GGUF V3 (latest)
0.00.017.186 I print_info: file type   = F16
0.00.017.187 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.019.553 I load: special tokens cache size = 5
0.00.020.743 I load: token to piece cache size = 0.2032 MB
0.00.020.765 I print_info: arch             = bert
0.00.020.766 I print_info: vocab_only       = 0
0.00.020.766 I print_info: n_ctx_train      = 512
0.00.020.767 I print_info: n_embd           = 384
0.00.020.767 I print_info: n_layer          = 12
0.00.020.770 I print_info: n_head           = 12
0.00.020.771 I print_info: n_head_kv        = 12
0.00.020.771 I print_info: n_rot            = 32
0.00.020.771 I print_info: n_swa            = 0
0.00.020.771 I print_info: n_embd_head_k    = 32
0.00.020.771 I print_info: n_embd_head_v    = 32
0.00.020.772 I print_info: n_gqa            = 1
0.00.020.773 I print_info: n_embd_k_gqa     = 384
0.00.020.773 I print_info: n_embd_v_gqa     = 384
0.00.020.774 I print_info: f_norm_eps       = 1.0e-12
0.00.020.774 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.020.774 I print_info: f_clamp_kqv      = 0.0e+00
0.00.020.774 I print_info: f_max_alibi_bias = 0.0e+00
0.00.020.774 I print_info: f_logit_scale    = 0.0e+00
0.00.020.775 I print_info: n_ff             = 1536
0.00.020.775 I print_info: n_expert         = 0
0.00.020.775 I print_info: n_expert_used    = 0
0.00.020.775 I print_info: causal attn      = 0
0.00.020.776 I print_info: pooling type     = 2
0.00.020.776 I print_info: rope type        = 2
0.00.020.776 I print_info: rope scaling     = linear
0.00.020.776 I print_info: freq_base_train  = 10000.0
0.00.020.776 I print_info: freq_scale_train = 1
0.00.020.776 I print_info: n_ctx_orig_yarn  = 512
0.00.020.777 I print_info: rope_finetuned   = unknown
0.00.020.777 I print_info: ssm_d_conv       = 0
0.00.020.777 I print_info: ssm_d_inner      = 0
0.00.020.777 I print_info: ssm_d_state      = 0
0.00.020.777 I print_info: ssm_dt_rank      = 0
0.00.020.777 I print_info: ssm_dt_b_c_rms   = 0
0.00.020.777 I print_info: model type       = 33M
0.00.020.778 I print_info: model params     = 33.21 M
0.00.020.778 I print_info: general.name     = Bge Small
0.00.020.779 I print_info: vocab type       = WPM
0.00.020.779 I print_info: n_vocab          = 30522
0.00.020.779 I print_info: n_merges         = 0
0.00.020.779 I print_info: BOS token        = 101 '[CLS]'
0.00.020.779 I print_info: UNK token        = 100 '[UNK]'
0.00.020.782 I print_info: SEP token        = 102 '[SEP]'
0.00.020.782 I print_info: PAD token        = 0 '[PAD]'
0.00.020.783 I print_info: MASK token       = 103 '[MASK]'
0.00.020.783 I print_info: LF token         = 0 '[PAD]'
0.00.020.783 I print_info: max token length = 21
0.00.020.783 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.022.879 I load_tensors: offloading 12 repeating layers to GPU
0.00.022.880 I load_tensors: offloading output layer to GPU
0.00.022.880 I load_tensors: offloaded 13/13 layers to GPU
0.00.022.900 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.022.901 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.023.107 I llama_init_from_model: n_seq_max     = 1
0.00.023.107 I llama_init_from_model: n_ctx         = 512
0.00.023.108 I llama_init_from_model: n_ctx_per_seq = 512
0.00.023.108 I llama_init_from_model: n_batch       = 2048
0.00.023.108 I llama_init_from_model: n_ubatch      = 2048
0.00.023.108 I llama_init_from_model: flash_attn    = 0
0.00.023.108 I llama_init_from_model: freq_base     = 10000.0
0.00.023.108 I llama_init_from_model: freq_scale    = 1
0.00.023.109 I ggml_metal_init: allocating
0.00.023.112 I ggml_metal_init: found device: Apple M4
0.00.023.115 I ggml_metal_init: picking default device: Apple M4
0.00.023.582 I ggml_metal_init: using embedded metal library
0.00.026.146 I ggml_metal_init: GPU name:   Apple M4
0.00.026.148 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.026.149 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.026.149 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.026.150 I ggml_metal_init: simdgroup reduction   = true
0.00.026.150 I ggml_metal_init: simdgroup matrix mul. = true
0.00.026.150 I ggml_metal_init: has residency sets    = true
0.00.026.150 I ggml_metal_init: has bfloat            = true
0.00.026.150 I ggml_metal_init: use bfloat            = true
0.00.026.151 I ggml_metal_init: hasUnifiedMemory      = true
0.00.026.151 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.036.724 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.037.317 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.037.319 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.037.321 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.038.419 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.038.421 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.038.421 I llama_init_from_model: graph nodes  = 429
0.00.038.421 I llama_init_from_model: graph splits = 2
0.00.038.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.038.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.908 I 
0.00.042.940 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.043.463 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.047.820 I llama_perf_context_print:        load time =      30.82 ms
0.00.047.821 I llama_perf_context_print: prompt eval time =       4.24 ms /     9 tokens (    0.47 ms per token,  2124.65 tokens per second)
0.00.047.822 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.822 I llama_perf_context_print:       total time =       4.91 ms /    10 tokens
0.00.048.038 I ggml_metal_free: deallocating

real	0m0.232s
user	0m0.033s
sys	0m0.024s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.048 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.910 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.299 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.304 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.307 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.307 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.308 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.308 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.309 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.309 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.309 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.310 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.312 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.313 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.313 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.314 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.314 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.314 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.468 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.076 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.077 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.077 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.077 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.078 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.078 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.078 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.079 I llama_model_loader: - type  f32:  124 tensors
0.00.014.079 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.080 I print_info: file format = GGUF V3 (latest)
0.00.014.080 I print_info: file type   = Q8_0
0.00.014.081 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.312 I load: special tokens cache size = 5
0.00.017.572 I load: token to piece cache size = 0.2032 MB
0.00.017.581 I print_info: arch             = bert
0.00.017.582 I print_info: vocab_only       = 0
0.00.017.583 I print_info: n_ctx_train      = 512
0.00.017.583 I print_info: n_embd           = 384
0.00.017.583 I print_info: n_layer          = 12
0.00.017.586 I print_info: n_head           = 12
0.00.017.586 I print_info: n_head_kv        = 12
0.00.017.586 I print_info: n_rot            = 32
0.00.017.586 I print_info: n_swa            = 0
0.00.017.587 I print_info: n_embd_head_k    = 32
0.00.017.587 I print_info: n_embd_head_v    = 32
0.00.017.587 I print_info: n_gqa            = 1
0.00.017.588 I print_info: n_embd_k_gqa     = 384
0.00.017.589 I print_info: n_embd_v_gqa     = 384
0.00.017.590 I print_info: f_norm_eps       = 1.0e-12
0.00.017.590 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.592 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.592 I print_info: f_logit_scale    = 0.0e+00
0.00.017.592 I print_info: n_ff             = 1536
0.00.017.593 I print_info: n_expert         = 0
0.00.017.593 I print_info: n_expert_used    = 0
0.00.017.593 I print_info: causal attn      = 0
0.00.017.593 I print_info: pooling type     = 2
0.00.017.593 I print_info: rope type        = 2
0.00.017.594 I print_info: rope scaling     = linear
0.00.017.594 I print_info: freq_base_train  = 10000.0
0.00.017.596 I print_info: freq_scale_train = 1
0.00.017.597 I print_info: n_ctx_orig_yarn  = 512
0.00.017.597 I print_info: rope_finetuned   = unknown
0.00.017.597 I print_info: ssm_d_conv       = 0
0.00.017.597 I print_info: ssm_d_inner      = 0
0.00.017.597 I print_info: ssm_d_state      = 0
0.00.017.597 I print_info: ssm_dt_rank      = 0
0.00.017.597 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.598 I print_info: model type       = 33M
0.00.017.598 I print_info: model params     = 33.21 M
0.00.017.598 I print_info: general.name     = Bge Small
0.00.017.599 I print_info: vocab type       = WPM
0.00.017.599 I print_info: n_vocab          = 30522
0.00.017.599 I print_info: n_merges         = 0
0.00.017.604 I print_info: BOS token        = 101 '[CLS]'
0.00.017.604 I print_info: UNK token        = 100 '[UNK]'
0.00.017.606 I print_info: SEP token        = 102 '[SEP]'
0.00.017.606 I print_info: PAD token        = 0 '[PAD]'
0.00.017.606 I print_info: MASK token       = 103 '[MASK]'
0.00.017.606 I print_info: LF token         = 0 '[PAD]'
0.00.017.607 I print_info: max token length = 21
0.00.017.607 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.222 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.223 I load_tensors: offloading output layer to GPU
0.00.019.223 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.229 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.229 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.396 I llama_init_from_model: n_seq_max     = 1
0.00.019.397 I llama_init_from_model: n_ctx         = 512
0.00.019.397 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.397 I llama_init_from_model: n_batch       = 2048
0.00.019.398 I llama_init_from_model: n_ubatch      = 2048
0.00.019.398 I llama_init_from_model: flash_attn    = 0
0.00.019.398 I llama_init_from_model: freq_base     = 10000.0
0.00.019.398 I llama_init_from_model: freq_scale    = 1
0.00.019.399 I ggml_metal_init: allocating
0.00.019.402 I ggml_metal_init: found device: Apple M4
0.00.019.406 I ggml_metal_init: picking default device: Apple M4
0.00.019.838 I ggml_metal_init: using embedded metal library
0.00.022.186 I ggml_metal_init: GPU name:   Apple M4
0.00.022.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.188 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.188 I ggml_metal_init: simdgroup reduction   = true
0.00.022.189 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.189 I ggml_metal_init: has residency sets    = true
0.00.022.189 I ggml_metal_init: has bfloat            = true
0.00.022.189 I ggml_metal_init: use bfloat            = true
0.00.022.190 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.190 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.860 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.497 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.499 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.501 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.417 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.418 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.419 I llama_init_from_model: graph nodes  = 429
0.00.034.419 I llama_init_from_model: graph splits = 2
0.00.034.420 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.420 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.163 I 
0.00.038.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.038.733 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.085 I llama_perf_context_print:        load time =      29.25 ms
0.00.042.087 I llama_perf_context_print: prompt eval time =       3.24 ms /     9 tokens (    0.36 ms per token,  2776.06 tokens per second)
0.00.042.088 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.089 I llama_perf_context_print:       total time =       3.92 ms /    10 tokens
0.00.042.297 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.029s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.270 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.675 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.346 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.351 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.354 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.363 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.364 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.365 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.366 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.367 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.370 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.371 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.371 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.375 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.375 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.376 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.377 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.377 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.220 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.495 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.495 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.496 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.496 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.497 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.497 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.497 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.498 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.498 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.499 I llama_model_loader: - type  f32:   40 tensors
0.00.048.499 I llama_model_loader: - type  f16:   30 tensors
0.00.048.500 I print_info: file format = GGUF V3 (latest)
0.00.048.500 I print_info: file type   = F16
0.00.048.501 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.706 W load: empty token at index 5
0.00.057.658 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.080 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.115 I load: special tokens cache size = 5
0.00.322.452 I load: token to piece cache size = 1.5060 MB
0.00.322.482 I print_info: arch             = jina-bert-v2
0.00.322.483 I print_info: vocab_only       = 0
0.00.322.484 I print_info: n_ctx_train      = 8192
0.00.322.484 I print_info: n_embd           = 384
0.00.322.484 I print_info: n_layer          = 4
0.00.322.490 I print_info: n_head           = 12
0.00.322.490 I print_info: n_head_kv        = 12
0.00.322.490 I print_info: n_rot            = 32
0.00.322.491 I print_info: n_swa            = 0
0.00.322.491 I print_info: n_embd_head_k    = 32
0.00.322.491 I print_info: n_embd_head_v    = 32
0.00.322.491 I print_info: n_gqa            = 1
0.00.322.492 I print_info: n_embd_k_gqa     = 384
0.00.322.492 I print_info: n_embd_v_gqa     = 384
0.00.322.493 I print_info: f_norm_eps       = 1.0e-12
0.00.322.494 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.322.494 I print_info: f_clamp_kqv      = 0.0e+00
0.00.322.494 I print_info: f_max_alibi_bias = 8.0e+00
0.00.322.494 I print_info: f_logit_scale    = 0.0e+00
0.00.322.496 I print_info: n_ff             = 1536
0.00.322.496 I print_info: n_expert         = 0
0.00.322.496 I print_info: n_expert_used    = 0
0.00.322.498 I print_info: causal attn      = 0
0.00.322.499 I print_info: pooling type     = -1
0.00.322.499 I print_info: rope type        = -1
0.00.322.499 I print_info: rope scaling     = linear
0.00.322.500 I print_info: freq_base_train  = 10000.0
0.00.322.500 I print_info: freq_scale_train = 1
0.00.322.500 I print_info: n_ctx_orig_yarn  = 8192
0.00.322.500 I print_info: rope_finetuned   = unknown
0.00.322.500 I print_info: ssm_d_conv       = 0
0.00.322.501 I print_info: ssm_d_inner      = 0
0.00.322.501 I print_info: ssm_d_state      = 0
0.00.322.501 I print_info: ssm_dt_rank      = 0
0.00.322.501 I print_info: ssm_dt_b_c_rms   = 0
0.00.322.501 I print_info: model type       = 33M
0.00.322.502 I print_info: model params     = 32.90 M
0.00.322.502 I print_info: general.name     = Jina Bert Implementation
0.00.322.503 I print_info: vocab type       = BPE
0.00.322.503 I print_info: n_vocab          = 61056
0.00.322.504 I print_info: n_merges         = 39382
0.00.322.504 I print_info: BOS token        = 0 '<s>'
0.00.322.504 I print_info: EOS token        = 2 '</s>'
0.00.322.504 I print_info: UNK token        = 3 '<unk>'
0.00.322.504 I print_info: SEP token        = 2 '</s>'
0.00.322.504 I print_info: PAD token        = 1 '<pad>'
0.00.322.505 I print_info: MASK token       = 4 '<mask>'
0.00.322.505 I print_info: EOG token        = 2 '</s>'
0.00.322.505 I print_info: max token length = 45
0.00.322.505 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.324.582 I load_tensors: offloading 4 repeating layers to GPU
0.00.324.583 I load_tensors: offloading output layer to GPU
0.00.324.583 I load_tensors: offloaded 5/5 layers to GPU
0.00.324.607 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.324.607 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.325.086 I llama_init_from_model: n_seq_max     = 1
0.00.325.087 I llama_init_from_model: n_ctx         = 8192
0.00.325.087 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.325.088 I llama_init_from_model: n_batch       = 2048
0.00.325.088 I llama_init_from_model: n_ubatch      = 2048
0.00.325.088 I llama_init_from_model: flash_attn    = 0
0.00.325.088 I llama_init_from_model: freq_base     = 10000.0
0.00.325.088 I llama_init_from_model: freq_scale    = 1
0.00.325.089 I ggml_metal_init: allocating
0.00.325.092 I ggml_metal_init: found device: Apple M4
0.00.325.095 I ggml_metal_init: picking default device: Apple M4
0.00.325.825 I ggml_metal_init: using embedded metal library
0.00.328.775 I ggml_metal_init: GPU name:   Apple M4
0.00.328.776 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.328.776 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.328.777 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.328.777 I ggml_metal_init: simdgroup reduction   = true
0.00.328.777 I ggml_metal_init: simdgroup matrix mul. = true
0.00.328.777 I ggml_metal_init: has residency sets    = true
0.00.328.777 I ggml_metal_init: has bfloat            = true
0.00.328.778 I ggml_metal_init: use bfloat            = true
0.00.328.778 I ggml_metal_init: hasUnifiedMemory      = true
0.00.328.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.338.417 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.341.518 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.341.520 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.521 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.347.720 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.347.722 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.347.722 I llama_init_from_model: graph nodes  = 154
0.00.347.722 I llama_init_from_model: graph splits = 2
0.00.347.723 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.347.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.503 I 
0.00.356.535 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.777 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.778 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.795 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.795 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.800 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.800 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.341 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.361.094 I llama_perf_context_print:        load time =     333.82 ms
0.00.361.095 I llama_perf_context_print: prompt eval time =       3.75 ms /    62 tokens (    0.06 ms per token, 16546.57 tokens per second)
0.00.361.095 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.361.096 I llama_perf_context_print:       total time =       4.59 ms /    63 tokens
0.00.361.330 I ggml_metal_free: deallocating

real	0m1.130s
user	0m0.329s
sys	0m0.051s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.135 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.308 I main: llama backend init
0.00.000.316 I main: load the model and apply lora adapter, if any
0.00.058.181 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.074.533 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.074.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.074.563 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.074.563 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.074.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.074.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.074.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.074.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.074.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.074.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.074.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.074.580 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.074.580 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.074.581 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.074.585 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.074.585 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.074.586 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.082.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.085.317 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.093.071 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.093.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.093.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.093.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.093.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.093.077 I llama_model_loader: - type  f32:  194 tensors
0.00.093.077 I llama_model_loader: - type  f16:   98 tensors
0.00.093.078 I print_info: file format = GGUF V3 (latest)
0.00.093.079 I print_info: file type   = all F32 (guessed)
0.00.093.083 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.107.162 I load: special tokens cache size = 25
0.00.116.463 I load: token to piece cache size = 0.2984 MB
0.00.116.495 I print_info: arch             = gptneox
0.00.116.495 I print_info: vocab_only       = 0
0.00.116.496 I print_info: n_ctx_train      = 2048
0.00.116.496 I print_info: n_embd           = 2048
0.00.116.496 I print_info: n_layer          = 24
0.00.116.500 I print_info: n_head           = 16
0.00.116.502 I print_info: n_head_kv        = 16
0.00.116.502 I print_info: n_rot            = 32
0.00.116.502 I print_info: n_swa            = 0
0.00.116.502 I print_info: n_embd_head_k    = 128
0.00.116.502 I print_info: n_embd_head_v    = 128
0.00.116.506 I print_info: n_gqa            = 1
0.00.116.507 I print_info: n_embd_k_gqa     = 2048
0.00.116.507 I print_info: n_embd_v_gqa     = 2048
0.00.116.508 I print_info: f_norm_eps       = 1.0e-05
0.00.116.509 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.116.509 I print_info: f_clamp_kqv      = 0.0e+00
0.00.116.509 I print_info: f_max_alibi_bias = 0.0e+00
0.00.116.509 I print_info: f_logit_scale    = 0.0e+00
0.00.116.510 I print_info: n_ff             = 8192
0.00.116.510 I print_info: n_expert         = 0
0.00.116.511 I print_info: n_expert_used    = 0
0.00.116.511 I print_info: causal attn      = 1
0.00.116.512 I print_info: pooling type     = 0
0.00.116.512 I print_info: rope type        = 2
0.00.116.512 I print_info: rope scaling     = linear
0.00.116.513 I print_info: freq_base_train  = 10000.0
0.00.116.514 I print_info: freq_scale_train = 1
0.00.116.514 I print_info: n_ctx_orig_yarn  = 2048
0.00.116.515 I print_info: rope_finetuned   = unknown
0.00.116.515 I print_info: ssm_d_conv       = 0
0.00.116.515 I print_info: ssm_d_inner      = 0
0.00.116.515 I print_info: ssm_d_state      = 0
0.00.116.519 I print_info: ssm_dt_rank      = 0
0.00.116.519 I print_info: ssm_dt_b_c_rms   = 0
0.00.116.520 I print_info: model type       = 1.4B
0.00.116.521 I print_info: model params     = 1.41 B
0.00.116.521 I print_info: general.name     = 1.4B
0.00.116.522 I print_info: vocab type       = BPE
0.00.116.522 I print_info: n_vocab          = 50304
0.00.116.523 I print_info: n_merges         = 50009
0.00.116.526 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.116.526 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.116.527 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.116.527 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.116.527 I print_info: LF token         = 187 ''
0.00.116.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.116.528 I print_info: max token length = 1024
0.00.116.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.209.371 I load_tensors: offloading 24 repeating layers to GPU
0.00.209.375 I load_tensors: offloading output layer to GPU
0.00.209.375 I load_tensors: offloaded 25/25 layers to GPU
0.00.209.402 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.209.403 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.210.116 I llama_init_from_model: n_seq_max     = 1
0.00.210.117 I llama_init_from_model: n_ctx         = 2048
0.00.210.117 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.210.117 I llama_init_from_model: n_batch       = 2048
0.00.210.118 I llama_init_from_model: n_ubatch      = 512
0.00.210.118 I llama_init_from_model: flash_attn    = 0
0.00.210.118 I llama_init_from_model: freq_base     = 10000.0
0.00.210.119 I llama_init_from_model: freq_scale    = 1
0.00.210.119 I ggml_metal_init: allocating
0.00.210.184 I ggml_metal_init: found device: Apple M4
0.00.210.190 I ggml_metal_init: picking default device: Apple M4
0.00.210.925 I ggml_metal_init: using embedded metal library
0.00.234.420 I ggml_metal_init: GPU name:   Apple M4
0.00.234.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.234.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.234.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.234.423 I ggml_metal_init: simdgroup reduction   = true
0.00.234.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.234.424 I ggml_metal_init: has residency sets    = true
0.00.234.424 I ggml_metal_init: has bfloat            = true
0.00.234.424 I ggml_metal_init: use bfloat            = true
0.00.234.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.234.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.383.584 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.412.808 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.412.815 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.412.837 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.417.910 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.417.913 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.417.913 I llama_init_from_model: graph nodes  = 967
0.00.417.914 I llama_init_from_model: graph splits = 2
0.00.417.922 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.418.045 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.418.046 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.511 I main: llama threadpool init, n_threads = 4
0.00.483.576 I 
0.00.483.603 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.483.604 I 
0.00.483.793 I sampler seed: 1234
0.00.483.798 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.483.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.483.834 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.483.834 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.313.055 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.02.313.055 I llama_perf_context_print:        load time =     424.45 ms
0.02.313.057 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.24 tokens per second)
0.02.313.058 I llama_perf_context_print:        eval time =    1782.56 ms /    63 runs   (   28.29 ms per token,    35.34 tokens per second)
0.02.313.058 I llama_perf_context_print:       total time =    1830.42 ms /    70 tokens
0.02.313.298 I ggml_metal_free: deallocating

real	0m2.675s
user	0m0.136s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.647 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.433 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.255 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.262 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.268 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.269 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.276 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.277 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.499 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.758 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.760 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.760 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.760 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.761 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.762 I llama_model_loader: - type  f32:  194 tensors
0.00.052.762 I llama_model_loader: - type  f16:   98 tensors
0.00.052.763 I print_info: file format = GGUF V3 (latest)
0.00.052.763 I print_info: file type   = all F32 (guessed)
0.00.052.764 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.867 I load: special tokens cache size = 25
0.00.072.900 I load: token to piece cache size = 0.2984 MB
0.00.072.915 I print_info: arch             = gptneox
0.00.072.916 I print_info: vocab_only       = 0
0.00.072.916 I print_info: n_ctx_train      = 2048
0.00.072.916 I print_info: n_embd           = 2048
0.00.072.917 I print_info: n_layer          = 24
0.00.072.920 I print_info: n_head           = 16
0.00.072.921 I print_info: n_head_kv        = 16
0.00.072.921 I print_info: n_rot            = 32
0.00.072.921 I print_info: n_swa            = 0
0.00.072.921 I print_info: n_embd_head_k    = 128
0.00.072.921 I print_info: n_embd_head_v    = 128
0.00.072.922 I print_info: n_gqa            = 1
0.00.072.923 I print_info: n_embd_k_gqa     = 2048
0.00.072.924 I print_info: n_embd_v_gqa     = 2048
0.00.072.924 I print_info: f_norm_eps       = 1.0e-05
0.00.072.924 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.925 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.925 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.925 I print_info: f_logit_scale    = 0.0e+00
0.00.072.926 I print_info: n_ff             = 8192
0.00.072.926 I print_info: n_expert         = 0
0.00.072.926 I print_info: n_expert_used    = 0
0.00.072.928 I print_info: causal attn      = 1
0.00.072.928 I print_info: pooling type     = 0
0.00.072.929 I print_info: rope type        = 2
0.00.072.929 I print_info: rope scaling     = linear
0.00.072.929 I print_info: freq_base_train  = 10000.0
0.00.072.930 I print_info: freq_scale_train = 1
0.00.072.930 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.930 I print_info: rope_finetuned   = unknown
0.00.072.930 I print_info: ssm_d_conv       = 0
0.00.072.930 I print_info: ssm_d_inner      = 0
0.00.072.930 I print_info: ssm_d_state      = 0
0.00.072.931 I print_info: ssm_dt_rank      = 0
0.00.072.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.931 I print_info: model type       = 1.4B
0.00.072.931 I print_info: model params     = 1.41 B
0.00.072.932 I print_info: general.name     = 1.4B
0.00.072.932 I print_info: vocab type       = BPE
0.00.072.932 I print_info: n_vocab          = 50304
0.00.072.933 I print_info: n_merges         = 50009
0.00.072.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.933 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.933 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.934 I print_info: LF token         = 187 ''
0.00.072.934 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.934 I print_info: max token length = 1024
0.00.072.935 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.413.770 I load_tensors: offloading 24 repeating layers to GPU
0.01.413.775 I load_tensors: offloading output layer to GPU
0.01.413.775 I load_tensors: offloaded 25/25 layers to GPU
0.01.413.802 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.413.803 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.414.879 I llama_init_from_model: n_seq_max     = 1
0.01.414.880 I llama_init_from_model: n_ctx         = 128
0.01.414.881 I llama_init_from_model: n_ctx_per_seq = 128
0.01.414.881 I llama_init_from_model: n_batch       = 128
0.01.414.881 I llama_init_from_model: n_ubatch      = 128
0.01.414.881 I llama_init_from_model: flash_attn    = 0
0.01.414.882 I llama_init_from_model: freq_base     = 10000.0
0.01.414.882 I llama_init_from_model: freq_scale    = 1
0.01.414.882 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.414.886 I ggml_metal_init: allocating
0.01.414.933 I ggml_metal_init: found device: Apple M4
0.01.414.939 I ggml_metal_init: picking default device: Apple M4
0.01.415.836 I ggml_metal_init: using embedded metal library
0.01.419.759 I ggml_metal_init: GPU name:   Apple M4
0.01.419.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.419.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.419.762 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.419.763 I ggml_metal_init: simdgroup reduction   = true
0.01.419.763 I ggml_metal_init: simdgroup matrix mul. = true
0.01.419.763 I ggml_metal_init: has residency sets    = true
0.01.419.763 I ggml_metal_init: has bfloat            = true
0.01.419.763 I ggml_metal_init: use bfloat            = true
0.01.419.764 I ggml_metal_init: hasUnifiedMemory      = true
0.01.419.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.431.125 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.432.893 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.432.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.432.910 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.434.704 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.434.705 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.434.705 I llama_init_from_model: graph nodes  = 967
0.01.434.706 I llama_init_from_model: graph splits = 2
0.01.434.707 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.434.707 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.470.762 I 
0.01.470.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.470.838 I perplexity: tokenizing the input ..
0.01.476.170 I perplexity: tokenization took 5.33 ms
0.01.476.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.595.082 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.596.474 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.596.490 I llama_perf_context_print:        load time =    1449.32 ms
0.01.596.490 I llama_perf_context_print: prompt eval time =     118.57 ms /   128 tokens (    0.93 ms per token,  1079.55 tokens per second)
0.01.596.493 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.596.494 I llama_perf_context_print:       total time =     125.73 ms /   129 tokens
0.01.596.847 I ggml_metal_free: deallocating

real	0m1.809s
user	0m0.097s
sys	0m0.255s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.536 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.703 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.703 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.705 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.705 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.706 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.657 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.440 I llama_model_loader: - type  f32:  194 tensors
0.00.034.440 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.441 I print_info: file format = GGUF V3 (latest)
0.00.034.441 I print_info: file type   = Q8_0
0.00.034.442 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.215 I load: special tokens cache size = 25
0.00.050.307 I load: token to piece cache size = 0.2984 MB
0.00.050.324 I print_info: arch             = gptneox
0.00.050.325 I print_info: vocab_only       = 0
0.00.050.326 I print_info: n_ctx_train      = 2048
0.00.050.326 I print_info: n_embd           = 2048
0.00.050.326 I print_info: n_layer          = 24
0.00.050.332 I print_info: n_head           = 16
0.00.050.333 I print_info: n_head_kv        = 16
0.00.050.333 I print_info: n_rot            = 32
0.00.050.333 I print_info: n_swa            = 0
0.00.050.333 I print_info: n_embd_head_k    = 128
0.00.050.334 I print_info: n_embd_head_v    = 128
0.00.050.334 I print_info: n_gqa            = 1
0.00.050.335 I print_info: n_embd_k_gqa     = 2048
0.00.050.336 I print_info: n_embd_v_gqa     = 2048
0.00.050.337 I print_info: f_norm_eps       = 1.0e-05
0.00.050.340 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.340 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.342 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.342 I print_info: f_logit_scale    = 0.0e+00
0.00.050.343 I print_info: n_ff             = 8192
0.00.050.343 I print_info: n_expert         = 0
0.00.050.343 I print_info: n_expert_used    = 0
0.00.050.343 I print_info: causal attn      = 1
0.00.050.343 I print_info: pooling type     = 0
0.00.050.344 I print_info: rope type        = 2
0.00.050.344 I print_info: rope scaling     = linear
0.00.050.345 I print_info: freq_base_train  = 10000.0
0.00.050.345 I print_info: freq_scale_train = 1
0.00.050.345 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.346 I print_info: rope_finetuned   = unknown
0.00.050.346 I print_info: ssm_d_conv       = 0
0.00.050.346 I print_info: ssm_d_inner      = 0
0.00.050.346 I print_info: ssm_d_state      = 0
0.00.050.346 I print_info: ssm_dt_rank      = 0
0.00.050.346 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.351 I print_info: model type       = 1.4B
0.00.050.354 I print_info: model params     = 1.41 B
0.00.050.355 I print_info: general.name     = 1.4B
0.00.050.355 I print_info: vocab type       = BPE
0.00.050.356 I print_info: n_vocab          = 50304
0.00.050.357 I print_info: n_merges         = 50009
0.00.050.357 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.357 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.358 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.359 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.360 I print_info: LF token         = 187 ''
0.00.050.360 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.360 I print_info: max token length = 1024
0.00.050.361 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.136.893 I load_tensors: offloading 24 repeating layers to GPU
0.01.136.898 I load_tensors: offloading output layer to GPU
0.01.136.900 I load_tensors: offloaded 25/25 layers to GPU
0.01.136.925 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.136.928 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.137.798 I llama_init_from_model: n_seq_max     = 1
0.01.137.800 I llama_init_from_model: n_ctx         = 2048
0.01.137.801 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.137.801 I llama_init_from_model: n_batch       = 2048
0.01.137.801 I llama_init_from_model: n_ubatch      = 512
0.01.137.802 I llama_init_from_model: flash_attn    = 0
0.01.137.802 I llama_init_from_model: freq_base     = 10000.0
0.01.137.803 I llama_init_from_model: freq_scale    = 1
0.01.137.804 I ggml_metal_init: allocating
0.01.137.820 I ggml_metal_init: found device: Apple M4
0.01.137.828 I ggml_metal_init: picking default device: Apple M4
0.01.138.979 I ggml_metal_init: using embedded metal library
0.01.144.766 I ggml_metal_init: GPU name:   Apple M4
0.01.144.769 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.144.770 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.144.770 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.144.771 I ggml_metal_init: simdgroup reduction   = true
0.01.144.771 I ggml_metal_init: simdgroup matrix mul. = true
0.01.144.772 I ggml_metal_init: has residency sets    = true
0.01.144.772 I ggml_metal_init: has bfloat            = true
0.01.144.772 I ggml_metal_init: use bfloat            = true
0.01.144.773 I ggml_metal_init: hasUnifiedMemory      = true
0.01.144.777 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.163.118 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.214.108 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.214.115 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.214.137 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.218.346 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.218.348 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.218.349 I llama_init_from_model: graph nodes  = 967
0.01.218.349 I llama_init_from_model: graph splits = 2
0.01.218.353 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.218.486 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.218.487 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.271.416 I main: llama threadpool init, n_threads = 4
0.01.271.461 I 
0.01.271.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.271.481 I 
0.01.271.627 I sampler seed: 1234
0.01.271.631 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.271.646 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.271.647 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.271.647 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.357.415 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.02.357.416 I llama_perf_context_print:        load time =    1260.10 ms
0.02.357.417 I llama_perf_context_print: prompt eval time =      39.46 ms /     7 tokens (    5.64 ms per token,   177.40 tokens per second)
0.02.357.417 I llama_perf_context_print:        eval time =    1043.46 ms /    63 runs   (   16.56 ms per token,    60.38 tokens per second)
0.02.357.418 I llama_perf_context_print:       total time =    1086.78 ms /    70 tokens
0.02.357.634 I ggml_metal_free: deallocating

real	0m2.377s
user	0m0.110s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.282 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.255 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.316 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.324 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.327 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.330 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.331 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.333 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.333 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.334 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.336 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.336 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.336 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.115 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.127 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.852 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.855 I llama_model_loader: - type  f32:  194 tensors
0.00.025.856 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.856 I print_info: file format = GGUF V3 (latest)
0.00.025.857 I print_info: file type   = Q8_0
0.00.025.858 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.884 I load: special tokens cache size = 25
0.00.040.367 I load: token to piece cache size = 0.2984 MB
0.00.040.379 I print_info: arch             = gptneox
0.00.040.380 I print_info: vocab_only       = 0
0.00.040.381 I print_info: n_ctx_train      = 2048
0.00.040.381 I print_info: n_embd           = 2048
0.00.040.381 I print_info: n_layer          = 24
0.00.040.385 I print_info: n_head           = 16
0.00.040.385 I print_info: n_head_kv        = 16
0.00.040.385 I print_info: n_rot            = 32
0.00.040.385 I print_info: n_swa            = 0
0.00.040.386 I print_info: n_embd_head_k    = 128
0.00.040.386 I print_info: n_embd_head_v    = 128
0.00.040.386 I print_info: n_gqa            = 1
0.00.040.387 I print_info: n_embd_k_gqa     = 2048
0.00.040.388 I print_info: n_embd_v_gqa     = 2048
0.00.040.388 I print_info: f_norm_eps       = 1.0e-05
0.00.040.389 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.389 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.391 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.392 I print_info: f_logit_scale    = 0.0e+00
0.00.040.392 I print_info: n_ff             = 8192
0.00.040.392 I print_info: n_expert         = 0
0.00.040.392 I print_info: n_expert_used    = 0
0.00.040.393 I print_info: causal attn      = 1
0.00.040.393 I print_info: pooling type     = 0
0.00.040.393 I print_info: rope type        = 2
0.00.040.393 I print_info: rope scaling     = linear
0.00.040.393 I print_info: freq_base_train  = 10000.0
0.00.040.394 I print_info: freq_scale_train = 1
0.00.040.394 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.394 I print_info: rope_finetuned   = unknown
0.00.040.398 I print_info: ssm_d_conv       = 0
0.00.040.398 I print_info: ssm_d_inner      = 0
0.00.040.398 I print_info: ssm_d_state      = 0
0.00.040.398 I print_info: ssm_dt_rank      = 0
0.00.040.398 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.398 I print_info: model type       = 1.4B
0.00.040.400 I print_info: model params     = 1.41 B
0.00.040.400 I print_info: general.name     = 1.4B
0.00.040.400 I print_info: vocab type       = BPE
0.00.040.401 I print_info: n_vocab          = 50304
0.00.040.401 I print_info: n_merges         = 50009
0.00.040.401 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.401 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.401 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.401 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: LF token         = 187 ''
0.00.040.402 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.402 I print_info: max token length = 1024
0.00.040.403 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.816.743 I load_tensors: offloading 24 repeating layers to GPU
0.00.816.753 I load_tensors: offloading output layer to GPU
0.00.816.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.816.779 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.816.781 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.818.303 I llama_init_from_model: n_seq_max     = 1
0.00.818.305 I llama_init_from_model: n_ctx         = 128
0.00.818.305 I llama_init_from_model: n_ctx_per_seq = 128
0.00.818.305 I llama_init_from_model: n_batch       = 128
0.00.818.306 I llama_init_from_model: n_ubatch      = 128
0.00.818.306 I llama_init_from_model: flash_attn    = 0
0.00.818.307 I llama_init_from_model: freq_base     = 10000.0
0.00.818.308 I llama_init_from_model: freq_scale    = 1
0.00.818.308 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.818.310 I ggml_metal_init: allocating
0.00.818.390 I ggml_metal_init: found device: Apple M4
0.00.818.403 I ggml_metal_init: picking default device: Apple M4
0.00.819.574 I ggml_metal_init: using embedded metal library
0.00.824.964 I ggml_metal_init: GPU name:   Apple M4
0.00.824.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.824.969 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.824.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.824.973 I ggml_metal_init: simdgroup reduction   = true
0.00.824.973 I ggml_metal_init: simdgroup matrix mul. = true
0.00.824.973 I ggml_metal_init: has residency sets    = true
0.00.824.974 I ggml_metal_init: has bfloat            = true
0.00.824.974 I ggml_metal_init: use bfloat            = true
0.00.824.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.824.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.842.379 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.845.796 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.845.799 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.845.824 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.848.935 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.848.937 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.848.937 I llama_init_from_model: graph nodes  = 967
0.00.848.938 I llama_init_from_model: graph splits = 2
0.00.848.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.848.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.877.909 I 
0.00.878.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.878.027 I perplexity: tokenizing the input ..
0.00.885.582 I perplexity: tokenization took 7.552 ms
0.00.885.596 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.024.860 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.026.199 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.026.222 I llama_perf_context_print:        load time =     867.64 ms
0.01.026.223 I llama_perf_context_print: prompt eval time =     138.39 ms /   128 tokens (    1.08 ms per token,   924.94 tokens per second)
0.01.026.224 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.026.224 I llama_perf_context_print:       total time =     148.32 ms /   129 tokens
0.01.026.603 I ggml_metal_free: deallocating

real	0m1.042s
user	0m0.079s
sys	0m0.169s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.012.379 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.029 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.038 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.038 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.039 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.039 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.039 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.040 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.041 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.041 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.816 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.500 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.501 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.501 I llama_model_loader: - type  f32:  194 tensors
0.00.028.502 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.502 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.503 I print_info: file format = GGUF V3 (latest)
0.00.028.503 I print_info: file type   = Q4_0
0.00.028.504 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.522 I load: special tokens cache size = 25
0.00.042.895 I load: token to piece cache size = 0.2984 MB
0.00.042.910 I print_info: arch             = gptneox
0.00.042.911 I print_info: vocab_only       = 0
0.00.042.912 I print_info: n_ctx_train      = 2048
0.00.042.912 I print_info: n_embd           = 2048
0.00.042.912 I print_info: n_layer          = 24
0.00.042.917 I print_info: n_head           = 16
0.00.042.917 I print_info: n_head_kv        = 16
0.00.042.917 I print_info: n_rot            = 32
0.00.042.918 I print_info: n_swa            = 0
0.00.042.919 I print_info: n_embd_head_k    = 128
0.00.042.919 I print_info: n_embd_head_v    = 128
0.00.042.919 I print_info: n_gqa            = 1
0.00.042.920 I print_info: n_embd_k_gqa     = 2048
0.00.042.922 I print_info: n_embd_v_gqa     = 2048
0.00.042.922 I print_info: f_norm_eps       = 1.0e-05
0.00.042.923 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.923 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.923 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.923 I print_info: f_logit_scale    = 0.0e+00
0.00.042.924 I print_info: n_ff             = 8192
0.00.042.924 I print_info: n_expert         = 0
0.00.042.924 I print_info: n_expert_used    = 0
0.00.042.924 I print_info: causal attn      = 1
0.00.042.924 I print_info: pooling type     = 0
0.00.042.924 I print_info: rope type        = 2
0.00.042.925 I print_info: rope scaling     = linear
0.00.042.925 I print_info: freq_base_train  = 10000.0
0.00.042.925 I print_info: freq_scale_train = 1
0.00.042.925 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.926 I print_info: rope_finetuned   = unknown
0.00.042.926 I print_info: ssm_d_conv       = 0
0.00.042.926 I print_info: ssm_d_inner      = 0
0.00.042.926 I print_info: ssm_d_state      = 0
0.00.042.926 I print_info: ssm_dt_rank      = 0
0.00.042.926 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.927 I print_info: model type       = 1.4B
0.00.042.928 I print_info: model params     = 1.41 B
0.00.042.928 I print_info: general.name     = 1.4B
0.00.042.929 I print_info: vocab type       = BPE
0.00.042.929 I print_info: n_vocab          = 50304
0.00.042.930 I print_info: n_merges         = 50009
0.00.042.931 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.931 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.931 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.931 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.932 I print_info: LF token         = 187 ''
0.00.042.932 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.932 I print_info: max token length = 1024
0.00.042.933 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.918 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.933 I load_tensors: offloading output layer to GPU
0.00.593.934 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.968 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.593.969 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.595.597 I llama_init_from_model: n_seq_max     = 1
0.00.595.600 I llama_init_from_model: n_ctx         = 2048
0.00.595.601 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.602 I llama_init_from_model: n_batch       = 2048
0.00.595.602 I llama_init_from_model: n_ubatch      = 512
0.00.595.603 I llama_init_from_model: flash_attn    = 0
0.00.595.604 I llama_init_from_model: freq_base     = 10000.0
0.00.595.605 I llama_init_from_model: freq_scale    = 1
0.00.595.607 I ggml_metal_init: allocating
0.00.595.696 I ggml_metal_init: found device: Apple M4
0.00.595.710 I ggml_metal_init: picking default device: Apple M4
0.00.597.269 I ggml_metal_init: using embedded metal library
0.00.603.016 I ggml_metal_init: GPU name:   Apple M4
0.00.603.024 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.603.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.603.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.603.028 I ggml_metal_init: simdgroup reduction   = true
0.00.603.028 I ggml_metal_init: simdgroup matrix mul. = true
0.00.603.029 I ggml_metal_init: has residency sets    = true
0.00.603.029 I ggml_metal_init: has bfloat            = true
0.00.603.030 I ggml_metal_init: use bfloat            = true
0.00.603.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.603.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.623.142 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.324 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.682.336 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.682.358 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.687.033 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.687.037 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.687.037 I llama_init_from_model: graph nodes  = 967
0.00.687.037 I llama_init_from_model: graph splits = 2
0.00.687.042 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.687.174 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.687.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.851 I main: llama threadpool init, n_threads = 4
0.00.740.895 I 
0.00.740.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.917 I 
0.00.741.093 I sampler seed: 1234
0.00.741.097 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.114 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.114 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.429.984 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48965.52 tokens per second)
0.01.429.984 I llama_perf_context_print:        load time =     727.70 ms
0.01.429.985 I llama_perf_context_print: prompt eval time =      49.91 ms /     7 tokens (    7.13 ms per token,   140.25 tokens per second)
0.01.429.986 I llama_perf_context_print:        eval time =     636.05 ms /    63 runs   (   10.10 ms per token,    99.05 tokens per second)
0.01.429.986 I llama_perf_context_print:       total time =     689.91 ms /    70 tokens
0.01.430.223 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.111s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.268 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.817 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.268 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.276 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.276 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.276 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.279 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.280 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.280 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.281 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.281 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.282 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.282 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.285 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.285 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.816 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.818 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.820 I llama_model_loader: - type  f32:  194 tensors
0.00.025.820 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.821 I print_info: file format = GGUF V3 (latest)
0.00.025.822 I print_info: file type   = Q4_0
0.00.025.823 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.489 I load: special tokens cache size = 25
0.00.040.902 I load: token to piece cache size = 0.2984 MB
0.00.040.919 I print_info: arch             = gptneox
0.00.040.920 I print_info: vocab_only       = 0
0.00.040.920 I print_info: n_ctx_train      = 2048
0.00.040.920 I print_info: n_embd           = 2048
0.00.040.920 I print_info: n_layer          = 24
0.00.040.924 I print_info: n_head           = 16
0.00.040.925 I print_info: n_head_kv        = 16
0.00.040.931 I print_info: n_rot            = 32
0.00.040.931 I print_info: n_swa            = 0
0.00.040.931 I print_info: n_embd_head_k    = 128
0.00.040.931 I print_info: n_embd_head_v    = 128
0.00.040.932 I print_info: n_gqa            = 1
0.00.040.933 I print_info: n_embd_k_gqa     = 2048
0.00.040.933 I print_info: n_embd_v_gqa     = 2048
0.00.040.934 I print_info: f_norm_eps       = 1.0e-05
0.00.040.934 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.934 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.935 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.936 I print_info: f_logit_scale    = 0.0e+00
0.00.040.936 I print_info: n_ff             = 8192
0.00.040.937 I print_info: n_expert         = 0
0.00.040.937 I print_info: n_expert_used    = 0
0.00.040.937 I print_info: causal attn      = 1
0.00.040.937 I print_info: pooling type     = 0
0.00.040.937 I print_info: rope type        = 2
0.00.040.937 I print_info: rope scaling     = linear
0.00.040.938 I print_info: freq_base_train  = 10000.0
0.00.040.939 I print_info: freq_scale_train = 1
0.00.040.939 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.940 I print_info: rope_finetuned   = unknown
0.00.040.940 I print_info: ssm_d_conv       = 0
0.00.040.940 I print_info: ssm_d_inner      = 0
0.00.040.940 I print_info: ssm_d_state      = 0
0.00.040.940 I print_info: ssm_dt_rank      = 0
0.00.040.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.940 I print_info: model type       = 1.4B
0.00.040.941 I print_info: model params     = 1.41 B
0.00.040.941 I print_info: general.name     = 1.4B
0.00.040.941 I print_info: vocab type       = BPE
0.00.040.942 I print_info: n_vocab          = 50304
0.00.040.943 I print_info: n_merges         = 50009
0.00.040.943 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.943 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.943 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.944 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.944 I print_info: LF token         = 187 ''
0.00.040.944 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.944 I print_info: max token length = 1024
0.00.040.945 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.863 I load_tensors: offloading output layer to GPU
0.00.583.864 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.896 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.583.898 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.585.614 I llama_init_from_model: n_seq_max     = 1
0.00.585.617 I llama_init_from_model: n_ctx         = 128
0.00.585.618 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.618 I llama_init_from_model: n_batch       = 128
0.00.585.619 I llama_init_from_model: n_ubatch      = 128
0.00.585.619 I llama_init_from_model: flash_attn    = 0
0.00.585.621 I llama_init_from_model: freq_base     = 10000.0
0.00.585.621 I llama_init_from_model: freq_scale    = 1
0.00.585.622 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.624 I ggml_metal_init: allocating
0.00.585.705 I ggml_metal_init: found device: Apple M4
0.00.585.719 I ggml_metal_init: picking default device: Apple M4
0.00.587.304 I ggml_metal_init: using embedded metal library
0.00.592.811 I ggml_metal_init: GPU name:   Apple M4
0.00.592.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.592.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.592.821 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.592.822 I ggml_metal_init: simdgroup reduction   = true
0.00.592.822 I ggml_metal_init: simdgroup matrix mul. = true
0.00.592.823 I ggml_metal_init: has residency sets    = true
0.00.592.823 I ggml_metal_init: has bfloat            = true
0.00.592.824 I ggml_metal_init: use bfloat            = true
0.00.592.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.592.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.041 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.616.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.616.622 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.616.669 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.954 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.619.956 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.619.956 I llama_init_from_model: graph nodes  = 967
0.00.619.957 I llama_init_from_model: graph splits = 2
0.00.619.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.619.960 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.616 I 
0.00.643.706 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.750 I perplexity: tokenizing the input ..
0.00.650.212 I perplexity: tokenization took 6.46 ms
0.00.650.217 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.772.435 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.773.766 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.773.781 I llama_perf_context_print:        load time =     633.79 ms
0.00.773.782 I llama_perf_context_print: prompt eval time =     121.83 ms /   128 tokens (    0.95 ms per token,  1050.63 tokens per second)
0.00.773.782 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.773.783 I llama_perf_context_print:       total time =     130.17 ms /   129 tokens
0.00.774.181 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.080s
sys	0m0.122s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.177 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.732 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.737 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.743 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.743 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.744 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.744 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.744 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.745 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.747 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.750 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.561 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.238 I llama_model_loader: - type  f32:  194 tensors
0.00.026.238 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.238 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.239 I print_info: file format = GGUF V3 (latest)
0.00.026.240 I print_info: file type   = Q4_1
0.00.026.240 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.421 I load: special tokens cache size = 25
0.00.040.799 I load: token to piece cache size = 0.2984 MB
0.00.040.808 I print_info: arch             = gptneox
0.00.040.808 I print_info: vocab_only       = 0
0.00.040.809 I print_info: n_ctx_train      = 2048
0.00.040.809 I print_info: n_embd           = 2048
0.00.040.809 I print_info: n_layer          = 24
0.00.040.812 I print_info: n_head           = 16
0.00.040.813 I print_info: n_head_kv        = 16
0.00.040.813 I print_info: n_rot            = 32
0.00.040.813 I print_info: n_swa            = 0
0.00.040.813 I print_info: n_embd_head_k    = 128
0.00.040.814 I print_info: n_embd_head_v    = 128
0.00.040.814 I print_info: n_gqa            = 1
0.00.040.815 I print_info: n_embd_k_gqa     = 2048
0.00.040.816 I print_info: n_embd_v_gqa     = 2048
0.00.040.818 I print_info: f_norm_eps       = 1.0e-05
0.00.040.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.819 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.819 I print_info: f_logit_scale    = 0.0e+00
0.00.040.820 I print_info: n_ff             = 8192
0.00.040.820 I print_info: n_expert         = 0
0.00.040.820 I print_info: n_expert_used    = 0
0.00.040.820 I print_info: causal attn      = 1
0.00.040.821 I print_info: pooling type     = 0
0.00.040.821 I print_info: rope type        = 2
0.00.040.821 I print_info: rope scaling     = linear
0.00.040.821 I print_info: freq_base_train  = 10000.0
0.00.040.823 I print_info: freq_scale_train = 1
0.00.040.823 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.824 I print_info: rope_finetuned   = unknown
0.00.040.824 I print_info: ssm_d_conv       = 0
0.00.040.824 I print_info: ssm_d_inner      = 0
0.00.040.824 I print_info: ssm_d_state      = 0
0.00.040.824 I print_info: ssm_dt_rank      = 0
0.00.040.824 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.826 I print_info: model type       = 1.4B
0.00.040.826 I print_info: model params     = 1.41 B
0.00.040.826 I print_info: general.name     = 1.4B
0.00.040.826 I print_info: vocab type       = BPE
0.00.040.827 I print_info: n_vocab          = 50304
0.00.040.827 I print_info: n_merges         = 50009
0.00.040.827 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.827 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.827 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.827 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.828 I print_info: LF token         = 187 ''
0.00.040.828 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.828 I print_info: max token length = 1024
0.00.040.829 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.385 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.399 I load_tensors: offloading output layer to GPU
0.00.596.400 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.434 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.596.436 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.597.918 I llama_init_from_model: n_seq_max     = 1
0.00.597.922 I llama_init_from_model: n_ctx         = 2048
0.00.597.922 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.597.923 I llama_init_from_model: n_batch       = 2048
0.00.597.924 I llama_init_from_model: n_ubatch      = 512
0.00.597.924 I llama_init_from_model: flash_attn    = 0
0.00.597.926 I llama_init_from_model: freq_base     = 10000.0
0.00.597.927 I llama_init_from_model: freq_scale    = 1
0.00.597.929 I ggml_metal_init: allocating
0.00.598.002 I ggml_metal_init: found device: Apple M4
0.00.598.016 I ggml_metal_init: picking default device: Apple M4
0.00.599.724 I ggml_metal_init: using embedded metal library
0.00.606.293 I ggml_metal_init: GPU name:   Apple M4
0.00.606.297 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.298 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.299 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.299 I ggml_metal_init: simdgroup reduction   = true
0.00.606.300 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.300 I ggml_metal_init: has residency sets    = true
0.00.606.300 I ggml_metal_init: has bfloat            = true
0.00.606.300 I ggml_metal_init: use bfloat            = true
0.00.606.301 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.303 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.624.368 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.356 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.677.364 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.391 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.022 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.682.024 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.682.024 I llama_init_from_model: graph nodes  = 967
0.00.682.025 I llama_init_from_model: graph splits = 2
0.00.682.029 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.682.153 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.154 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.697 I main: llama threadpool init, n_threads = 4
0.00.737.748 I 
0.00.737.768 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.768 I 
0.00.737.923 I sampler seed: 1234
0.00.737.927 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.942 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.943 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.943 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.462.332 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53625.38 tokens per second)
0.01.462.333 I llama_perf_context_print:        load time =     727.80 ms
0.01.462.337 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.40 tokens per second)
0.01.462.339 I llama_perf_context_print:        eval time =     672.89 ms /    63 runs   (   10.68 ms per token,    93.63 tokens per second)
0.01.462.339 I llama_perf_context_print:       total time =     725.35 ms /    70 tokens
0.01.462.628 I ggml_metal_free: deallocating

real	0m1.478s
user	0m0.110s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.901 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.383 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.383 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.384 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.385 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.386 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.386 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.391 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.392 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.392 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.188 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.998 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.999 I llama_model_loader: - type  f32:  194 tensors
0.00.024.999 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.000 I print_info: file format = GGUF V3 (latest)
0.00.025.001 I print_info: file type   = Q4_1
0.00.025.002 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.365 I load: special tokens cache size = 25
0.00.039.828 I load: token to piece cache size = 0.2984 MB
0.00.039.845 I print_info: arch             = gptneox
0.00.039.846 I print_info: vocab_only       = 0
0.00.039.847 I print_info: n_ctx_train      = 2048
0.00.039.847 I print_info: n_embd           = 2048
0.00.039.847 I print_info: n_layer          = 24
0.00.039.851 I print_info: n_head           = 16
0.00.039.852 I print_info: n_head_kv        = 16
0.00.039.852 I print_info: n_rot            = 32
0.00.039.852 I print_info: n_swa            = 0
0.00.039.852 I print_info: n_embd_head_k    = 128
0.00.039.852 I print_info: n_embd_head_v    = 128
0.00.039.853 I print_info: n_gqa            = 1
0.00.039.853 I print_info: n_embd_k_gqa     = 2048
0.00.039.854 I print_info: n_embd_v_gqa     = 2048
0.00.039.854 I print_info: f_norm_eps       = 1.0e-05
0.00.039.855 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.855 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.855 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.855 I print_info: f_logit_scale    = 0.0e+00
0.00.039.856 I print_info: n_ff             = 8192
0.00.039.856 I print_info: n_expert         = 0
0.00.039.856 I print_info: n_expert_used    = 0
0.00.039.856 I print_info: causal attn      = 1
0.00.039.856 I print_info: pooling type     = 0
0.00.039.856 I print_info: rope type        = 2
0.00.039.857 I print_info: rope scaling     = linear
0.00.039.857 I print_info: freq_base_train  = 10000.0
0.00.039.857 I print_info: freq_scale_train = 1
0.00.039.857 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.859 I print_info: rope_finetuned   = unknown
0.00.039.859 I print_info: ssm_d_conv       = 0
0.00.039.859 I print_info: ssm_d_inner      = 0
0.00.039.860 I print_info: ssm_d_state      = 0
0.00.039.860 I print_info: ssm_dt_rank      = 0
0.00.039.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.861 I print_info: model type       = 1.4B
0.00.039.862 I print_info: model params     = 1.41 B
0.00.039.862 I print_info: general.name     = 1.4B
0.00.039.862 I print_info: vocab type       = BPE
0.00.039.863 I print_info: n_vocab          = 50304
0.00.039.863 I print_info: n_merges         = 50009
0.00.039.863 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.863 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.863 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: LF token         = 187 ''
0.00.039.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.893 I print_info: max token length = 1024
0.00.039.894 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.639.810 I load_tensors: offloading 24 repeating layers to GPU
0.00.639.824 I load_tensors: offloading output layer to GPU
0.00.639.825 I load_tensors: offloaded 25/25 layers to GPU
0.00.639.858 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.639.863 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.641.559 I llama_init_from_model: n_seq_max     = 1
0.00.641.562 I llama_init_from_model: n_ctx         = 128
0.00.641.563 I llama_init_from_model: n_ctx_per_seq = 128
0.00.641.563 I llama_init_from_model: n_batch       = 128
0.00.641.563 I llama_init_from_model: n_ubatch      = 128
0.00.641.564 I llama_init_from_model: flash_attn    = 0
0.00.641.565 I llama_init_from_model: freq_base     = 10000.0
0.00.641.566 I llama_init_from_model: freq_scale    = 1
0.00.641.566 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.641.569 I ggml_metal_init: allocating
0.00.641.632 I ggml_metal_init: found device: Apple M4
0.00.641.646 I ggml_metal_init: picking default device: Apple M4
0.00.643.151 I ggml_metal_init: using embedded metal library
0.00.649.890 I ggml_metal_init: GPU name:   Apple M4
0.00.649.899 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.649.900 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.649.901 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.649.902 I ggml_metal_init: simdgroup reduction   = true
0.00.649.902 I ggml_metal_init: simdgroup matrix mul. = true
0.00.649.902 I ggml_metal_init: has residency sets    = true
0.00.649.903 I ggml_metal_init: has bfloat            = true
0.00.649.903 I ggml_metal_init: use bfloat            = true
0.00.649.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.649.918 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.132 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.659 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.672.666 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.672.694 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.675.965 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.675.967 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.675.967 I llama_init_from_model: graph nodes  = 967
0.00.675.968 I llama_init_from_model: graph splits = 2
0.00.675.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.675.971 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.646 I 
0.00.702.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.753 I perplexity: tokenizing the input ..
0.00.709.719 I perplexity: tokenization took 6.963 ms
0.00.709.726 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.846.366 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.847.699 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.847.720 I llama_perf_context_print:        load time =     693.74 ms
0.00.847.721 I llama_perf_context_print: prompt eval time =     135.70 ms /   128 tokens (    1.06 ms per token,   943.25 tokens per second)
0.00.847.722 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.847.724 I llama_perf_context_print:       total time =     145.08 ms /   129 tokens
0.00.848.156 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.081s
sys	0m0.142s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.010.286 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.067 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.077 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.081 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.081 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.083 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.864 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.778 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.780 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.781 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.782 I llama_model_loader: - type  f32:  194 tensors
0.00.026.782 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.782 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.783 I print_info: file format = GGUF V3 (latest)
0.00.026.783 I print_info: file type   = Q5_0
0.00.026.785 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.264 I load: special tokens cache size = 25
0.00.041.858 I load: token to piece cache size = 0.2984 MB
0.00.041.876 I print_info: arch             = gptneox
0.00.041.877 I print_info: vocab_only       = 0
0.00.041.877 I print_info: n_ctx_train      = 2048
0.00.041.877 I print_info: n_embd           = 2048
0.00.041.877 I print_info: n_layer          = 24
0.00.041.881 I print_info: n_head           = 16
0.00.041.882 I print_info: n_head_kv        = 16
0.00.041.882 I print_info: n_rot            = 32
0.00.041.882 I print_info: n_swa            = 0
0.00.041.883 I print_info: n_embd_head_k    = 128
0.00.041.883 I print_info: n_embd_head_v    = 128
0.00.041.883 I print_info: n_gqa            = 1
0.00.041.884 I print_info: n_embd_k_gqa     = 2048
0.00.041.887 I print_info: n_embd_v_gqa     = 2048
0.00.041.887 I print_info: f_norm_eps       = 1.0e-05
0.00.041.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.889 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.890 I print_info: f_logit_scale    = 0.0e+00
0.00.041.890 I print_info: n_ff             = 8192
0.00.041.891 I print_info: n_expert         = 0
0.00.041.891 I print_info: n_expert_used    = 0
0.00.041.891 I print_info: causal attn      = 1
0.00.041.891 I print_info: pooling type     = 0
0.00.041.891 I print_info: rope type        = 2
0.00.041.892 I print_info: rope scaling     = linear
0.00.041.892 I print_info: freq_base_train  = 10000.0
0.00.041.892 I print_info: freq_scale_train = 1
0.00.041.893 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.893 I print_info: rope_finetuned   = unknown
0.00.041.893 I print_info: ssm_d_conv       = 0
0.00.041.893 I print_info: ssm_d_inner      = 0
0.00.041.893 I print_info: ssm_d_state      = 0
0.00.041.893 I print_info: ssm_dt_rank      = 0
0.00.041.896 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.896 I print_info: model type       = 1.4B
0.00.041.896 I print_info: model params     = 1.41 B
0.00.041.896 I print_info: general.name     = 1.4B
0.00.041.897 I print_info: vocab type       = BPE
0.00.041.897 I print_info: n_vocab          = 50304
0.00.041.897 I print_info: n_merges         = 50009
0.00.041.898 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.898 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.898 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.898 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.898 I print_info: LF token         = 187 ''
0.00.041.899 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.899 I print_info: max token length = 1024
0.00.041.899 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.637 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.653 I load_tensors: offloading output layer to GPU
0.00.692.654 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.688 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.692.689 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.694.298 I llama_init_from_model: n_seq_max     = 1
0.00.694.302 I llama_init_from_model: n_ctx         = 2048
0.00.694.302 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.694.303 I llama_init_from_model: n_batch       = 2048
0.00.694.303 I llama_init_from_model: n_ubatch      = 512
0.00.694.304 I llama_init_from_model: flash_attn    = 0
0.00.694.306 I llama_init_from_model: freq_base     = 10000.0
0.00.694.306 I llama_init_from_model: freq_scale    = 1
0.00.694.309 I ggml_metal_init: allocating
0.00.694.394 I ggml_metal_init: found device: Apple M4
0.00.694.408 I ggml_metal_init: picking default device: Apple M4
0.00.695.922 I ggml_metal_init: using embedded metal library
0.00.702.511 I ggml_metal_init: GPU name:   Apple M4
0.00.702.517 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.518 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.519 I ggml_metal_init: simdgroup reduction   = true
0.00.702.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.519 I ggml_metal_init: has residency sets    = true
0.00.702.520 I ggml_metal_init: has bfloat            = true
0.00.702.520 I ggml_metal_init: use bfloat            = true
0.00.702.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.525 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.719.823 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.793.172 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.793.179 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.793.202 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.797.480 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.797.482 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.797.482 I llama_init_from_model: graph nodes  = 967
0.00.797.482 I llama_init_from_model: graph splits = 2
0.00.797.487 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.797.611 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.797.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.856.995 I main: llama threadpool init, n_threads = 4
0.00.857.046 I 
0.00.857.066 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.068 I 
0.00.857.248 I sampler seed: 1234
0.00.857.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.857.268 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.857.270 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.857.270 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.642.643 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52398.52 tokens per second)
0.01.642.644 I llama_perf_context_print:        load time =     845.99 ms
0.01.642.645 I llama_perf_context_print: prompt eval time =      50.37 ms /     7 tokens (    7.20 ms per token,   138.97 tokens per second)
0.01.642.646 I llama_perf_context_print:        eval time =     732.17 ms /    63 runs   (   11.62 ms per token,    86.05 tokens per second)
0.01.642.647 I llama_perf_context_print:       total time =     786.36 ms /    70 tokens
0.01.642.878 I ggml_metal_free: deallocating

real	0m1.663s
user	0m0.111s
sys	0m0.223s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.416 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.690 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.696 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.702 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.703 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.703 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.704 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.705 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.705 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.707 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.707 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.709 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.430 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.309 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.309 I llama_model_loader: - type  f32:  194 tensors
0.00.026.310 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.310 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.311 I print_info: file format = GGUF V3 (latest)
0.00.026.311 I print_info: file type   = Q5_0
0.00.026.312 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.330 I load: special tokens cache size = 25
0.00.040.872 I load: token to piece cache size = 0.2984 MB
0.00.040.890 I print_info: arch             = gptneox
0.00.040.891 I print_info: vocab_only       = 0
0.00.040.891 I print_info: n_ctx_train      = 2048
0.00.040.892 I print_info: n_embd           = 2048
0.00.040.892 I print_info: n_layer          = 24
0.00.040.896 I print_info: n_head           = 16
0.00.040.896 I print_info: n_head_kv        = 16
0.00.040.897 I print_info: n_rot            = 32
0.00.040.897 I print_info: n_swa            = 0
0.00.040.897 I print_info: n_embd_head_k    = 128
0.00.040.897 I print_info: n_embd_head_v    = 128
0.00.040.897 I print_info: n_gqa            = 1
0.00.040.898 I print_info: n_embd_k_gqa     = 2048
0.00.040.899 I print_info: n_embd_v_gqa     = 2048
0.00.040.899 I print_info: f_norm_eps       = 1.0e-05
0.00.040.901 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.901 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.901 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.901 I print_info: f_logit_scale    = 0.0e+00
0.00.040.902 I print_info: n_ff             = 8192
0.00.040.902 I print_info: n_expert         = 0
0.00.040.902 I print_info: n_expert_used    = 0
0.00.040.902 I print_info: causal attn      = 1
0.00.040.902 I print_info: pooling type     = 0
0.00.040.902 I print_info: rope type        = 2
0.00.040.905 I print_info: rope scaling     = linear
0.00.040.905 I print_info: freq_base_train  = 10000.0
0.00.040.905 I print_info: freq_scale_train = 1
0.00.040.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.905 I print_info: rope_finetuned   = unknown
0.00.040.906 I print_info: ssm_d_conv       = 0
0.00.040.906 I print_info: ssm_d_inner      = 0
0.00.040.906 I print_info: ssm_d_state      = 0
0.00.040.906 I print_info: ssm_dt_rank      = 0
0.00.040.906 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.906 I print_info: model type       = 1.4B
0.00.040.907 I print_info: model params     = 1.41 B
0.00.040.907 I print_info: general.name     = 1.4B
0.00.040.907 I print_info: vocab type       = BPE
0.00.040.908 I print_info: n_vocab          = 50304
0.00.040.909 I print_info: n_merges         = 50009
0.00.040.909 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.909 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.910 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.910 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.910 I print_info: LF token         = 187 ''
0.00.040.910 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.910 I print_info: max token length = 1024
0.00.040.911 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.475 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.493 I load_tensors: offloading output layer to GPU
0.00.664.494 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.525 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.664.527 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.665.840 I llama_init_from_model: n_seq_max     = 1
0.00.665.844 I llama_init_from_model: n_ctx         = 128
0.00.665.845 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.845 I llama_init_from_model: n_batch       = 128
0.00.665.845 I llama_init_from_model: n_ubatch      = 128
0.00.665.846 I llama_init_from_model: flash_attn    = 0
0.00.665.848 I llama_init_from_model: freq_base     = 10000.0
0.00.665.848 I llama_init_from_model: freq_scale    = 1
0.00.665.849 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.851 I ggml_metal_init: allocating
0.00.665.952 I ggml_metal_init: found device: Apple M4
0.00.665.967 I ggml_metal_init: picking default device: Apple M4
0.00.667.458 I ggml_metal_init: using embedded metal library
0.00.673.482 I ggml_metal_init: GPU name:   Apple M4
0.00.673.489 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.673.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.673.490 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.673.493 I ggml_metal_init: simdgroup reduction   = true
0.00.673.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.673.494 I ggml_metal_init: has residency sets    = true
0.00.673.494 I ggml_metal_init: has bfloat            = true
0.00.673.494 I ggml_metal_init: use bfloat            = true
0.00.673.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.673.501 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.691.694 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.231 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.236 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.271 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.698.492 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.698.494 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.698.494 I llama_init_from_model: graph nodes  = 967
0.00.698.495 I llama_init_from_model: graph splits = 2
0.00.698.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.698.499 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.723 I 
0.00.724.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.844 I perplexity: tokenizing the input ..
0.00.731.075 I perplexity: tokenization took 6.227 ms
0.00.731.085 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.865.456 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.866.780 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.866.794 I llama_perf_context_print:        load time =     714.30 ms
0.00.866.795 I llama_perf_context_print: prompt eval time =     133.53 ms /   128 tokens (    1.04 ms per token,   958.56 tokens per second)
0.00.866.795 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.866.796 I llama_perf_context_print:       total time =     142.07 ms /   129 tokens
0.00.867.143 I ggml_metal_free: deallocating

real	0m0.883s
user	0m0.077s
sys	0m0.124s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.030 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.035 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.572 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.573 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.574 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.574 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.574 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.575 I llama_model_loader: - type  f32:  194 tensors
0.00.025.575 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.576 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.576 I print_info: file format = GGUF V3 (latest)
0.00.025.577 I print_info: file type   = Q5_1
0.00.025.578 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.754 I load: special tokens cache size = 25
0.00.040.013 I load: token to piece cache size = 0.2984 MB
0.00.040.027 I print_info: arch             = gptneox
0.00.040.028 I print_info: vocab_only       = 0
0.00.040.028 I print_info: n_ctx_train      = 2048
0.00.040.028 I print_info: n_embd           = 2048
0.00.040.028 I print_info: n_layer          = 24
0.00.040.031 I print_info: n_head           = 16
0.00.040.032 I print_info: n_head_kv        = 16
0.00.040.032 I print_info: n_rot            = 32
0.00.040.032 I print_info: n_swa            = 0
0.00.040.032 I print_info: n_embd_head_k    = 128
0.00.040.033 I print_info: n_embd_head_v    = 128
0.00.040.033 I print_info: n_gqa            = 1
0.00.040.034 I print_info: n_embd_k_gqa     = 2048
0.00.040.035 I print_info: n_embd_v_gqa     = 2048
0.00.040.035 I print_info: f_norm_eps       = 1.0e-05
0.00.040.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.036 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.036 I print_info: f_logit_scale    = 0.0e+00
0.00.040.038 I print_info: n_ff             = 8192
0.00.040.038 I print_info: n_expert         = 0
0.00.040.038 I print_info: n_expert_used    = 0
0.00.040.038 I print_info: causal attn      = 1
0.00.040.038 I print_info: pooling type     = 0
0.00.040.038 I print_info: rope type        = 2
0.00.040.039 I print_info: rope scaling     = linear
0.00.040.039 I print_info: freq_base_train  = 10000.0
0.00.040.039 I print_info: freq_scale_train = 1
0.00.040.039 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.040 I print_info: rope_finetuned   = unknown
0.00.040.040 I print_info: ssm_d_conv       = 0
0.00.040.040 I print_info: ssm_d_inner      = 0
0.00.040.040 I print_info: ssm_d_state      = 0
0.00.040.040 I print_info: ssm_dt_rank      = 0
0.00.040.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.040 I print_info: model type       = 1.4B
0.00.040.041 I print_info: model params     = 1.41 B
0.00.040.041 I print_info: general.name     = 1.4B
0.00.040.041 I print_info: vocab type       = BPE
0.00.040.041 I print_info: n_vocab          = 50304
0.00.040.042 I print_info: n_merges         = 50009
0.00.040.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.042 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: LF token         = 187 ''
0.00.040.043 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.043 I print_info: max token length = 1024
0.00.040.044 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.651.265 I load_tensors: offloading 24 repeating layers to GPU
0.00.651.283 I load_tensors: offloading output layer to GPU
0.00.651.284 I load_tensors: offloaded 25/25 layers to GPU
0.00.651.318 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.651.319 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.653.357 I llama_init_from_model: n_seq_max     = 1
0.00.653.361 I llama_init_from_model: n_ctx         = 2048
0.00.653.362 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.653.362 I llama_init_from_model: n_batch       = 2048
0.00.653.363 I llama_init_from_model: n_ubatch      = 512
0.00.653.364 I llama_init_from_model: flash_attn    = 0
0.00.653.365 I llama_init_from_model: freq_base     = 10000.0
0.00.653.366 I llama_init_from_model: freq_scale    = 1
0.00.653.368 I ggml_metal_init: allocating
0.00.653.382 I ggml_metal_init: found device: Apple M4
0.00.653.395 I ggml_metal_init: picking default device: Apple M4
0.00.654.618 I ggml_metal_init: using embedded metal library
0.00.661.189 I ggml_metal_init: GPU name:   Apple M4
0.00.661.193 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.194 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.195 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.195 I ggml_metal_init: simdgroup reduction   = true
0.00.661.196 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.196 I ggml_metal_init: has residency sets    = true
0.00.661.196 I ggml_metal_init: has bfloat            = true
0.00.661.196 I ggml_metal_init: use bfloat            = true
0.00.661.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.437 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.733.435 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.733.441 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.733.464 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.712 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.714 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.715 I llama_init_from_model: graph nodes  = 967
0.00.738.715 I llama_init_from_model: graph splits = 2
0.00.738.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.853 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.681 I main: llama threadpool init, n_threads = 4
0.00.799.726 I 
0.00.799.745 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.747 I 
0.00.799.909 I sampler seed: 1234
0.00.799.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.953 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.956 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.956 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.643.646 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51636.36 tokens per second)
0.01.643.646 I llama_perf_context_print:        load time =     790.09 ms
0.01.643.648 I llama_perf_context_print: prompt eval time =      52.57 ms /     7 tokens (    7.51 ms per token,   133.17 tokens per second)
0.01.643.654 I llama_perf_context_print:        eval time =     788.21 ms /    63 runs   (   12.51 ms per token,    79.93 tokens per second)
0.01.643.656 I llama_perf_context_print:       total time =     844.69 ms /    70 tokens
0.01.643.904 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.109s
sys	0m0.220s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.761 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.960 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.965 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.967 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.968 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.968 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.969 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.969 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.970 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.970 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.971 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.971 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.971 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.973 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.975 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.976 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.699 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.710 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.562 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.564 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.564 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.565 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.566 I llama_model_loader: - type  f32:  194 tensors
0.00.024.566 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.566 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.567 I print_info: file format = GGUF V3 (latest)
0.00.024.568 I print_info: file type   = Q5_1
0.00.024.569 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.919 I load: special tokens cache size = 25
0.00.040.584 I load: token to piece cache size = 0.2984 MB
0.00.040.602 I print_info: arch             = gptneox
0.00.040.603 I print_info: vocab_only       = 0
0.00.040.603 I print_info: n_ctx_train      = 2048
0.00.040.604 I print_info: n_embd           = 2048
0.00.040.604 I print_info: n_layer          = 24
0.00.040.610 I print_info: n_head           = 16
0.00.040.610 I print_info: n_head_kv        = 16
0.00.040.611 I print_info: n_rot            = 32
0.00.040.611 I print_info: n_swa            = 0
0.00.040.611 I print_info: n_embd_head_k    = 128
0.00.040.611 I print_info: n_embd_head_v    = 128
0.00.040.611 I print_info: n_gqa            = 1
0.00.040.612 I print_info: n_embd_k_gqa     = 2048
0.00.040.613 I print_info: n_embd_v_gqa     = 2048
0.00.040.613 I print_info: f_norm_eps       = 1.0e-05
0.00.040.614 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.614 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.614 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.614 I print_info: f_logit_scale    = 0.0e+00
0.00.040.615 I print_info: n_ff             = 8192
0.00.040.616 I print_info: n_expert         = 0
0.00.040.618 I print_info: n_expert_used    = 0
0.00.040.618 I print_info: causal attn      = 1
0.00.040.618 I print_info: pooling type     = 0
0.00.040.618 I print_info: rope type        = 2
0.00.040.619 I print_info: rope scaling     = linear
0.00.040.619 I print_info: freq_base_train  = 10000.0
0.00.040.619 I print_info: freq_scale_train = 1
0.00.040.620 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.620 I print_info: rope_finetuned   = unknown
0.00.040.620 I print_info: ssm_d_conv       = 0
0.00.040.621 I print_info: ssm_d_inner      = 0
0.00.040.621 I print_info: ssm_d_state      = 0
0.00.040.621 I print_info: ssm_dt_rank      = 0
0.00.040.621 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.621 I print_info: model type       = 1.4B
0.00.040.621 I print_info: model params     = 1.41 B
0.00.040.622 I print_info: general.name     = 1.4B
0.00.040.622 I print_info: vocab type       = BPE
0.00.040.622 I print_info: n_vocab          = 50304
0.00.040.623 I print_info: n_merges         = 50009
0.00.040.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.623 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.623 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.623 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.623 I print_info: LF token         = 187 ''
0.00.040.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.624 I print_info: max token length = 1024
0.00.040.624 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.646.356 I load_tensors: offloading 24 repeating layers to GPU
0.00.646.371 I load_tensors: offloading output layer to GPU
0.00.646.372 I load_tensors: offloaded 25/25 layers to GPU
0.00.646.403 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.646.405 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.648.296 I llama_init_from_model: n_seq_max     = 1
0.00.648.301 I llama_init_from_model: n_ctx         = 128
0.00.648.301 I llama_init_from_model: n_ctx_per_seq = 128
0.00.648.302 I llama_init_from_model: n_batch       = 128
0.00.648.302 I llama_init_from_model: n_ubatch      = 128
0.00.648.302 I llama_init_from_model: flash_attn    = 0
0.00.648.305 I llama_init_from_model: freq_base     = 10000.0
0.00.648.305 I llama_init_from_model: freq_scale    = 1
0.00.648.306 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.648.309 I ggml_metal_init: allocating
0.00.648.358 I ggml_metal_init: found device: Apple M4
0.00.648.369 I ggml_metal_init: picking default device: Apple M4
0.00.649.704 I ggml_metal_init: using embedded metal library
0.00.656.006 I ggml_metal_init: GPU name:   Apple M4
0.00.656.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.010 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.012 I ggml_metal_init: simdgroup reduction   = true
0.00.656.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.012 I ggml_metal_init: has residency sets    = true
0.00.656.013 I ggml_metal_init: has bfloat            = true
0.00.656.013 I ggml_metal_init: use bfloat            = true
0.00.656.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.017 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.673.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.218 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.677.222 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.254 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.307 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.680.309 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.680.309 I llama_init_from_model: graph nodes  = 967
0.00.680.310 I llama_init_from_model: graph splits = 2
0.00.680.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.313 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.302 I 
0.00.708.387 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.412 I perplexity: tokenizing the input ..
0.00.715.659 I perplexity: tokenization took 7.242 ms
0.00.715.667 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.554 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.852.989 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.853.010 I llama_perf_context_print:        load time =     699.53 ms
0.00.853.012 I llama_perf_context_print: prompt eval time =     134.92 ms /   128 tokens (    1.05 ms per token,   948.74 tokens per second)
0.00.853.015 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.015 I llama_perf_context_print:       total time =     144.71 ms /   129 tokens
0.00.853.420 I ggml_metal_free: deallocating

real	0m0.867s
user	0m0.081s
sys	0m0.141s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.162 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.569 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.582 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.583 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.583 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.584 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.584 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.585 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.586 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.587 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.749 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.750 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.751 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.752 I llama_model_loader: - type  f32:  194 tensors
0.00.025.752 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.752 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.752 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.753 I print_info: file format = GGUF V3 (latest)
0.00.025.753 I print_info: file type   = Q2_K - Medium
0.00.025.754 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.573 I load: special tokens cache size = 25
0.00.039.792 I load: token to piece cache size = 0.2984 MB
0.00.039.807 I print_info: arch             = gptneox
0.00.039.808 I print_info: vocab_only       = 0
0.00.039.808 I print_info: n_ctx_train      = 2048
0.00.039.808 I print_info: n_embd           = 2048
0.00.039.808 I print_info: n_layer          = 24
0.00.039.811 I print_info: n_head           = 16
0.00.039.812 I print_info: n_head_kv        = 16
0.00.039.812 I print_info: n_rot            = 32
0.00.039.812 I print_info: n_swa            = 0
0.00.039.812 I print_info: n_embd_head_k    = 128
0.00.039.812 I print_info: n_embd_head_v    = 128
0.00.039.813 I print_info: n_gqa            = 1
0.00.039.814 I print_info: n_embd_k_gqa     = 2048
0.00.039.815 I print_info: n_embd_v_gqa     = 2048
0.00.039.815 I print_info: f_norm_eps       = 1.0e-05
0.00.039.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.816 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.816 I print_info: f_logit_scale    = 0.0e+00
0.00.039.817 I print_info: n_ff             = 8192
0.00.039.817 I print_info: n_expert         = 0
0.00.039.817 I print_info: n_expert_used    = 0
0.00.039.817 I print_info: causal attn      = 1
0.00.039.817 I print_info: pooling type     = 0
0.00.039.819 I print_info: rope type        = 2
0.00.039.820 I print_info: rope scaling     = linear
0.00.039.820 I print_info: freq_base_train  = 10000.0
0.00.039.820 I print_info: freq_scale_train = 1
0.00.039.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.821 I print_info: rope_finetuned   = unknown
0.00.039.821 I print_info: ssm_d_conv       = 0
0.00.039.821 I print_info: ssm_d_inner      = 0
0.00.039.821 I print_info: ssm_d_state      = 0
0.00.039.821 I print_info: ssm_dt_rank      = 0
0.00.039.821 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.821 I print_info: model type       = 1.4B
0.00.039.822 I print_info: model params     = 1.41 B
0.00.039.822 I print_info: general.name     = 1.4B
0.00.039.822 I print_info: vocab type       = BPE
0.00.039.822 I print_info: n_vocab          = 50304
0.00.039.823 I print_info: n_merges         = 50009
0.00.039.823 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.826 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: LF token         = 187 ''
0.00.039.830 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: max token length = 1024
0.00.039.830 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.338.266 I load_tensors: offloading 24 repeating layers to GPU
0.00.338.280 I load_tensors: offloading output layer to GPU
0.00.338.281 I load_tensors: offloaded 25/25 layers to GPU
0.00.338.314 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.338.316 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.340.019 I llama_init_from_model: n_seq_max     = 1
0.00.340.022 I llama_init_from_model: n_ctx         = 2048
0.00.340.022 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.340.023 I llama_init_from_model: n_batch       = 2048
0.00.340.023 I llama_init_from_model: n_ubatch      = 512
0.00.340.023 I llama_init_from_model: flash_attn    = 0
0.00.340.026 I llama_init_from_model: freq_base     = 10000.0
0.00.340.026 I llama_init_from_model: freq_scale    = 1
0.00.340.029 I ggml_metal_init: allocating
0.00.340.151 I ggml_metal_init: found device: Apple M4
0.00.340.169 I ggml_metal_init: picking default device: Apple M4
0.00.341.749 I ggml_metal_init: using embedded metal library
0.00.347.495 I ggml_metal_init: GPU name:   Apple M4
0.00.347.505 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.347.506 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.347.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.347.508 I ggml_metal_init: simdgroup reduction   = true
0.00.347.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.347.508 I ggml_metal_init: has residency sets    = true
0.00.347.508 I ggml_metal_init: has bfloat            = true
0.00.347.509 I ggml_metal_init: use bfloat            = true
0.00.347.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.347.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.368.856 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.428.866 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.428.872 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.428.896 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.433.637 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.433.639 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.433.640 I llama_init_from_model: graph nodes  = 967
0.00.433.640 I llama_init_from_model: graph splits = 2
0.00.433.646 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.433.775 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.433.776 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.860 I main: llama threadpool init, n_threads = 4
0.00.494.913 I 
0.00.494.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.935 I 
0.00.495.118 I sampler seed: 1234
0.00.495.122 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.495.170 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.495.173 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.495.173 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.172.677 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.172.678 I llama_perf_context_print:        load time =     483.97 ms
0.01.172.679 I llama_perf_context_print: prompt eval time =      39.82 ms /     7 tokens (    5.69 ms per token,   175.80 tokens per second)
0.01.172.681 I llama_perf_context_print:        eval time =     634.95 ms /    63 runs   (   10.08 ms per token,    99.22 tokens per second)
0.01.172.682 I llama_perf_context_print:       total time =     678.55 ms /    70 tokens
0.01.172.893 I ggml_metal_free: deallocating

real	0m1.194s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.555 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.830 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.833 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.834 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.841 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.842 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.842 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.656 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.664 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.774 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.777 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.777 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.778 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.778 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.778 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.779 I llama_model_loader: - type  f32:  194 tensors
0.00.026.779 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.780 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.780 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.781 I print_info: file format = GGUF V3 (latest)
0.00.026.781 I print_info: file type   = Q2_K - Medium
0.00.026.788 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.941 I load: special tokens cache size = 25
0.00.041.222 I load: token to piece cache size = 0.2984 MB
0.00.041.241 I print_info: arch             = gptneox
0.00.041.241 I print_info: vocab_only       = 0
0.00.041.242 I print_info: n_ctx_train      = 2048
0.00.041.242 I print_info: n_embd           = 2048
0.00.041.242 I print_info: n_layer          = 24
0.00.041.246 I print_info: n_head           = 16
0.00.041.247 I print_info: n_head_kv        = 16
0.00.041.247 I print_info: n_rot            = 32
0.00.041.247 I print_info: n_swa            = 0
0.00.041.247 I print_info: n_embd_head_k    = 128
0.00.041.247 I print_info: n_embd_head_v    = 128
0.00.041.248 I print_info: n_gqa            = 1
0.00.041.249 I print_info: n_embd_k_gqa     = 2048
0.00.041.249 I print_info: n_embd_v_gqa     = 2048
0.00.041.250 I print_info: f_norm_eps       = 1.0e-05
0.00.041.250 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.250 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.250 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.252 I print_info: f_logit_scale    = 0.0e+00
0.00.041.256 I print_info: n_ff             = 8192
0.00.041.256 I print_info: n_expert         = 0
0.00.041.257 I print_info: n_expert_used    = 0
0.00.041.257 I print_info: causal attn      = 1
0.00.041.257 I print_info: pooling type     = 0
0.00.041.257 I print_info: rope type        = 2
0.00.041.257 I print_info: rope scaling     = linear
0.00.041.257 I print_info: freq_base_train  = 10000.0
0.00.041.259 I print_info: freq_scale_train = 1
0.00.041.259 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.259 I print_info: rope_finetuned   = unknown
0.00.041.260 I print_info: ssm_d_conv       = 0
0.00.041.260 I print_info: ssm_d_inner      = 0
0.00.041.260 I print_info: ssm_d_state      = 0
0.00.041.260 I print_info: ssm_dt_rank      = 0
0.00.041.260 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.260 I print_info: model type       = 1.4B
0.00.041.261 I print_info: model params     = 1.41 B
0.00.041.261 I print_info: general.name     = 1.4B
0.00.041.261 I print_info: vocab type       = BPE
0.00.041.261 I print_info: n_vocab          = 50304
0.00.041.261 I print_info: n_merges         = 50009
0.00.041.262 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.262 I print_info: LF token         = 187 ''
0.00.041.263 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.263 I print_info: max token length = 1024
0.00.041.263 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.355.640 I load_tensors: offloading 24 repeating layers to GPU
0.00.355.656 I load_tensors: offloading output layer to GPU
0.00.355.656 I load_tensors: offloaded 25/25 layers to GPU
0.00.355.689 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.355.690 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.356.841 I llama_init_from_model: n_seq_max     = 1
0.00.356.844 I llama_init_from_model: n_ctx         = 128
0.00.356.844 I llama_init_from_model: n_ctx_per_seq = 128
0.00.356.845 I llama_init_from_model: n_batch       = 128
0.00.356.845 I llama_init_from_model: n_ubatch      = 128
0.00.356.845 I llama_init_from_model: flash_attn    = 0
0.00.356.847 I llama_init_from_model: freq_base     = 10000.0
0.00.356.848 I llama_init_from_model: freq_scale    = 1
0.00.356.848 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.356.851 I ggml_metal_init: allocating
0.00.356.941 I ggml_metal_init: found device: Apple M4
0.00.356.954 I ggml_metal_init: picking default device: Apple M4
0.00.358.569 I ggml_metal_init: using embedded metal library
0.00.364.007 I ggml_metal_init: GPU name:   Apple M4
0.00.364.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.364.021 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.364.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.364.022 I ggml_metal_init: simdgroup reduction   = true
0.00.364.022 I ggml_metal_init: simdgroup matrix mul. = true
0.00.364.023 I ggml_metal_init: has residency sets    = true
0.00.364.023 I ggml_metal_init: has bfloat            = true
0.00.364.023 I ggml_metal_init: use bfloat            = true
0.00.364.025 I ggml_metal_init: hasUnifiedMemory      = true
0.00.364.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.973 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.389.614 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.389.618 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.389.654 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.393.243 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.393.245 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.393.246 I llama_init_from_model: graph nodes  = 967
0.00.393.246 I llama_init_from_model: graph splits = 2
0.00.393.250 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.393.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.792 I 
0.00.422.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.915 I perplexity: tokenizing the input ..
0.00.429.279 I perplexity: tokenization took 6.362 ms
0.00.429.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.964 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.562.378 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.562.396 I llama_perf_context_print:        load time =     412.23 ms
0.00.562.397 I llama_perf_context_print: prompt eval time =     131.04 ms /   128 tokens (    1.02 ms per token,   976.82 tokens per second)
0.00.562.397 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.562.398 I llama_perf_context_print:       total time =     139.61 ms /   129 tokens
0.00.562.803 I ggml_metal_free: deallocating

real	0m0.578s
user	0m0.081s
sys	0m0.099s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.863 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.462 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.475 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.476 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.479 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.482 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.482 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.976 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.976 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.977 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.977 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.978 I llama_model_loader: - type  f32:  194 tensors
0.00.024.978 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.978 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.979 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.979 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.979 I print_info: file format = GGUF V3 (latest)
0.00.024.980 I print_info: file type   = Q3_K - Medium
0.00.024.981 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.135 I load: special tokens cache size = 25
0.00.039.500 I load: token to piece cache size = 0.2984 MB
0.00.039.515 I print_info: arch             = gptneox
0.00.039.516 I print_info: vocab_only       = 0
0.00.039.516 I print_info: n_ctx_train      = 2048
0.00.039.516 I print_info: n_embd           = 2048
0.00.039.517 I print_info: n_layer          = 24
0.00.039.519 I print_info: n_head           = 16
0.00.039.520 I print_info: n_head_kv        = 16
0.00.039.523 I print_info: n_rot            = 32
0.00.039.523 I print_info: n_swa            = 0
0.00.039.523 I print_info: n_embd_head_k    = 128
0.00.039.523 I print_info: n_embd_head_v    = 128
0.00.039.524 I print_info: n_gqa            = 1
0.00.039.525 I print_info: n_embd_k_gqa     = 2048
0.00.039.526 I print_info: n_embd_v_gqa     = 2048
0.00.039.526 I print_info: f_norm_eps       = 1.0e-05
0.00.039.527 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.527 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.527 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.527 I print_info: f_logit_scale    = 0.0e+00
0.00.039.528 I print_info: n_ff             = 8192
0.00.039.528 I print_info: n_expert         = 0
0.00.039.528 I print_info: n_expert_used    = 0
0.00.039.528 I print_info: causal attn      = 1
0.00.039.528 I print_info: pooling type     = 0
0.00.039.528 I print_info: rope type        = 2
0.00.039.528 I print_info: rope scaling     = linear
0.00.039.529 I print_info: freq_base_train  = 10000.0
0.00.039.530 I print_info: freq_scale_train = 1
0.00.039.530 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.530 I print_info: rope_finetuned   = unknown
0.00.039.531 I print_info: ssm_d_conv       = 0
0.00.039.531 I print_info: ssm_d_inner      = 0
0.00.039.531 I print_info: ssm_d_state      = 0
0.00.039.531 I print_info: ssm_dt_rank      = 0
0.00.039.531 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.531 I print_info: model type       = 1.4B
0.00.039.532 I print_info: model params     = 1.41 B
0.00.039.532 I print_info: general.name     = 1.4B
0.00.039.532 I print_info: vocab type       = BPE
0.00.039.532 I print_info: n_vocab          = 50304
0.00.039.533 I print_info: n_merges         = 50009
0.00.039.533 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.533 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.533 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.533 I print_info: LF token         = 187 ''
0.00.039.534 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.534 I print_info: max token length = 1024
0.00.039.534 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.299 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.314 I load_tensors: offloading output layer to GPU
0.00.443.315 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.346 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.348 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.994 I llama_init_from_model: n_seq_max     = 1
0.00.444.997 I llama_init_from_model: n_ctx         = 2048
0.00.444.998 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.998 I llama_init_from_model: n_batch       = 2048
0.00.444.999 I llama_init_from_model: n_ubatch      = 512
0.00.444.999 I llama_init_from_model: flash_attn    = 0
0.00.445.001 I llama_init_from_model: freq_base     = 10000.0
0.00.445.002 I llama_init_from_model: freq_scale    = 1
0.00.445.016 I ggml_metal_init: allocating
0.00.445.096 I ggml_metal_init: found device: Apple M4
0.00.445.119 I ggml_metal_init: picking default device: Apple M4
0.00.446.706 I ggml_metal_init: using embedded metal library
0.00.452.852 I ggml_metal_init: GPU name:   Apple M4
0.00.452.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.864 I ggml_metal_init: simdgroup reduction   = true
0.00.452.865 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.865 I ggml_metal_init: has residency sets    = true
0.00.452.865 I ggml_metal_init: has bfloat            = true
0.00.452.865 I ggml_metal_init: use bfloat            = true
0.00.452.868 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.873 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.330 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.535.337 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.535.359 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.539.828 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.539.830 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.539.830 I llama_init_from_model: graph nodes  = 967
0.00.539.831 I llama_init_from_model: graph splits = 2
0.00.539.836 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.539.965 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.539.966 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.213 I main: llama threadpool init, n_threads = 4
0.00.594.264 I 
0.00.594.284 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.286 I 
0.00.594.444 I sampler seed: 1234
0.00.594.448 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.594.463 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.594.465 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.594.465 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.327.981 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53183.52 tokens per second)
0.01.327.981 I llama_perf_context_print:        load time =     584.60 ms
0.01.327.983 I llama_perf_context_print: prompt eval time =      40.10 ms /     7 tokens (    5.73 ms per token,   174.56 tokens per second)
0.01.327.984 I llama_perf_context_print:        eval time =     690.61 ms /    63 runs   (   10.96 ms per token,    91.22 tokens per second)
0.01.327.986 I llama_perf_context_print:       total time =     734.51 ms /    70 tokens
0.01.328.197 I ggml_metal_free: deallocating

real	0m1.344s
user	0m0.112s
sys	0m0.193s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.991 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.275 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.282 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.284 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.072 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.734 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.736 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.736 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.737 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.737 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.738 I llama_model_loader: - type  f32:  194 tensors
0.00.024.738 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.739 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.739 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.739 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.740 I print_info: file format = GGUF V3 (latest)
0.00.024.741 I print_info: file type   = Q3_K - Medium
0.00.024.742 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.999 I load: special tokens cache size = 25
0.00.039.654 I load: token to piece cache size = 0.2984 MB
0.00.039.673 I print_info: arch             = gptneox
0.00.039.674 I print_info: vocab_only       = 0
0.00.039.674 I print_info: n_ctx_train      = 2048
0.00.039.674 I print_info: n_embd           = 2048
0.00.039.674 I print_info: n_layer          = 24
0.00.039.678 I print_info: n_head           = 16
0.00.039.679 I print_info: n_head_kv        = 16
0.00.039.679 I print_info: n_rot            = 32
0.00.039.679 I print_info: n_swa            = 0
0.00.039.679 I print_info: n_embd_head_k    = 128
0.00.039.679 I print_info: n_embd_head_v    = 128
0.00.039.680 I print_info: n_gqa            = 1
0.00.039.680 I print_info: n_embd_k_gqa     = 2048
0.00.039.681 I print_info: n_embd_v_gqa     = 2048
0.00.039.681 I print_info: f_norm_eps       = 1.0e-05
0.00.039.682 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.682 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.682 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.685 I print_info: f_logit_scale    = 0.0e+00
0.00.039.685 I print_info: n_ff             = 8192
0.00.039.686 I print_info: n_expert         = 0
0.00.039.686 I print_info: n_expert_used    = 0
0.00.039.686 I print_info: causal attn      = 1
0.00.039.687 I print_info: pooling type     = 0
0.00.039.687 I print_info: rope type        = 2
0.00.039.688 I print_info: rope scaling     = linear
0.00.039.688 I print_info: freq_base_train  = 10000.0
0.00.039.688 I print_info: freq_scale_train = 1
0.00.039.689 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.689 I print_info: rope_finetuned   = unknown
0.00.039.689 I print_info: ssm_d_conv       = 0
0.00.039.689 I print_info: ssm_d_inner      = 0
0.00.039.689 I print_info: ssm_d_state      = 0
0.00.039.689 I print_info: ssm_dt_rank      = 0
0.00.039.689 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.690 I print_info: model type       = 1.4B
0.00.039.690 I print_info: model params     = 1.41 B
0.00.039.690 I print_info: general.name     = 1.4B
0.00.039.690 I print_info: vocab type       = BPE
0.00.039.691 I print_info: n_vocab          = 50304
0.00.039.691 I print_info: n_merges         = 50009
0.00.039.692 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.692 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.692 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.692 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.692 I print_info: LF token         = 187 ''
0.00.039.693 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.693 I print_info: max token length = 1024
0.00.039.693 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.433.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.433.795 I load_tensors: offloading output layer to GPU
0.00.433.796 I load_tensors: offloaded 25/25 layers to GPU
0.00.433.829 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.433.831 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.435.500 I llama_init_from_model: n_seq_max     = 1
0.00.435.502 I llama_init_from_model: n_ctx         = 128
0.00.435.503 I llama_init_from_model: n_ctx_per_seq = 128
0.00.435.503 I llama_init_from_model: n_batch       = 128
0.00.435.504 I llama_init_from_model: n_ubatch      = 128
0.00.435.504 I llama_init_from_model: flash_attn    = 0
0.00.435.506 I llama_init_from_model: freq_base     = 10000.0
0.00.435.507 I llama_init_from_model: freq_scale    = 1
0.00.435.508 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.435.514 I ggml_metal_init: allocating
0.00.435.581 I ggml_metal_init: found device: Apple M4
0.00.435.595 I ggml_metal_init: picking default device: Apple M4
0.00.437.101 I ggml_metal_init: using embedded metal library
0.00.442.576 I ggml_metal_init: GPU name:   Apple M4
0.00.442.605 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.442.606 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.442.607 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.442.608 I ggml_metal_init: simdgroup reduction   = true
0.00.442.608 I ggml_metal_init: simdgroup matrix mul. = true
0.00.442.608 I ggml_metal_init: has residency sets    = true
0.00.442.609 I ggml_metal_init: has bfloat            = true
0.00.442.609 I ggml_metal_init: use bfloat            = true
0.00.442.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.442.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.463.432 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.467.068 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.467.076 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.467.117 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.470.515 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.470.517 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.470.518 I llama_init_from_model: graph nodes  = 967
0.00.470.518 I llama_init_from_model: graph splits = 2
0.00.470.522 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.470.525 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.731 I 
0.00.495.814 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.842 I perplexity: tokenizing the input ..
0.00.503.251 I perplexity: tokenization took 7.406 ms
0.00.503.261 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.635.999 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.299 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.313 I llama_perf_context_print:        load time =     486.73 ms
0.00.637.315 I llama_perf_context_print: prompt eval time =     131.81 ms /   128 tokens (    1.03 ms per token,   971.12 tokens per second)
0.00.637.315 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.316 I llama_perf_context_print:       total time =     141.59 ms /   129 tokens
0.00.637.707 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.081s
sys	0m0.110s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.187 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.823 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.834 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.837 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.837 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.837 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.839 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.839 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.839 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.840 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.669 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.379 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.380 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.381 I llama_model_loader: - type  f32:  194 tensors
0.00.025.381 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.382 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.382 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.382 I print_info: file format = GGUF V3 (latest)
0.00.025.383 I print_info: file type   = Q4_K - Medium
0.00.025.384 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.643 I load: special tokens cache size = 25
0.00.040.132 I load: token to piece cache size = 0.2984 MB
0.00.040.142 I print_info: arch             = gptneox
0.00.040.143 I print_info: vocab_only       = 0
0.00.040.143 I print_info: n_ctx_train      = 2048
0.00.040.144 I print_info: n_embd           = 2048
0.00.040.144 I print_info: n_layer          = 24
0.00.040.146 I print_info: n_head           = 16
0.00.040.147 I print_info: n_head_kv        = 16
0.00.040.147 I print_info: n_rot            = 32
0.00.040.148 I print_info: n_swa            = 0
0.00.040.148 I print_info: n_embd_head_k    = 128
0.00.040.148 I print_info: n_embd_head_v    = 128
0.00.040.149 I print_info: n_gqa            = 1
0.00.040.149 I print_info: n_embd_k_gqa     = 2048
0.00.040.150 I print_info: n_embd_v_gqa     = 2048
0.00.040.151 I print_info: f_norm_eps       = 1.0e-05
0.00.040.151 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.151 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.152 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.152 I print_info: f_logit_scale    = 0.0e+00
0.00.040.152 I print_info: n_ff             = 8192
0.00.040.153 I print_info: n_expert         = 0
0.00.040.155 I print_info: n_expert_used    = 0
0.00.040.155 I print_info: causal attn      = 1
0.00.040.155 I print_info: pooling type     = 0
0.00.040.156 I print_info: rope type        = 2
0.00.040.156 I print_info: rope scaling     = linear
0.00.040.156 I print_info: freq_base_train  = 10000.0
0.00.040.156 I print_info: freq_scale_train = 1
0.00.040.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.158 I print_info: rope_finetuned   = unknown
0.00.040.158 I print_info: ssm_d_conv       = 0
0.00.040.158 I print_info: ssm_d_inner      = 0
0.00.040.158 I print_info: ssm_d_state      = 0
0.00.040.158 I print_info: ssm_dt_rank      = 0
0.00.040.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.158 I print_info: model type       = 1.4B
0.00.040.159 I print_info: model params     = 1.41 B
0.00.040.159 I print_info: general.name     = 1.4B
0.00.040.159 I print_info: vocab type       = BPE
0.00.040.160 I print_info: n_vocab          = 50304
0.00.040.160 I print_info: n_merges         = 50009
0.00.040.161 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: LF token         = 187 ''
0.00.040.162 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.165 I print_info: max token length = 1024
0.00.040.166 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.536.332 I load_tensors: offloading 24 repeating layers to GPU
0.00.536.348 I load_tensors: offloading output layer to GPU
0.00.536.349 I load_tensors: offloaded 25/25 layers to GPU
0.00.536.383 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.536.384 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.537.817 I llama_init_from_model: n_seq_max     = 1
0.00.537.820 I llama_init_from_model: n_ctx         = 2048
0.00.537.821 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.537.821 I llama_init_from_model: n_batch       = 2048
0.00.537.822 I llama_init_from_model: n_ubatch      = 512
0.00.537.822 I llama_init_from_model: flash_attn    = 0
0.00.537.824 I llama_init_from_model: freq_base     = 10000.0
0.00.537.824 I llama_init_from_model: freq_scale    = 1
0.00.537.826 I ggml_metal_init: allocating
0.00.537.909 I ggml_metal_init: found device: Apple M4
0.00.537.922 I ggml_metal_init: picking default device: Apple M4
0.00.539.505 I ggml_metal_init: using embedded metal library
0.00.545.905 I ggml_metal_init: GPU name:   Apple M4
0.00.545.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.545.911 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.545.912 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.545.912 I ggml_metal_init: simdgroup reduction   = true
0.00.545.913 I ggml_metal_init: simdgroup matrix mul. = true
0.00.545.913 I ggml_metal_init: has residency sets    = true
0.00.545.913 I ggml_metal_init: has bfloat            = true
0.00.545.914 I ggml_metal_init: use bfloat            = true
0.00.545.915 I ggml_metal_init: hasUnifiedMemory      = true
0.00.545.919 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.565.318 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.826 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.624.832 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.624.857 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.629.230 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.629.232 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.629.233 I llama_init_from_model: graph nodes  = 967
0.00.629.233 I llama_init_from_model: graph splits = 2
0.00.629.239 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.629.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.629.368 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.128 I main: llama threadpool init, n_threads = 4
0.00.684.177 I 
0.00.684.197 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.198 I 
0.00.684.353 I sampler seed: 1234
0.00.684.357 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.373 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.375 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.375 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.434.691 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50461.98 tokens per second)
0.01.434.692 I llama_perf_context_print:        load time =     674.21 ms
0.01.434.693 I llama_perf_context_print: prompt eval time =      47.17 ms /     7 tokens (    6.74 ms per token,   148.41 tokens per second)
0.01.434.693 I llama_perf_context_print:        eval time =     700.23 ms /    63 runs   (   11.11 ms per token,    89.97 tokens per second)
0.01.434.694 I llama_perf_context_print:       total time =     751.30 ms /    70 tokens
0.01.434.964 I ggml_metal_free: deallocating

real	0m1.451s
user	0m0.110s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.672 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.541 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.542 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.543 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.543 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.544 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.545 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.545 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.545 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.546 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.546 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.546 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.547 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.549 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.549 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.549 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.173 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.174 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.175 I llama_model_loader: - type  f32:  194 tensors
0.00.024.175 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.175 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.175 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.176 I print_info: file format = GGUF V3 (latest)
0.00.024.177 I print_info: file type   = Q4_K - Medium
0.00.024.178 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.822 I load: special tokens cache size = 25
0.00.039.197 I load: token to piece cache size = 0.2984 MB
0.00.039.215 I print_info: arch             = gptneox
0.00.039.216 I print_info: vocab_only       = 0
0.00.039.216 I print_info: n_ctx_train      = 2048
0.00.039.216 I print_info: n_embd           = 2048
0.00.039.216 I print_info: n_layer          = 24
0.00.039.220 I print_info: n_head           = 16
0.00.039.221 I print_info: n_head_kv        = 16
0.00.039.221 I print_info: n_rot            = 32
0.00.039.221 I print_info: n_swa            = 0
0.00.039.221 I print_info: n_embd_head_k    = 128
0.00.039.222 I print_info: n_embd_head_v    = 128
0.00.039.222 I print_info: n_gqa            = 1
0.00.039.223 I print_info: n_embd_k_gqa     = 2048
0.00.039.223 I print_info: n_embd_v_gqa     = 2048
0.00.039.224 I print_info: f_norm_eps       = 1.0e-05
0.00.039.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.225 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.225 I print_info: f_logit_scale    = 0.0e+00
0.00.039.225 I print_info: n_ff             = 8192
0.00.039.226 I print_info: n_expert         = 0
0.00.039.226 I print_info: n_expert_used    = 0
0.00.039.226 I print_info: causal attn      = 1
0.00.039.228 I print_info: pooling type     = 0
0.00.039.228 I print_info: rope type        = 2
0.00.039.228 I print_info: rope scaling     = linear
0.00.039.228 I print_info: freq_base_train  = 10000.0
0.00.039.229 I print_info: freq_scale_train = 1
0.00.039.229 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.229 I print_info: rope_finetuned   = unknown
0.00.039.229 I print_info: ssm_d_conv       = 0
0.00.039.229 I print_info: ssm_d_inner      = 0
0.00.039.229 I print_info: ssm_d_state      = 0
0.00.039.230 I print_info: ssm_dt_rank      = 0
0.00.039.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.230 I print_info: model type       = 1.4B
0.00.039.230 I print_info: model params     = 1.41 B
0.00.039.230 I print_info: general.name     = 1.4B
0.00.039.231 I print_info: vocab type       = BPE
0.00.039.231 I print_info: n_vocab          = 50304
0.00.039.231 I print_info: n_merges         = 50009
0.00.039.231 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: LF token         = 187 ''
0.00.039.232 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.233 I print_info: max token length = 1024
0.00.039.233 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.505.735 I load_tensors: offloading 24 repeating layers to GPU
0.00.505.749 I load_tensors: offloading output layer to GPU
0.00.505.750 I load_tensors: offloaded 25/25 layers to GPU
0.00.505.784 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.505.785 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.507.470 I llama_init_from_model: n_seq_max     = 1
0.00.507.473 I llama_init_from_model: n_ctx         = 128
0.00.507.473 I llama_init_from_model: n_ctx_per_seq = 128
0.00.507.474 I llama_init_from_model: n_batch       = 128
0.00.507.474 I llama_init_from_model: n_ubatch      = 128
0.00.507.475 I llama_init_from_model: flash_attn    = 0
0.00.507.477 I llama_init_from_model: freq_base     = 10000.0
0.00.507.477 I llama_init_from_model: freq_scale    = 1
0.00.507.478 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.507.481 I ggml_metal_init: allocating
0.00.507.591 I ggml_metal_init: found device: Apple M4
0.00.507.605 I ggml_metal_init: picking default device: Apple M4
0.00.509.232 I ggml_metal_init: using embedded metal library
0.00.516.017 I ggml_metal_init: GPU name:   Apple M4
0.00.516.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.516.029 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.516.030 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.516.030 I ggml_metal_init: simdgroup reduction   = true
0.00.516.031 I ggml_metal_init: simdgroup matrix mul. = true
0.00.516.031 I ggml_metal_init: has residency sets    = true
0.00.516.031 I ggml_metal_init: has bfloat            = true
0.00.516.032 I ggml_metal_init: use bfloat            = true
0.00.516.033 I ggml_metal_init: hasUnifiedMemory      = true
0.00.516.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.535.080 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.538.746 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.538.753 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.538.801 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.541.941 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.541.943 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.541.943 I llama_init_from_model: graph nodes  = 967
0.00.541.944 I llama_init_from_model: graph splits = 2
0.00.541.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.541.950 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.186 I 
0.00.571.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.310 I perplexity: tokenizing the input ..
0.00.578.313 I perplexity: tokenization took 6.999 ms
0.00.578.328 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.721.139 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.722.579 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.722.595 I llama_perf_context_print:        load time =     562.51 ms
0.00.722.595 I llama_perf_context_print: prompt eval time =     141.87 ms /   128 tokens (    1.11 ms per token,   902.21 tokens per second)
0.00.722.596 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.596 I llama_perf_context_print:       total time =     151.41 ms /   129 tokens
0.00.722.979 I ggml_metal_free: deallocating

real	0m0.737s
user	0m0.081s
sys	0m0.118s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.750 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.241 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.243 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.244 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.244 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.245 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.245 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.246 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.249 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.250 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.065 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.800 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.802 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.802 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.802 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.803 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.803 I llama_model_loader: - type  f32:  194 tensors
0.00.026.804 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.804 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.805 I print_info: file format = GGUF V3 (latest)
0.00.026.805 I print_info: file type   = Q5_K - Medium
0.00.026.806 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.020 I load: special tokens cache size = 25
0.00.041.514 I load: token to piece cache size = 0.2984 MB
0.00.041.528 I print_info: arch             = gptneox
0.00.041.529 I print_info: vocab_only       = 0
0.00.041.529 I print_info: n_ctx_train      = 2048
0.00.041.530 I print_info: n_embd           = 2048
0.00.041.530 I print_info: n_layer          = 24
0.00.041.533 I print_info: n_head           = 16
0.00.041.533 I print_info: n_head_kv        = 16
0.00.041.534 I print_info: n_rot            = 32
0.00.041.534 I print_info: n_swa            = 0
0.00.041.534 I print_info: n_embd_head_k    = 128
0.00.041.534 I print_info: n_embd_head_v    = 128
0.00.041.535 I print_info: n_gqa            = 1
0.00.041.538 I print_info: n_embd_k_gqa     = 2048
0.00.041.538 I print_info: n_embd_v_gqa     = 2048
0.00.041.539 I print_info: f_norm_eps       = 1.0e-05
0.00.041.539 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.540 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.540 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.540 I print_info: f_logit_scale    = 0.0e+00
0.00.041.540 I print_info: n_ff             = 8192
0.00.041.541 I print_info: n_expert         = 0
0.00.041.541 I print_info: n_expert_used    = 0
0.00.041.541 I print_info: causal attn      = 1
0.00.041.541 I print_info: pooling type     = 0
0.00.041.541 I print_info: rope type        = 2
0.00.041.541 I print_info: rope scaling     = linear
0.00.041.546 I print_info: freq_base_train  = 10000.0
0.00.041.546 I print_info: freq_scale_train = 1
0.00.041.546 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.546 I print_info: rope_finetuned   = unknown
0.00.041.546 I print_info: ssm_d_conv       = 0
0.00.041.546 I print_info: ssm_d_inner      = 0
0.00.041.547 I print_info: ssm_d_state      = 0
0.00.041.547 I print_info: ssm_dt_rank      = 0
0.00.041.547 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.547 I print_info: model type       = 1.4B
0.00.041.547 I print_info: model params     = 1.41 B
0.00.041.548 I print_info: general.name     = 1.4B
0.00.041.548 I print_info: vocab type       = BPE
0.00.041.548 I print_info: n_vocab          = 50304
0.00.041.548 I print_info: n_merges         = 50009
0.00.041.549 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: LF token         = 187 ''
0.00.041.549 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.550 I print_info: max token length = 1024
0.00.041.550 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.465 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.479 I load_tensors: offloading output layer to GPU
0.00.592.480 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.510 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.592.511 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.783 I llama_init_from_model: n_seq_max     = 1
0.00.593.785 I llama_init_from_model: n_ctx         = 2048
0.00.593.786 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.593.786 I llama_init_from_model: n_batch       = 2048
0.00.593.786 I llama_init_from_model: n_ubatch      = 512
0.00.593.787 I llama_init_from_model: flash_attn    = 0
0.00.593.788 I llama_init_from_model: freq_base     = 10000.0
0.00.593.789 I llama_init_from_model: freq_scale    = 1
0.00.593.790 I ggml_metal_init: allocating
0.00.593.805 I ggml_metal_init: found device: Apple M4
0.00.593.813 I ggml_metal_init: picking default device: Apple M4
0.00.595.034 I ggml_metal_init: using embedded metal library
0.00.601.421 I ggml_metal_init: GPU name:   Apple M4
0.00.601.425 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.425 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.426 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.427 I ggml_metal_init: simdgroup reduction   = true
0.00.601.427 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.427 I ggml_metal_init: has residency sets    = true
0.00.601.428 I ggml_metal_init: has bfloat            = true
0.00.601.428 I ggml_metal_init: use bfloat            = true
0.00.601.429 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.432 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.097 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.460 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.468 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.490 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.675.794 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.675.796 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.675.796 I llama_init_from_model: graph nodes  = 967
0.00.675.796 I llama_init_from_model: graph splits = 2
0.00.675.801 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.675.918 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.675.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.564 I main: llama threadpool init, n_threads = 4
0.00.742.610 I 
0.00.742.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.742.631 I 
0.00.742.787 I sampler seed: 1234
0.00.742.792 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.806 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.808 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.808 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.609.452 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.01.609.453 I llama_perf_context_print:        load time =     731.09 ms
0.01.609.454 I llama_perf_context_print: prompt eval time =      63.84 ms /     7 tokens (    9.12 ms per token,   109.65 tokens per second)
0.01.609.455 I llama_perf_context_print:        eval time =     800.26 ms /    63 runs   (   12.70 ms per token,    78.72 tokens per second)
0.01.609.455 I llama_perf_context_print:       total time =     867.61 ms /    70 tokens
0.01.609.718 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.112s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.964 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.970 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.972 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.972 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.973 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.973 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.973 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.974 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.975 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.976 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.977 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.979 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.979 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.808 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.588 I llama_model_loader: - type  f32:  194 tensors
0.00.025.589 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.589 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.590 I print_info: file format = GGUF V3 (latest)
0.00.025.592 I print_info: file type   = Q5_K - Medium
0.00.025.593 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.596 I load: special tokens cache size = 25
0.00.040.002 I load: token to piece cache size = 0.2984 MB
0.00.040.019 I print_info: arch             = gptneox
0.00.040.020 I print_info: vocab_only       = 0
0.00.040.020 I print_info: n_ctx_train      = 2048
0.00.040.021 I print_info: n_embd           = 2048
0.00.040.021 I print_info: n_layer          = 24
0.00.040.025 I print_info: n_head           = 16
0.00.040.026 I print_info: n_head_kv        = 16
0.00.040.026 I print_info: n_rot            = 32
0.00.040.026 I print_info: n_swa            = 0
0.00.040.026 I print_info: n_embd_head_k    = 128
0.00.040.026 I print_info: n_embd_head_v    = 128
0.00.040.027 I print_info: n_gqa            = 1
0.00.040.028 I print_info: n_embd_k_gqa     = 2048
0.00.040.028 I print_info: n_embd_v_gqa     = 2048
0.00.040.029 I print_info: f_norm_eps       = 1.0e-05
0.00.040.029 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.029 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.029 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.030 I print_info: f_logit_scale    = 0.0e+00
0.00.040.030 I print_info: n_ff             = 8192
0.00.040.030 I print_info: n_expert         = 0
0.00.040.031 I print_info: n_expert_used    = 0
0.00.040.031 I print_info: causal attn      = 1
0.00.040.031 I print_info: pooling type     = 0
0.00.040.031 I print_info: rope type        = 2
0.00.040.031 I print_info: rope scaling     = linear
0.00.040.031 I print_info: freq_base_train  = 10000.0
0.00.040.032 I print_info: freq_scale_train = 1
0.00.040.032 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.032 I print_info: rope_finetuned   = unknown
0.00.040.032 I print_info: ssm_d_conv       = 0
0.00.040.032 I print_info: ssm_d_inner      = 0
0.00.040.033 I print_info: ssm_d_state      = 0
0.00.040.033 I print_info: ssm_dt_rank      = 0
0.00.040.033 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.033 I print_info: model type       = 1.4B
0.00.040.033 I print_info: model params     = 1.41 B
0.00.040.033 I print_info: general.name     = 1.4B
0.00.040.034 I print_info: vocab type       = BPE
0.00.040.034 I print_info: n_vocab          = 50304
0.00.040.034 I print_info: n_merges         = 50009
0.00.040.034 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: LF token         = 187 ''
0.00.040.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.035 I print_info: max token length = 1024
0.00.040.036 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.628.493 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.510 I load_tensors: offloading output layer to GPU
0.00.628.511 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.541 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.628.542 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.630.291 I llama_init_from_model: n_seq_max     = 1
0.00.630.295 I llama_init_from_model: n_ctx         = 128
0.00.630.296 I llama_init_from_model: n_ctx_per_seq = 128
0.00.630.296 I llama_init_from_model: n_batch       = 128
0.00.630.297 I llama_init_from_model: n_ubatch      = 128
0.00.630.297 I llama_init_from_model: flash_attn    = 0
0.00.630.299 I llama_init_from_model: freq_base     = 10000.0
0.00.630.300 I llama_init_from_model: freq_scale    = 1
0.00.630.301 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.303 I ggml_metal_init: allocating
0.00.630.371 I ggml_metal_init: found device: Apple M4
0.00.630.385 I ggml_metal_init: picking default device: Apple M4
0.00.631.714 I ggml_metal_init: using embedded metal library
0.00.638.174 I ggml_metal_init: GPU name:   Apple M4
0.00.638.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.638.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.638.181 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.638.181 I ggml_metal_init: simdgroup reduction   = true
0.00.638.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.638.182 I ggml_metal_init: has residency sets    = true
0.00.638.182 I ggml_metal_init: has bfloat            = true
0.00.638.183 I ggml_metal_init: use bfloat            = true
0.00.638.183 I ggml_metal_init: hasUnifiedMemory      = true
0.00.638.187 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.608 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.660.089 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.660.094 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.660.123 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.663.387 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.663.389 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.663.390 I llama_init_from_model: graph nodes  = 967
0.00.663.390 I llama_init_from_model: graph splits = 2
0.00.663.393 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.663.393 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.150 I 
0.00.701.247 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.275 I perplexity: tokenizing the input ..
0.00.706.646 I perplexity: tokenization took 5.37 ms
0.00.706.650 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.769 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.846.125 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.846.143 I llama_perf_context_print:        load time =     691.04 ms
0.00.846.144 I llama_perf_context_print: prompt eval time =     137.89 ms /   128 tokens (    1.08 ms per token,   928.30 tokens per second)
0.00.846.144 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.145 I llama_perf_context_print:       total time =     145.00 ms /   129 tokens
0.00.846.546 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.077s
sys	0m0.160s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.926 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.148 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.153 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.154 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.154 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.154 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.054 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.876 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.877 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.878 I llama_model_loader: - type  f32:  194 tensors
0.00.025.878 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.878 I print_info: file format = GGUF V3 (latest)
0.00.025.879 I print_info: file type   = Q6_K
0.00.025.880 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.286 I load: special tokens cache size = 25
0.00.040.803 I load: token to piece cache size = 0.2984 MB
0.00.040.819 I print_info: arch             = gptneox
0.00.040.820 I print_info: vocab_only       = 0
0.00.040.820 I print_info: n_ctx_train      = 2048
0.00.040.820 I print_info: n_embd           = 2048
0.00.040.820 I print_info: n_layer          = 24
0.00.040.824 I print_info: n_head           = 16
0.00.040.825 I print_info: n_head_kv        = 16
0.00.040.825 I print_info: n_rot            = 32
0.00.040.825 I print_info: n_swa            = 0
0.00.040.825 I print_info: n_embd_head_k    = 128
0.00.040.825 I print_info: n_embd_head_v    = 128
0.00.040.826 I print_info: n_gqa            = 1
0.00.040.829 I print_info: n_embd_k_gqa     = 2048
0.00.040.830 I print_info: n_embd_v_gqa     = 2048
0.00.040.830 I print_info: f_norm_eps       = 1.0e-05
0.00.040.831 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.831 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.831 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.833 I print_info: f_logit_scale    = 0.0e+00
0.00.040.834 I print_info: n_ff             = 8192
0.00.040.835 I print_info: n_expert         = 0
0.00.040.835 I print_info: n_expert_used    = 0
0.00.040.835 I print_info: causal attn      = 1
0.00.040.835 I print_info: pooling type     = 0
0.00.040.835 I print_info: rope type        = 2
0.00.040.835 I print_info: rope scaling     = linear
0.00.040.836 I print_info: freq_base_train  = 10000.0
0.00.040.836 I print_info: freq_scale_train = 1
0.00.040.836 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.836 I print_info: rope_finetuned   = unknown
0.00.040.836 I print_info: ssm_d_conv       = 0
0.00.040.837 I print_info: ssm_d_inner      = 0
0.00.040.837 I print_info: ssm_d_state      = 0
0.00.040.837 I print_info: ssm_dt_rank      = 0
0.00.040.837 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.837 I print_info: model type       = 1.4B
0.00.040.837 I print_info: model params     = 1.41 B
0.00.040.838 I print_info: general.name     = 1.4B
0.00.040.838 I print_info: vocab type       = BPE
0.00.040.839 I print_info: n_vocab          = 50304
0.00.040.840 I print_info: n_merges         = 50009
0.00.040.840 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.840 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.841 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.841 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.841 I print_info: LF token         = 187 ''
0.00.040.842 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.842 I print_info: max token length = 1024
0.00.040.843 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.676.401 I load_tensors: offloading 24 repeating layers to GPU
0.00.676.409 I load_tensors: offloading output layer to GPU
0.00.676.410 I load_tensors: offloaded 25/25 layers to GPU
0.00.676.437 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.676.441 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.677.791 I llama_init_from_model: n_seq_max     = 1
0.00.677.793 I llama_init_from_model: n_ctx         = 2048
0.00.677.794 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.677.794 I llama_init_from_model: n_batch       = 2048
0.00.677.795 I llama_init_from_model: n_ubatch      = 512
0.00.677.795 I llama_init_from_model: flash_attn    = 0
0.00.677.797 I llama_init_from_model: freq_base     = 10000.0
0.00.677.797 I llama_init_from_model: freq_scale    = 1
0.00.677.799 I ggml_metal_init: allocating
0.00.677.856 I ggml_metal_init: found device: Apple M4
0.00.677.869 I ggml_metal_init: picking default device: Apple M4
0.00.679.219 I ggml_metal_init: using embedded metal library
0.00.685.271 I ggml_metal_init: GPU name:   Apple M4
0.00.685.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.685.276 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.685.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.685.277 I ggml_metal_init: simdgroup reduction   = true
0.00.685.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.685.278 I ggml_metal_init: has residency sets    = true
0.00.685.278 I ggml_metal_init: has bfloat            = true
0.00.685.278 I ggml_metal_init: use bfloat            = true
0.00.685.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.685.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.177 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.754.508 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.754.519 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.754.550 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.758.885 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.758.887 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.758.887 I llama_init_from_model: graph nodes  = 967
0.00.758.888 I llama_init_from_model: graph splits = 2
0.00.758.893 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.759.018 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.759.019 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.202 I main: llama threadpool init, n_threads = 4
0.00.814.248 I 
0.00.814.267 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.268 I 
0.00.814.401 I sampler seed: 1234
0.00.814.406 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.428 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.429 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.429 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.701.780 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.701.780 I llama_perf_context_print:        load time =     804.56 ms
0.01.701.781 I llama_perf_context_print: prompt eval time =      57.47 ms /     7 tokens (    8.21 ms per token,   121.80 tokens per second)
0.01.701.782 I llama_perf_context_print:        eval time =     827.05 ms /    63 runs   (   13.13 ms per token,    76.17 tokens per second)
0.01.701.782 I llama_perf_context_print:       total time =     888.29 ms /    70 tokens
0.01.702.058 I ggml_metal_free: deallocating

real	0m1.719s
user	0m0.110s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4832 (07d15723) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.338 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.246 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.252 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.254 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.256 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.256 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.257 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.257 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.257 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.258 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.992 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.759 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.760 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.760 I llama_model_loader: - type  f32:  194 tensors
0.00.024.761 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.761 I print_info: file format = GGUF V3 (latest)
0.00.024.762 I print_info: file type   = Q6_K
0.00.024.763 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.782 I load: special tokens cache size = 25
0.00.039.081 I load: token to piece cache size = 0.2984 MB
0.00.039.098 I print_info: arch             = gptneox
0.00.039.099 I print_info: vocab_only       = 0
0.00.039.100 I print_info: n_ctx_train      = 2048
0.00.039.100 I print_info: n_embd           = 2048
0.00.039.100 I print_info: n_layer          = 24
0.00.039.104 I print_info: n_head           = 16
0.00.039.105 I print_info: n_head_kv        = 16
0.00.039.105 I print_info: n_rot            = 32
0.00.039.106 I print_info: n_swa            = 0
0.00.039.106 I print_info: n_embd_head_k    = 128
0.00.039.106 I print_info: n_embd_head_v    = 128
0.00.039.107 I print_info: n_gqa            = 1
0.00.039.107 I print_info: n_embd_k_gqa     = 2048
0.00.039.108 I print_info: n_embd_v_gqa     = 2048
0.00.039.109 I print_info: f_norm_eps       = 1.0e-05
0.00.039.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.109 I print_info: f_logit_scale    = 0.0e+00
0.00.039.110 I print_info: n_ff             = 8192
0.00.039.110 I print_info: n_expert         = 0
0.00.039.111 I print_info: n_expert_used    = 0
0.00.039.111 I print_info: causal attn      = 1
0.00.039.111 I print_info: pooling type     = 0
0.00.039.112 I print_info: rope type        = 2
0.00.039.134 I print_info: rope scaling     = linear
0.00.039.137 I print_info: freq_base_train  = 10000.0
0.00.039.137 I print_info: freq_scale_train = 1
0.00.039.137 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.137 I print_info: rope_finetuned   = unknown
0.00.039.138 I print_info: ssm_d_conv       = 0
0.00.039.138 I print_info: ssm_d_inner      = 0
0.00.039.138 I print_info: ssm_d_state      = 0
0.00.039.138 I print_info: ssm_dt_rank      = 0
0.00.039.138 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.138 I print_info: model type       = 1.4B
0.00.039.139 I print_info: model params     = 1.41 B
0.00.039.139 I print_info: general.name     = 1.4B
0.00.039.141 I print_info: vocab type       = BPE
0.00.039.141 I print_info: n_vocab          = 50304
0.00.039.141 I print_info: n_merges         = 50009
0.00.039.141 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.141 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.143 I print_info: LF token         = 187 ''
0.00.039.143 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.144 I print_info: max token length = 1024
0.00.039.144 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.021 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.038 I load_tensors: offloading output layer to GPU
0.00.599.038 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.073 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.599.075 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.600.821 I llama_init_from_model: n_seq_max     = 1
0.00.600.824 I llama_init_from_model: n_ctx         = 128
0.00.600.824 I llama_init_from_model: n_ctx_per_seq = 128
0.00.600.824 I llama_init_from_model: n_batch       = 128
0.00.600.825 I llama_init_from_model: n_ubatch      = 128
0.00.600.825 I llama_init_from_model: flash_attn    = 0
0.00.600.828 I llama_init_from_model: freq_base     = 10000.0
0.00.600.828 I llama_init_from_model: freq_scale    = 1
0.00.600.829 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.600.832 I ggml_metal_init: allocating
0.00.600.879 I ggml_metal_init: found device: Apple M4
0.00.600.893 I ggml_metal_init: picking default device: Apple M4
0.00.602.184 I ggml_metal_init: using embedded metal library
0.00.608.432 I ggml_metal_init: GPU name:   Apple M4
0.00.608.436 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.437 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.438 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.438 I ggml_metal_init: simdgroup reduction   = true
0.00.608.439 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.439 I ggml_metal_init: has residency sets    = true
0.00.608.439 I ggml_metal_init: has bfloat            = true
0.00.608.439 I ggml_metal_init: use bfloat            = true
0.00.608.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.905 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.324 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.330 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.364 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.632.586 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.632.588 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.632.588 I llama_init_from_model: graph nodes  = 967
0.00.632.589 I llama_init_from_model: graph splits = 2
0.00.632.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.632.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.990 I 
0.00.664.078 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.105 I perplexity: tokenizing the input ..
0.00.671.433 I perplexity: tokenization took 7.326 ms
0.00.671.442 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.363 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.805.703 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.805.722 I llama_perf_context_print:        load time =     654.64 ms
0.00.805.724 I llama_perf_context_print: prompt eval time =     131.96 ms /   128 tokens (    1.03 ms per token,   970.00 tokens per second)
0.00.805.726 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.727 I llama_perf_context_print:       total time =     141.74 ms /   129 tokens
0.00.806.130 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.079s
sys	0m0.132s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4832 (07d15723)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126604a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126605160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126605710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126605cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126606270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126606820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126606dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126607380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126607930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126607e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126608330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126608830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126609350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126609b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12660a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12660aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12660b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12660b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12660bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12660c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12660ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12660d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12660dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12660e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12660ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12660ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12660f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1266101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1266109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126610e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1266119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126611ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1266121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126612650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126612af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126613430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1266138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126613d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1266146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126614b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126614e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126615420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126615a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126616350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126616f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126617580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1266181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1266187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126618fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126619440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1266198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12661a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12661a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12661ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12661b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12661b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12661ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12661bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12661c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12661c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12661ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12661d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12661d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12661daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12661df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12661e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12661e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12661ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12661f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12661f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12661fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1266203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126620910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1266213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126621e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1266223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1266228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126623390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1266238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126623e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1266248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126624e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126625370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1266258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126625e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1266267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1266274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126627a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126627f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1266284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126628a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126628f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1266294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126629a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126629f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12662a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12662a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12662af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12662b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12662b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12662bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12662c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12662c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12662cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12662d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12662d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12662d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12662de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12662e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12662e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12662ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12662f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12662f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12662f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12662fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126630330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1266307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126630c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126631110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1266315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126631a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1266343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1266351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1266368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1266376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1266384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12663a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12663a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12663a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12663ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12663b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12663b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12663bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12663c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12663c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12663ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12663ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12663d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12663d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12663dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12663e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12663e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12663ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12663ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12663f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12663f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12663fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1266418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1266421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126643130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126643680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126643e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1266444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1266450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1266458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126646010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126646c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126647420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1266478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126647d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1266489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126648f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126649450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1266499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126649ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12664a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12664a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12664aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12664b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12664b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12664bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12664c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12664c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12664cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12664d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12664d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12664deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12664e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12664e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12664eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12664f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12664f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12664fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1266503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126650930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126650e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1266513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126651920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1266523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126652910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126652e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1266533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126653900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1266543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1266548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126654e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126655390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1266558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126655e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126656380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1266568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126656e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1266578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126657e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126658360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1266588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126658e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1266598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126659df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12665a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12665a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12665ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12665b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12665b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12665bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12665c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12665c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12665ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12665cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12665d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12665d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12665dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12665e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12665e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12665eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12665ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12665f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12665f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12665fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1266601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x126660670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x126660b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x126660fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x126661450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1266618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x126661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x126662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1266626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126662c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126663340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126663a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126664180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1266648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126664b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126665350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126665610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126665c20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.723.444 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.723.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1266658d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1266462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126644150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126619e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1266468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12660f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126615cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126616610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1266150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126617230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12660e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12661a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126626a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126664e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1266113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1266116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126646ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126645380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12660f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12660fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12660fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126666080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126666340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126666600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1266668c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126666b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126666e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126667100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1266673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126667940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126667c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126667ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126668180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126668440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126668700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1266689c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126668c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126668f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126669200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1266694c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126669780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126669a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126669d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126669fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12666a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12666a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12666a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12666aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12666ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12666b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12666b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12666b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12666b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12666bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12666be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12666c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12666c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12666c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12666c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12666cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12666ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12666d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12666d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12666d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12666d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12666dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12666df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12666e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12666e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12666e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12666ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12666ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12666ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12666f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12666f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12666f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12666fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12666fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126670000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1266702c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126670580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126670840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126670b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126670dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126671080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126671340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126671600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1266718c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126671b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126671e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126672100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1266723c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126672680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126672940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126672c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126672ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126673180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126673440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126673700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1266739c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126673c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126673f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126674200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1266744c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126674780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126674a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126674d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126674fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126675280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126675540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126675800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126675ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126675d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126676040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126676300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1266765c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126676880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126676b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126676e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1266770c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126677380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126677640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126677900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126677bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126677e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126678140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126678400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1266786c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126678980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126678c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126678f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1266791c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126679480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126679740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126679a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126679cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126679f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12667a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12667a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12667a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12667aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12667ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12667b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12667b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12667b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12667b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12667bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12667bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12667c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12667c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12667c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12667c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12667cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12667ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12667d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12667d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12667d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12667d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12667dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12667dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12667e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12667e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12667e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12667e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12667ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12667ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12667f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12667f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12667f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12667fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12667fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12667ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126680280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126680540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126680800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126680ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126680d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126681040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126681300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1266815c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126681880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126681b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126681e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1266820c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126682380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126682640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126682900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126682bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126682e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126683140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126683400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1266836c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126683980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126683c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126683f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1266841c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126684480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126684740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126684a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126684cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126684f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126685240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126685500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1266857c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126685a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126686050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126686310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1266865d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126686890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126686de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126687330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126687880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126687dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126688320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126688870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126688dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126689310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126689860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126689db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12668a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12668a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12668ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12668b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12668b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12668bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12668c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12668c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12668cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12668d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12668d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12668dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12668e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12668e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12668ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12668f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12668f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12668fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1266902a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1266907f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126690d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126691290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1266917e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126691d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126692280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1266927d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126692d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126693270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1266937c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126693d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126694260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1266947b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126694d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126695250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1266957a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126695cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126696240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126696790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126696ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126697230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126697780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126697cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126698220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1266984e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1266987a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126698a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126698ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126699340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1266997b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126699c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12669a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12669a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12669a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12669ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12669b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12669b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12669bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12669bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x12669c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x12669c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12669ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x12669d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x12669d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x12669da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x12669deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12669e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12669e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x12669ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12669f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12669fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1266a01f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1266a0910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1266a1030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1266a12f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1266a1ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1266a1da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1266a23b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f4044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f404950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f404dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f405230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f4056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f405b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f405f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f4063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f406860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f406cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f407140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f407860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f408380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f408b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f409340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f409a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f40a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f40a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f40afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f40b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f40be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f40c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f40cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f40d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f40da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f40dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f40e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f40e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f40e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f40ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f40f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f40f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f40fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f40fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f4102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f410710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f410b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f410ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f411460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f4118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f411d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f4121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f412620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f412a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f412f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f413370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f4137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f413c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f4140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f414530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f4149a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f414e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f415280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f4156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f415b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f415fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f416540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f416a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f416eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f417320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f417790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f417c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f418070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f4184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f418950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f418dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f419230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f4196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f419b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f419f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f41a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f41a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11f41acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11f41b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11f41b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11f41ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11f41be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11f41c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11f41c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11f41cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11f41d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11f41d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11f41d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11f41dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11f41e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11f41e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11f41eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11f41ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11f41f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11f41f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11f41fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11f420120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11f420590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11f420a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11f420e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11f4212e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11f421750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11f421bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11f422420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11f422940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11f422ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11f4234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11f423a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11f424000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11f4245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11f424b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11f425110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11f4256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11f425c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11f426220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11f4267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11f426d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11f427330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11f4278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11f427de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11f4282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11f4287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11f428ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11f4291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11f4296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11f429be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11f42a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11f42a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11f42aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11f42afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11f42b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11f42b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11f42bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11f42c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11f42c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11f42cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11f42d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11f42d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11f42dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11f42e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11f42e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11f42ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11f42f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11f42f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11f42fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11f42ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11f4304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11f4309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11f430ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11f4313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11f4318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11f431de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11f4322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11f4327e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11f432ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11f4331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11f4336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11f433be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11f4340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11f4345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11f434ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11f434fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11f4354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11f4359e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11f435ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11f4363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11f4368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11f436de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11f4372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11f4377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11f437ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11f4381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11f4386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11f438be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11f4390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11f4395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11f439ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11f439fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11f43a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11f43a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11f43aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11f43b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11f43b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11f43bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11f43c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11f43c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11f43cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11f43d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11f43d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11f43dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11f43e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11f43e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11f43eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11f43efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11f43f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11f43f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11f43fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11f4403e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11f4408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11f440e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11f441440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11f4419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11f441fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11f4425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11f442bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11f4431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11f4439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11f443e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11f444120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11f444730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11f444d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11f445530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11f4459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11f445e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11f446310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11f446ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11f447010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11f447560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11f447ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11f448000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11f448550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11f448aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11f448ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11f449540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11f449a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11f449fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11f44a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11f44aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11f44afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11f44b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11f44ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11f44bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11f44c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11f44ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11f44cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11f44d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11f44da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11f44dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11f44e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11f44ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11f44ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11f44f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11f44fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11f44ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11f4504d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11f450a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11f450f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11f4514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11f451a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11f451f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11f4524b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11f452a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11f452f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11f4534a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11f4539f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11f453f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11f454490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11f4549e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11f454f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11f455480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11f4559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11f455f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11f456470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11f4569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11f456f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11f457460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11f4579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11f457f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11f458450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11f4589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11f458ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f459440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f4598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f459d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f45a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f45a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f45ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f45b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f45b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f45b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f45bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f45c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f45c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f45cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f45d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f45d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f45d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11f45de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11f45e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11f45e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11f45ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11f45f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11f45f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11f45fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11f45fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11f460340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11f4607e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f460d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f461450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f461b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f462290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f4629b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f462c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f463460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f463720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f463d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.803s
user	0m0.276s
sys	0m0.333s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4832 (07d15723)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133e0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133e0dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133e0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133e0e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133e0ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133e0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133e0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133e0fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133e10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133e10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133e10e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133e11340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133e11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133e12610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133e12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133e13540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133e13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133e14380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133e14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133e15270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133e15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133e160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133e167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133e17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133e17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133e17a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133e18060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133e18cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133e19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133e194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133e19970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133e19c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133e1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133e1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133e1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133e1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133e1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133e1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133e1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133e1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133e1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133e1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133e1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133e1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133f073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133f07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133f07cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133f08130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133f088c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133f08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133f091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133f09610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133f09a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133f09ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133f0a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133f0ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133f0afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133f0b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133f0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133f0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133f0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133f0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133f0ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133f0d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133f0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133f0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133f0e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133f0e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133f0ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133f0eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133f0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133f0fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133f101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133f10740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133f10c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133f111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133f11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133f11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133f121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133f12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133f12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133f131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133f13710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133f13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133f141b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133f14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133f14c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133f151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133f156f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133f16190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133f166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133f16c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133f17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133f176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133f17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133f083f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133f18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133f18840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133f18d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133f192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133f19830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133f19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133f1a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133f1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133f1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133f1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133f1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133f1bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133f1c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133f1c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133f1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133f1d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133f1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133f1dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133f1e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133f1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133f1edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133f1f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133f1f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133f1fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133f202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133f20560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133f20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133f20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133f21460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133f21960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133f21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133f22360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133f22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133f22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133f23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133f23760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133f23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133f24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133f24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133f24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133f25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133f25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133f25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133f25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133f26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133f26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133f27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133f27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133f28260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133f28760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133f28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133f29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133f29660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133f29b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133f2a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133f2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133f2aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133f2af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133f2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133f2b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133f2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133f2c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133f2c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133f2cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133f2d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133f2d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133f2dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133f2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133f2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133f2eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133f2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133f2f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133f2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133f2ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133f30460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133f30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133f30e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133f31360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133f31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133f31d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133f32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133f32760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133f32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133f33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133f33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133f33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133f34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133f34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133f34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133f34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133f35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133f35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133f35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133f36410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133f369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133f36f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133f37520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133f37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133f38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133f38750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133f38f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133f393e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133f396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133f3a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133f3aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133f3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133f3b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133f3b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133f3c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133f3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133f3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133f3d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133f3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133f3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133f3e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133f3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133f3f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133f3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133f3fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133f40000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133f40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133f40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133f40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133f41540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133f41a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133f41fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133f42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133f42a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133f42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133f43520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133f43a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133f43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133f44510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133f44a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133f44fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133f45500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133f45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133f45fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133f464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133f46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133f46f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133f474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133f47a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133f47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133f484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133f48a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133f48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133f494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133f49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133f49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133f4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133f4aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133f4af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133f4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133f4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133f4bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133f4c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133f4c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133f4cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133f4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133f4d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133f4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133f4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133f4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133f4ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133f4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133f4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133f4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133f500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133f50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133f50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133f51360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133f51800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133f51ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133f52140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133f525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133f52a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133f52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x133f533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x133f53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x133f53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x133f541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x133f54640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x133f54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x133f54f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x133f55420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x133f558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x133f55d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133f562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133f569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133f570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133f57810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133f57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133f581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133f589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133f58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133f592b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.104.142 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.147 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135805e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1358062d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135806740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135806bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135807020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135807490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135807900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135807d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1358081e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135808730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135808ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135809220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135809d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13580a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13580ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13580b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13580bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13580c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13580c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13580d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13580d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13580df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13580e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13580edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13580f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13580f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13580fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13580fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135810350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1358107c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x135810c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135811160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1358115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135811890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135811d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135812170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1358125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135812a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135812ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135813330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1358137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135813c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135814080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1358144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135814960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135814dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135815240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1358156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135815b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135815f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135816400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135816870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135816ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135817150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1358175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135817a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135817fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1358184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135818910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135818d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1358191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135819660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135819ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x135819f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13581a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13581a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13581ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13581b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13581b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13581b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13581be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13581c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13581c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13581cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13581d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13581d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13581d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13581dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13581e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13581e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13581eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13581ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13581f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13581f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13581fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1358200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x135820550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1358209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x135820e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1358212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135821710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135821b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x135821ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135822460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1358228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135822d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1358231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135823620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135823a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135823f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135824370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1358247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135824c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1358250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135825530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1358259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135825e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135826280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1358266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135826b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135826fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135827440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1358278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135827d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135828190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135828600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135828a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135828ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135829350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1358297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135829c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13582a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13582a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13582a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13582adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13582b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13582b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13582bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13582bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13582c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13582c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13582cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13582d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13582d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13582da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13582dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13582e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13582e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13582ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13582f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13582f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13582f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13582fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135830240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1358306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x135830b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135830f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135831400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135831870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135831ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135832150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1358325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135832a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135832ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x135833310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135833780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x135833bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x135834060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1358344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x135834940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x135834db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135835220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135835690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x135835b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x135835f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1358363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x135836b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135836e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1358372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135837730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135837ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135838010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135838480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1358388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135838d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1358391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135839640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135839ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135839f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13583a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13583a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13583ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13583b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13583b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13583b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13583be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13583c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13583c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13583cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13583cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13583d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13583d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13583dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13583e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13583e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13583ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13583ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13583f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13583f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13583fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1358400c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x135840530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1358409a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x135840e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135841280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1358416f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135841b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135841fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135842440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1358428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135843460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135843720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1358439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135843e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1358442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135844730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135844ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135845010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135845480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1358458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135845d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1358461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135846640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135846ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135846f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135847390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135847800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135847c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1358480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135848550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1358489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135848e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1358492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135849710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135849b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135849ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13584a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13584a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13584ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13584b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13584b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13584ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13584bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13584c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13584c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13584cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13584d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13584d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13584d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13584de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13584e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13584e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13584eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13584efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13584f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13584f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13584fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135850190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135850600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x135850a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135850ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135851350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1358517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135851c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1358520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135852510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135852980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135852df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135853260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1358536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135853b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135853fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135854420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135854890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135854d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135855170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1358555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135855a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135855ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x135856330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1358567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x135856c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x135857080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1358574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x135857960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x135857dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x135858240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1358586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x135858b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x135858f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x135859400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x135859870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135859ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13585a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13585aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13585b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13585bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13585bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13585c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13585c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13585cb50 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x133e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x133e0f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x133e0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x133e0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x133e0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x133e17d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x133e16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x133e11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x133e19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x133e1a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x133e18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x133e185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x133e1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x133e1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x133e1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x133e1e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x133e1eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x133e1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x133e1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x133e1f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x133e1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x133e20270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x133e207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x133e20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x133e21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x133e214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x133e217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x133e21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x133e22090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x133e22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x133e22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x133e22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x133e23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x133e237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x133e23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x133e23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x133e24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x133e24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x133e24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x133e25300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x133e257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x133e25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x133e26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x133e26640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x133e26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x133e26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x133e273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x133e27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x133e27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x133e28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x133e285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x133e28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x133e28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x133e29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x133e29970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x133e29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x133e2a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x133e2a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x133e2a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x133e2ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x133e2b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x133e2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x133e2bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x133e2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x133e2c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x133e2cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x133e2d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x133e2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x133e2dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x133e2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x133e2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x133e2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x133e2f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x133e2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x133e2fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x133e300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x133e30690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x133e30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x133e31210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x133e317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x133e31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x133e32350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x133e32910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x133e32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x133e33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x133e33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x133e34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x133e345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x133e34b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x133e35150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x133e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x133e35cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x133e36290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x133e36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x133e36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x133e373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x133e37990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x133e37f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x133e38510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x133e38ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x133e39090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x133e39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x133e39c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x133e3a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x133e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x133e3ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x133e3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x133e3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x133e3be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x133e3c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x133e3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x133e3cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x133e3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x133e3db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x133e3e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x133e3e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x133e3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x133e3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x133e3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x133e3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x133e3fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x133e403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x133e408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x133e40df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x133e41300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x133e41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x133e41d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x133e42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x133e42740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x133e42c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x133e43160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x133e43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x133e43b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x133e44090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x133e445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x133e44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x133e44fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x133e454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x133e459e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x133e45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x133e46400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x133e46910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x133e46e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x133e47330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x133e47840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x133e47d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x133e48260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x133e48770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x133e48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x133e49190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x133e49690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x133e49b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x133e4a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x133e4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x133e4aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x133e4afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x133e4b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x133e4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x133e4bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x133e4c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x133e4c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x133e4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x133e4d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x133e4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x133e4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x133e4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x133e4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x133e4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x133e4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x133e4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x133e4fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x133e500d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x133e505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x133e50af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x133e51000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x133e51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x133e51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x133e51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x133e52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x133e52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x133e52e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x133e53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x133e53880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x133e53d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x133e542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x133e547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x133e54cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x133e551d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x133e556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x133e55bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x133e56100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x133e56610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x133e56b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x133e57030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x133e575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x133e57b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x133e58140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x133e586f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x133e58d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x133e59310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x133e59920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x133e5a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x133e5a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x133e5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x133e5ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x133e5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x133e5bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x133e5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x133e5c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x133e5ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x133e5d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x133e5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x133e5dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x133e5e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x133e5e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x133e5eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x133e5f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x133e5f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x133e5fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x133e601e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x133e60730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x133e60c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x133e611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x133e61720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x133e61c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x133e621c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x133e62710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x133e62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x133e631b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x133e63700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x133e63c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x133e641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x133e646f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x133e64c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x133e65190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x133e656e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x133e65c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x133e66180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x133e666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x133e66c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x133e67170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x133e676c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x133e67c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x133e68160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x133e686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x133e68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x133e69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x133e696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x133e69bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x133e6a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x133e6a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x133e6abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x133e6b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x133e6b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x133e6bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x133e6c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x133e6c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x133e6cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x133e6d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x133e6d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x133e6dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x133e6e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x133e6e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x133e6eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x133e6f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x133e6f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x133e6fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x133e70030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x133e704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x133e70970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x133e70e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x133e712b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x133e71750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x133e71bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x133e72090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x133e72530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x133e729d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x133e72e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x133e73310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x133e737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x133e73c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x133e740f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x133e74590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x133e74a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x133e74ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x133e75370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x133e75810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x133e75cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x133e76150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x133e765f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x133e76a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x133e76f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x133e77480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x133e77ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x133e782c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x133e789e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x133e79100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x133e793c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x133e79bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x133e79e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x133e7a480 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.957s
user	0m0.231s
sys	0m0.186s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.55 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.00 sec*proc (2 tests)

Total Test time (real) =   2.01 sec
        2.03 real         0.52 user         0.25 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.12 user         0.08 sys
```
