### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.31 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.10 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.46 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.27 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.18 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.08 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.27 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.87 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.74 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.91 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.92 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.22 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 253.93 sec*proc (29 tests)

Total Test time (real) = 253.94 sec

real	4m13.970s
user	8m37.651s
sys	0m7.252s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.16 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.04 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.78 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.19 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.48 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.40 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.50 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.48 sec*proc (29 tests)

Total Test time (real) =  54.49 sec

real	0m54.504s
user	1m15.911s
sys	0m6.309s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.315 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.078 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.627 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.635 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.638 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.028.639 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.639 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.028.640 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.028.641 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.028.643 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.028.643 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.028.644 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.028.645 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.028.645 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.028.648 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.652 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.653 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.028.653 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.028.654 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.028.655 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.028.655 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.033.747 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.035.073 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.075 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.035.075 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.035.076 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.035.076 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.035.077 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.035.077 I llama_model_loader: - type  f32:  124 tensors
0.00.035.078 I llama_model_loader: - type  f16:   73 tensors
0.00.035.079 I print_info: file format = GGUF V3 (latest)
0.00.035.079 I print_info: file type   = F16
0.00.035.081 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.039.692 I load: special tokens cache size = 5
0.00.041.907 I load: token to piece cache size = 0.2032 MB
0.00.041.912 I print_info: arch             = bert
0.00.041.912 I print_info: vocab_only       = 0
0.00.041.912 I print_info: n_ctx_train      = 512
0.00.041.913 I print_info: n_embd           = 384
0.00.041.913 I print_info: n_layer          = 12
0.00.041.917 I print_info: n_head           = 12
0.00.041.918 I print_info: n_head_kv        = 12
0.00.041.918 I print_info: n_rot            = 32
0.00.041.918 I print_info: n_swa            = 0
0.00.041.918 I print_info: n_embd_head_k    = 32
0.00.041.919 I print_info: n_embd_head_v    = 32
0.00.041.920 I print_info: n_gqa            = 1
0.00.041.920 I print_info: n_embd_k_gqa     = 384
0.00.041.921 I print_info: n_embd_v_gqa     = 384
0.00.041.922 I print_info: f_norm_eps       = 1.0e-12
0.00.041.922 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.923 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.923 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.923 I print_info: f_logit_scale    = 0.0e+00
0.00.041.924 I print_info: n_ff             = 1536
0.00.041.925 I print_info: n_expert         = 0
0.00.041.925 I print_info: n_expert_used    = 0
0.00.041.926 I print_info: causal attn      = 0
0.00.041.926 I print_info: pooling type     = 2
0.00.041.926 I print_info: rope type        = 2
0.00.041.926 I print_info: rope scaling     = linear
0.00.041.927 I print_info: freq_base_train  = 10000.0
0.00.041.927 I print_info: freq_scale_train = 1
0.00.041.928 I print_info: n_ctx_orig_yarn  = 512
0.00.041.929 I print_info: rope_finetuned   = unknown
0.00.041.929 I print_info: ssm_d_conv       = 0
0.00.041.929 I print_info: ssm_d_inner      = 0
0.00.041.929 I print_info: ssm_d_state      = 0
0.00.041.930 I print_info: ssm_dt_rank      = 0
0.00.041.930 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.930 I print_info: model type       = 33M
0.00.041.931 I print_info: model params     = 33.21 M
0.00.041.931 I print_info: general.name     = Bge Small
0.00.041.931 I print_info: vocab type       = WPM
0.00.041.932 I print_info: n_vocab          = 30522
0.00.041.932 I print_info: n_merges         = 0
0.00.041.932 I print_info: BOS token        = 101 '[CLS]'
0.00.041.933 I print_info: UNK token        = 100 '[UNK]'
0.00.041.933 I print_info: SEP token        = 102 '[SEP]'
0.00.041.934 I print_info: PAD token        = 0 '[PAD]'
0.00.041.934 I print_info: MASK token       = 103 '[MASK]'
0.00.041.934 I print_info: LF token         = 0 '[PAD]'
0.00.041.935 I print_info: max token length = 21
0.00.045.263 I load_tensors: offloading 12 repeating layers to GPU
0.00.045.265 I load_tensors: offloading output layer to GPU
0.00.045.266 I load_tensors: offloaded 13/13 layers to GPU
0.00.045.293 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.045.295 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.045.630 I llama_init_from_model: n_seq_max     = 1
0.00.045.632 I llama_init_from_model: n_ctx         = 512
0.00.045.632 I llama_init_from_model: n_ctx_per_seq = 512
0.00.045.632 I llama_init_from_model: n_batch       = 2048
0.00.045.632 I llama_init_from_model: n_ubatch      = 2048
0.00.045.633 I llama_init_from_model: flash_attn    = 0
0.00.045.633 I llama_init_from_model: freq_base     = 10000.0
0.00.045.634 I llama_init_from_model: freq_scale    = 1
0.00.045.634 I ggml_metal_init: allocating
0.00.045.648 I ggml_metal_init: found device: Apple M4
0.00.045.654 I ggml_metal_init: picking default device: Apple M4
0.00.046.439 I ggml_metal_init: using embedded metal library
0.00.050.710 I ggml_metal_init: GPU name:   Apple M4
0.00.050.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.050.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.050.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.050.714 I ggml_metal_init: simdgroup reduction   = true
0.00.050.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.050.715 I ggml_metal_init: has residency sets    = true
0.00.050.715 I ggml_metal_init: has bfloat            = true
0.00.050.715 I ggml_metal_init: use bfloat            = true
0.00.050.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.050.716 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.842 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.063.530 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.063.532 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.063.555 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.064.787 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.064.788 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.064.789 I llama_init_from_model: graph nodes  = 429
0.00.064.789 I llama_init_from_model: graph splits = 2
0.00.064.790 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.064.790 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.495 I 
0.00.070.525 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.071.216 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.390 I llama_perf_context_print:        load time =      47.41 ms
0.00.076.391 I llama_perf_context_print: prompt eval time =       5.03 ms /     9 tokens (    0.56 ms per token,  1788.91 tokens per second)
0.00.076.392 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.392 I llama_perf_context_print:       total time =       5.89 ms /    10 tokens
0.00.076.540 I ggml_metal_free: deallocating

real	0m0.266s
user	0m0.052s
sys	0m0.039s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.048 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.368 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.054 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.058 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.059 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.061 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.062 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.062 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.062 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.063 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.064 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.064 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.064 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.065 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.066 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.069 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.069 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.069 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.069 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.070 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.609 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.277 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.278 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.278 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.279 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.279 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.279 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.279 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.280 I llama_model_loader: - type  f32:  124 tensors
0.00.015.280 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.280 I print_info: file format = GGUF V3 (latest)
0.00.015.281 I print_info: file type   = Q8_0
0.00.015.282 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.769 I load: special tokens cache size = 5
0.00.019.021 I load: token to piece cache size = 0.2032 MB
0.00.019.024 I print_info: arch             = bert
0.00.019.024 I print_info: vocab_only       = 0
0.00.019.024 I print_info: n_ctx_train      = 512
0.00.019.025 I print_info: n_embd           = 384
0.00.019.025 I print_info: n_layer          = 12
0.00.019.027 I print_info: n_head           = 12
0.00.019.028 I print_info: n_head_kv        = 12
0.00.019.028 I print_info: n_rot            = 32
0.00.019.028 I print_info: n_swa            = 0
0.00.019.029 I print_info: n_embd_head_k    = 32
0.00.019.029 I print_info: n_embd_head_v    = 32
0.00.019.030 I print_info: n_gqa            = 1
0.00.019.030 I print_info: n_embd_k_gqa     = 384
0.00.019.031 I print_info: n_embd_v_gqa     = 384
0.00.019.032 I print_info: f_norm_eps       = 1.0e-12
0.00.019.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.032 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.033 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.033 I print_info: f_logit_scale    = 0.0e+00
0.00.019.033 I print_info: n_ff             = 1536
0.00.019.033 I print_info: n_expert         = 0
0.00.019.033 I print_info: n_expert_used    = 0
0.00.019.034 I print_info: causal attn      = 0
0.00.019.034 I print_info: pooling type     = 2
0.00.019.034 I print_info: rope type        = 2
0.00.019.035 I print_info: rope scaling     = linear
0.00.019.035 I print_info: freq_base_train  = 10000.0
0.00.019.036 I print_info: freq_scale_train = 1
0.00.019.036 I print_info: n_ctx_orig_yarn  = 512
0.00.019.036 I print_info: rope_finetuned   = unknown
0.00.019.036 I print_info: ssm_d_conv       = 0
0.00.019.038 I print_info: ssm_d_inner      = 0
0.00.019.038 I print_info: ssm_d_state      = 0
0.00.019.038 I print_info: ssm_dt_rank      = 0
0.00.019.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.038 I print_info: model type       = 33M
0.00.019.038 I print_info: model params     = 33.21 M
0.00.019.038 I print_info: general.name     = Bge Small
0.00.019.039 I print_info: vocab type       = WPM
0.00.019.039 I print_info: n_vocab          = 30522
0.00.019.039 I print_info: n_merges         = 0
0.00.019.039 I print_info: BOS token        = 101 '[CLS]'
0.00.019.040 I print_info: UNK token        = 100 '[UNK]'
0.00.019.040 I print_info: SEP token        = 102 '[SEP]'
0.00.019.040 I print_info: PAD token        = 0 '[PAD]'
0.00.019.040 I print_info: MASK token       = 103 '[MASK]'
0.00.019.040 I print_info: LF token         = 0 '[PAD]'
0.00.019.045 I print_info: max token length = 21
0.00.020.931 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.932 I load_tensors: offloading output layer to GPU
0.00.020.932 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.940 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.941 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.021.159 I llama_init_from_model: n_seq_max     = 1
0.00.021.160 I llama_init_from_model: n_ctx         = 512
0.00.021.161 I llama_init_from_model: n_ctx_per_seq = 512
0.00.021.161 I llama_init_from_model: n_batch       = 2048
0.00.021.161 I llama_init_from_model: n_ubatch      = 2048
0.00.021.161 I llama_init_from_model: flash_attn    = 0
0.00.021.162 I llama_init_from_model: freq_base     = 10000.0
0.00.021.162 I llama_init_from_model: freq_scale    = 1
0.00.021.162 I ggml_metal_init: allocating
0.00.021.181 I ggml_metal_init: found device: Apple M4
0.00.021.185 I ggml_metal_init: picking default device: Apple M4
0.00.021.767 I ggml_metal_init: using embedded metal library
0.00.024.349 I ggml_metal_init: GPU name:   Apple M4
0.00.024.351 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.351 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.352 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.352 I ggml_metal_init: simdgroup reduction   = true
0.00.024.352 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.352 I ggml_metal_init: has residency sets    = true
0.00.024.353 I ggml_metal_init: has bfloat            = true
0.00.024.353 I ggml_metal_init: use bfloat            = true
0.00.024.353 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.718 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.331 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.332 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.346 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.334 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.335 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.335 I llama_init_from_model: graph nodes  = 429
0.00.036.335 I llama_init_from_model: graph splits = 2
0.00.036.337 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.337 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.571 I 
0.00.040.596 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.128 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.586 I llama_perf_context_print:        load time =      31.20 ms
0.00.045.587 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2079.00 tokens per second)
0.00.045.588 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.588 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.045.791 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.278 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.411 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.190 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.198 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.037.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.200 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.037.201 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.037.201 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.037.203 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.037.204 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.037.204 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.037.205 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.037.205 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.037.212 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.037.212 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.037.213 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.037.214 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.214 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.449 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.073 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.075 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.075 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.076 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.076 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.076 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.077 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.077 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.077 I llama_model_loader: - type  f32:   40 tensors
0.00.051.078 I llama_model_loader: - type  f16:   30 tensors
0.00.051.078 I print_info: file format = GGUF V3 (latest)
0.00.051.079 I print_info: file type   = F16
0.00.051.081 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.055.374 W load: empty token at index 5
0.00.060.390 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.912 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.952 I load: special tokens cache size = 5
0.00.328.150 I load: token to piece cache size = 1.5060 MB
0.00.328.155 I print_info: arch             = jina-bert-v2
0.00.328.155 I print_info: vocab_only       = 0
0.00.328.156 I print_info: n_ctx_train      = 8192
0.00.328.156 I print_info: n_embd           = 384
0.00.328.156 I print_info: n_layer          = 4
0.00.328.162 I print_info: n_head           = 12
0.00.328.163 I print_info: n_head_kv        = 12
0.00.328.163 I print_info: n_rot            = 32
0.00.328.164 I print_info: n_swa            = 0
0.00.328.164 I print_info: n_embd_head_k    = 32
0.00.328.164 I print_info: n_embd_head_v    = 32
0.00.328.164 I print_info: n_gqa            = 1
0.00.328.165 I print_info: n_embd_k_gqa     = 384
0.00.328.166 I print_info: n_embd_v_gqa     = 384
0.00.328.167 I print_info: f_norm_eps       = 1.0e-12
0.00.328.167 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.328.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.328.170 I print_info: f_max_alibi_bias = 8.0e+00
0.00.328.171 I print_info: f_logit_scale    = 0.0e+00
0.00.328.171 I print_info: n_ff             = 1536
0.00.328.171 I print_info: n_expert         = 0
0.00.328.172 I print_info: n_expert_used    = 0
0.00.328.172 I print_info: causal attn      = 0
0.00.328.174 I print_info: pooling type     = -1
0.00.328.174 I print_info: rope type        = -1
0.00.328.174 I print_info: rope scaling     = linear
0.00.328.175 I print_info: freq_base_train  = 10000.0
0.00.328.175 I print_info: freq_scale_train = 1
0.00.328.175 I print_info: n_ctx_orig_yarn  = 8192
0.00.328.176 I print_info: rope_finetuned   = unknown
0.00.328.176 I print_info: ssm_d_conv       = 0
0.00.328.176 I print_info: ssm_d_inner      = 0
0.00.328.176 I print_info: ssm_d_state      = 0
0.00.328.176 I print_info: ssm_dt_rank      = 0
0.00.328.176 I print_info: ssm_dt_b_c_rms   = 0
0.00.328.176 I print_info: model type       = 33M
0.00.328.177 I print_info: model params     = 32.90 M
0.00.328.177 I print_info: general.name     = Jina Bert Implementation
0.00.328.178 I print_info: vocab type       = BPE
0.00.328.179 I print_info: n_vocab          = 61056
0.00.328.179 I print_info: n_merges         = 39382
0.00.328.179 I print_info: BOS token        = 0 '<s>'
0.00.328.179 I print_info: EOS token        = 2 '</s>'
0.00.328.179 I print_info: UNK token        = 3 '<unk>'
0.00.328.179 I print_info: SEP token        = 2 '</s>'
0.00.328.180 I print_info: PAD token        = 1 '<pad>'
0.00.328.180 I print_info: MASK token       = 4 '<mask>'
0.00.328.180 I print_info: EOG token        = 2 '</s>'
0.00.328.180 I print_info: max token length = 45
0.00.330.439 I load_tensors: offloading 4 repeating layers to GPU
0.00.330.440 I load_tensors: offloading output layer to GPU
0.00.330.440 I load_tensors: offloaded 5/5 layers to GPU
0.00.330.464 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.330.465 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.331.119 I llama_init_from_model: n_seq_max     = 1
0.00.331.120 I llama_init_from_model: n_ctx         = 8192
0.00.331.121 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.331.121 I llama_init_from_model: n_batch       = 2048
0.00.331.121 I llama_init_from_model: n_ubatch      = 2048
0.00.331.121 I llama_init_from_model: flash_attn    = 0
0.00.331.122 I llama_init_from_model: freq_base     = 10000.0
0.00.331.122 I llama_init_from_model: freq_scale    = 1
0.00.331.123 I ggml_metal_init: allocating
0.00.331.126 I ggml_metal_init: found device: Apple M4
0.00.331.130 I ggml_metal_init: picking default device: Apple M4
0.00.332.007 I ggml_metal_init: using embedded metal library
0.00.334.789 I ggml_metal_init: GPU name:   Apple M4
0.00.334.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.334.791 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.334.791 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.334.792 I ggml_metal_init: simdgroup reduction   = true
0.00.334.792 I ggml_metal_init: simdgroup matrix mul. = true
0.00.334.792 I ggml_metal_init: has residency sets    = true
0.00.334.792 I ggml_metal_init: has bfloat            = true
0.00.334.792 I ggml_metal_init: use bfloat            = true
0.00.334.793 I ggml_metal_init: hasUnifiedMemory      = true
0.00.334.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.344.337 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.347.391 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.347.394 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.347.415 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.353.698 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.353.700 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.353.700 I llama_init_from_model: graph nodes  = 154
0.00.353.700 I llama_init_from_model: graph splits = 2
0.00.353.701 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.701 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.949 I 
0.00.360.980 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.384 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.384 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.398 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.399 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.403 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.405 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.951 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.498 I llama_perf_context_print:        load time =     336.53 ms
0.00.365.499 I llama_perf_context_print: prompt eval time =       3.54 ms /    62 tokens (    0.06 ms per token, 17519.07 tokens per second)
0.00.365.500 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.500 I llama_perf_context_print:       total time =       4.55 ms /    63 tokens
0.00.365.682 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.333s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.149 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.359 I main: llama backend init
0.00.000.370 I main: load the model and apply lora adapter, if any
0.00.057.573 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.071.293 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.071.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.071.340 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.071.341 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.071.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.071.342 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.071.343 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.071.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.071.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.071.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.071.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.071.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.071.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.071.351 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.071.358 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.071.358 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.071.359 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.080.086 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.082.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.090.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.090.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.090.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.090.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.090.370 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.090.372 I llama_model_loader: - type  f32:  194 tensors
0.00.090.372 I llama_model_loader: - type  f16:   98 tensors
0.00.090.376 I print_info: file format = GGUF V3 (latest)
0.00.090.378 I print_info: file type   = all F32 (guessed)
0.00.090.384 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.104.712 I load: special tokens cache size = 25
0.00.113.112 I load: token to piece cache size = 0.2984 MB
0.00.113.116 I print_info: arch             = gptneox
0.00.113.116 I print_info: vocab_only       = 0
0.00.113.116 I print_info: n_ctx_train      = 2048
0.00.113.116 I print_info: n_embd           = 2048
0.00.113.117 I print_info: n_layer          = 24
0.00.113.120 I print_info: n_head           = 16
0.00.113.121 I print_info: n_head_kv        = 16
0.00.113.121 I print_info: n_rot            = 32
0.00.113.122 I print_info: n_swa            = 0
0.00.113.122 I print_info: n_embd_head_k    = 128
0.00.113.122 I print_info: n_embd_head_v    = 128
0.00.113.125 I print_info: n_gqa            = 1
0.00.113.126 I print_info: n_embd_k_gqa     = 2048
0.00.113.127 I print_info: n_embd_v_gqa     = 2048
0.00.113.127 I print_info: f_norm_eps       = 1.0e-05
0.00.113.128 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.128 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.128 I print_info: f_logit_scale    = 0.0e+00
0.00.113.129 I print_info: n_ff             = 8192
0.00.113.129 I print_info: n_expert         = 0
0.00.113.130 I print_info: n_expert_used    = 0
0.00.113.130 I print_info: causal attn      = 1
0.00.113.130 I print_info: pooling type     = 0
0.00.113.130 I print_info: rope type        = 2
0.00.113.130 I print_info: rope scaling     = linear
0.00.113.132 I print_info: freq_base_train  = 10000.0
0.00.113.132 I print_info: freq_scale_train = 1
0.00.113.133 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.133 I print_info: rope_finetuned   = unknown
0.00.113.133 I print_info: ssm_d_conv       = 0
0.00.113.133 I print_info: ssm_d_inner      = 0
0.00.113.134 I print_info: ssm_d_state      = 0
0.00.113.134 I print_info: ssm_dt_rank      = 0
0.00.113.134 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.134 I print_info: model type       = 1.4B
0.00.113.135 I print_info: model params     = 1.41 B
0.00.113.136 I print_info: general.name     = 1.4B
0.00.113.137 I print_info: vocab type       = BPE
0.00.113.137 I print_info: n_vocab          = 50304
0.00.113.137 I print_info: n_merges         = 50009
0.00.113.137 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.137 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.138 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.138 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.140 I print_info: LF token         = 187 'Ċ'
0.00.113.140 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.140 I print_info: max token length = 1024
0.00.157.523 I load_tensors: offloading 24 repeating layers to GPU
0.00.157.529 I load_tensors: offloading output layer to GPU
0.00.157.529 I load_tensors: offloaded 25/25 layers to GPU
0.00.157.554 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.157.555 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.157.889 I llama_init_from_model: n_seq_max     = 1
0.00.157.891 I llama_init_from_model: n_ctx         = 2048
0.00.157.891 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.157.891 I llama_init_from_model: n_batch       = 2048
0.00.157.891 I llama_init_from_model: n_ubatch      = 512
0.00.157.892 I llama_init_from_model: flash_attn    = 0
0.00.157.892 I llama_init_from_model: freq_base     = 10000.0
0.00.157.892 I llama_init_from_model: freq_scale    = 1
0.00.157.893 I ggml_metal_init: allocating
0.00.157.915 I ggml_metal_init: found device: Apple M4
0.00.157.920 I ggml_metal_init: picking default device: Apple M4
0.00.158.604 I ggml_metal_init: using embedded metal library
0.00.205.579 I ggml_metal_init: GPU name:   Apple M4
0.00.205.584 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.205.584 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.205.585 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.205.585 I ggml_metal_init: simdgroup reduction   = true
0.00.205.585 I ggml_metal_init: simdgroup matrix mul. = true
0.00.205.585 I ggml_metal_init: has residency sets    = true
0.00.205.585 I ggml_metal_init: has bfloat            = true
0.00.205.585 I ggml_metal_init: use bfloat            = true
0.00.205.586 I ggml_metal_init: hasUnifiedMemory      = true
0.00.205.588 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.273.831 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.311.329 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.311.336 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.311.385 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.316.331 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.316.332 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.316.333 I llama_init_from_model: graph nodes  = 967
0.00.316.333 I llama_init_from_model: graph splits = 2
0.00.316.337 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.316.468 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.316.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.376.814 I main: llama threadpool init, n_threads = 4
0.00.376.860 I 
0.00.376.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.376.894 I 
0.00.376.940 I sampler seed: 1234
0.00.376.944 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.376.968 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.376.969 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.376.969 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.288.166 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.02.288.167 I llama_perf_context_print:        load time =     318.44 ms
0.02.288.168 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.63 tokens per second)
0.02.288.168 I llama_perf_context_print:        eval time =    1864.48 ms /    63 runs   (   29.59 ms per token,    33.79 tokens per second)
0.02.288.169 I llama_perf_context_print:       total time =    1912.14 ms /    70 tokens
0.02.288.413 I ggml_metal_free: deallocating

real	0m3.360s
user	0m0.137s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.654 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.152 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.924 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.936 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.942 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.943 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.950 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.777 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.204 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.208 I llama_model_loader: - type  f32:  194 tensors
0.00.058.208 I llama_model_loader: - type  f16:   98 tensors
0.00.058.209 I print_info: file format = GGUF V3 (latest)
0.00.058.211 I print_info: file type   = all F32 (guessed)
0.00.058.212 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.574 I load: special tokens cache size = 25
0.00.078.507 I load: token to piece cache size = 0.2984 MB
0.00.078.511 I print_info: arch             = gptneox
0.00.078.511 I print_info: vocab_only       = 0
0.00.078.511 I print_info: n_ctx_train      = 2048
0.00.078.512 I print_info: n_embd           = 2048
0.00.078.512 I print_info: n_layer          = 24
0.00.078.515 I print_info: n_head           = 16
0.00.078.516 I print_info: n_head_kv        = 16
0.00.078.516 I print_info: n_rot            = 32
0.00.078.516 I print_info: n_swa            = 0
0.00.078.517 I print_info: n_embd_head_k    = 128
0.00.078.517 I print_info: n_embd_head_v    = 128
0.00.078.518 I print_info: n_gqa            = 1
0.00.078.518 I print_info: n_embd_k_gqa     = 2048
0.00.078.519 I print_info: n_embd_v_gqa     = 2048
0.00.078.520 I print_info: f_norm_eps       = 1.0e-05
0.00.078.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.521 I print_info: f_logit_scale    = 0.0e+00
0.00.078.522 I print_info: n_ff             = 8192
0.00.078.522 I print_info: n_expert         = 0
0.00.078.522 I print_info: n_expert_used    = 0
0.00.078.522 I print_info: causal attn      = 1
0.00.078.522 I print_info: pooling type     = 0
0.00.078.523 I print_info: rope type        = 2
0.00.078.523 I print_info: rope scaling     = linear
0.00.078.523 I print_info: freq_base_train  = 10000.0
0.00.078.524 I print_info: freq_scale_train = 1
0.00.078.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.524 I print_info: rope_finetuned   = unknown
0.00.078.525 I print_info: ssm_d_conv       = 0
0.00.078.525 I print_info: ssm_d_inner      = 0
0.00.078.525 I print_info: ssm_d_state      = 0
0.00.078.525 I print_info: ssm_dt_rank      = 0
0.00.078.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.526 I print_info: model type       = 1.4B
0.00.078.526 I print_info: model params     = 1.41 B
0.00.078.526 I print_info: general.name     = 1.4B
0.00.078.527 I print_info: vocab type       = BPE
0.00.078.528 I print_info: n_vocab          = 50304
0.00.078.529 I print_info: n_merges         = 50009
0.00.078.529 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.529 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.529 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.530 I print_info: LF token         = 187 'Ċ'
0.00.078.530 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.530 I print_info: max token length = 1024
0.01.095.147 I load_tensors: offloading 24 repeating layers to GPU
0.01.095.157 I load_tensors: offloading output layer to GPU
0.01.095.158 I load_tensors: offloaded 25/25 layers to GPU
0.01.095.185 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.095.187 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.096.172 I llama_init_from_model: n_seq_max     = 1
0.01.096.173 I llama_init_from_model: n_ctx         = 128
0.01.096.173 I llama_init_from_model: n_ctx_per_seq = 128
0.01.096.173 I llama_init_from_model: n_batch       = 128
0.01.096.174 I llama_init_from_model: n_ubatch      = 128
0.01.096.174 I llama_init_from_model: flash_attn    = 0
0.01.096.174 I llama_init_from_model: freq_base     = 10000.0
0.01.096.175 I llama_init_from_model: freq_scale    = 1
0.01.096.175 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.096.176 I ggml_metal_init: allocating
0.01.096.220 I ggml_metal_init: found device: Apple M4
0.01.096.226 I ggml_metal_init: picking default device: Apple M4
0.01.097.261 I ggml_metal_init: using embedded metal library
0.01.101.140 I ggml_metal_init: GPU name:   Apple M4
0.01.101.142 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.101.143 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.101.143 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.101.144 I ggml_metal_init: simdgroup reduction   = true
0.01.101.144 I ggml_metal_init: simdgroup matrix mul. = true
0.01.101.144 I ggml_metal_init: has residency sets    = true
0.01.101.144 I ggml_metal_init: has bfloat            = true
0.01.101.144 I ggml_metal_init: use bfloat            = true
0.01.101.145 I ggml_metal_init: hasUnifiedMemory      = true
0.01.101.146 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.111.687 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.113.381 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.113.384 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.113.409 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.115.024 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.115.025 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.115.025 I llama_init_from_model: graph nodes  = 967
0.01.115.025 I llama_init_from_model: graph splits = 2
0.01.115.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.115.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.150.110 I 
0.01.150.149 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.150.170 I perplexity: tokenizing the input ..
0.01.155.326 I perplexity: tokenization took 5.154 ms
0.01.155.350 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.273.836 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.275.177 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.275.197 I llama_perf_context_print:        load time =    1124.95 ms
0.01.275.198 I llama_perf_context_print: prompt eval time =     118.21 ms /   128 tokens (    0.92 ms per token,  1082.80 tokens per second)
0.01.275.199 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.275.200 I llama_perf_context_print:       total time =     125.09 ms /   129 tokens
0.01.275.596 I ggml_metal_free: deallocating

real	0m1.767s
user	0m0.099s
sys	0m0.217s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.011.978 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.514 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.522 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.524 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.527 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.527 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.528 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.528 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.530 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.530 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.530 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.531 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.531 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.531 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.532 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.534 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.538 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.540 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.541 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.543 I llama_model_loader: - type  f32:  194 tensors
0.00.038.543 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.544 I print_info: file format = GGUF V3 (latest)
0.00.038.545 I print_info: file type   = Q8_0
0.00.038.546 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.848 I load: special tokens cache size = 25
0.00.054.684 I load: token to piece cache size = 0.2984 MB
0.00.054.688 I print_info: arch             = gptneox
0.00.054.688 I print_info: vocab_only       = 0
0.00.054.688 I print_info: n_ctx_train      = 2048
0.00.054.688 I print_info: n_embd           = 2048
0.00.054.689 I print_info: n_layer          = 24
0.00.054.695 I print_info: n_head           = 16
0.00.054.696 I print_info: n_head_kv        = 16
0.00.054.696 I print_info: n_rot            = 32
0.00.054.696 I print_info: n_swa            = 0
0.00.054.696 I print_info: n_embd_head_k    = 128
0.00.054.696 I print_info: n_embd_head_v    = 128
0.00.054.697 I print_info: n_gqa            = 1
0.00.054.698 I print_info: n_embd_k_gqa     = 2048
0.00.054.698 I print_info: n_embd_v_gqa     = 2048
0.00.054.699 I print_info: f_norm_eps       = 1.0e-05
0.00.054.699 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.699 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.700 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.700 I print_info: f_logit_scale    = 0.0e+00
0.00.054.700 I print_info: n_ff             = 8192
0.00.054.701 I print_info: n_expert         = 0
0.00.054.701 I print_info: n_expert_used    = 0
0.00.054.701 I print_info: causal attn      = 1
0.00.054.701 I print_info: pooling type     = 0
0.00.054.701 I print_info: rope type        = 2
0.00.054.701 I print_info: rope scaling     = linear
0.00.054.702 I print_info: freq_base_train  = 10000.0
0.00.054.702 I print_info: freq_scale_train = 1
0.00.054.702 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.703 I print_info: rope_finetuned   = unknown
0.00.054.703 I print_info: ssm_d_conv       = 0
0.00.054.703 I print_info: ssm_d_inner      = 0
0.00.054.703 I print_info: ssm_d_state      = 0
0.00.054.703 I print_info: ssm_dt_rank      = 0
0.00.054.703 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.704 I print_info: model type       = 1.4B
0.00.054.704 I print_info: model params     = 1.41 B
0.00.054.704 I print_info: general.name     = 1.4B
0.00.054.705 I print_info: vocab type       = BPE
0.00.054.708 I print_info: n_vocab          = 50304
0.00.054.709 I print_info: n_merges         = 50009
0.00.054.709 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.709 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.709 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.709 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.710 I print_info: LF token         = 187 'Ċ'
0.00.054.710 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.710 I print_info: max token length = 1024
0.01.041.462 I load_tensors: offloading 24 repeating layers to GPU
0.01.041.466 I load_tensors: offloading output layer to GPU
0.01.041.467 I load_tensors: offloaded 25/25 layers to GPU
0.01.041.490 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.041.491 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.042.171 I llama_init_from_model: n_seq_max     = 1
0.01.042.173 I llama_init_from_model: n_ctx         = 2048
0.01.042.173 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.042.173 I llama_init_from_model: n_batch       = 2048
0.01.042.174 I llama_init_from_model: n_ubatch      = 512
0.01.042.174 I llama_init_from_model: flash_attn    = 0
0.01.042.174 I llama_init_from_model: freq_base     = 10000.0
0.01.042.175 I llama_init_from_model: freq_scale    = 1
0.01.042.176 I ggml_metal_init: allocating
0.01.042.183 I ggml_metal_init: found device: Apple M4
0.01.042.189 I ggml_metal_init: picking default device: Apple M4
0.01.043.420 I ggml_metal_init: using embedded metal library
0.01.048.551 I ggml_metal_init: GPU name:   Apple M4
0.01.048.555 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.048.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.048.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.048.557 I ggml_metal_init: simdgroup reduction   = true
0.01.048.557 I ggml_metal_init: simdgroup matrix mul. = true
0.01.048.557 I ggml_metal_init: has residency sets    = true
0.01.048.558 I ggml_metal_init: has bfloat            = true
0.01.048.558 I ggml_metal_init: use bfloat            = true
0.01.048.558 I ggml_metal_init: hasUnifiedMemory      = true
0.01.048.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.063.560 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.117.807 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.117.813 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.117.845 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.122.758 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.122.759 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.122.760 I llama_init_from_model: graph nodes  = 967
0.01.122.760 I llama_init_from_model: graph splits = 2
0.01.122.765 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.122.893 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.122.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.178.102 I main: llama threadpool init, n_threads = 4
0.01.178.139 I 
0.01.178.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.178.161 I 
0.01.178.310 I sampler seed: 1234
0.01.178.315 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.178.357 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.178.360 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.178.361 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.284.118 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48797.25 tokens per second)
0.02.284.118 I llama_perf_context_print:        load time =    1165.49 ms
0.02.284.119 I llama_perf_context_print: prompt eval time =      49.13 ms /     7 tokens (    7.02 ms per token,   142.46 tokens per second)
0.02.284.120 I llama_perf_context_print:        eval time =    1054.07 ms /    63 runs   (   16.73 ms per token,    59.77 tokens per second)
0.02.284.120 I llama_perf_context_print:       total time =    1106.65 ms /    70 tokens
0.02.284.378 I ggml_metal_free: deallocating

real	0m2.307s
user	0m0.108s
sys	0m0.259s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.182 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.577 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.381 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.282 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.283 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.284 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.285 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.286 I llama_model_loader: - type  f32:  194 tensors
0.00.025.286 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.287 I print_info: file format = GGUF V3 (latest)
0.00.025.287 I print_info: file type   = Q8_0
0.00.025.288 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.155 I load: special tokens cache size = 25
0.00.039.251 I load: token to piece cache size = 0.2984 MB
0.00.039.255 I print_info: arch             = gptneox
0.00.039.255 I print_info: vocab_only       = 0
0.00.039.256 I print_info: n_ctx_train      = 2048
0.00.039.256 I print_info: n_embd           = 2048
0.00.039.256 I print_info: n_layer          = 24
0.00.039.260 I print_info: n_head           = 16
0.00.039.261 I print_info: n_head_kv        = 16
0.00.039.261 I print_info: n_rot            = 32
0.00.039.261 I print_info: n_swa            = 0
0.00.039.261 I print_info: n_embd_head_k    = 128
0.00.039.262 I print_info: n_embd_head_v    = 128
0.00.039.262 I print_info: n_gqa            = 1
0.00.039.266 I print_info: n_embd_k_gqa     = 2048
0.00.039.266 I print_info: n_embd_v_gqa     = 2048
0.00.039.267 I print_info: f_norm_eps       = 1.0e-05
0.00.039.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.267 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.267 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.267 I print_info: f_logit_scale    = 0.0e+00
0.00.039.268 I print_info: n_ff             = 8192
0.00.039.268 I print_info: n_expert         = 0
0.00.039.269 I print_info: n_expert_used    = 0
0.00.039.269 I print_info: causal attn      = 1
0.00.039.269 I print_info: pooling type     = 0
0.00.039.269 I print_info: rope type        = 2
0.00.039.269 I print_info: rope scaling     = linear
0.00.039.270 I print_info: freq_base_train  = 10000.0
0.00.039.270 I print_info: freq_scale_train = 1
0.00.039.270 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.270 I print_info: rope_finetuned   = unknown
0.00.039.270 I print_info: ssm_d_conv       = 0
0.00.039.270 I print_info: ssm_d_inner      = 0
0.00.039.271 I print_info: ssm_d_state      = 0
0.00.039.271 I print_info: ssm_dt_rank      = 0
0.00.039.271 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.272 I print_info: model type       = 1.4B
0.00.039.272 I print_info: model params     = 1.41 B
0.00.039.272 I print_info: general.name     = 1.4B
0.00.039.273 I print_info: vocab type       = BPE
0.00.039.273 I print_info: n_vocab          = 50304
0.00.039.273 I print_info: n_merges         = 50009
0.00.039.273 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.273 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.273 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.274 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.274 I print_info: LF token         = 187 'Ċ'
0.00.039.274 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.275 I print_info: max token length = 1024
0.00.895.927 I load_tensors: offloading 24 repeating layers to GPU
0.00.895.932 I load_tensors: offloading output layer to GPU
0.00.895.932 I load_tensors: offloaded 25/25 layers to GPU
0.00.895.959 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.895.963 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.897.201 I llama_init_from_model: n_seq_max     = 1
0.00.897.203 I llama_init_from_model: n_ctx         = 128
0.00.897.203 I llama_init_from_model: n_ctx_per_seq = 128
0.00.897.204 I llama_init_from_model: n_batch       = 128
0.00.897.207 I llama_init_from_model: n_ubatch      = 128
0.00.897.207 I llama_init_from_model: flash_attn    = 0
0.00.897.208 I llama_init_from_model: freq_base     = 10000.0
0.00.897.208 I llama_init_from_model: freq_scale    = 1
0.00.897.216 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.897.217 I ggml_metal_init: allocating
0.00.897.278 I ggml_metal_init: found device: Apple M4
0.00.897.286 I ggml_metal_init: picking default device: Apple M4
0.00.898.510 I ggml_metal_init: using embedded metal library
0.00.903.784 I ggml_metal_init: GPU name:   Apple M4
0.00.903.788 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.903.789 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.903.789 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.903.790 I ggml_metal_init: simdgroup reduction   = true
0.00.903.790 I ggml_metal_init: simdgroup matrix mul. = true
0.00.903.791 I ggml_metal_init: has residency sets    = true
0.00.903.791 I ggml_metal_init: has bfloat            = true
0.00.903.791 I ggml_metal_init: use bfloat            = true
0.00.903.792 I ggml_metal_init: hasUnifiedMemory      = true
0.00.903.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.918.938 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.922.396 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.922.399 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.922.543 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.925.807 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.925.809 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.925.809 I llama_init_from_model: graph nodes  = 967
0.00.925.810 I llama_init_from_model: graph splits = 2
0.00.925.813 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.925.813 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.955.759 I 
0.00.955.829 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.955.850 I perplexity: tokenizing the input ..
0.00.962.921 I perplexity: tokenization took 7.069 ms
0.00.962.940 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.102.621 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.103.957 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.103.971 I llama_perf_context_print:        load time =     946.57 ms
0.01.103.972 I llama_perf_context_print: prompt eval time =     138.77 ms /   128 tokens (    1.08 ms per token,   922.37 tokens per second)
0.01.103.973 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.103.973 I llama_perf_context_print:       total time =     148.22 ms /   129 tokens
0.01.104.377 I ggml_metal_free: deallocating

real	0m1.119s
user	0m0.076s
sys	0m0.175s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.018.288 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.028.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.099 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.103 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.104 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.105 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.886 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.153 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.191 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.192 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.192 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.192 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.193 I llama_model_loader: - type  f32:  194 tensors
0.00.039.193 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.194 I llama_model_loader: - type q6_K:    1 tensors
0.00.039.194 I print_info: file format = GGUF V3 (latest)
0.00.039.195 I print_info: file type   = Q4_0
0.00.039.196 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.050.724 I load: special tokens cache size = 25
0.00.061.192 I load: token to piece cache size = 0.2984 MB
0.00.061.197 I print_info: arch             = gptneox
0.00.061.198 I print_info: vocab_only       = 0
0.00.061.198 I print_info: n_ctx_train      = 2048
0.00.061.198 I print_info: n_embd           = 2048
0.00.061.199 I print_info: n_layer          = 24
0.00.061.204 I print_info: n_head           = 16
0.00.061.205 I print_info: n_head_kv        = 16
0.00.061.206 I print_info: n_rot            = 32
0.00.061.210 I print_info: n_swa            = 0
0.00.061.210 I print_info: n_embd_head_k    = 128
0.00.061.210 I print_info: n_embd_head_v    = 128
0.00.061.212 I print_info: n_gqa            = 1
0.00.061.213 I print_info: n_embd_k_gqa     = 2048
0.00.061.214 I print_info: n_embd_v_gqa     = 2048
0.00.061.215 I print_info: f_norm_eps       = 1.0e-05
0.00.061.216 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.217 I print_info: f_logit_scale    = 0.0e+00
0.00.061.218 I print_info: n_ff             = 8192
0.00.061.218 I print_info: n_expert         = 0
0.00.061.219 I print_info: n_expert_used    = 0
0.00.061.219 I print_info: causal attn      = 1
0.00.061.219 I print_info: pooling type     = 0
0.00.061.222 I print_info: rope type        = 2
0.00.061.224 I print_info: rope scaling     = linear
0.00.061.225 I print_info: freq_base_train  = 10000.0
0.00.061.226 I print_info: freq_scale_train = 1
0.00.061.226 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.226 I print_info: rope_finetuned   = unknown
0.00.061.227 I print_info: ssm_d_conv       = 0
0.00.061.227 I print_info: ssm_d_inner      = 0
0.00.061.227 I print_info: ssm_d_state      = 0
0.00.061.228 I print_info: ssm_dt_rank      = 0
0.00.061.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.228 I print_info: model type       = 1.4B
0.00.061.229 I print_info: model params     = 1.41 B
0.00.061.232 I print_info: general.name     = 1.4B
0.00.061.232 I print_info: vocab type       = BPE
0.00.061.233 I print_info: n_vocab          = 50304
0.00.061.233 I print_info: n_merges         = 50009
0.00.061.234 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.234 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.234 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.235 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.235 I print_info: LF token         = 187 'Ċ'
0.00.061.236 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.236 I print_info: max token length = 1024
0.00.636.234 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.246 I load_tensors: offloading output layer to GPU
0.00.636.246 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.282 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.636.283 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.637.261 I llama_init_from_model: n_seq_max     = 1
0.00.637.266 I llama_init_from_model: n_ctx         = 2048
0.00.637.266 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.637.266 I llama_init_from_model: n_batch       = 2048
0.00.637.267 I llama_init_from_model: n_ubatch      = 512
0.00.637.267 I llama_init_from_model: flash_attn    = 0
0.00.637.269 I llama_init_from_model: freq_base     = 10000.0
0.00.637.269 I llama_init_from_model: freq_scale    = 1
0.00.637.275 I ggml_metal_init: allocating
0.00.637.385 I ggml_metal_init: found device: Apple M4
0.00.637.400 I ggml_metal_init: picking default device: Apple M4
0.00.639.297 I ggml_metal_init: using embedded metal library
0.00.644.715 I ggml_metal_init: GPU name:   Apple M4
0.00.644.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.644.731 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.644.732 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.644.733 I ggml_metal_init: simdgroup reduction   = true
0.00.644.733 I ggml_metal_init: simdgroup matrix mul. = true
0.00.644.733 I ggml_metal_init: has residency sets    = true
0.00.644.733 I ggml_metal_init: has bfloat            = true
0.00.644.734 I ggml_metal_init: use bfloat            = true
0.00.644.738 I ggml_metal_init: hasUnifiedMemory      = true
0.00.644.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.758 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.787 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.720.792 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.720.829 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.725.025 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.725.027 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.725.027 I llama_init_from_model: graph nodes  = 967
0.00.725.028 I llama_init_from_model: graph splits = 2
0.00.725.034 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.725.166 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.725.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.781.044 I main: llama threadpool init, n_threads = 4
0.00.781.089 I 
0.00.781.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.781.117 I 
0.00.783.801 I sampler seed: 1234
0.00.783.812 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.842 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.844 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.844 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.476.490 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51079.14 tokens per second)
0.01.476.490 I llama_perf_context_print:        load time =     762.08 ms
0.01.476.491 I llama_perf_context_print: prompt eval time =      47.09 ms /     7 tokens (    6.73 ms per token,   148.65 tokens per second)
0.01.476.492 I llama_perf_context_print:        eval time =     642.68 ms /    63 runs   (   10.20 ms per token,    98.03 tokens per second)
0.01.476.492 I llama_perf_context_print:       total time =     696.12 ms /    70 tokens
0.01.476.681 I ggml_metal_free: deallocating

real	0m1.514s
user	0m0.123s
sys	0m0.203s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.534 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.034 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.042 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.043 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.821 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.548 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.549 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.551 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.552 I llama_model_loader: - type  f32:  194 tensors
0.00.025.552 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.553 I print_info: file format = GGUF V3 (latest)
0.00.025.553 I print_info: file type   = Q4_0
0.00.025.555 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.387 I load: special tokens cache size = 25
0.00.039.266 I load: token to piece cache size = 0.2984 MB
0.00.039.269 I print_info: arch             = gptneox
0.00.039.269 I print_info: vocab_only       = 0
0.00.039.270 I print_info: n_ctx_train      = 2048
0.00.039.270 I print_info: n_embd           = 2048
0.00.039.270 I print_info: n_layer          = 24
0.00.039.274 I print_info: n_head           = 16
0.00.039.276 I print_info: n_head_kv        = 16
0.00.039.277 I print_info: n_rot            = 32
0.00.039.277 I print_info: n_swa            = 0
0.00.039.277 I print_info: n_embd_head_k    = 128
0.00.039.277 I print_info: n_embd_head_v    = 128
0.00.039.278 I print_info: n_gqa            = 1
0.00.039.279 I print_info: n_embd_k_gqa     = 2048
0.00.039.279 I print_info: n_embd_v_gqa     = 2048
0.00.039.280 I print_info: f_norm_eps       = 1.0e-05
0.00.039.280 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.280 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.280 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.281 I print_info: f_logit_scale    = 0.0e+00
0.00.039.282 I print_info: n_ff             = 8192
0.00.039.282 I print_info: n_expert         = 0
0.00.039.282 I print_info: n_expert_used    = 0
0.00.039.283 I print_info: causal attn      = 1
0.00.039.283 I print_info: pooling type     = 0
0.00.039.283 I print_info: rope type        = 2
0.00.039.283 I print_info: rope scaling     = linear
0.00.039.283 I print_info: freq_base_train  = 10000.0
0.00.039.284 I print_info: freq_scale_train = 1
0.00.039.284 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.284 I print_info: rope_finetuned   = unknown
0.00.039.284 I print_info: ssm_d_conv       = 0
0.00.039.286 I print_info: ssm_d_inner      = 0
0.00.039.286 I print_info: ssm_d_state      = 0
0.00.039.286 I print_info: ssm_dt_rank      = 0
0.00.039.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.286 I print_info: model type       = 1.4B
0.00.039.287 I print_info: model params     = 1.41 B
0.00.039.287 I print_info: general.name     = 1.4B
0.00.039.288 I print_info: vocab type       = BPE
0.00.039.288 I print_info: n_vocab          = 50304
0.00.039.288 I print_info: n_merges         = 50009
0.00.039.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: LF token         = 187 'Ċ'
0.00.039.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.290 I print_info: max token length = 1024
0.00.601.532 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.550 I load_tensors: offloading output layer to GPU
0.00.601.551 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.587 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.601.588 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.603.093 I llama_init_from_model: n_seq_max     = 1
0.00.603.098 I llama_init_from_model: n_ctx         = 128
0.00.603.098 I llama_init_from_model: n_ctx_per_seq = 128
0.00.603.103 I llama_init_from_model: n_batch       = 128
0.00.603.103 I llama_init_from_model: n_ubatch      = 128
0.00.603.104 I llama_init_from_model: flash_attn    = 0
0.00.603.105 I llama_init_from_model: freq_base     = 10000.0
0.00.603.110 I llama_init_from_model: freq_scale    = 1
0.00.603.111 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.603.118 I ggml_metal_init: allocating
0.00.603.193 I ggml_metal_init: found device: Apple M4
0.00.603.208 I ggml_metal_init: picking default device: Apple M4
0.00.605.082 I ggml_metal_init: using embedded metal library
0.00.611.370 I ggml_metal_init: GPU name:   Apple M4
0.00.611.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.611.376 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.611.377 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.611.377 I ggml_metal_init: simdgroup reduction   = true
0.00.611.378 I ggml_metal_init: simdgroup matrix mul. = true
0.00.611.378 I ggml_metal_init: has residency sets    = true
0.00.611.378 I ggml_metal_init: has bfloat            = true
0.00.611.379 I ggml_metal_init: use bfloat            = true
0.00.611.380 I ggml_metal_init: hasUnifiedMemory      = true
0.00.611.381 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.986 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.576 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.580 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.715 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.637.141 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.637.143 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.637.143 I llama_init_from_model: graph nodes  = 967
0.00.637.144 I llama_init_from_model: graph splits = 2
0.00.637.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.914 I 
0.00.660.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.019 I perplexity: tokenizing the input ..
0.00.668.167 I perplexity: tokenization took 7.144 ms
0.00.668.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.791.508 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.792.819 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.792.832 I llama_perf_context_print:        load time =     651.37 ms
0.00.792.833 I llama_perf_context_print: prompt eval time =     122.37 ms /   128 tokens (    0.96 ms per token,  1046.00 tokens per second)
0.00.792.836 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.836 I llama_perf_context_print:       total time =     131.92 ms /   129 tokens
0.00.793.210 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.079s
sys	0m0.124s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.736 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.019.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.252 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.254 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.255 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.257 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.258 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.258 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.926 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.928 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.929 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.929 I llama_model_loader: - type  f32:  194 tensors
0.00.027.930 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.930 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.930 I print_info: file format = GGUF V3 (latest)
0.00.027.931 I print_info: file type   = Q4_1
0.00.027.932 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.766 I load: special tokens cache size = 25
0.00.041.718 I load: token to piece cache size = 0.2984 MB
0.00.041.721 I print_info: arch             = gptneox
0.00.041.721 I print_info: vocab_only       = 0
0.00.041.721 I print_info: n_ctx_train      = 2048
0.00.041.722 I print_info: n_embd           = 2048
0.00.041.722 I print_info: n_layer          = 24
0.00.041.725 I print_info: n_head           = 16
0.00.041.725 I print_info: n_head_kv        = 16
0.00.041.725 I print_info: n_rot            = 32
0.00.041.726 I print_info: n_swa            = 0
0.00.041.727 I print_info: n_embd_head_k    = 128
0.00.041.727 I print_info: n_embd_head_v    = 128
0.00.041.728 I print_info: n_gqa            = 1
0.00.041.729 I print_info: n_embd_k_gqa     = 2048
0.00.041.729 I print_info: n_embd_v_gqa     = 2048
0.00.041.730 I print_info: f_norm_eps       = 1.0e-05
0.00.041.730 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.730 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.734 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.734 I print_info: f_logit_scale    = 0.0e+00
0.00.041.735 I print_info: n_ff             = 8192
0.00.041.735 I print_info: n_expert         = 0
0.00.041.736 I print_info: n_expert_used    = 0
0.00.041.736 I print_info: causal attn      = 1
0.00.041.736 I print_info: pooling type     = 0
0.00.041.736 I print_info: rope type        = 2
0.00.041.738 I print_info: rope scaling     = linear
0.00.041.738 I print_info: freq_base_train  = 10000.0
0.00.041.738 I print_info: freq_scale_train = 1
0.00.041.739 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.739 I print_info: rope_finetuned   = unknown
0.00.041.739 I print_info: ssm_d_conv       = 0
0.00.041.739 I print_info: ssm_d_inner      = 0
0.00.041.739 I print_info: ssm_d_state      = 0
0.00.041.739 I print_info: ssm_dt_rank      = 0
0.00.041.740 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.740 I print_info: model type       = 1.4B
0.00.041.740 I print_info: model params     = 1.41 B
0.00.041.740 I print_info: general.name     = 1.4B
0.00.041.741 I print_info: vocab type       = BPE
0.00.041.741 I print_info: n_vocab          = 50304
0.00.041.741 I print_info: n_merges         = 50009
0.00.041.741 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.742 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.742 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.742 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.742 I print_info: LF token         = 187 'Ċ'
0.00.041.742 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.743 I print_info: max token length = 1024
0.00.723.855 I load_tensors: offloading 24 repeating layers to GPU
0.00.723.870 I load_tensors: offloading output layer to GPU
0.00.723.871 I load_tensors: offloaded 25/25 layers to GPU
0.00.723.905 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.723.917 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.725.409 I llama_init_from_model: n_seq_max     = 1
0.00.725.414 I llama_init_from_model: n_ctx         = 2048
0.00.725.415 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.725.415 I llama_init_from_model: n_batch       = 2048
0.00.725.416 I llama_init_from_model: n_ubatch      = 512
0.00.725.416 I llama_init_from_model: flash_attn    = 0
0.00.725.418 I llama_init_from_model: freq_base     = 10000.0
0.00.725.419 I llama_init_from_model: freq_scale    = 1
0.00.725.421 I ggml_metal_init: allocating
0.00.725.496 I ggml_metal_init: found device: Apple M4
0.00.725.511 I ggml_metal_init: picking default device: Apple M4
0.00.727.365 I ggml_metal_init: using embedded metal library
0.00.733.893 I ggml_metal_init: GPU name:   Apple M4
0.00.733.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.733.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.733.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.733.901 I ggml_metal_init: simdgroup reduction   = true
0.00.733.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.733.902 I ggml_metal_init: has residency sets    = true
0.00.733.902 I ggml_metal_init: has bfloat            = true
0.00.733.902 I ggml_metal_init: use bfloat            = true
0.00.733.903 I ggml_metal_init: hasUnifiedMemory      = true
0.00.733.905 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.752.106 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.807.799 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.807.807 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.807.840 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.812.044 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.812.046 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.812.047 I llama_init_from_model: graph nodes  = 967
0.00.812.047 I llama_init_from_model: graph splits = 2
0.00.812.052 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.812.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.812.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.868.555 I main: llama threadpool init, n_threads = 4
0.00.868.600 I 
0.00.868.623 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.868.623 I 
0.00.868.778 I sampler seed: 1234
0.00.868.782 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.868.801 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.868.801 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.868.801 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.608.953 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.608.954 I llama_perf_context_print:        load time =     859.17 ms
0.01.608.954 I llama_perf_context_print: prompt eval time =      48.99 ms /     7 tokens (    7.00 ms per token,   142.90 tokens per second)
0.01.608.955 I llama_perf_context_print:        eval time =     688.35 ms /    63 runs   (   10.93 ms per token,    91.52 tokens per second)
0.01.608.956 I llama_perf_context_print:       total time =     741.04 ms /    70 tokens
0.01.609.215 I ggml_metal_free: deallocating

real	0m1.626s
user	0m0.108s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.999 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.688 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.694 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.696 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.696 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.700 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.701 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.602 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.467 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.467 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.468 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.469 I llama_model_loader: - type  f32:  194 tensors
0.00.025.469 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.469 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.470 I print_info: file format = GGUF V3 (latest)
0.00.025.471 I print_info: file type   = Q4_1
0.00.025.472 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.779 I load: special tokens cache size = 25
0.00.039.605 I load: token to piece cache size = 0.2984 MB
0.00.039.609 I print_info: arch             = gptneox
0.00.039.609 I print_info: vocab_only       = 0
0.00.039.610 I print_info: n_ctx_train      = 2048
0.00.039.610 I print_info: n_embd           = 2048
0.00.039.610 I print_info: n_layer          = 24
0.00.039.614 I print_info: n_head           = 16
0.00.039.615 I print_info: n_head_kv        = 16
0.00.039.615 I print_info: n_rot            = 32
0.00.039.615 I print_info: n_swa            = 0
0.00.039.616 I print_info: n_embd_head_k    = 128
0.00.039.616 I print_info: n_embd_head_v    = 128
0.00.039.618 I print_info: n_gqa            = 1
0.00.039.619 I print_info: n_embd_k_gqa     = 2048
0.00.039.619 I print_info: n_embd_v_gqa     = 2048
0.00.039.620 I print_info: f_norm_eps       = 1.0e-05
0.00.039.620 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.620 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.620 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.621 I print_info: f_logit_scale    = 0.0e+00
0.00.039.621 I print_info: n_ff             = 8192
0.00.039.621 I print_info: n_expert         = 0
0.00.039.622 I print_info: n_expert_used    = 0
0.00.039.623 I print_info: causal attn      = 1
0.00.039.623 I print_info: pooling type     = 0
0.00.039.624 I print_info: rope type        = 2
0.00.039.624 I print_info: rope scaling     = linear
0.00.039.624 I print_info: freq_base_train  = 10000.0
0.00.039.626 I print_info: freq_scale_train = 1
0.00.039.626 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.626 I print_info: rope_finetuned   = unknown
0.00.039.626 I print_info: ssm_d_conv       = 0
0.00.039.626 I print_info: ssm_d_inner      = 0
0.00.039.627 I print_info: ssm_d_state      = 0
0.00.039.628 I print_info: ssm_dt_rank      = 0
0.00.039.628 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.628 I print_info: model type       = 1.4B
0.00.039.629 I print_info: model params     = 1.41 B
0.00.039.629 I print_info: general.name     = 1.4B
0.00.039.629 I print_info: vocab type       = BPE
0.00.039.630 I print_info: n_vocab          = 50304
0.00.039.630 I print_info: n_merges         = 50009
0.00.039.630 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.630 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: LF token         = 187 'Ċ'
0.00.039.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.631 I print_info: max token length = 1024
0.00.672.175 I load_tensors: offloading 24 repeating layers to GPU
0.00.672.185 I load_tensors: offloading output layer to GPU
0.00.672.186 I load_tensors: offloaded 25/25 layers to GPU
0.00.672.217 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.672.221 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.673.768 I llama_init_from_model: n_seq_max     = 1
0.00.673.774 I llama_init_from_model: n_ctx         = 128
0.00.673.774 I llama_init_from_model: n_ctx_per_seq = 128
0.00.673.775 I llama_init_from_model: n_batch       = 128
0.00.673.775 I llama_init_from_model: n_ubatch      = 128
0.00.673.776 I llama_init_from_model: flash_attn    = 0
0.00.673.778 I llama_init_from_model: freq_base     = 10000.0
0.00.673.779 I llama_init_from_model: freq_scale    = 1
0.00.673.779 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.673.781 I ggml_metal_init: allocating
0.00.673.837 I ggml_metal_init: found device: Apple M4
0.00.673.851 I ggml_metal_init: picking default device: Apple M4
0.00.675.566 I ggml_metal_init: using embedded metal library
0.00.682.410 I ggml_metal_init: GPU name:   Apple M4
0.00.682.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.682.415 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.682.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.682.420 I ggml_metal_init: simdgroup reduction   = true
0.00.682.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.682.421 I ggml_metal_init: has residency sets    = true
0.00.682.421 I ggml_metal_init: has bfloat            = true
0.00.682.422 I ggml_metal_init: use bfloat            = true
0.00.682.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.682.428 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.700.355 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.703.811 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.703.815 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.703.861 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.707.088 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.707.090 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.707.090 I llama_init_from_model: graph nodes  = 967
0.00.707.091 I llama_init_from_model: graph splits = 2
0.00.707.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.707.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.778 I 
0.00.732.857 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.876 I perplexity: tokenizing the input ..
0.00.740.069 I perplexity: tokenization took 7.189 ms
0.00.740.094 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.876.615 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.878.018 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.878.031 I llama_perf_context_print:        load time =     723.77 ms
0.00.878.032 I llama_perf_context_print: prompt eval time =     135.66 ms /   128 tokens (    1.06 ms per token,   943.51 tokens per second)
0.00.878.033 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.033 I llama_perf_context_print:       total time =     145.26 ms /   129 tokens
0.00.878.387 I ggml_metal_free: deallocating

real	0m0.892s
user	0m0.079s
sys	0m0.126s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.010.047 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.331 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.343 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.343 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.347 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.167 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.938 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.940 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.940 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.941 I llama_model_loader: - type  f32:  194 tensors
0.00.025.941 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.942 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.942 I print_info: file format = GGUF V3 (latest)
0.00.025.943 I print_info: file type   = Q5_0
0.00.025.944 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.072 I load: special tokens cache size = 25
0.00.040.010 I load: token to piece cache size = 0.2984 MB
0.00.040.012 I print_info: arch             = gptneox
0.00.040.012 I print_info: vocab_only       = 0
0.00.040.013 I print_info: n_ctx_train      = 2048
0.00.040.013 I print_info: n_embd           = 2048
0.00.040.013 I print_info: n_layer          = 24
0.00.040.016 I print_info: n_head           = 16
0.00.040.017 I print_info: n_head_kv        = 16
0.00.040.017 I print_info: n_rot            = 32
0.00.040.017 I print_info: n_swa            = 0
0.00.040.017 I print_info: n_embd_head_k    = 128
0.00.040.019 I print_info: n_embd_head_v    = 128
0.00.040.020 I print_info: n_gqa            = 1
0.00.040.020 I print_info: n_embd_k_gqa     = 2048
0.00.040.021 I print_info: n_embd_v_gqa     = 2048
0.00.040.026 I print_info: f_norm_eps       = 1.0e-05
0.00.040.026 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.027 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.027 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.027 I print_info: f_logit_scale    = 0.0e+00
0.00.040.028 I print_info: n_ff             = 8192
0.00.040.028 I print_info: n_expert         = 0
0.00.040.028 I print_info: n_expert_used    = 0
0.00.040.028 I print_info: causal attn      = 1
0.00.040.028 I print_info: pooling type     = 0
0.00.040.028 I print_info: rope type        = 2
0.00.040.032 I print_info: rope scaling     = linear
0.00.040.033 I print_info: freq_base_train  = 10000.0
0.00.040.034 I print_info: freq_scale_train = 1
0.00.040.034 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.035 I print_info: rope_finetuned   = unknown
0.00.040.035 I print_info: ssm_d_conv       = 0
0.00.040.035 I print_info: ssm_d_inner      = 0
0.00.040.035 I print_info: ssm_d_state      = 0
0.00.040.035 I print_info: ssm_dt_rank      = 0
0.00.040.035 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.035 I print_info: model type       = 1.4B
0.00.040.036 I print_info: model params     = 1.41 B
0.00.040.036 I print_info: general.name     = 1.4B
0.00.040.036 I print_info: vocab type       = BPE
0.00.040.036 I print_info: n_vocab          = 50304
0.00.040.037 I print_info: n_merges         = 50009
0.00.040.037 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.037 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.037 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.037 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.037 I print_info: LF token         = 187 'Ċ'
0.00.040.038 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.038 I print_info: max token length = 1024
0.00.655.209 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.223 I load_tensors: offloading output layer to GPU
0.00.655.223 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.257 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.655.258 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.656.695 I llama_init_from_model: n_seq_max     = 1
0.00.656.701 I llama_init_from_model: n_ctx         = 2048
0.00.656.701 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.656.702 I llama_init_from_model: n_batch       = 2048
0.00.656.702 I llama_init_from_model: n_ubatch      = 512
0.00.656.703 I llama_init_from_model: flash_attn    = 0
0.00.656.705 I llama_init_from_model: freq_base     = 10000.0
0.00.656.706 I llama_init_from_model: freq_scale    = 1
0.00.656.708 I ggml_metal_init: allocating
0.00.656.783 I ggml_metal_init: found device: Apple M4
0.00.656.797 I ggml_metal_init: picking default device: Apple M4
0.00.658.449 I ggml_metal_init: using embedded metal library
0.00.664.915 I ggml_metal_init: GPU name:   Apple M4
0.00.664.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.921 I ggml_metal_init: simdgroup reduction   = true
0.00.664.921 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.922 I ggml_metal_init: has residency sets    = true
0.00.664.922 I ggml_metal_init: has bfloat            = true
0.00.664.922 I ggml_metal_init: use bfloat            = true
0.00.664.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.924 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.628 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.018 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.025 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.061 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.739.861 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.739.863 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.739.863 I llama_init_from_model: graph nodes  = 967
0.00.739.863 I llama_init_from_model: graph splits = 2
0.00.739.868 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.697 I main: llama threadpool init, n_threads = 4
0.00.797.741 I 
0.00.797.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.766 I 
0.00.797.947 I sampler seed: 1234
0.00.797.952 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.990 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.993 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.993 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.600.147 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.01.600.148 I llama_perf_context_print:        load time =     786.95 ms
0.01.600.149 I llama_perf_context_print: prompt eval time =      52.92 ms /     7 tokens (    7.56 ms per token,   132.27 tokens per second)
0.01.600.150 I llama_perf_context_print:        eval time =     746.26 ms /    63 runs   (   11.85 ms per token,    84.42 tokens per second)
0.01.600.151 I llama_perf_context_print:       total time =     803.15 ms /    70 tokens
0.01.600.409 I ggml_metal_free: deallocating

real	0m1.618s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.008 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.045 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.046 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.046 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.046 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.048 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.048 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.049 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.049 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.053 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.879 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.753 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.754 I llama_model_loader: - type  f32:  194 tensors
0.00.025.755 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.755 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.756 I print_info: file format = GGUF V3 (latest)
0.00.025.756 I print_info: file type   = Q5_0
0.00.025.757 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.794 I load: special tokens cache size = 25
0.00.039.949 I load: token to piece cache size = 0.2984 MB
0.00.039.953 I print_info: arch             = gptneox
0.00.039.953 I print_info: vocab_only       = 0
0.00.039.953 I print_info: n_ctx_train      = 2048
0.00.039.953 I print_info: n_embd           = 2048
0.00.039.954 I print_info: n_layer          = 24
0.00.039.957 I print_info: n_head           = 16
0.00.039.957 I print_info: n_head_kv        = 16
0.00.039.958 I print_info: n_rot            = 32
0.00.039.958 I print_info: n_swa            = 0
0.00.039.958 I print_info: n_embd_head_k    = 128
0.00.039.958 I print_info: n_embd_head_v    = 128
0.00.039.959 I print_info: n_gqa            = 1
0.00.039.960 I print_info: n_embd_k_gqa     = 2048
0.00.039.960 I print_info: n_embd_v_gqa     = 2048
0.00.039.961 I print_info: f_norm_eps       = 1.0e-05
0.00.039.961 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.961 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.962 I print_info: f_logit_scale    = 0.0e+00
0.00.039.962 I print_info: n_ff             = 8192
0.00.039.963 I print_info: n_expert         = 0
0.00.039.963 I print_info: n_expert_used    = 0
0.00.039.963 I print_info: causal attn      = 1
0.00.039.963 I print_info: pooling type     = 0
0.00.039.963 I print_info: rope type        = 2
0.00.039.963 I print_info: rope scaling     = linear
0.00.039.964 I print_info: freq_base_train  = 10000.0
0.00.039.964 I print_info: freq_scale_train = 1
0.00.039.964 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.965 I print_info: rope_finetuned   = unknown
0.00.039.965 I print_info: ssm_d_conv       = 0
0.00.039.967 I print_info: ssm_d_inner      = 0
0.00.039.967 I print_info: ssm_d_state      = 0
0.00.039.967 I print_info: ssm_dt_rank      = 0
0.00.039.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.968 I print_info: model type       = 1.4B
0.00.039.968 I print_info: model params     = 1.41 B
0.00.039.968 I print_info: general.name     = 1.4B
0.00.039.969 I print_info: vocab type       = BPE
0.00.039.969 I print_info: n_vocab          = 50304
0.00.039.969 I print_info: n_merges         = 50009
0.00.039.969 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.974 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.975 I print_info: LF token         = 187 'Ċ'
0.00.039.975 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.976 I print_info: max token length = 1024
0.00.715.026 I load_tensors: offloading 24 repeating layers to GPU
0.00.715.043 I load_tensors: offloading output layer to GPU
0.00.715.044 I load_tensors: offloaded 25/25 layers to GPU
0.00.715.078 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.715.085 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.716.618 I llama_init_from_model: n_seq_max     = 1
0.00.716.623 I llama_init_from_model: n_ctx         = 128
0.00.716.623 I llama_init_from_model: n_ctx_per_seq = 128
0.00.716.624 I llama_init_from_model: n_batch       = 128
0.00.716.624 I llama_init_from_model: n_ubatch      = 128
0.00.716.625 I llama_init_from_model: flash_attn    = 0
0.00.716.628 I llama_init_from_model: freq_base     = 10000.0
0.00.716.628 I llama_init_from_model: freq_scale    = 1
0.00.716.629 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.716.632 I ggml_metal_init: allocating
0.00.716.710 I ggml_metal_init: found device: Apple M4
0.00.716.724 I ggml_metal_init: picking default device: Apple M4
0.00.718.500 I ggml_metal_init: using embedded metal library
0.00.725.269 I ggml_metal_init: GPU name:   Apple M4
0.00.725.274 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.725.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.725.275 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.725.279 I ggml_metal_init: simdgroup reduction   = true
0.00.725.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.725.280 I ggml_metal_init: has residency sets    = true
0.00.725.280 I ggml_metal_init: has bfloat            = true
0.00.725.280 I ggml_metal_init: use bfloat            = true
0.00.725.281 I ggml_metal_init: hasUnifiedMemory      = true
0.00.725.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.742.658 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.236 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.746.240 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.746.284 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.539 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.749.540 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.749.541 I llama_init_from_model: graph nodes  = 967
0.00.749.541 I llama_init_from_model: graph splits = 2
0.00.749.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.749.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.843 I 
0.00.783.931 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.952 I perplexity: tokenizing the input ..
0.00.790.955 I perplexity: tokenization took 7 ms
0.00.790.976 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.938.760 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.940.104 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.940.118 I llama_perf_context_print:        load time =     773.83 ms
0.00.940.119 I llama_perf_context_print: prompt eval time =     146.90 ms /   128 tokens (    1.15 ms per token,   871.32 tokens per second)
0.00.940.119 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.940.120 I llama_perf_context_print:       total time =     156.28 ms /   129 tokens
0.00.940.506 I ggml_metal_free: deallocating

real	0m0.957s
user	0m0.079s
sys	0m0.157s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.709 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.027 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.048 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.048 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.770 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.780 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.470 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.472 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.473 I llama_model_loader: - type  f32:  194 tensors
0.00.024.474 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.474 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.474 I print_info: file format = GGUF V3 (latest)
0.00.024.475 I print_info: file type   = Q5_1
0.00.024.479 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.264 I load: special tokens cache size = 25
0.00.038.325 I load: token to piece cache size = 0.2984 MB
0.00.038.327 I print_info: arch             = gptneox
0.00.038.328 I print_info: vocab_only       = 0
0.00.038.328 I print_info: n_ctx_train      = 2048
0.00.038.328 I print_info: n_embd           = 2048
0.00.038.328 I print_info: n_layer          = 24
0.00.038.331 I print_info: n_head           = 16
0.00.038.331 I print_info: n_head_kv        = 16
0.00.038.331 I print_info: n_rot            = 32
0.00.038.332 I print_info: n_swa            = 0
0.00.038.332 I print_info: n_embd_head_k    = 128
0.00.038.332 I print_info: n_embd_head_v    = 128
0.00.038.333 I print_info: n_gqa            = 1
0.00.038.334 I print_info: n_embd_k_gqa     = 2048
0.00.038.334 I print_info: n_embd_v_gqa     = 2048
0.00.038.335 I print_info: f_norm_eps       = 1.0e-05
0.00.038.337 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.337 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.337 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.337 I print_info: f_logit_scale    = 0.0e+00
0.00.038.338 I print_info: n_ff             = 8192
0.00.038.338 I print_info: n_expert         = 0
0.00.038.338 I print_info: n_expert_used    = 0
0.00.038.339 I print_info: causal attn      = 1
0.00.038.339 I print_info: pooling type     = 0
0.00.038.339 I print_info: rope type        = 2
0.00.038.339 I print_info: rope scaling     = linear
0.00.038.340 I print_info: freq_base_train  = 10000.0
0.00.038.340 I print_info: freq_scale_train = 1
0.00.038.340 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.340 I print_info: rope_finetuned   = unknown
0.00.038.341 I print_info: ssm_d_conv       = 0
0.00.038.341 I print_info: ssm_d_inner      = 0
0.00.038.341 I print_info: ssm_d_state      = 0
0.00.038.341 I print_info: ssm_dt_rank      = 0
0.00.038.341 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.341 I print_info: model type       = 1.4B
0.00.038.342 I print_info: model params     = 1.41 B
0.00.038.342 I print_info: general.name     = 1.4B
0.00.038.342 I print_info: vocab type       = BPE
0.00.038.342 I print_info: n_vocab          = 50304
0.00.038.343 I print_info: n_merges         = 50009
0.00.038.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.343 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.343 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.344 I print_info: LF token         = 187 'Ċ'
0.00.038.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.344 I print_info: max token length = 1024
0.00.600.949 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.954 I load_tensors: offloading output layer to GPU
0.00.600.955 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.978 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.600.979 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.602.269 I llama_init_from_model: n_seq_max     = 1
0.00.602.272 I llama_init_from_model: n_ctx         = 2048
0.00.602.272 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.602.272 I llama_init_from_model: n_batch       = 2048
0.00.602.273 I llama_init_from_model: n_ubatch      = 512
0.00.602.273 I llama_init_from_model: flash_attn    = 0
0.00.602.274 I llama_init_from_model: freq_base     = 10000.0
0.00.602.275 I llama_init_from_model: freq_scale    = 1
0.00.602.276 I ggml_metal_init: allocating
0.00.602.291 I ggml_metal_init: found device: Apple M4
0.00.602.300 I ggml_metal_init: picking default device: Apple M4
0.00.603.734 I ggml_metal_init: using embedded metal library
0.00.609.927 I ggml_metal_init: GPU name:   Apple M4
0.00.609.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.933 I ggml_metal_init: simdgroup reduction   = true
0.00.609.933 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.934 I ggml_metal_init: has residency sets    = true
0.00.609.934 I ggml_metal_init: has bfloat            = true
0.00.609.934 I ggml_metal_init: use bfloat            = true
0.00.609.935 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.341 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.432 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.437 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.472 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.208 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.210 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.211 I llama_init_from_model: graph nodes  = 967
0.00.686.211 I llama_init_from_model: graph splits = 2
0.00.686.217 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.349 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.350 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.744.983 I main: llama threadpool init, n_threads = 4
0.00.745.026 I 
0.00.745.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.049 I 
0.00.745.229 I sampler seed: 1234
0.00.745.234 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.745.245 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.745.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.745.245 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.594.697 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.594.698 I llama_perf_context_print:        load time =     735.62 ms
0.01.594.698 I llama_perf_context_print: prompt eval time =      51.87 ms /     7 tokens (    7.41 ms per token,   134.96 tokens per second)
0.01.594.699 I llama_perf_context_print:        eval time =     794.74 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.594.699 I llama_perf_context_print:       total time =     850.36 ms /    70 tokens
0.01.594.945 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.107s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.012 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.017 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.019 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.019 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.019 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.020 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.021 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.021 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.022 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.022 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.022 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.023 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.023 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.026 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.027 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.027 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.726 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.473 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.474 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.474 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.475 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.475 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.476 I llama_model_loader: - type  f32:  194 tensors
0.00.024.476 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.477 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.477 I print_info: file format = GGUF V3 (latest)
0.00.024.477 I print_info: file type   = Q5_1
0.00.024.478 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.301 I load: special tokens cache size = 25
0.00.038.512 I load: token to piece cache size = 0.2984 MB
0.00.038.515 I print_info: arch             = gptneox
0.00.038.515 I print_info: vocab_only       = 0
0.00.038.516 I print_info: n_ctx_train      = 2048
0.00.038.516 I print_info: n_embd           = 2048
0.00.038.516 I print_info: n_layer          = 24
0.00.038.519 I print_info: n_head           = 16
0.00.038.520 I print_info: n_head_kv        = 16
0.00.038.520 I print_info: n_rot            = 32
0.00.038.520 I print_info: n_swa            = 0
0.00.038.520 I print_info: n_embd_head_k    = 128
0.00.038.520 I print_info: n_embd_head_v    = 128
0.00.038.521 I print_info: n_gqa            = 1
0.00.038.522 I print_info: n_embd_k_gqa     = 2048
0.00.038.523 I print_info: n_embd_v_gqa     = 2048
0.00.038.523 I print_info: f_norm_eps       = 1.0e-05
0.00.038.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.524 I print_info: f_logit_scale    = 0.0e+00
0.00.038.525 I print_info: n_ff             = 8192
0.00.038.525 I print_info: n_expert         = 0
0.00.038.525 I print_info: n_expert_used    = 0
0.00.038.525 I print_info: causal attn      = 1
0.00.038.525 I print_info: pooling type     = 0
0.00.038.526 I print_info: rope type        = 2
0.00.038.528 I print_info: rope scaling     = linear
0.00.038.529 I print_info: freq_base_train  = 10000.0
0.00.038.529 I print_info: freq_scale_train = 1
0.00.038.529 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.530 I print_info: rope_finetuned   = unknown
0.00.038.530 I print_info: ssm_d_conv       = 0
0.00.038.530 I print_info: ssm_d_inner      = 0
0.00.038.530 I print_info: ssm_d_state      = 0
0.00.038.530 I print_info: ssm_dt_rank      = 0
0.00.038.530 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.530 I print_info: model type       = 1.4B
0.00.038.531 I print_info: model params     = 1.41 B
0.00.038.531 I print_info: general.name     = 1.4B
0.00.038.532 I print_info: vocab type       = BPE
0.00.038.532 I print_info: n_vocab          = 50304
0.00.038.532 I print_info: n_merges         = 50009
0.00.038.532 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.532 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.533 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.534 I print_info: LF token         = 187 'Ċ'
0.00.038.534 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.538 I print_info: max token length = 1024
0.00.616.361 I load_tensors: offloading 24 repeating layers to GPU
0.00.616.376 I load_tensors: offloading output layer to GPU
0.00.616.376 I load_tensors: offloaded 25/25 layers to GPU
0.00.616.409 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.616.410 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.617.806 I llama_init_from_model: n_seq_max     = 1
0.00.617.811 I llama_init_from_model: n_ctx         = 128
0.00.617.812 I llama_init_from_model: n_ctx_per_seq = 128
0.00.617.813 I llama_init_from_model: n_batch       = 128
0.00.617.813 I llama_init_from_model: n_ubatch      = 128
0.00.617.814 I llama_init_from_model: flash_attn    = 0
0.00.617.816 I llama_init_from_model: freq_base     = 10000.0
0.00.617.817 I llama_init_from_model: freq_scale    = 1
0.00.617.818 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.617.824 I ggml_metal_init: allocating
0.00.617.902 I ggml_metal_init: found device: Apple M4
0.00.617.916 I ggml_metal_init: picking default device: Apple M4
0.00.619.587 I ggml_metal_init: using embedded metal library
0.00.626.009 I ggml_metal_init: GPU name:   Apple M4
0.00.626.013 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.015 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.015 I ggml_metal_init: simdgroup reduction   = true
0.00.626.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.016 I ggml_metal_init: has residency sets    = true
0.00.626.016 I ggml_metal_init: has bfloat            = true
0.00.626.016 I ggml_metal_init: use bfloat            = true
0.00.626.017 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.018 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.703 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.646.199 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.646.205 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.646.262 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.649.607 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.649.608 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.649.609 I llama_init_from_model: graph nodes  = 967
0.00.649.609 I llama_init_from_model: graph splits = 2
0.00.649.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.649.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.219 I 
0.00.681.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.329 I perplexity: tokenizing the input ..
0.00.688.376 I perplexity: tokenization took 7.042 ms
0.00.688.397 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.640 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.837.994 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.838.013 I llama_perf_context_print:        load time =     672.37 ms
0.00.838.014 I llama_perf_context_print: prompt eval time =     147.45 ms /   128 tokens (    1.15 ms per token,   868.10 tokens per second)
0.00.838.015 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.838.018 I llama_perf_context_print:       total time =     156.80 ms /   129 tokens
0.00.838.423 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.078s
sys	0m0.135s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.888 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.704 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.706 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.707 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.707 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.707 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.709 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.709 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.712 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.344 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.344 I llama_model_loader: - type  f32:  194 tensors
0.00.025.345 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.345 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.345 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.346 I print_info: file format = GGUF V3 (latest)
0.00.025.346 I print_info: file type   = Q2_K - Medium
0.00.025.347 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.528 I load: special tokens cache size = 25
0.00.039.220 I load: token to piece cache size = 0.2984 MB
0.00.039.223 I print_info: arch             = gptneox
0.00.039.223 I print_info: vocab_only       = 0
0.00.039.224 I print_info: n_ctx_train      = 2048
0.00.039.224 I print_info: n_embd           = 2048
0.00.039.224 I print_info: n_layer          = 24
0.00.039.227 I print_info: n_head           = 16
0.00.039.227 I print_info: n_head_kv        = 16
0.00.039.228 I print_info: n_rot            = 32
0.00.039.228 I print_info: n_swa            = 0
0.00.039.228 I print_info: n_embd_head_k    = 128
0.00.039.228 I print_info: n_embd_head_v    = 128
0.00.039.229 I print_info: n_gqa            = 1
0.00.039.230 I print_info: n_embd_k_gqa     = 2048
0.00.039.230 I print_info: n_embd_v_gqa     = 2048
0.00.039.233 I print_info: f_norm_eps       = 1.0e-05
0.00.039.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.234 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.234 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.234 I print_info: f_logit_scale    = 0.0e+00
0.00.039.235 I print_info: n_ff             = 8192
0.00.039.235 I print_info: n_expert         = 0
0.00.039.235 I print_info: n_expert_used    = 0
0.00.039.235 I print_info: causal attn      = 1
0.00.039.236 I print_info: pooling type     = 0
0.00.039.237 I print_info: rope type        = 2
0.00.039.238 I print_info: rope scaling     = linear
0.00.039.238 I print_info: freq_base_train  = 10000.0
0.00.039.238 I print_info: freq_scale_train = 1
0.00.039.238 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.238 I print_info: rope_finetuned   = unknown
0.00.039.239 I print_info: ssm_d_conv       = 0
0.00.039.239 I print_info: ssm_d_inner      = 0
0.00.039.239 I print_info: ssm_d_state      = 0
0.00.039.239 I print_info: ssm_dt_rank      = 0
0.00.039.239 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.239 I print_info: model type       = 1.4B
0.00.039.240 I print_info: model params     = 1.41 B
0.00.039.240 I print_info: general.name     = 1.4B
0.00.039.240 I print_info: vocab type       = BPE
0.00.039.240 I print_info: n_vocab          = 50304
0.00.039.241 I print_info: n_merges         = 50009
0.00.039.241 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.241 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.241 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.241 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: LF token         = 187 'Ċ'
0.00.039.246 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.246 I print_info: max token length = 1024
0.00.341.967 I load_tensors: offloading 24 repeating layers to GPU
0.00.341.979 I load_tensors: offloading output layer to GPU
0.00.341.980 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.016 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.018 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.343.516 I llama_init_from_model: n_seq_max     = 1
0.00.343.521 I llama_init_from_model: n_ctx         = 2048
0.00.343.522 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.343.522 I llama_init_from_model: n_batch       = 2048
0.00.343.523 I llama_init_from_model: n_ubatch      = 512
0.00.343.523 I llama_init_from_model: flash_attn    = 0
0.00.343.525 I llama_init_from_model: freq_base     = 10000.0
0.00.343.530 I llama_init_from_model: freq_scale    = 1
0.00.343.540 I ggml_metal_init: allocating
0.00.343.624 I ggml_metal_init: found device: Apple M4
0.00.343.638 I ggml_metal_init: picking default device: Apple M4
0.00.345.492 I ggml_metal_init: using embedded metal library
0.00.350.953 I ggml_metal_init: GPU name:   Apple M4
0.00.350.968 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.350.968 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.350.969 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.350.970 I ggml_metal_init: simdgroup reduction   = true
0.00.350.970 I ggml_metal_init: simdgroup matrix mul. = true
0.00.350.971 I ggml_metal_init: has residency sets    = true
0.00.350.971 I ggml_metal_init: has bfloat            = true
0.00.350.971 I ggml_metal_init: use bfloat            = true
0.00.350.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.350.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.371.926 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.431.659 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.431.666 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.431.700 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.435.867 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.435.869 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.435.869 I llama_init_from_model: graph nodes  = 967
0.00.435.869 I llama_init_from_model: graph splits = 2
0.00.435.875 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.436.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.436.005 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.148 I main: llama threadpool init, n_threads = 4
0.00.496.188 I 
0.00.496.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.213 I 
0.00.496.388 I sampler seed: 1234
0.00.496.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.496.403 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.496.403 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.496.403 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.177.633 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.177.634 I llama_perf_context_print:        load time =     485.59 ms
0.01.177.634 I llama_perf_context_print: prompt eval time =      44.47 ms /     7 tokens (    6.35 ms per token,   157.40 tokens per second)
0.01.177.636 I llama_perf_context_print:        eval time =     633.89 ms /    63 runs   (   10.06 ms per token,    99.39 tokens per second)
0.01.177.637 I llama_perf_context_print:       total time =     682.15 ms /    70 tokens
0.01.177.859 I ggml_metal_free: deallocating

real	0m1.196s
user	0m0.111s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.060 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.466 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.469 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.470 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.473 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.475 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.475 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.476 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.918 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.919 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.920 I llama_model_loader: - type  f32:  194 tensors
0.00.024.921 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.921 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.921 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.922 I print_info: file format = GGUF V3 (latest)
0.00.024.922 I print_info: file type   = Q2_K - Medium
0.00.024.923 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.676 I load: special tokens cache size = 25
0.00.038.659 I load: token to piece cache size = 0.2984 MB
0.00.038.661 I print_info: arch             = gptneox
0.00.038.662 I print_info: vocab_only       = 0
0.00.038.662 I print_info: n_ctx_train      = 2048
0.00.038.662 I print_info: n_embd           = 2048
0.00.038.662 I print_info: n_layer          = 24
0.00.038.665 I print_info: n_head           = 16
0.00.038.665 I print_info: n_head_kv        = 16
0.00.038.665 I print_info: n_rot            = 32
0.00.038.666 I print_info: n_swa            = 0
0.00.038.666 I print_info: n_embd_head_k    = 128
0.00.038.666 I print_info: n_embd_head_v    = 128
0.00.038.667 I print_info: n_gqa            = 1
0.00.038.668 I print_info: n_embd_k_gqa     = 2048
0.00.038.668 I print_info: n_embd_v_gqa     = 2048
0.00.038.669 I print_info: f_norm_eps       = 1.0e-05
0.00.038.669 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.670 I print_info: f_logit_scale    = 0.0e+00
0.00.038.670 I print_info: n_ff             = 8192
0.00.038.670 I print_info: n_expert         = 0
0.00.038.670 I print_info: n_expert_used    = 0
0.00.038.671 I print_info: causal attn      = 1
0.00.038.671 I print_info: pooling type     = 0
0.00.038.671 I print_info: rope type        = 2
0.00.038.671 I print_info: rope scaling     = linear
0.00.038.672 I print_info: freq_base_train  = 10000.0
0.00.038.673 I print_info: freq_scale_train = 1
0.00.038.673 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.673 I print_info: rope_finetuned   = unknown
0.00.038.673 I print_info: ssm_d_conv       = 0
0.00.038.673 I print_info: ssm_d_inner      = 0
0.00.038.673 I print_info: ssm_d_state      = 0
0.00.038.673 I print_info: ssm_dt_rank      = 0
0.00.038.674 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.674 I print_info: model type       = 1.4B
0.00.038.674 I print_info: model params     = 1.41 B
0.00.038.674 I print_info: general.name     = 1.4B
0.00.038.675 I print_info: vocab type       = BPE
0.00.038.675 I print_info: n_vocab          = 50304
0.00.038.677 I print_info: n_merges         = 50009
0.00.038.677 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.677 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.677 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: LF token         = 187 'Ċ'
0.00.038.678 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.678 I print_info: max token length = 1024
0.00.337.286 I load_tensors: offloading 24 repeating layers to GPU
0.00.337.300 I load_tensors: offloading output layer to GPU
0.00.337.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.337.334 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.337.335 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.338.828 I llama_init_from_model: n_seq_max     = 1
0.00.338.834 I llama_init_from_model: n_ctx         = 128
0.00.338.835 I llama_init_from_model: n_ctx_per_seq = 128
0.00.338.835 I llama_init_from_model: n_batch       = 128
0.00.338.836 I llama_init_from_model: n_ubatch      = 128
0.00.338.837 I llama_init_from_model: flash_attn    = 0
0.00.338.839 I llama_init_from_model: freq_base     = 10000.0
0.00.338.840 I llama_init_from_model: freq_scale    = 1
0.00.338.840 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.338.842 I ggml_metal_init: allocating
0.00.338.886 I ggml_metal_init: found device: Apple M4
0.00.338.898 I ggml_metal_init: picking default device: Apple M4
0.00.340.592 I ggml_metal_init: using embedded metal library
0.00.346.306 I ggml_metal_init: GPU name:   Apple M4
0.00.346.316 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.317 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.318 I ggml_metal_init: simdgroup reduction   = true
0.00.346.318 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.319 I ggml_metal_init: has residency sets    = true
0.00.346.319 I ggml_metal_init: has bfloat            = true
0.00.346.319 I ggml_metal_init: use bfloat            = true
0.00.346.322 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.367.945 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.371.630 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.371.640 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.371.698 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.375.212 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.375.214 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.375.214 I llama_init_from_model: graph nodes  = 967
0.00.375.215 I llama_init_from_model: graph splits = 2
0.00.375.218 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.375.218 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.408.309 I 
0.00.408.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.408.416 I perplexity: tokenizing the input ..
0.00.415.764 I perplexity: tokenization took 7.343 ms
0.00.415.793 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.557.287 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.558.729 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.558.743 I llama_perf_context_print:        load time =     398.24 ms
0.00.558.744 I llama_perf_context_print: prompt eval time =     140.68 ms /   128 tokens (    1.10 ms per token,   909.89 tokens per second)
0.00.558.744 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.558.744 I llama_perf_context_print:       total time =     150.44 ms /   129 tokens
0.00.559.122 I ggml_metal_free: deallocating

real	0m0.575s
user	0m0.080s
sys	0m0.095s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.319 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.841 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.845 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.847 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.848 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.848 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.848 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.849 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.849 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.850 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.850 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.851 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.851 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.853 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.853 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.855 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.859 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.860 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.663 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.694 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.444 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.445 I llama_model_loader: - type  f32:  194 tensors
0.00.025.445 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.445 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.445 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.446 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.446 I print_info: file format = GGUF V3 (latest)
0.00.025.447 I print_info: file type   = Q3_K - Medium
0.00.025.447 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.718 I load: special tokens cache size = 25
0.00.039.681 I load: token to piece cache size = 0.2984 MB
0.00.039.684 I print_info: arch             = gptneox
0.00.039.684 I print_info: vocab_only       = 0
0.00.039.685 I print_info: n_ctx_train      = 2048
0.00.039.685 I print_info: n_embd           = 2048
0.00.039.685 I print_info: n_layer          = 24
0.00.039.688 I print_info: n_head           = 16
0.00.039.689 I print_info: n_head_kv        = 16
0.00.039.689 I print_info: n_rot            = 32
0.00.039.689 I print_info: n_swa            = 0
0.00.039.690 I print_info: n_embd_head_k    = 128
0.00.039.690 I print_info: n_embd_head_v    = 128
0.00.039.691 I print_info: n_gqa            = 1
0.00.039.691 I print_info: n_embd_k_gqa     = 2048
0.00.039.694 I print_info: n_embd_v_gqa     = 2048
0.00.039.695 I print_info: f_norm_eps       = 1.0e-05
0.00.039.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.696 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.697 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.698 I print_info: f_logit_scale    = 0.0e+00
0.00.039.698 I print_info: n_ff             = 8192
0.00.039.699 I print_info: n_expert         = 0
0.00.039.699 I print_info: n_expert_used    = 0
0.00.039.700 I print_info: causal attn      = 1
0.00.039.702 I print_info: pooling type     = 0
0.00.039.702 I print_info: rope type        = 2
0.00.039.702 I print_info: rope scaling     = linear
0.00.039.703 I print_info: freq_base_train  = 10000.0
0.00.039.703 I print_info: freq_scale_train = 1
0.00.039.703 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.710 I print_info: rope_finetuned   = unknown
0.00.039.712 I print_info: ssm_d_conv       = 0
0.00.039.712 I print_info: ssm_d_inner      = 0
0.00.039.712 I print_info: ssm_d_state      = 0
0.00.039.713 I print_info: ssm_dt_rank      = 0
0.00.039.714 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.714 I print_info: model type       = 1.4B
0.00.039.714 I print_info: model params     = 1.41 B
0.00.039.715 I print_info: general.name     = 1.4B
0.00.039.715 I print_info: vocab type       = BPE
0.00.039.715 I print_info: n_vocab          = 50304
0.00.039.716 I print_info: n_merges         = 50009
0.00.039.716 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.716 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.718 I print_info: LF token         = 187 'Ċ'
0.00.039.718 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.718 I print_info: max token length = 1024
0.00.447.890 I load_tensors: offloading 24 repeating layers to GPU
0.00.447.904 I load_tensors: offloading output layer to GPU
0.00.447.904 I load_tensors: offloaded 25/25 layers to GPU
0.00.447.938 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.447.939 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.449.274 I llama_init_from_model: n_seq_max     = 1
0.00.449.279 I llama_init_from_model: n_ctx         = 2048
0.00.449.279 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.449.280 I llama_init_from_model: n_batch       = 2048
0.00.449.280 I llama_init_from_model: n_ubatch      = 512
0.00.449.281 I llama_init_from_model: flash_attn    = 0
0.00.449.287 I llama_init_from_model: freq_base     = 10000.0
0.00.449.290 I llama_init_from_model: freq_scale    = 1
0.00.449.293 I ggml_metal_init: allocating
0.00.449.371 I ggml_metal_init: found device: Apple M4
0.00.449.386 I ggml_metal_init: picking default device: Apple M4
0.00.451.272 I ggml_metal_init: using embedded metal library
0.00.456.751 I ggml_metal_init: GPU name:   Apple M4
0.00.456.756 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.757 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.759 I ggml_metal_init: simdgroup reduction   = true
0.00.456.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.759 I ggml_metal_init: has residency sets    = true
0.00.456.760 I ggml_metal_init: has bfloat            = true
0.00.456.760 I ggml_metal_init: use bfloat            = true
0.00.456.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.763 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.476.586 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.883 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.535.891 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.535.930 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.540.225 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.540.228 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.540.228 I llama_init_from_model: graph nodes  = 967
0.00.540.228 I llama_init_from_model: graph splits = 2
0.00.540.235 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.540.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.540.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.598.288 I main: llama threadpool init, n_threads = 4
0.00.598.333 I 
0.00.598.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.362 I 
0.00.598.512 I sampler seed: 1234
0.00.598.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.598.537 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.598.537 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.598.537 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.302 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48596.85 tokens per second)
0.01.342.302 I llama_perf_context_print:        load time =     588.32 ms
0.01.342.303 I llama_perf_context_print: prompt eval time =      49.73 ms /     7 tokens (    7.10 ms per token,   140.77 tokens per second)
0.01.342.304 I llama_perf_context_print:        eval time =     691.36 ms /    63 runs   (   10.97 ms per token,    91.13 tokens per second)
0.01.342.304 I llama_perf_context_print:       total time =     744.66 ms /    70 tokens
0.01.342.564 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.111s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.784 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.903 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.910 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.913 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.916 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.917 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.919 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.919 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.919 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.747 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.822 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.823 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.823 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.824 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.824 I llama_model_loader: - type  f32:  194 tensors
0.00.024.824 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.825 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.825 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.825 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.825 I print_info: file format = GGUF V3 (latest)
0.00.024.826 I print_info: file type   = Q3_K - Medium
0.00.024.826 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.922 I load: special tokens cache size = 25
0.00.038.897 I load: token to piece cache size = 0.2984 MB
0.00.038.900 I print_info: arch             = gptneox
0.00.038.900 I print_info: vocab_only       = 0
0.00.038.901 I print_info: n_ctx_train      = 2048
0.00.038.901 I print_info: n_embd           = 2048
0.00.038.901 I print_info: n_layer          = 24
0.00.038.904 I print_info: n_head           = 16
0.00.038.905 I print_info: n_head_kv        = 16
0.00.038.905 I print_info: n_rot            = 32
0.00.038.906 I print_info: n_swa            = 0
0.00.038.906 I print_info: n_embd_head_k    = 128
0.00.038.906 I print_info: n_embd_head_v    = 128
0.00.038.907 I print_info: n_gqa            = 1
0.00.038.907 I print_info: n_embd_k_gqa     = 2048
0.00.038.908 I print_info: n_embd_v_gqa     = 2048
0.00.038.909 I print_info: f_norm_eps       = 1.0e-05
0.00.038.909 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.909 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.910 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.910 I print_info: f_logit_scale    = 0.0e+00
0.00.038.910 I print_info: n_ff             = 8192
0.00.038.911 I print_info: n_expert         = 0
0.00.038.911 I print_info: n_expert_used    = 0
0.00.038.911 I print_info: causal attn      = 1
0.00.038.911 I print_info: pooling type     = 0
0.00.038.911 I print_info: rope type        = 2
0.00.038.913 I print_info: rope scaling     = linear
0.00.038.913 I print_info: freq_base_train  = 10000.0
0.00.038.914 I print_info: freq_scale_train = 1
0.00.038.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.914 I print_info: rope_finetuned   = unknown
0.00.038.914 I print_info: ssm_d_conv       = 0
0.00.038.914 I print_info: ssm_d_inner      = 0
0.00.038.914 I print_info: ssm_d_state      = 0
0.00.038.914 I print_info: ssm_dt_rank      = 0
0.00.038.916 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.917 I print_info: model type       = 1.4B
0.00.038.917 I print_info: model params     = 1.41 B
0.00.038.917 I print_info: general.name     = 1.4B
0.00.038.918 I print_info: vocab type       = BPE
0.00.038.918 I print_info: n_vocab          = 50304
0.00.038.918 I print_info: n_merges         = 50009
0.00.038.918 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.918 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.919 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.919 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.919 I print_info: LF token         = 187 'Ċ'
0.00.038.920 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.920 I print_info: max token length = 1024
0.00.444.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.966 I load_tensors: offloading output layer to GPU
0.00.444.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.999 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.445.001 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.446.380 I llama_init_from_model: n_seq_max     = 1
0.00.446.385 I llama_init_from_model: n_ctx         = 128
0.00.446.386 I llama_init_from_model: n_ctx_per_seq = 128
0.00.446.386 I llama_init_from_model: n_batch       = 128
0.00.446.386 I llama_init_from_model: n_ubatch      = 128
0.00.446.387 I llama_init_from_model: flash_attn    = 0
0.00.446.389 I llama_init_from_model: freq_base     = 10000.0
0.00.446.390 I llama_init_from_model: freq_scale    = 1
0.00.446.390 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.446.396 I ggml_metal_init: allocating
0.00.446.465 I ggml_metal_init: found device: Apple M4
0.00.446.478 I ggml_metal_init: picking default device: Apple M4
0.00.448.224 I ggml_metal_init: using embedded metal library
0.00.453.716 I ggml_metal_init: GPU name:   Apple M4
0.00.453.753 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.766 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.766 I ggml_metal_init: simdgroup reduction   = true
0.00.453.767 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.768 I ggml_metal_init: has residency sets    = true
0.00.453.768 I ggml_metal_init: has bfloat            = true
0.00.453.769 I ggml_metal_init: use bfloat            = true
0.00.453.772 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.153 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.838 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.476.842 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.887 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.480.228 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.480.230 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.480.231 I llama_init_from_model: graph nodes  = 967
0.00.480.231 I llama_init_from_model: graph splits = 2
0.00.480.234 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.480.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.506.689 I 
0.00.506.771 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.506.789 I perplexity: tokenizing the input ..
0.00.513.908 I perplexity: tokenization took 7.115 ms
0.00.513.926 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.647.292 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.648.640 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.648.665 I llama_perf_context_print:        load time =     497.90 ms
0.00.648.667 I llama_perf_context_print: prompt eval time =     132.41 ms /   128 tokens (    1.03 ms per token,   966.66 tokens per second)
0.00.648.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.648.668 I llama_perf_context_print:       total time =     141.98 ms /   129 tokens
0.00.649.059 I ggml_metal_free: deallocating

real	0m0.663s
user	0m0.080s
sys	0m0.106s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.682 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.179 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.192 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.194 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.194 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.196 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.197 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.737 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.738 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.739 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.739 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.739 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.740 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.744 I llama_model_loader: - type  f32:  194 tensors
0.00.025.745 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.745 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.745 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.746 I print_info: file format = GGUF V3 (latest)
0.00.025.747 I print_info: file type   = Q4_K - Medium
0.00.025.748 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.574 I load: special tokens cache size = 25
0.00.039.509 I load: token to piece cache size = 0.2984 MB
0.00.039.512 I print_info: arch             = gptneox
0.00.039.512 I print_info: vocab_only       = 0
0.00.039.512 I print_info: n_ctx_train      = 2048
0.00.039.513 I print_info: n_embd           = 2048
0.00.039.513 I print_info: n_layer          = 24
0.00.039.516 I print_info: n_head           = 16
0.00.039.516 I print_info: n_head_kv        = 16
0.00.039.517 I print_info: n_rot            = 32
0.00.039.518 I print_info: n_swa            = 0
0.00.039.518 I print_info: n_embd_head_k    = 128
0.00.039.519 I print_info: n_embd_head_v    = 128
0.00.039.519 I print_info: n_gqa            = 1
0.00.039.520 I print_info: n_embd_k_gqa     = 2048
0.00.039.525 I print_info: n_embd_v_gqa     = 2048
0.00.039.525 I print_info: f_norm_eps       = 1.0e-05
0.00.039.526 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.526 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.526 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.527 I print_info: f_logit_scale    = 0.0e+00
0.00.039.527 I print_info: n_ff             = 8192
0.00.039.528 I print_info: n_expert         = 0
0.00.039.528 I print_info: n_expert_used    = 0
0.00.039.528 I print_info: causal attn      = 1
0.00.039.528 I print_info: pooling type     = 0
0.00.039.530 I print_info: rope type        = 2
0.00.039.530 I print_info: rope scaling     = linear
0.00.039.531 I print_info: freq_base_train  = 10000.0
0.00.039.531 I print_info: freq_scale_train = 1
0.00.039.531 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.532 I print_info: rope_finetuned   = unknown
0.00.039.532 I print_info: ssm_d_conv       = 0
0.00.039.532 I print_info: ssm_d_inner      = 0
0.00.039.532 I print_info: ssm_d_state      = 0
0.00.039.532 I print_info: ssm_dt_rank      = 0
0.00.039.532 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.533 I print_info: model type       = 1.4B
0.00.039.533 I print_info: model params     = 1.41 B
0.00.039.534 I print_info: general.name     = 1.4B
0.00.039.534 I print_info: vocab type       = BPE
0.00.039.534 I print_info: n_vocab          = 50304
0.00.039.534 I print_info: n_merges         = 50009
0.00.039.535 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.536 I print_info: LF token         = 187 'Ċ'
0.00.039.536 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.537 I print_info: max token length = 1024
0.00.540.078 I load_tensors: offloading 24 repeating layers to GPU
0.00.540.096 I load_tensors: offloading output layer to GPU
0.00.540.096 I load_tensors: offloaded 25/25 layers to GPU
0.00.540.132 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.540.134 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.541.470 I llama_init_from_model: n_seq_max     = 1
0.00.541.474 I llama_init_from_model: n_ctx         = 2048
0.00.541.474 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.541.475 I llama_init_from_model: n_batch       = 2048
0.00.541.475 I llama_init_from_model: n_ubatch      = 512
0.00.541.475 I llama_init_from_model: flash_attn    = 0
0.00.541.478 I llama_init_from_model: freq_base     = 10000.0
0.00.541.478 I llama_init_from_model: freq_scale    = 1
0.00.541.481 I ggml_metal_init: allocating
0.00.541.563 I ggml_metal_init: found device: Apple M4
0.00.541.578 I ggml_metal_init: picking default device: Apple M4
0.00.543.436 I ggml_metal_init: using embedded metal library
0.00.550.072 I ggml_metal_init: GPU name:   Apple M4
0.00.550.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.550.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.550.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.550.080 I ggml_metal_init: simdgroup reduction   = true
0.00.550.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.550.081 I ggml_metal_init: has residency sets    = true
0.00.550.081 I ggml_metal_init: has bfloat            = true
0.00.550.081 I ggml_metal_init: use bfloat            = true
0.00.550.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.550.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.568.104 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.699 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.624.705 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.624.741 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.628.783 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.628.784 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.628.785 I llama_init_from_model: graph nodes  = 967
0.00.628.785 I llama_init_from_model: graph splits = 2
0.00.628.792 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.628.905 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.628.905 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.033 I main: llama threadpool init, n_threads = 4
0.00.687.079 I 
0.00.687.105 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.107 I 
0.00.687.260 I sampler seed: 1234
0.00.687.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.285 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.285 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.285 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.448.078 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.448.079 I llama_perf_context_print:        load time =     676.69 ms
0.01.448.079 I llama_perf_context_print: prompt eval time =      57.09 ms /     7 tokens (    8.16 ms per token,   122.61 tokens per second)
0.01.448.080 I llama_perf_context_print:        eval time =     700.91 ms /    63 runs   (   11.13 ms per token,    89.88 tokens per second)
0.01.448.081 I llama_perf_context_print:       total time =     761.70 ms /    70 tokens
0.01.448.348 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.108s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.892 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.153 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.167 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.169 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.169 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.170 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.069 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.063 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.881 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.882 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.882 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.883 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.883 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.884 I llama_model_loader: - type  f32:  194 tensors
0.00.024.884 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.884 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.885 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.886 I print_info: file format = GGUF V3 (latest)
0.00.024.886 I print_info: file type   = Q4_K - Medium
0.00.024.887 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.983 I load: special tokens cache size = 25
0.00.038.940 I load: token to piece cache size = 0.2984 MB
0.00.038.942 I print_info: arch             = gptneox
0.00.038.943 I print_info: vocab_only       = 0
0.00.038.943 I print_info: n_ctx_train      = 2048
0.00.038.943 I print_info: n_embd           = 2048
0.00.038.943 I print_info: n_layer          = 24
0.00.038.947 I print_info: n_head           = 16
0.00.038.947 I print_info: n_head_kv        = 16
0.00.038.950 I print_info: n_rot            = 32
0.00.038.950 I print_info: n_swa            = 0
0.00.038.950 I print_info: n_embd_head_k    = 128
0.00.038.950 I print_info: n_embd_head_v    = 128
0.00.038.951 I print_info: n_gqa            = 1
0.00.038.952 I print_info: n_embd_k_gqa     = 2048
0.00.038.952 I print_info: n_embd_v_gqa     = 2048
0.00.038.953 I print_info: f_norm_eps       = 1.0e-05
0.00.038.953 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.953 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.954 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.954 I print_info: f_logit_scale    = 0.0e+00
0.00.038.954 I print_info: n_ff             = 8192
0.00.038.955 I print_info: n_expert         = 0
0.00.038.955 I print_info: n_expert_used    = 0
0.00.038.955 I print_info: causal attn      = 1
0.00.038.959 I print_info: pooling type     = 0
0.00.038.959 I print_info: rope type        = 2
0.00.038.959 I print_info: rope scaling     = linear
0.00.038.960 I print_info: freq_base_train  = 10000.0
0.00.038.960 I print_info: freq_scale_train = 1
0.00.038.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.960 I print_info: rope_finetuned   = unknown
0.00.038.961 I print_info: ssm_d_conv       = 0
0.00.038.961 I print_info: ssm_d_inner      = 0
0.00.038.961 I print_info: ssm_d_state      = 0
0.00.038.961 I print_info: ssm_dt_rank      = 0
0.00.038.963 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.963 I print_info: model type       = 1.4B
0.00.038.963 I print_info: model params     = 1.41 B
0.00.038.964 I print_info: general.name     = 1.4B
0.00.038.964 I print_info: vocab type       = BPE
0.00.038.964 I print_info: n_vocab          = 50304
0.00.038.965 I print_info: n_merges         = 50009
0.00.038.965 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.966 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.966 I print_info: LF token         = 187 'Ċ'
0.00.038.966 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.966 I print_info: max token length = 1024
0.00.515.133 I load_tensors: offloading 24 repeating layers to GPU
0.00.515.150 I load_tensors: offloading output layer to GPU
0.00.515.151 I load_tensors: offloaded 25/25 layers to GPU
0.00.515.182 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.515.184 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.516.745 I llama_init_from_model: n_seq_max     = 1
0.00.516.750 I llama_init_from_model: n_ctx         = 128
0.00.516.751 I llama_init_from_model: n_ctx_per_seq = 128
0.00.516.751 I llama_init_from_model: n_batch       = 128
0.00.516.752 I llama_init_from_model: n_ubatch      = 128
0.00.516.753 I llama_init_from_model: flash_attn    = 0
0.00.516.754 I llama_init_from_model: freq_base     = 10000.0
0.00.516.755 I llama_init_from_model: freq_scale    = 1
0.00.516.756 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.516.757 I ggml_metal_init: allocating
0.00.516.825 I ggml_metal_init: found device: Apple M4
0.00.516.840 I ggml_metal_init: picking default device: Apple M4
0.00.518.620 I ggml_metal_init: using embedded metal library
0.00.525.197 I ggml_metal_init: GPU name:   Apple M4
0.00.525.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.525.201 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.525.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.525.202 I ggml_metal_init: simdgroup reduction   = true
0.00.525.202 I ggml_metal_init: simdgroup matrix mul. = true
0.00.525.203 I ggml_metal_init: has residency sets    = true
0.00.525.203 I ggml_metal_init: has bfloat            = true
0.00.525.203 I ggml_metal_init: use bfloat            = true
0.00.525.204 I ggml_metal_init: hasUnifiedMemory      = true
0.00.525.206 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.542.467 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.545.892 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.545.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.545.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.290 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.549.292 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.549.292 I llama_init_from_model: graph nodes  = 967
0.00.549.293 I llama_init_from_model: graph splits = 2
0.00.549.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.549.296 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.576.173 I 
0.00.576.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.576.267 I perplexity: tokenizing the input ..
0.00.581.680 I perplexity: tokenization took 5.411 ms
0.00.581.692 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.714.935 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.716.276 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.716.296 I llama_perf_context_print:        load time =     567.27 ms
0.00.716.297 I llama_perf_context_print: prompt eval time =     133.01 ms /   128 tokens (    1.04 ms per token,   962.33 tokens per second)
0.00.716.298 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.716.299 I llama_perf_context_print:       total time =     140.13 ms /   129 tokens
0.00.716.670 I ggml_metal_free: deallocating

real	0m0.731s
user	0m0.077s
sys	0m0.117s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.843 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.622 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.627 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.633 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.634 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.634 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.634 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.635 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.638 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.638 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.514 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.514 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.280 I llama_model_loader: - type  f32:  194 tensors
0.00.025.281 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.281 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.281 I print_info: file format = GGUF V3 (latest)
0.00.025.282 I print_info: file type   = Q5_K - Medium
0.00.025.283 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.422 I load: special tokens cache size = 25
0.00.039.588 I load: token to piece cache size = 0.2984 MB
0.00.039.591 I print_info: arch             = gptneox
0.00.039.592 I print_info: vocab_only       = 0
0.00.039.592 I print_info: n_ctx_train      = 2048
0.00.039.592 I print_info: n_embd           = 2048
0.00.039.592 I print_info: n_layer          = 24
0.00.039.595 I print_info: n_head           = 16
0.00.039.596 I print_info: n_head_kv        = 16
0.00.039.596 I print_info: n_rot            = 32
0.00.039.598 I print_info: n_swa            = 0
0.00.039.598 I print_info: n_embd_head_k    = 128
0.00.039.598 I print_info: n_embd_head_v    = 128
0.00.039.599 I print_info: n_gqa            = 1
0.00.039.599 I print_info: n_embd_k_gqa     = 2048
0.00.039.605 I print_info: n_embd_v_gqa     = 2048
0.00.039.606 I print_info: f_norm_eps       = 1.0e-05
0.00.039.606 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.606 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.606 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.606 I print_info: f_logit_scale    = 0.0e+00
0.00.039.607 I print_info: n_ff             = 8192
0.00.039.608 I print_info: n_expert         = 0
0.00.039.608 I print_info: n_expert_used    = 0
0.00.039.608 I print_info: causal attn      = 1
0.00.039.608 I print_info: pooling type     = 0
0.00.039.608 I print_info: rope type        = 2
0.00.039.608 I print_info: rope scaling     = linear
0.00.039.610 I print_info: freq_base_train  = 10000.0
0.00.039.611 I print_info: freq_scale_train = 1
0.00.039.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.611 I print_info: rope_finetuned   = unknown
0.00.039.611 I print_info: ssm_d_conv       = 0
0.00.039.611 I print_info: ssm_d_inner      = 0
0.00.039.611 I print_info: ssm_d_state      = 0
0.00.039.611 I print_info: ssm_dt_rank      = 0
0.00.039.612 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.612 I print_info: model type       = 1.4B
0.00.039.612 I print_info: model params     = 1.41 B
0.00.039.612 I print_info: general.name     = 1.4B
0.00.039.613 I print_info: vocab type       = BPE
0.00.039.613 I print_info: n_vocab          = 50304
0.00.039.613 I print_info: n_merges         = 50009
0.00.039.614 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.614 I print_info: LF token         = 187 'Ċ'
0.00.039.615 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.615 I print_info: max token length = 1024
0.00.599.847 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.860 I load_tensors: offloading output layer to GPU
0.00.599.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.890 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.599.891 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.601.198 I llama_init_from_model: n_seq_max     = 1
0.00.601.208 I llama_init_from_model: n_ctx         = 2048
0.00.601.208 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.209 I llama_init_from_model: n_batch       = 2048
0.00.601.209 I llama_init_from_model: n_ubatch      = 512
0.00.601.210 I llama_init_from_model: flash_attn    = 0
0.00.601.211 I llama_init_from_model: freq_base     = 10000.0
0.00.601.211 I llama_init_from_model: freq_scale    = 1
0.00.601.213 I ggml_metal_init: allocating
0.00.601.265 I ggml_metal_init: found device: Apple M4
0.00.601.278 I ggml_metal_init: picking default device: Apple M4
0.00.603.054 I ggml_metal_init: using embedded metal library
0.00.609.852 I ggml_metal_init: GPU name:   Apple M4
0.00.609.856 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.857 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.858 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.859 I ggml_metal_init: simdgroup reduction   = true
0.00.609.859 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.859 I ggml_metal_init: has residency sets    = true
0.00.609.859 I ggml_metal_init: has bfloat            = true
0.00.609.860 I ggml_metal_init: use bfloat            = true
0.00.609.861 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.268 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.492 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.683.497 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.531 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.688.007 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.688.009 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.688.009 I llama_init_from_model: graph nodes  = 967
0.00.688.009 I llama_init_from_model: graph splits = 2
0.00.688.014 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.402 I main: llama threadpool init, n_threads = 4
0.00.750.441 I 
0.00.750.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.465 I 
0.00.750.618 I sampler seed: 1234
0.00.750.622 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.658 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.661 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.661 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.592.144 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50141.24 tokens per second)
0.01.592.145 I llama_perf_context_print:        load time =     740.87 ms
0.01.592.145 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.80 tokens per second)
0.01.592.146 I llama_perf_context_print:        eval time =     787.50 ms /    63 runs   (   12.50 ms per token,    80.00 tokens per second)
0.01.592.146 I llama_perf_context_print:       total time =     842.43 ms /    70 tokens
0.01.592.387 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.041 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.145 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.151 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.153 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.154 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.154 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.155 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.155 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.156 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.158 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.158 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.158 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.033 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.811 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.812 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.813 I llama_model_loader: - type  f32:  194 tensors
0.00.025.814 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.814 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.815 I print_info: file format = GGUF V3 (latest)
0.00.025.815 I print_info: file type   = Q5_K - Medium
0.00.025.817 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.947 I load: special tokens cache size = 25
0.00.039.699 I load: token to piece cache size = 0.2984 MB
0.00.039.702 I print_info: arch             = gptneox
0.00.039.702 I print_info: vocab_only       = 0
0.00.039.702 I print_info: n_ctx_train      = 2048
0.00.039.702 I print_info: n_embd           = 2048
0.00.039.702 I print_info: n_layer          = 24
0.00.039.706 I print_info: n_head           = 16
0.00.039.706 I print_info: n_head_kv        = 16
0.00.039.706 I print_info: n_rot            = 32
0.00.039.707 I print_info: n_swa            = 0
0.00.039.707 I print_info: n_embd_head_k    = 128
0.00.039.707 I print_info: n_embd_head_v    = 128
0.00.039.708 I print_info: n_gqa            = 1
0.00.039.708 I print_info: n_embd_k_gqa     = 2048
0.00.039.709 I print_info: n_embd_v_gqa     = 2048
0.00.039.710 I print_info: f_norm_eps       = 1.0e-05
0.00.039.710 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.711 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.711 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.711 I print_info: f_logit_scale    = 0.0e+00
0.00.039.712 I print_info: n_ff             = 8192
0.00.039.712 I print_info: n_expert         = 0
0.00.039.712 I print_info: n_expert_used    = 0
0.00.039.712 I print_info: causal attn      = 1
0.00.039.712 I print_info: pooling type     = 0
0.00.039.712 I print_info: rope type        = 2
0.00.039.713 I print_info: rope scaling     = linear
0.00.039.720 I print_info: freq_base_train  = 10000.0
0.00.039.723 I print_info: freq_scale_train = 1
0.00.039.723 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.724 I print_info: rope_finetuned   = unknown
0.00.039.724 I print_info: ssm_d_conv       = 0
0.00.039.724 I print_info: ssm_d_inner      = 0
0.00.039.724 I print_info: ssm_d_state      = 0
0.00.039.724 I print_info: ssm_dt_rank      = 0
0.00.039.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.724 I print_info: model type       = 1.4B
0.00.039.726 I print_info: model params     = 1.41 B
0.00.039.726 I print_info: general.name     = 1.4B
0.00.039.726 I print_info: vocab type       = BPE
0.00.039.727 I print_info: n_vocab          = 50304
0.00.039.727 I print_info: n_merges         = 50009
0.00.039.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.728 I print_info: LF token         = 187 'Ċ'
0.00.039.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.729 I print_info: max token length = 1024
0.00.604.854 I load_tensors: offloading 24 repeating layers to GPU
0.00.604.865 I load_tensors: offloading output layer to GPU
0.00.604.866 I load_tensors: offloaded 25/25 layers to GPU
0.00.604.895 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.604.897 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.606.389 I llama_init_from_model: n_seq_max     = 1
0.00.606.396 I llama_init_from_model: n_ctx         = 128
0.00.606.397 I llama_init_from_model: n_ctx_per_seq = 128
0.00.606.397 I llama_init_from_model: n_batch       = 128
0.00.606.398 I llama_init_from_model: n_ubatch      = 128
0.00.606.399 I llama_init_from_model: flash_attn    = 0
0.00.606.399 I llama_init_from_model: freq_base     = 10000.0
0.00.606.400 I llama_init_from_model: freq_scale    = 1
0.00.606.401 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.606.403 I ggml_metal_init: allocating
0.00.606.457 I ggml_metal_init: found device: Apple M4
0.00.606.471 I ggml_metal_init: picking default device: Apple M4
0.00.608.147 I ggml_metal_init: using embedded metal library
0.00.614.955 I ggml_metal_init: GPU name:   Apple M4
0.00.614.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.614.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.614.960 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.614.961 I ggml_metal_init: simdgroup reduction   = true
0.00.614.961 I ggml_metal_init: simdgroup matrix mul. = true
0.00.614.961 I ggml_metal_init: has residency sets    = true
0.00.614.962 I ggml_metal_init: has bfloat            = true
0.00.614.962 I ggml_metal_init: use bfloat            = true
0.00.614.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.614.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.632.589 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.636.018 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.636.024 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.636.069 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.639.324 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.639.326 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.639.326 I llama_init_from_model: graph nodes  = 967
0.00.639.327 I llama_init_from_model: graph splits = 2
0.00.639.330 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.639.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.585 I 
0.00.675.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.669 I perplexity: tokenizing the input ..
0.00.682.077 I perplexity: tokenization took 6.407 ms
0.00.682.092 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.985 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.824.393 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.824.406 I llama_perf_context_print:        load time =     665.53 ms
0.00.824.407 I llama_perf_context_print: prompt eval time =     140.47 ms /   128 tokens (    1.10 ms per token,   911.21 tokens per second)
0.00.824.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.409 I llama_perf_context_print:       total time =     148.83 ms /   129 tokens
0.00.824.769 I ggml_metal_free: deallocating

real	0m0.841s
user	0m0.077s
sys	0m0.143s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.014.710 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.995 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.033.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.008 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.009 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.013 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.014 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.014 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.015 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.019 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.019 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.019 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.896 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.948 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.041.892 I llama_model_loader: - type  f32:  194 tensors
0.00.041.892 I llama_model_loader: - type q6_K:   98 tensors
0.00.041.893 I print_info: file format = GGUF V3 (latest)
0.00.041.893 I print_info: file type   = Q6_K
0.00.041.895 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.050.307 I load: special tokens cache size = 25
0.00.056.378 I load: token to piece cache size = 0.2984 MB
0.00.056.382 I print_info: arch             = gptneox
0.00.056.382 I print_info: vocab_only       = 0
0.00.056.383 I print_info: n_ctx_train      = 2048
0.00.056.383 I print_info: n_embd           = 2048
0.00.056.383 I print_info: n_layer          = 24
0.00.056.387 I print_info: n_head           = 16
0.00.056.388 I print_info: n_head_kv        = 16
0.00.056.388 I print_info: n_rot            = 32
0.00.056.388 I print_info: n_swa            = 0
0.00.056.388 I print_info: n_embd_head_k    = 128
0.00.056.389 I print_info: n_embd_head_v    = 128
0.00.056.389 I print_info: n_gqa            = 1
0.00.056.390 I print_info: n_embd_k_gqa     = 2048
0.00.056.391 I print_info: n_embd_v_gqa     = 2048
0.00.056.391 I print_info: f_norm_eps       = 1.0e-05
0.00.056.392 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.392 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.392 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.394 I print_info: f_logit_scale    = 0.0e+00
0.00.056.395 I print_info: n_ff             = 8192
0.00.056.395 I print_info: n_expert         = 0
0.00.056.395 I print_info: n_expert_used    = 0
0.00.056.395 I print_info: causal attn      = 1
0.00.056.395 I print_info: pooling type     = 0
0.00.056.396 I print_info: rope type        = 2
0.00.056.397 I print_info: rope scaling     = linear
0.00.056.397 I print_info: freq_base_train  = 10000.0
0.00.056.397 I print_info: freq_scale_train = 1
0.00.056.398 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.398 I print_info: rope_finetuned   = unknown
0.00.056.398 I print_info: ssm_d_conv       = 0
0.00.056.398 I print_info: ssm_d_inner      = 0
0.00.056.398 I print_info: ssm_d_state      = 0
0.00.056.398 I print_info: ssm_dt_rank      = 0
0.00.056.398 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.399 I print_info: model type       = 1.4B
0.00.056.399 I print_info: model params     = 1.41 B
0.00.056.399 I print_info: general.name     = 1.4B
0.00.056.400 I print_info: vocab type       = BPE
0.00.056.400 I print_info: n_vocab          = 50304
0.00.056.400 I print_info: n_merges         = 50009
0.00.056.400 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.400 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.400 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.401 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.401 I print_info: LF token         = 187 'Ċ'
0.00.056.401 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.401 I print_info: max token length = 1024
0.01.035.709 I load_tensors: offloading 24 repeating layers to GPU
0.01.035.716 I load_tensors: offloading output layer to GPU
0.01.035.717 I load_tensors: offloaded 25/25 layers to GPU
0.01.035.746 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.01.035.750 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.01.036.577 I llama_init_from_model: n_seq_max     = 1
0.01.036.580 I llama_init_from_model: n_ctx         = 2048
0.01.036.580 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.036.580 I llama_init_from_model: n_batch       = 2048
0.01.036.581 I llama_init_from_model: n_ubatch      = 512
0.01.036.581 I llama_init_from_model: flash_attn    = 0
0.01.036.583 I llama_init_from_model: freq_base     = 10000.0
0.01.036.583 I llama_init_from_model: freq_scale    = 1
0.01.036.585 I ggml_metal_init: allocating
0.01.036.638 I ggml_metal_init: found device: Apple M4
0.01.036.653 I ggml_metal_init: picking default device: Apple M4
0.01.038.194 I ggml_metal_init: using embedded metal library
0.01.044.233 I ggml_metal_init: GPU name:   Apple M4
0.01.044.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.044.237 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.044.238 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.044.239 I ggml_metal_init: simdgroup reduction   = true
0.01.044.239 I ggml_metal_init: simdgroup matrix mul. = true
0.01.044.239 I ggml_metal_init: has residency sets    = true
0.01.044.239 I ggml_metal_init: has bfloat            = true
0.01.044.240 I ggml_metal_init: use bfloat            = true
0.01.044.240 I ggml_metal_init: hasUnifiedMemory      = true
0.01.044.242 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.060.624 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.113.459 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.113.465 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.113.500 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.117.719 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.117.721 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.117.721 I llama_init_from_model: graph nodes  = 967
0.01.117.722 I llama_init_from_model: graph splits = 2
0.01.117.728 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.117.852 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.117.852 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.184.508 I main: llama threadpool init, n_threads = 4
0.01.184.556 I 
0.01.184.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.184.583 I 
0.01.184.766 I sampler seed: 1234
0.01.184.771 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.184.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.184.782 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.184.783 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.02.063.344 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.02.063.344 I llama_perf_context_print:        load time =    1169.15 ms
0.02.063.345 I llama_perf_context_print: prompt eval time =      54.39 ms /     7 tokens (    7.77 ms per token,   128.70 tokens per second)
0.02.063.346 I llama_perf_context_print:        eval time =     821.16 ms /    63 runs   (   13.03 ms per token,    76.72 tokens per second)
0.02.063.346 I llama_perf_context_print:       total time =     879.48 ms /    70 tokens
0.02.063.594 I ggml_metal_free: deallocating

real	0m2.100s
user	0m0.109s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4638 (3962fc1a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.191 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.123 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.129 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.133 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.134 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.134 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.137 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.137 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.138 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.139 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.599 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.600 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.601 I llama_model_loader: - type  f32:  194 tensors
0.00.024.602 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.602 I print_info: file format = GGUF V3 (latest)
0.00.024.603 I print_info: file type   = Q6_K
0.00.024.604 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.825 I load: special tokens cache size = 25
0.00.038.901 I load: token to piece cache size = 0.2984 MB
0.00.038.905 I print_info: arch             = gptneox
0.00.038.906 I print_info: vocab_only       = 0
0.00.038.906 I print_info: n_ctx_train      = 2048
0.00.038.906 I print_info: n_embd           = 2048
0.00.038.906 I print_info: n_layer          = 24
0.00.038.911 I print_info: n_head           = 16
0.00.038.911 I print_info: n_head_kv        = 16
0.00.038.911 I print_info: n_rot            = 32
0.00.038.912 I print_info: n_swa            = 0
0.00.038.912 I print_info: n_embd_head_k    = 128
0.00.038.912 I print_info: n_embd_head_v    = 128
0.00.038.912 I print_info: n_gqa            = 1
0.00.038.913 I print_info: n_embd_k_gqa     = 2048
0.00.038.914 I print_info: n_embd_v_gqa     = 2048
0.00.038.914 I print_info: f_norm_eps       = 1.0e-05
0.00.038.915 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.915 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.915 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.915 I print_info: f_logit_scale    = 0.0e+00
0.00.038.920 I print_info: n_ff             = 8192
0.00.038.920 I print_info: n_expert         = 0
0.00.038.920 I print_info: n_expert_used    = 0
0.00.038.920 I print_info: causal attn      = 1
0.00.038.920 I print_info: pooling type     = 0
0.00.038.920 I print_info: rope type        = 2
0.00.038.921 I print_info: rope scaling     = linear
0.00.038.921 I print_info: freq_base_train  = 10000.0
0.00.038.921 I print_info: freq_scale_train = 1
0.00.038.922 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.922 I print_info: rope_finetuned   = unknown
0.00.038.922 I print_info: ssm_d_conv       = 0
0.00.038.924 I print_info: ssm_d_inner      = 0
0.00.038.924 I print_info: ssm_d_state      = 0
0.00.038.924 I print_info: ssm_dt_rank      = 0
0.00.038.924 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.924 I print_info: model type       = 1.4B
0.00.038.924 I print_info: model params     = 1.41 B
0.00.038.924 I print_info: general.name     = 1.4B
0.00.038.925 I print_info: vocab type       = BPE
0.00.038.925 I print_info: n_vocab          = 50304
0.00.038.925 I print_info: n_merges         = 50009
0.00.038.925 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: LF token         = 187 'Ċ'
0.00.038.926 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.926 I print_info: max token length = 1024
0.00.329.608 I load_tensors: offloading 24 repeating layers to GPU
0.00.329.612 I load_tensors: offloading output layer to GPU
0.00.329.613 I load_tensors: offloaded 25/25 layers to GPU
0.00.329.633 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.329.634 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.330.557 I llama_init_from_model: n_seq_max     = 1
0.00.330.561 I llama_init_from_model: n_ctx         = 128
0.00.330.561 I llama_init_from_model: n_ctx_per_seq = 128
0.00.330.562 I llama_init_from_model: n_batch       = 128
0.00.330.562 I llama_init_from_model: n_ubatch      = 128
0.00.330.562 I llama_init_from_model: flash_attn    = 0
0.00.330.564 I llama_init_from_model: freq_base     = 10000.0
0.00.330.564 I llama_init_from_model: freq_scale    = 1
0.00.330.565 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.330.566 I ggml_metal_init: allocating
0.00.330.623 I ggml_metal_init: found device: Apple M4
0.00.330.635 I ggml_metal_init: picking default device: Apple M4
0.00.331.696 I ggml_metal_init: using embedded metal library
0.00.335.962 I ggml_metal_init: GPU name:   Apple M4
0.00.335.967 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.335.967 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.335.968 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.335.969 I ggml_metal_init: simdgroup reduction   = true
0.00.335.969 I ggml_metal_init: simdgroup matrix mul. = true
0.00.335.969 I ggml_metal_init: has residency sets    = true
0.00.335.969 I ggml_metal_init: has bfloat            = true
0.00.335.970 I ggml_metal_init: use bfloat            = true
0.00.335.971 I ggml_metal_init: hasUnifiedMemory      = true
0.00.335.973 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.478 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.351.051 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.351.055 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.351.084 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.352.736 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.352.737 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.352.738 I llama_init_from_model: graph nodes  = 967
0.00.352.738 I llama_init_from_model: graph splits = 2
0.00.352.740 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.352.740 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.385.304 I 
0.00.385.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.385.350 I perplexity: tokenizing the input ..
0.00.389.251 I perplexity: tokenization took 3.9 ms
0.00.389.263 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.257 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.529.428 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.529.443 I llama_perf_context_print:        load time =     376.11 ms
0.00.529.445 I llama_perf_context_print: prompt eval time =     138.77 ms /   128 tokens (    1.08 ms per token,   922.42 tokens per second)
0.00.529.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.446 I llama_perf_context_print:       total time =     144.14 ms /   129 tokens
0.00.529.823 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.068s
sys	0m0.073s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4638 (3962fc1a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117e04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117e054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117e05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117e08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117e08bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117e09060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117e094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117e09940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117e09db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117e0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117e0a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117e0ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117e0b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117e0c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117e0c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117e0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117e0d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117e0ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117e0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117e0ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117e0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117e0fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117e10170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117e10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117e11130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117e113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117e116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117e11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117e12240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117e126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117e12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117e13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117e135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117e138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117e13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117e14190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117e14600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117e14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117e14ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117e15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117e157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117e15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117e160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117e16510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117e16980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117e16df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117e17260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117e176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117e17e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117e182d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117e18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117e18bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117e19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117e19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117e19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117e1a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117e1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117e1a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117e1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117e1b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117e1b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117e1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117e1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117e1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117e1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117e1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117e1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117e1d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117e1dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117e1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117e1e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117e1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117e1f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117e1f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117e1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117e200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117e20690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117e20c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117e211f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117e217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117e21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117e22300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117e228b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117e22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117e23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117e239c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117e23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117e24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117e24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117e25080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117e25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117e25be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117e26190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117e26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117e26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117e272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117e27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117e17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117e27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117e28420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117e28890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117e28e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117e293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117e299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117e29f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117e2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117e2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117e2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117e2b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117e2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117e2c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117e2c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117e2ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117e2d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117e2d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117e2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117e2e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117e2e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117e2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117e2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117e2f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117e2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117e2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117e30480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117e30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117e30e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117e31380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117e31880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117e31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117e32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117e32780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117e32c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117e33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117e33680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117e33b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117e34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117e34580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117e34a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117e34f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117e35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117e35980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117e35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117e36380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117e36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117e36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117e37280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117e37780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117e37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117e38180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117e38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117e38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117e39080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117e39580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117e39a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117e39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117e3a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117e3a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117e3ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117e3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117e3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117e3bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117e3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117e3c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117e3cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117e3d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117e3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117e3db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117e3e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117e3e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117e3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117e3ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117e3f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117e3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117e3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117e40380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117e40880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117e40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117e41280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117e41780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117e41c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117e42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117e42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117e42b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117e43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117e43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117e43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117e43f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117e44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117e44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117e44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117e45380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117e45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117e45d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117e46280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117e46830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117e46de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117e47390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117e47940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117e47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117e48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117e48b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117e49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117e49800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117e49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117e4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117e4a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117e4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117e4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117e4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117e4bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117e4c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117e4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117e4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117e4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117e4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117e4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117e4e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117e4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117e4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117e4f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117e4fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117e50420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117e50970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117e50ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117e51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117e51960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117e51eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117e52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117e52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117e533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117e53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117e53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117e543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117e54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117e54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117e55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117e563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117e56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117e57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117e57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117e583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x112704230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1127046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x112704b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x112704f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1127053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x112705860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x112705cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x112706140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1127065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x112706a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x112706e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x112707300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x112707770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x112707be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x112708050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1127084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x112708930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x112708da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x112709210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x112709680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x112709af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x112709ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11270a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11270a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11270ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11270b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11270b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11270ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11270bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11270c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11270c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11270cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11270d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11270d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11270d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11270de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11270e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11270f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11270f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11270fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x112710180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x112710440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1127108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x112710d20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.732.093 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.732.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116d04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116d05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116d056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116d05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116d05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116d06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116d06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116d06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116d07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116d075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116d07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116d08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116d08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116d093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116d09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116d0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116d0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116d0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116d0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x116d0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116d0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116d0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116d0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116d0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116d0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116d0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116d0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116d0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116d0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116d0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116d0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116d0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116d10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116d106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116d10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116d10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116d11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116d118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116d11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116d12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116d12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116d12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116d12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116d13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116d137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116d13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116d140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116d14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116d14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116d14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116d15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116d156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116d15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116d15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116d16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116d16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116d16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116d17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116d17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116d17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116d18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116d184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116d18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116d18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116d19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116d19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116d19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116d19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116d1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116d1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116d1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x116d1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116d1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116d1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116d1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116d1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116d1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116d1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116d1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116d1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116d1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116d1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116d1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116d1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116d1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116d1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116d1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116d1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116d1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116d20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116d20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116d209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116d20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116d212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116d21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116d21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116d22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116d22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116d228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116d22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116d231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116d23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116d23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116d23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116d24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116d24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116d24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116d250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116d25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116d259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116d25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116d262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116d26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116d26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116d26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116d27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116d278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116d27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116d281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116d28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116d28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116d28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116d29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116d297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116d29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116d2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116d2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116d2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116d2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116d2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116d2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116d2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116d2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116d2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116d2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116d2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116d2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116d2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116d2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116d2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116d2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116d2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116d2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116d2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116d2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116d2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116d2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116d30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116d306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116d30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116d30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116d31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116d31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116d31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116d32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116d325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116d32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116d32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116d33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116d337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116d33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116d34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116d344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116d34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116d34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116d35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116d35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116d36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116d363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116d36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116d36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116d37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116d375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116d37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116d37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116d38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116d38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116d38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116d39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116d394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116d39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116d39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116d3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116d3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116d3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116d3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116d3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116d3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116d3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116d3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116d3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116d3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116d3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116d3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116d3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116d3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116d3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116d3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116d3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116d3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116d3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116d3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116d3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116d400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116d40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116d409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116d40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116d41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116d417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116d41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116d42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116d42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116d430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116d43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116d43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116d441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116d447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116d44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116d45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116d458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116d45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116d46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116d46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116d46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116d475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116d47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116d48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116d486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116d48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116d49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116d49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116d49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116d4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116d4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116d4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116d4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116d4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116d4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116d4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116d4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116d4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116d4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116d4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116d4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116d4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116d4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116d4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116d4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116d4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116d50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116d50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116d510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116d516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116d51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116d52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116d527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116d52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116d53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116d53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116d53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116d544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116d54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116d55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116d555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116d55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116d56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116d56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116d56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116d571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116d576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116d57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116d580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116d585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116d58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116d58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116d594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116d599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116d59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116d5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116d5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116d5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116d5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116d5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116d5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116d5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116d5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116d5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116d5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116d5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116d5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116d5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117e1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117e25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117e1f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117e27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117e24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117e2c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117e2be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117e2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117e26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117e21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117e29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117e46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117e26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117e214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117e247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117e23120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117e296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117e46540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117e2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117e26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117e20f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117e24230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117e22b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117e29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117e2ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117e25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117e20950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117e23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117e28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117e2a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117e258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117e236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117e2a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117e47c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117e48820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117e4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117e106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117e1aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117e19bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117e0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117e27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117e4a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117e48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117e11de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117e58660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117e58920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117e58be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117e58ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117e59160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117e59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117e596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117e599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117e59c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117e59f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117e5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117e5a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117e5a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117e5aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117e5ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117e5afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117e5b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117e5b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117e5b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117e5baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117e5bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117e5c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117e5c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117e5c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117e5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117e5cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117e5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117e5d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117e5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117e5d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117e5d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117e5dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117e5de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117e5e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117e5e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117e5e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117e5e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117e5ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117e5eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117e5f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117e5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117e5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117e5f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117e5fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117e5ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117e60220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117e604e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117e607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117e60a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117e60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117e60fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117e612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117e61560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117e61820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117e61ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117e61da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117e62060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117e62320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117e625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117e628a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117e62b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117e62e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117e630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117e633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117e63660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117e63920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117e63be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117e63ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117e64160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117e64420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117e646e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117e649a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117e64c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117e64f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117e651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117e654a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117e65760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117e65a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117e65ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117e65fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117e66260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117e66520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117e667e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117e66aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117e66d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117e67020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117e672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117e675a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117e67860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117e67b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117e67de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117e680a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117e68360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117e68620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117e688e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117e68ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117e68e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117e69120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117e693e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117e696a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117e69960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117e69c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117e69ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117e6a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117e6a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117e6a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117e6a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117e6aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117e6af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117e6b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117e6b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117e6b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117e6ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117e6bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117e6bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117e6c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117e6c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117e6c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117e6cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117e6cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117e6d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117e6d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117e6d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117e6d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117e6db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117e6de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117e6e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117e6e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117e6e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117e6e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117e6ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117e6eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117e6f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117e6f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117e6f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117e6f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117e6fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117e6ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117e701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117e704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117e70940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117e70de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117e71280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117e71720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117e71bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117e72060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117e72500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117e729a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117e72e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117e73390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117e738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117e73e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117e74380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117e74640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117e74c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117e75260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117e75870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117e76060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117e76500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117e767c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117e76dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117e773e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117e77bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117e78070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117e78510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117e789b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117e79160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117e796b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117e79c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117e7a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117e7a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117e7abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117e7b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117e7b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117e7bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117e7c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117e7c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117e7cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117e7d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117e7d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117e7dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117e7e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117e7e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117e7ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117e7f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117e7f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117e7fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117e800f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117e80640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117e80b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117e810e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117e81630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117e81b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117e820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117e82620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117e82b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117e830c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117e83610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117e83b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117e840b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117e84600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117e84b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117e850a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117e855f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117e85b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117e86090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117e865e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117e86b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117e87080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117e875d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117e87b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117e88070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117e885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117e88b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117e89060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117e895b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117e89b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117e8a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117e8a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117e8aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117e8b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117e8b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117e8bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117e8bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117e8c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117e8c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117e8cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117e8d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117e8d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117e8db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117e8dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117e8e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117e8e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117e8edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117e8f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117e8f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117e8fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117e90040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117e90590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117e90cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117e913d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117e91af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117e92210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117e924d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117e92cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117e92f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117e93590 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m2.075s
user	0m0.281s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4638 (3962fc1a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15980ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15980e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15980ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15980f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15980f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15980fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x159810150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x159810700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159810cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1598111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1598116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x159811bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1598126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x159812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159813690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x159813db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1598144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x159814bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x159815310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x159815ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159816200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x159816920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159817040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1598178e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159818000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1598182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1598188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x159819540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x159819a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x159819d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15981a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15981a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15981ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15981b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15981b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15981b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15981be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15981c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15981c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15981cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15981d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15981d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15981da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15981ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15981e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15981e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15981edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15981f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15981fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1598202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159820900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159820f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159821520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x159821b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159822320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1598227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159822c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159822f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159823530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159823d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159823fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x159824480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159824920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159824dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159825260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x159825700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159825ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1598264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159826980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159826e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1598272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159827760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x159827cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159828200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159828750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1598291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159829740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159829c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15982a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15982a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15982ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15982b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15982b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15982bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15982c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15982c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15982cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15982d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15982d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15982dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15982e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15982e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15982ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15982f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15982f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15981f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15982fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159830300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159830850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x159830da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1598312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159831840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159831d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1598322e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x159832830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x159832d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1598332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x159833820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x159833d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1598342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x159834810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x159834cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x159835150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1598355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159835a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159835f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1598363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159836870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x159836d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1598371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x159837650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159837af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x159837f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159838430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1598388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159838d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159839210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1598396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159839b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x159839ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15983a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15983a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15983add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15983b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15983b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15983bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15983c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15983c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15983c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15983ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15983d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15983d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15983dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15983e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15983e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15983e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15983ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15983f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15983f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15983fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159840110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1598405b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159840a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159840ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159841390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x159841830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159841cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159842170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159842610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159842ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159842f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1598433f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159843890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x159843d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1598441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159844670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x159844fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159845450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1598458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159845d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159846230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1598466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159846b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159847010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1598474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159847950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159847df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159848290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x159848730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159848bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159849070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159849510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1598499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159849e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15984a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15984a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15984ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15984b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15984b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15984ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15984bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15984c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15984ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15984cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15984d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15984d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15984de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15984e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15984ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15984f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15984f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15984f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15984ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1598507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159850c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1598510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x159851580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x159851d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x159852280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1598527d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x159852d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x159853270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1598537c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x159853d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x159854260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1598547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x159854d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x159855250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1598557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x159855cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x159856240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159856790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x159856ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159857230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x159857780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159857cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159858220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159858770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159858cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159859210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159859760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x159859cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15985a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15985a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15985aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15985b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15985b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15985bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15985c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15985c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15985cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15985d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15985d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15985dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15985e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15985e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15985ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15985f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15985f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15985fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1598601a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1598606f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159860c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159861190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1598616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159861c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x159862180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1598626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159862c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159863170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1598636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159863c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159864160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1598646b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x159864b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x159864ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x159865490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x159865930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x159865dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x159866270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x159866710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x159866bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x159867050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1598674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x159867990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x159867e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1598682d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x159868770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x159868c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x159869160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x159869880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x159869fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15986a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15986ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15986b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15986b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15986bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15986c160 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.842 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.847 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x158f079d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x158f07e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x158f082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x158f08720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158f08b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158f09000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158f09470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158f098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158f09d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x158f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x158f0a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158f0acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158f0b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158f0bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x158f0c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158f0cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158f0d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158f0dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158f0e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158f0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158f0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158f0fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158f108a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158f10fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x158f11280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x158f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x158f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x158f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x158f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x158f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x158f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x158f13360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x158f137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x158f13c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x158f140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x158f14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x158f14990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158f14e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158f15270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x158f156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x158f15b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x158f15fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x158f16430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158f168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158f16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158f17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158f175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158f17a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158f17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158f18340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158f187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158f18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158f19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x158f19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158f19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158f19f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158f1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158f1a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158f1acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x158f1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158f1b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158f1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158f1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158f1c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158f1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158f1cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158f1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x158f1d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158f1d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x158f1dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158f1e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x158f1e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x158f1eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x158f1ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x158f1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158f1f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158f1fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x158f20110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x158f20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158f209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158f20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x158f212d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x158f21740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x158f21bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x158f22020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158f22490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x158f22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158f22d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158f231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x158f23650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158f23ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x158f23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x158f243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x158f24810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158f24c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x158f250f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158f25560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158f259d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x158f25e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x158f262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x158f26720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158f26b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x158f27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158f27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158f278e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x158f27d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x158f281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x158f28630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158f28aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x158f28f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158f29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158f297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158f29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158f2a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158f2a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158f2a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x158f2ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x158f2b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x158f2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x158f2bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x158f2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x158f2c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x158f2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x158f2cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x158f2d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x158f2d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x158f2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x158f2def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x158f2e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x158f2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158f2ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x158f2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x158f2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158f2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x158f2fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x158f30270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158f306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158f30b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158f30fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x158f31430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158f318a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158f31d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158f32180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158f325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158f32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158f32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158f33340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158f337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158f33c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158f34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158f34500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158f34970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x158f34de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158f35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158f356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x158f35b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158f35fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158f36410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x158f36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158f36cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158f37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158f375d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158f37a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x158f37eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158f38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158f39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158f394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158f39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158f39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158f3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158f3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x158f3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x158f3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x158f3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158f3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x158f3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x158f3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158f3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x158f3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x158f3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x158f3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x158f3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x158f3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x158f3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x158f3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158f3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x158f3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x158f3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x158f3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x158f3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x158f3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158f403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158f40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158f40ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158f41110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x158f41580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x158f419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158f41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x158f422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158f42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x158f42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158f431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x158f43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x158f43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158f43f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158f44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158f44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158f454a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158f45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158f45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158f462e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158f468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158f46e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158f47420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158f479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x158f47fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158f48560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158f48b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x158f490e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158f496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158f49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x158f4a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158f4a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158f4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x158f4b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x158f4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158f4bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x158f4c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x158f4ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x158f4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x158f4d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x158f4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x158f4e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158f4e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158f4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x158f4f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158f4f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x158f4fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x158f503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x158f509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x158f50f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x158f51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158f51ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x158f520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158f52660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158f52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x158f531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158f537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158f53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158f54320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158f548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158f54ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158f55460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158f55a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158f55fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158f565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158f56b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158f57120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158f576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158f57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158f58260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158f58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158f58de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x158f593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x158f59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x158f59e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158f5a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158f5a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x158f5ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158f5b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158f5b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158f5bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158f5c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x158f5c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158f5cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x158f5d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x158f5d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x158f5da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x158f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x158f5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x158f5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x158f5f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x158f5fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x158f603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x158f60690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x158f60e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158f61140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158f61750 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15986be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15984dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15984d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15984e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1598211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x159820bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1598231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15984fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x159818580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15981f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15981f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15981ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15981e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1598205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x159817580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1598237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15982fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15986b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15981a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15981aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x159850270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15984e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x159818b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x159818e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x159819110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15986c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15986c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15986cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15986ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15986d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15986d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15986d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15986d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15986dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15986de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15986e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15986e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15986e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15986e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15986ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15986ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15986f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15986f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15986f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15986fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15986fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15986ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x159870240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x159870500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1598707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x159870a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x159870d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x159871000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1598712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x159871580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x159871840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x159871b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x159871dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x159872080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x159872340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x159872600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1598728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x159872b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x159872e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x159873100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1598733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x159873680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x159873940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x159873c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x159873ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x159874180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x159874440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159874700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1598749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159874c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159874f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159875200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1598754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159875780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159875a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159875d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159875fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159876280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159876540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x159876800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159876ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159876d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159877040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159877300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1598775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159877880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x159877b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159877e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1598780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x159878380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x159878640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x159878900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x159878bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x159878e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x159879140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x159879400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1598796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x159879980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x159879c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x159879f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15987a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15987a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15987a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15987aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15987acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15987af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15987b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15987b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15987b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15987ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15987bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15987c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15987c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15987c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15987c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15987cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15987cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15987d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15987d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15987d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15987d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15987db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15987de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15987e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15987e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15987e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15987e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15987ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15987eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15987f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15987f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15987f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15987f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15987fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15987ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x159880200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1598804c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159880780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159880a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159880d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x159880fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159881280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159881540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159881800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x159881ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159881d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x159882040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x159882300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1598825c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x159882880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x159882b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x159882e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1598830c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x159883380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x159883640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x159883900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x159883bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x159883e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x159884140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x159884400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1598846c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x159884980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x159884c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x159884f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1598851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x159885480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x159885740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x159885a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159885cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159885f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159886240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159886500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1598867c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159886a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159886d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159887000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1598872c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159887580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159887840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x159887b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159887dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159888080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159888340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x159888600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1598888c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159888b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159888e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159889100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1598893c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159889680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x159889940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159889c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159889ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15988a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15988a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15988a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15988a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15988ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15988af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15988b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15988b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15988b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15988ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15988be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15988c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15988ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15988cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15988d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15988d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15988d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15988dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15988e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15988e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15988eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15988ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15988f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15988f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15988fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1598900e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x159890550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1598909c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x159890e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1598912a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x159891710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x159891b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159891ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159892460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1598928d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159892d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1598931b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159893620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159893a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159893f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x159894370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1598947e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159894c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1598950c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159895530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1598959a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159895e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x159896280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1598966f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159896b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159896fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159897440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1598978b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159897d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x159898190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159898600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159898a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159898ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159899350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1598997c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159899c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15989a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15989a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15989a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15989adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15989b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15989b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15989bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15989bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15989c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15989c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15989cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15989d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15989d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15989da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15989dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15989e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15989e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15989ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15989f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15989f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15989f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15989fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1598a0240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1598a06b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1598a1120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1598a1840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1598a1f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1598a2680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1598a2940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1598a3130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1598a33f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1598a3a00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.971s
user	0m0.237s
sys	0m0.192s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.76 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    2.00 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.75 sec*proc (2 tests)

Total Test time (real) =   2.78 sec
        2.80 real         0.54 user         0.27 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.56 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.60 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.16 sec*proc (2 tests)

Total Test time (real) =   1.17 sec
        1.18 real         0.14 user         0.10 sys
```
