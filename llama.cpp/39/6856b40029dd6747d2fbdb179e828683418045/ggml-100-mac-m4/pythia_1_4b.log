Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.634s
user	0m0.895s
sys	0m1.241s
++ nproc
+ make -j10
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Built target sha1
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llava
[ 35%] Built target llama-simple
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Built target llava_shared
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-chat
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Built target test-log
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-backend-ops
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-arg-parser
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-autorelease
[ 62%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Built target test-quantize-perf
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-embedding
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-batched
[ 71%] Built target llama-gbnf-validator
[ 71%] Built target llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 72%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gguf-split
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Built target llama-infill
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Linking CXX executable ../../bin/llama-cli
[ 76%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Built target llama-bench
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-cli
[ 81%] Built target llama-passkey
[ 81%] Built target llama-parallel
[ 82%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Built target llama-quantize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-run
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 91%] Built target llama-tts
[ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 92%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 95%] Built target llama-gen-docs
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-cvector-generator
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-convert-llama2c-to-ggml
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.089s
user	0m6.628s
sys	0m10.087s

main: quantize time =  6395.18 ms
main:    total time =  6395.18 ms

main: quantize time =  1459.78 ms
main:    total time =  1459.78 ms

main: quantize time =  2975.40 ms
main:    total time =  2975.40 ms

main: quantize time =  2200.37 ms
main:    total time =  2200.37 ms

main: quantize time =  2066.55 ms
main:    total time =  2066.55 ms

main: quantize time =  5126.98 ms
main:    total time =  5126.98 ms

main: quantize time =  5726.38 ms
main:    total time =  5726.38 ms

main: quantize time =  7343.56 ms
main:    total time =  7343.56 ms

main: quantize time =  6220.36 ms
main:    total time =  6220.36 ms

main: quantize time =  4530.25 ms
main:    total time =  4530.25 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.140 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.298 I main: llama backend init
0.00.000.305 I main: load the model and apply lora adapter, if any
0.00.046.097 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.058.823 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.058.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.058.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.058.847 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.058.848 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.058.848 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.058.849 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.058.852 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.058.853 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.058.854 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.058.855 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.058.855 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.058.856 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.058.857 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.058.861 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.058.862 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.058.863 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.065.984 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.068.843 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.077.554 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.077.559 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.077.560 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.077.560 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.077.561 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.077.562 I llama_model_loader: - type  f32:  194 tensors
0.00.077.562 I llama_model_loader: - type  f16:   98 tensors
0.00.077.566 I print_info: file format = GGUF V3 (latest)
0.00.077.568 I print_info: file type   = all F32 (guessed)
0.00.077.570 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.092.499 I load: special tokens cache size = 25
0.00.101.716 I load: token to piece cache size = 0.2984 MB
0.00.101.719 I print_info: arch             = gptneox
0.00.101.720 I print_info: vocab_only       = 0
0.00.101.720 I print_info: n_ctx_train      = 2048
0.00.101.720 I print_info: n_embd           = 2048
0.00.101.721 I print_info: n_layer          = 24
0.00.101.724 I print_info: n_head           = 16
0.00.101.725 I print_info: n_head_kv        = 16
0.00.101.726 I print_info: n_rot            = 32
0.00.101.726 I print_info: n_swa            = 0
0.00.101.726 I print_info: n_embd_head_k    = 128
0.00.101.726 I print_info: n_embd_head_v    = 128
0.00.101.727 I print_info: n_gqa            = 1
0.00.101.728 I print_info: n_embd_k_gqa     = 2048
0.00.101.729 I print_info: n_embd_v_gqa     = 2048
0.00.101.730 I print_info: f_norm_eps       = 1.0e-05
0.00.101.730 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.101.730 I print_info: f_clamp_kqv      = 0.0e+00
0.00.101.730 I print_info: f_max_alibi_bias = 0.0e+00
0.00.101.730 I print_info: f_logit_scale    = 0.0e+00
0.00.101.731 I print_info: n_ff             = 8192
0.00.101.732 I print_info: n_expert         = 0
0.00.101.732 I print_info: n_expert_used    = 0
0.00.101.732 I print_info: causal attn      = 1
0.00.101.732 I print_info: pooling type     = 0
0.00.101.732 I print_info: rope type        = 2
0.00.101.732 I print_info: rope scaling     = linear
0.00.101.736 I print_info: freq_base_train  = 10000.0
0.00.101.736 I print_info: freq_scale_train = 1
0.00.101.736 I print_info: n_ctx_orig_yarn  = 2048
0.00.101.737 I print_info: rope_finetuned   = unknown
0.00.101.737 I print_info: ssm_d_conv       = 0
0.00.101.739 I print_info: ssm_d_inner      = 0
0.00.101.739 I print_info: ssm_d_state      = 0
0.00.101.739 I print_info: ssm_dt_rank      = 0
0.00.101.739 I print_info: ssm_dt_b_c_rms   = 0
0.00.101.739 I print_info: model type       = 1.4B
0.00.101.740 I print_info: model params     = 1.41 B
0.00.101.740 I print_info: general.name     = 1.4B
0.00.101.741 I print_info: vocab type       = BPE
0.00.101.741 I print_info: n_vocab          = 50304
0.00.101.741 I print_info: n_merges         = 50009
0.00.101.741 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.101.741 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.101.742 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.101.746 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.101.747 I print_info: LF token         = 187 'Ċ'
0.00.101.747 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.101.747 I print_info: max token length = 1024
0.00.144.546 I load_tensors: offloading 24 repeating layers to GPU
0.00.144.551 I load_tensors: offloading output layer to GPU
0.00.144.551 I load_tensors: offloaded 25/25 layers to GPU
0.00.144.574 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.144.575 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.144.894 I llama_init_from_model: n_seq_max     = 1
0.00.144.895 I llama_init_from_model: n_ctx         = 2048
0.00.144.895 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.144.895 I llama_init_from_model: n_batch       = 2048
0.00.144.895 I llama_init_from_model: n_ubatch      = 512
0.00.144.895 I llama_init_from_model: flash_attn    = 0
0.00.144.896 I llama_init_from_model: freq_base     = 10000.0
0.00.144.896 I llama_init_from_model: freq_scale    = 1
0.00.144.897 I ggml_metal_init: allocating
0.00.144.917 I ggml_metal_init: found device: Apple M4
0.00.144.922 I ggml_metal_init: picking default device: Apple M4
0.00.145.519 I ggml_metal_init: using embedded metal library
0.00.154.579 I ggml_metal_init: GPU name:   Apple M4
0.00.154.581 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.154.581 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.154.582 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.154.582 I ggml_metal_init: simdgroup reduction   = true
0.00.154.582 I ggml_metal_init: simdgroup matrix mul. = true
0.00.154.582 I ggml_metal_init: has residency sets    = true
0.00.154.582 I ggml_metal_init: has bfloat            = true
0.00.154.583 I ggml_metal_init: use bfloat            = true
0.00.154.583 I ggml_metal_init: hasUnifiedMemory      = true
0.00.154.584 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.178.916 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.205.716 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.205.723 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.205.774 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.209.283 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.209.285 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.209.286 I llama_init_from_model: graph nodes  = 967
0.00.209.286 I llama_init_from_model: graph splits = 2
0.00.209.289 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.209.413 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.209.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.274.817 I main: llama threadpool init, n_threads = 4
0.00.274.862 I 
0.00.274.893 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.274.894 I 
0.00.274.938 I sampler seed: 1234
0.00.274.942 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.274.967 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.274.968 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.274.968 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.013 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.103.014 I llama_perf_context_print:        load time =     227.66 ms
0.02.103.014 I llama_perf_context_print: prompt eval time =      43.69 ms /     7 tokens (    6.24 ms per token,   160.22 tokens per second)
0.02.103.015 I llama_perf_context_print:        eval time =    1781.49 ms /    63 runs   (   28.28 ms per token,    35.36 tokens per second)
0.02.103.015 I llama_perf_context_print:       total time =    1829.25 ms /    70 tokens
0.02.103.226 I ggml_metal_free: deallocating

real	0m2.444s
user	0m0.132s
sys	0m0.134s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.284 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.408 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.416 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.281 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.282 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.282 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.283 I llama_model_loader: - type  f32:  194 tensors
0.00.035.283 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.284 I print_info: file format = GGUF V3 (latest)
0.00.035.285 I print_info: file type   = Q8_0
0.00.035.286 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.454 I load: special tokens cache size = 25
0.00.051.173 I load: token to piece cache size = 0.2984 MB
0.00.051.181 I print_info: arch             = gptneox
0.00.051.181 I print_info: vocab_only       = 0
0.00.051.181 I print_info: n_ctx_train      = 2048
0.00.051.182 I print_info: n_embd           = 2048
0.00.051.182 I print_info: n_layer          = 24
0.00.051.187 I print_info: n_head           = 16
0.00.051.188 I print_info: n_head_kv        = 16
0.00.051.189 I print_info: n_rot            = 32
0.00.051.189 I print_info: n_swa            = 0
0.00.051.189 I print_info: n_embd_head_k    = 128
0.00.051.189 I print_info: n_embd_head_v    = 128
0.00.051.190 I print_info: n_gqa            = 1
0.00.051.191 I print_info: n_embd_k_gqa     = 2048
0.00.051.191 I print_info: n_embd_v_gqa     = 2048
0.00.051.192 I print_info: f_norm_eps       = 1.0e-05
0.00.051.192 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.192 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.193 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.195 I print_info: f_logit_scale    = 0.0e+00
0.00.051.196 I print_info: n_ff             = 8192
0.00.051.196 I print_info: n_expert         = 0
0.00.051.197 I print_info: n_expert_used    = 0
0.00.051.197 I print_info: causal attn      = 1
0.00.051.197 I print_info: pooling type     = 0
0.00.051.197 I print_info: rope type        = 2
0.00.051.197 I print_info: rope scaling     = linear
0.00.051.198 I print_info: freq_base_train  = 10000.0
0.00.051.199 I print_info: freq_scale_train = 1
0.00.051.199 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.199 I print_info: rope_finetuned   = unknown
0.00.051.199 I print_info: ssm_d_conv       = 0
0.00.051.201 I print_info: ssm_d_inner      = 0
0.00.051.201 I print_info: ssm_d_state      = 0
0.00.051.201 I print_info: ssm_dt_rank      = 0
0.00.051.201 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.201 I print_info: model type       = 1.4B
0.00.051.202 I print_info: model params     = 1.41 B
0.00.051.202 I print_info: general.name     = 1.4B
0.00.051.203 I print_info: vocab type       = BPE
0.00.051.203 I print_info: n_vocab          = 50304
0.00.051.203 I print_info: n_merges         = 50009
0.00.051.203 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.203 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.203 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.203 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.209 I print_info: LF token         = 187 'Ċ'
0.00.051.209 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.210 I print_info: max token length = 1024
0.01.203.463 I load_tensors: offloading 24 repeating layers to GPU
0.01.203.467 I load_tensors: offloading output layer to GPU
0.01.203.468 I load_tensors: offloaded 25/25 layers to GPU
0.01.203.491 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.203.492 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.204.175 I llama_init_from_model: n_seq_max     = 1
0.01.204.177 I llama_init_from_model: n_ctx         = 2048
0.01.204.177 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.204.178 I llama_init_from_model: n_batch       = 2048
0.01.204.178 I llama_init_from_model: n_ubatch      = 512
0.01.204.179 I llama_init_from_model: flash_attn    = 0
0.01.204.179 I llama_init_from_model: freq_base     = 10000.0
0.01.204.180 I llama_init_from_model: freq_scale    = 1
0.01.204.180 I ggml_metal_init: allocating
0.01.204.192 I ggml_metal_init: found device: Apple M4
0.01.204.198 I ggml_metal_init: picking default device: Apple M4
0.01.205.400 I ggml_metal_init: using embedded metal library
0.01.212.777 I ggml_metal_init: GPU name:   Apple M4
0.01.212.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.212.781 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.212.782 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.212.783 I ggml_metal_init: simdgroup reduction   = true
0.01.212.783 I ggml_metal_init: simdgroup matrix mul. = true
0.01.212.783 I ggml_metal_init: has residency sets    = true
0.01.212.784 I ggml_metal_init: has bfloat            = true
0.01.212.784 I ggml_metal_init: use bfloat            = true
0.01.212.785 I ggml_metal_init: hasUnifiedMemory      = true
0.01.212.786 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.230.739 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.293.503 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.293.509 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.293.542 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.297.887 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.297.888 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.297.889 I llama_init_from_model: graph nodes  = 967
0.01.297.889 I llama_init_from_model: graph splits = 2
0.01.297.898 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.298.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.298.032 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.353.817 I main: llama threadpool init, n_threads = 4
0.01.353.856 I 
0.01.353.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.353.882 I 
0.01.354.029 I sampler seed: 1234
0.01.354.033 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.354.043 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.354.044 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.354.045 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.460.712 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48563.61 tokens per second)
0.02.460.713 I llama_perf_context_print:        load time =    1342.61 ms
0.02.460.713 I llama_perf_context_print: prompt eval time =      48.79 ms /     7 tokens (    6.97 ms per token,   143.46 tokens per second)
0.02.460.715 I llama_perf_context_print:        eval time =    1055.37 ms /    63 runs   (   16.75 ms per token,    59.69 tokens per second)
0.02.460.715 I llama_perf_context_print:       total time =    1107.81 ms /    70 tokens
0.02.460.969 I ggml_metal_free: deallocating

real	0m2.479s
user	0m0.112s
sys	0m0.273s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.022.242 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.411 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.415 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.422 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.424 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.424 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.664 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.056 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.057 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.057 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.057 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.058 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.058 I llama_model_loader: - type  f32:  194 tensors
0.00.042.059 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.059 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.059 I print_info: file format = GGUF V3 (latest)
0.00.042.060 I print_info: file type   = Q4_0
0.00.042.061 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.051.144 I load: special tokens cache size = 25
0.00.058.543 I load: token to piece cache size = 0.2984 MB
0.00.058.546 I print_info: arch             = gptneox
0.00.058.546 I print_info: vocab_only       = 0
0.00.058.546 I print_info: n_ctx_train      = 2048
0.00.058.546 I print_info: n_embd           = 2048
0.00.058.547 I print_info: n_layer          = 24
0.00.058.549 I print_info: n_head           = 16
0.00.058.550 I print_info: n_head_kv        = 16
0.00.058.551 I print_info: n_rot            = 32
0.00.058.551 I print_info: n_swa            = 0
0.00.058.551 I print_info: n_embd_head_k    = 128
0.00.058.551 I print_info: n_embd_head_v    = 128
0.00.058.553 I print_info: n_gqa            = 1
0.00.058.553 I print_info: n_embd_k_gqa     = 2048
0.00.058.554 I print_info: n_embd_v_gqa     = 2048
0.00.058.554 I print_info: f_norm_eps       = 1.0e-05
0.00.058.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.555 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.555 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.555 I print_info: f_logit_scale    = 0.0e+00
0.00.058.556 I print_info: n_ff             = 8192
0.00.058.556 I print_info: n_expert         = 0
0.00.058.556 I print_info: n_expert_used    = 0
0.00.058.556 I print_info: causal attn      = 1
0.00.058.557 I print_info: pooling type     = 0
0.00.058.557 I print_info: rope type        = 2
0.00.058.557 I print_info: rope scaling     = linear
0.00.058.557 I print_info: freq_base_train  = 10000.0
0.00.058.558 I print_info: freq_scale_train = 1
0.00.058.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.558 I print_info: rope_finetuned   = unknown
0.00.058.558 I print_info: ssm_d_conv       = 0
0.00.058.558 I print_info: ssm_d_inner      = 0
0.00.058.558 I print_info: ssm_d_state      = 0
0.00.058.558 I print_info: ssm_dt_rank      = 0
0.00.058.560 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.561 I print_info: model type       = 1.4B
0.00.058.561 I print_info: model params     = 1.41 B
0.00.058.561 I print_info: general.name     = 1.4B
0.00.058.561 I print_info: vocab type       = BPE
0.00.058.562 I print_info: n_vocab          = 50304
0.00.058.562 I print_info: n_merges         = 50009
0.00.058.562 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.562 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.562 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.563 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.563 I print_info: LF token         = 187 'Ċ'
0.00.058.563 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.563 I print_info: max token length = 1024
0.00.600.589 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.605 I load_tensors: offloading output layer to GPU
0.00.600.605 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.640 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.600.641 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.601.880 I llama_init_from_model: n_seq_max     = 1
0.00.601.886 I llama_init_from_model: n_ctx         = 2048
0.00.601.886 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.601.887 I llama_init_from_model: n_batch       = 2048
0.00.601.887 I llama_init_from_model: n_ubatch      = 512
0.00.601.887 I llama_init_from_model: flash_attn    = 0
0.00.601.889 I llama_init_from_model: freq_base     = 10000.0
0.00.601.890 I llama_init_from_model: freq_scale    = 1
0.00.601.892 I ggml_metal_init: allocating
0.00.601.963 I ggml_metal_init: found device: Apple M4
0.00.601.977 I ggml_metal_init: picking default device: Apple M4
0.00.603.702 I ggml_metal_init: using embedded metal library
0.00.610.294 I ggml_metal_init: GPU name:   Apple M4
0.00.610.299 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.300 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.301 I ggml_metal_init: simdgroup reduction   = true
0.00.610.302 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.302 I ggml_metal_init: has residency sets    = true
0.00.610.302 I ggml_metal_init: has bfloat            = true
0.00.610.302 I ggml_metal_init: use bfloat            = true
0.00.610.303 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.291 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.686.242 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.686.248 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.686.282 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.437 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.690.439 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.690.439 I llama_init_from_model: graph nodes  = 967
0.00.690.439 I llama_init_from_model: graph splits = 2
0.00.690.442 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.580 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.581 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.743.497 I main: llama threadpool init, n_threads = 4
0.00.743.541 I 
0.00.743.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.743.565 I 
0.00.743.739 I sampler seed: 1234
0.00.743.744 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.743.754 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.743.755 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.743.755 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.420.695 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.420.695 I llama_perf_context_print:        load time =     720.33 ms
0.01.420.697 I llama_perf_context_print: prompt eval time =      39.45 ms /     7 tokens (    5.63 ms per token,   177.46 tokens per second)
0.01.420.697 I llama_perf_context_print:        eval time =     634.54 ms /    63 runs   (   10.07 ms per token,    99.28 tokens per second)
0.01.420.699 I llama_perf_context_print:       total time =     678.12 ms /    70 tokens
0.01.420.933 I ggml_metal_free: deallocating

real	0m1.441s
user	0m0.115s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.312 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.314 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.317 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.317 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.318 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.032 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.048 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.004 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.005 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.005 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.006 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.006 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.006 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.007 I llama_model_loader: - type  f32:  194 tensors
0.00.033.007 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.007 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.008 I print_info: file format = GGUF V3 (latest)
0.00.033.008 I print_info: file type   = Q4_1
0.00.033.009 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.041.302 I load: special tokens cache size = 25
0.00.047.147 I load: token to piece cache size = 0.2984 MB
0.00.047.150 I print_info: arch             = gptneox
0.00.047.150 I print_info: vocab_only       = 0
0.00.047.151 I print_info: n_ctx_train      = 2048
0.00.047.151 I print_info: n_embd           = 2048
0.00.047.151 I print_info: n_layer          = 24
0.00.047.154 I print_info: n_head           = 16
0.00.047.155 I print_info: n_head_kv        = 16
0.00.047.155 I print_info: n_rot            = 32
0.00.047.155 I print_info: n_swa            = 0
0.00.047.155 I print_info: n_embd_head_k    = 128
0.00.047.155 I print_info: n_embd_head_v    = 128
0.00.047.156 I print_info: n_gqa            = 1
0.00.047.157 I print_info: n_embd_k_gqa     = 2048
0.00.047.158 I print_info: n_embd_v_gqa     = 2048
0.00.047.158 I print_info: f_norm_eps       = 1.0e-05
0.00.047.158 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.159 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.159 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.163 I print_info: f_logit_scale    = 0.0e+00
0.00.047.164 I print_info: n_ff             = 8192
0.00.047.164 I print_info: n_expert         = 0
0.00.047.164 I print_info: n_expert_used    = 0
0.00.047.165 I print_info: causal attn      = 1
0.00.047.165 I print_info: pooling type     = 0
0.00.047.166 I print_info: rope type        = 2
0.00.047.167 I print_info: rope scaling     = linear
0.00.047.168 I print_info: freq_base_train  = 10000.0
0.00.047.168 I print_info: freq_scale_train = 1
0.00.047.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.168 I print_info: rope_finetuned   = unknown
0.00.047.168 I print_info: ssm_d_conv       = 0
0.00.047.168 I print_info: ssm_d_inner      = 0
0.00.047.169 I print_info: ssm_d_state      = 0
0.00.047.169 I print_info: ssm_dt_rank      = 0
0.00.047.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.169 I print_info: model type       = 1.4B
0.00.047.169 I print_info: model params     = 1.41 B
0.00.047.169 I print_info: general.name     = 1.4B
0.00.047.170 I print_info: vocab type       = BPE
0.00.047.170 I print_info: n_vocab          = 50304
0.00.047.170 I print_info: n_merges         = 50009
0.00.047.170 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.170 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.171 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.171 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.171 I print_info: LF token         = 187 'Ċ'
0.00.047.171 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.171 I print_info: max token length = 1024
0.00.723.587 I load_tensors: offloading 24 repeating layers to GPU
0.00.723.603 I load_tensors: offloading output layer to GPU
0.00.723.603 I load_tensors: offloaded 25/25 layers to GPU
0.00.723.638 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.723.640 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.725.059 I llama_init_from_model: n_seq_max     = 1
0.00.725.065 I llama_init_from_model: n_ctx         = 2048
0.00.725.066 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.725.066 I llama_init_from_model: n_batch       = 2048
0.00.725.066 I llama_init_from_model: n_ubatch      = 512
0.00.725.067 I llama_init_from_model: flash_attn    = 0
0.00.725.069 I llama_init_from_model: freq_base     = 10000.0
0.00.725.069 I llama_init_from_model: freq_scale    = 1
0.00.725.075 I ggml_metal_init: allocating
0.00.725.165 I ggml_metal_init: found device: Apple M4
0.00.725.179 I ggml_metal_init: picking default device: Apple M4
0.00.727.032 I ggml_metal_init: using embedded metal library
0.00.734.203 I ggml_metal_init: GPU name:   Apple M4
0.00.734.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.734.209 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.734.210 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.734.211 I ggml_metal_init: simdgroup reduction   = true
0.00.734.211 I ggml_metal_init: simdgroup matrix mul. = true
0.00.734.211 I ggml_metal_init: has residency sets    = true
0.00.734.212 I ggml_metal_init: has bfloat            = true
0.00.734.212 I ggml_metal_init: use bfloat            = true
0.00.734.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.734.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.752.197 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.809.002 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.809.008 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.809.043 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.813.427 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.813.429 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.813.429 I llama_init_from_model: graph nodes  = 967
0.00.813.429 I llama_init_from_model: graph splits = 2
0.00.813.435 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.813.563 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.813.564 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.868.526 I main: llama threadpool init, n_threads = 4
0.00.868.569 I 
0.00.868.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.868.594 I 
0.00.868.760 I sampler seed: 1234
0.00.868.764 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.868.775 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.868.775 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.868.775 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.597.614 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55861.53 tokens per second)
0.01.597.614 I llama_perf_context_print:        load time =     858.88 ms
0.01.597.615 I llama_perf_context_print: prompt eval time =      48.85 ms /     7 tokens (    6.98 ms per token,   143.29 tokens per second)
0.01.597.616 I llama_perf_context_print:        eval time =     677.24 ms /    63 runs   (   10.75 ms per token,    93.03 tokens per second)
0.01.597.617 I llama_perf_context_print:       total time =     730.06 ms /    70 tokens
0.01.597.872 I ggml_metal_free: deallocating

real	0m1.614s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.014.382 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.163 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.169 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.169 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.170 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.858 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.858 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.858 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.859 I llama_model_loader: - type  f32:  194 tensors
0.00.033.859 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.860 I print_info: file format = GGUF V3 (latest)
0.00.033.860 I print_info: file type   = Q5_0
0.00.033.861 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.042.510 I load: special tokens cache size = 25
0.00.049.269 I load: token to piece cache size = 0.2984 MB
0.00.049.272 I print_info: arch             = gptneox
0.00.049.272 I print_info: vocab_only       = 0
0.00.049.272 I print_info: n_ctx_train      = 2048
0.00.049.273 I print_info: n_embd           = 2048
0.00.049.273 I print_info: n_layer          = 24
0.00.049.275 I print_info: n_head           = 16
0.00.049.276 I print_info: n_head_kv        = 16
0.00.049.276 I print_info: n_rot            = 32
0.00.049.276 I print_info: n_swa            = 0
0.00.049.277 I print_info: n_embd_head_k    = 128
0.00.049.279 I print_info: n_embd_head_v    = 128
0.00.049.280 I print_info: n_gqa            = 1
0.00.049.280 I print_info: n_embd_k_gqa     = 2048
0.00.049.281 I print_info: n_embd_v_gqa     = 2048
0.00.049.282 I print_info: f_norm_eps       = 1.0e-05
0.00.049.286 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.286 I print_info: f_logit_scale    = 0.0e+00
0.00.049.287 I print_info: n_ff             = 8192
0.00.049.287 I print_info: n_expert         = 0
0.00.049.287 I print_info: n_expert_used    = 0
0.00.049.287 I print_info: causal attn      = 1
0.00.049.287 I print_info: pooling type     = 0
0.00.049.289 I print_info: rope type        = 2
0.00.049.290 I print_info: rope scaling     = linear
0.00.049.290 I print_info: freq_base_train  = 10000.0
0.00.049.291 I print_info: freq_scale_train = 1
0.00.049.291 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.291 I print_info: rope_finetuned   = unknown
0.00.049.291 I print_info: ssm_d_conv       = 0
0.00.049.292 I print_info: ssm_d_inner      = 0
0.00.049.292 I print_info: ssm_d_state      = 0
0.00.049.292 I print_info: ssm_dt_rank      = 0
0.00.049.292 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.292 I print_info: model type       = 1.4B
0.00.049.295 I print_info: model params     = 1.41 B
0.00.049.295 I print_info: general.name     = 1.4B
0.00.049.296 I print_info: vocab type       = BPE
0.00.049.296 I print_info: n_vocab          = 50304
0.00.049.296 I print_info: n_merges         = 50009
0.00.049.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.297 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.298 I print_info: LF token         = 187 'Ċ'
0.00.049.299 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.299 I print_info: max token length = 1024
0.00.809.193 I load_tensors: offloading 24 repeating layers to GPU
0.00.809.207 I load_tensors: offloading output layer to GPU
0.00.809.208 I load_tensors: offloaded 25/25 layers to GPU
0.00.809.243 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.809.249 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.810.773 I llama_init_from_model: n_seq_max     = 1
0.00.810.779 I llama_init_from_model: n_ctx         = 2048
0.00.810.779 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.810.780 I llama_init_from_model: n_batch       = 2048
0.00.810.780 I llama_init_from_model: n_ubatch      = 512
0.00.810.781 I llama_init_from_model: flash_attn    = 0
0.00.810.783 I llama_init_from_model: freq_base     = 10000.0
0.00.810.783 I llama_init_from_model: freq_scale    = 1
0.00.810.794 I ggml_metal_init: allocating
0.00.810.873 I ggml_metal_init: found device: Apple M4
0.00.810.886 I ggml_metal_init: picking default device: Apple M4
0.00.812.710 I ggml_metal_init: using embedded metal library
0.00.819.205 I ggml_metal_init: GPU name:   Apple M4
0.00.819.209 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.819.210 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.819.211 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.819.211 I ggml_metal_init: simdgroup reduction   = true
0.00.819.212 I ggml_metal_init: simdgroup matrix mul. = true
0.00.819.212 I ggml_metal_init: has residency sets    = true
0.00.819.212 I ggml_metal_init: has bfloat            = true
0.00.819.212 I ggml_metal_init: use bfloat            = true
0.00.819.213 I ggml_metal_init: hasUnifiedMemory      = true
0.00.819.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.836.422 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.893.208 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.893.217 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.893.300 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.897.805 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.897.806 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.897.807 I llama_init_from_model: graph nodes  = 967
0.00.897.807 I llama_init_from_model: graph splits = 2
0.00.897.813 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.897.948 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.897.949 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.955.361 I main: llama threadpool init, n_threads = 4
0.00.955.405 I 
0.00.955.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.955.429 I 
0.00.955.596 I sampler seed: 1234
0.00.955.601 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.955.611 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.955.612 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.955.612 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.738.388 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.738.389 I llama_perf_context_print:        load time =     940.02 ms
0.01.738.390 I llama_perf_context_print: prompt eval time =      42.80 ms /     7 tokens (    6.11 ms per token,   163.55 tokens per second)
0.01.738.390 I llama_perf_context_print:        eval time =     737.13 ms /    63 runs   (   11.70 ms per token,    85.47 tokens per second)
0.01.738.391 I llama_perf_context_print:       total time =     783.98 ms /    70 tokens
0.01.738.674 I ggml_metal_free: deallocating

real	0m1.759s
user	0m0.111s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.784 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.790 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.792 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.795 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.795 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.541 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.514 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.515 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.515 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.516 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.516 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.516 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.517 I llama_model_loader: - type  f32:  194 tensors
0.00.036.517 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.517 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.518 I print_info: file format = GGUF V3 (latest)
0.00.036.518 I print_info: file type   = Q5_1
0.00.036.519 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.875 I load: special tokens cache size = 25
0.00.051.844 I load: token to piece cache size = 0.2984 MB
0.00.051.847 I print_info: arch             = gptneox
0.00.051.847 I print_info: vocab_only       = 0
0.00.051.847 I print_info: n_ctx_train      = 2048
0.00.051.847 I print_info: n_embd           = 2048
0.00.051.848 I print_info: n_layer          = 24
0.00.051.850 I print_info: n_head           = 16
0.00.051.851 I print_info: n_head_kv        = 16
0.00.051.851 I print_info: n_rot            = 32
0.00.051.851 I print_info: n_swa            = 0
0.00.051.851 I print_info: n_embd_head_k    = 128
0.00.051.852 I print_info: n_embd_head_v    = 128
0.00.051.852 I print_info: n_gqa            = 1
0.00.051.853 I print_info: n_embd_k_gqa     = 2048
0.00.051.854 I print_info: n_embd_v_gqa     = 2048
0.00.051.854 I print_info: f_norm_eps       = 1.0e-05
0.00.051.855 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.855 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.856 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.856 I print_info: f_logit_scale    = 0.0e+00
0.00.051.857 I print_info: n_ff             = 8192
0.00.051.857 I print_info: n_expert         = 0
0.00.051.857 I print_info: n_expert_used    = 0
0.00.051.857 I print_info: causal attn      = 1
0.00.051.857 I print_info: pooling type     = 0
0.00.051.859 I print_info: rope type        = 2
0.00.051.859 I print_info: rope scaling     = linear
0.00.051.860 I print_info: freq_base_train  = 10000.0
0.00.051.860 I print_info: freq_scale_train = 1
0.00.051.860 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.860 I print_info: rope_finetuned   = unknown
0.00.051.860 I print_info: ssm_d_conv       = 0
0.00.051.860 I print_info: ssm_d_inner      = 0
0.00.051.861 I print_info: ssm_d_state      = 0
0.00.051.861 I print_info: ssm_dt_rank      = 0
0.00.051.861 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.861 I print_info: model type       = 1.4B
0.00.051.862 I print_info: model params     = 1.41 B
0.00.051.863 I print_info: general.name     = 1.4B
0.00.051.863 I print_info: vocab type       = BPE
0.00.051.863 I print_info: n_vocab          = 50304
0.00.051.863 I print_info: n_merges         = 50009
0.00.051.864 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.864 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.864 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.864 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.864 I print_info: LF token         = 187 'Ċ'
0.00.051.865 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.869 I print_info: max token length = 1024
0.00.729.170 I load_tensors: offloading 24 repeating layers to GPU
0.00.729.186 I load_tensors: offloading output layer to GPU
0.00.729.186 I load_tensors: offloaded 25/25 layers to GPU
0.00.729.223 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.729.224 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.730.664 I llama_init_from_model: n_seq_max     = 1
0.00.730.670 I llama_init_from_model: n_ctx         = 2048
0.00.730.670 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.730.671 I llama_init_from_model: n_batch       = 2048
0.00.730.671 I llama_init_from_model: n_ubatch      = 512
0.00.730.672 I llama_init_from_model: flash_attn    = 0
0.00.730.674 I llama_init_from_model: freq_base     = 10000.0
0.00.730.674 I llama_init_from_model: freq_scale    = 1
0.00.730.685 I ggml_metal_init: allocating
0.00.730.784 I ggml_metal_init: found device: Apple M4
0.00.730.798 I ggml_metal_init: picking default device: Apple M4
0.00.732.583 I ggml_metal_init: using embedded metal library
0.00.738.885 I ggml_metal_init: GPU name:   Apple M4
0.00.738.889 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.738.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.738.891 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.738.892 I ggml_metal_init: simdgroup reduction   = true
0.00.738.892 I ggml_metal_init: simdgroup matrix mul. = true
0.00.738.892 I ggml_metal_init: has residency sets    = true
0.00.738.892 I ggml_metal_init: has bfloat            = true
0.00.738.893 I ggml_metal_init: use bfloat            = true
0.00.738.893 I ggml_metal_init: hasUnifiedMemory      = true
0.00.738.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.756.338 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.810.708 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.810.716 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.810.754 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.815.034 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.815.036 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.815.037 I llama_init_from_model: graph nodes  = 967
0.00.815.037 I llama_init_from_model: graph splits = 2
0.00.815.043 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.815.183 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.815.183 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.953 I main: llama threadpool init, n_threads = 4
0.00.875.997 I 
0.00.876.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.876.024 I 
0.00.876.199 I sampler seed: 1234
0.00.876.203 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.876.224 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.876.225 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.876.225 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.716.723 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.716.724 I llama_perf_context_print:        load time =     866.13 ms
0.01.716.725 I llama_perf_context_print: prompt eval time =      51.93 ms /     7 tokens (    7.42 ms per token,   134.80 tokens per second)
0.01.716.725 I llama_perf_context_print:        eval time =     785.72 ms /    63 runs   (   12.47 ms per token,    80.18 tokens per second)
0.01.716.726 I llama_perf_context_print:       total time =     841.69 ms /    70 tokens
0.01.716.967 I ggml_metal_free: deallocating

real	0m1.734s
user	0m0.110s
sys	0m0.227s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.408 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.986 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.991 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.992 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.993 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.993 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.993 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.995 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.998 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.998 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.999 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.999 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.999 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.000 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.000 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.005 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.569 I llama_model_loader: - type  f32:  194 tensors
0.00.025.569 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.570 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.570 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.570 I print_info: file format = GGUF V3 (latest)
0.00.025.571 I print_info: file type   = Q2_K - Medium
0.00.025.576 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.637 I load: special tokens cache size = 25
0.00.039.740 I load: token to piece cache size = 0.2984 MB
0.00.039.743 I print_info: arch             = gptneox
0.00.039.743 I print_info: vocab_only       = 0
0.00.039.744 I print_info: n_ctx_train      = 2048
0.00.039.744 I print_info: n_embd           = 2048
0.00.039.744 I print_info: n_layer          = 24
0.00.039.747 I print_info: n_head           = 16
0.00.039.747 I print_info: n_head_kv        = 16
0.00.039.747 I print_info: n_rot            = 32
0.00.039.749 I print_info: n_swa            = 0
0.00.039.749 I print_info: n_embd_head_k    = 128
0.00.039.750 I print_info: n_embd_head_v    = 128
0.00.039.750 I print_info: n_gqa            = 1
0.00.039.751 I print_info: n_embd_k_gqa     = 2048
0.00.039.756 I print_info: n_embd_v_gqa     = 2048
0.00.039.756 I print_info: f_norm_eps       = 1.0e-05
0.00.039.757 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.757 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.757 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.757 I print_info: f_logit_scale    = 0.0e+00
0.00.039.758 I print_info: n_ff             = 8192
0.00.039.758 I print_info: n_expert         = 0
0.00.039.758 I print_info: n_expert_used    = 0
0.00.039.758 I print_info: causal attn      = 1
0.00.039.758 I print_info: pooling type     = 0
0.00.039.758 I print_info: rope type        = 2
0.00.039.762 I print_info: rope scaling     = linear
0.00.039.762 I print_info: freq_base_train  = 10000.0
0.00.039.762 I print_info: freq_scale_train = 1
0.00.039.764 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.764 I print_info: rope_finetuned   = unknown
0.00.039.764 I print_info: ssm_d_conv       = 0
0.00.039.765 I print_info: ssm_d_inner      = 0
0.00.039.765 I print_info: ssm_d_state      = 0
0.00.039.765 I print_info: ssm_dt_rank      = 0
0.00.039.765 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.765 I print_info: model type       = 1.4B
0.00.039.765 I print_info: model params     = 1.41 B
0.00.039.766 I print_info: general.name     = 1.4B
0.00.039.766 I print_info: vocab type       = BPE
0.00.039.766 I print_info: n_vocab          = 50304
0.00.039.766 I print_info: n_merges         = 50009
0.00.039.767 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.767 I print_info: LF token         = 187 'Ċ'
0.00.039.768 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.768 I print_info: max token length = 1024
0.00.360.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.360.150 I load_tensors: offloading output layer to GPU
0.00.360.151 I load_tensors: offloaded 25/25 layers to GPU
0.00.360.182 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.360.183 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.361.628 I llama_init_from_model: n_seq_max     = 1
0.00.361.633 I llama_init_from_model: n_ctx         = 2048
0.00.361.633 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.361.634 I llama_init_from_model: n_batch       = 2048
0.00.361.634 I llama_init_from_model: n_ubatch      = 512
0.00.361.635 I llama_init_from_model: flash_attn    = 0
0.00.361.637 I llama_init_from_model: freq_base     = 10000.0
0.00.361.641 I llama_init_from_model: freq_scale    = 1
0.00.361.644 I ggml_metal_init: allocating
0.00.361.744 I ggml_metal_init: found device: Apple M4
0.00.361.758 I ggml_metal_init: picking default device: Apple M4
0.00.363.602 I ggml_metal_init: using embedded metal library
0.00.369.087 I ggml_metal_init: GPU name:   Apple M4
0.00.369.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.369.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.369.108 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.369.108 I ggml_metal_init: simdgroup reduction   = true
0.00.369.109 I ggml_metal_init: simdgroup matrix mul. = true
0.00.369.109 I ggml_metal_init: has residency sets    = true
0.00.369.109 I ggml_metal_init: has bfloat            = true
0.00.369.109 I ggml_metal_init: use bfloat            = true
0.00.369.111 I ggml_metal_init: hasUnifiedMemory      = true
0.00.369.115 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.132 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.444.146 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.444.233 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.448.515 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.448.517 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.448.517 I llama_init_from_model: graph nodes  = 967
0.00.448.518 I llama_init_from_model: graph splits = 2
0.00.448.523 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.448.656 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.448.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.185 I main: llama threadpool init, n_threads = 4
0.00.509.228 I 
0.00.509.250 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.252 I 
0.00.509.422 I sampler seed: 1234
0.00.509.426 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.509.437 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.509.437 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.509.437 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.191.461 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.01.191.462 I llama_perf_context_print:        load time =     497.82 ms
0.01.191.462 I llama_perf_context_print: prompt eval time =      44.33 ms /     7 tokens (    6.33 ms per token,   157.91 tokens per second)
0.01.191.463 I llama_perf_context_print:        eval time =     634.87 ms /    63 runs   (   10.08 ms per token,    99.23 tokens per second)
0.01.191.464 I llama_perf_context_print:       total time =     683.24 ms /    70 tokens
0.01.191.689 I ggml_metal_free: deallocating

real	0m1.210s
user	0m0.111s
sys	0m0.178s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.743 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.545 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.550 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.552 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.559 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.559 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.560 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.314 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.041 I llama_model_loader: - type  f32:  194 tensors
0.00.025.041 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.041 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.042 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.042 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.042 I print_info: file format = GGUF V3 (latest)
0.00.025.043 I print_info: file type   = Q3_K - Medium
0.00.025.046 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.849 I load: special tokens cache size = 25
0.00.038.909 I load: token to piece cache size = 0.2984 MB
0.00.038.912 I print_info: arch             = gptneox
0.00.038.912 I print_info: vocab_only       = 0
0.00.038.912 I print_info: n_ctx_train      = 2048
0.00.038.912 I print_info: n_embd           = 2048
0.00.038.912 I print_info: n_layer          = 24
0.00.038.915 I print_info: n_head           = 16
0.00.038.916 I print_info: n_head_kv        = 16
0.00.038.916 I print_info: n_rot            = 32
0.00.038.916 I print_info: n_swa            = 0
0.00.038.916 I print_info: n_embd_head_k    = 128
0.00.038.917 I print_info: n_embd_head_v    = 128
0.00.038.917 I print_info: n_gqa            = 1
0.00.038.918 I print_info: n_embd_k_gqa     = 2048
0.00.038.919 I print_info: n_embd_v_gqa     = 2048
0.00.038.921 I print_info: f_norm_eps       = 1.0e-05
0.00.038.922 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.922 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.922 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.922 I print_info: f_logit_scale    = 0.0e+00
0.00.038.923 I print_info: n_ff             = 8192
0.00.038.923 I print_info: n_expert         = 0
0.00.038.923 I print_info: n_expert_used    = 0
0.00.038.925 I print_info: causal attn      = 1
0.00.038.926 I print_info: pooling type     = 0
0.00.038.926 I print_info: rope type        = 2
0.00.038.926 I print_info: rope scaling     = linear
0.00.038.927 I print_info: freq_base_train  = 10000.0
0.00.038.927 I print_info: freq_scale_train = 1
0.00.038.931 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.931 I print_info: rope_finetuned   = unknown
0.00.038.931 I print_info: ssm_d_conv       = 0
0.00.038.933 I print_info: ssm_d_inner      = 0
0.00.038.933 I print_info: ssm_d_state      = 0
0.00.038.933 I print_info: ssm_dt_rank      = 0
0.00.038.933 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.933 I print_info: model type       = 1.4B
0.00.038.934 I print_info: model params     = 1.41 B
0.00.038.934 I print_info: general.name     = 1.4B
0.00.038.934 I print_info: vocab type       = BPE
0.00.038.934 I print_info: n_vocab          = 50304
0.00.038.934 I print_info: n_merges         = 50009
0.00.038.935 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.935 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.935 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.935 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.935 I print_info: LF token         = 187 'Ċ'
0.00.038.936 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.936 I print_info: max token length = 1024
0.00.443.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.568 I load_tensors: offloading output layer to GPU
0.00.443.569 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.596 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.597 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.444.925 I llama_init_from_model: n_seq_max     = 1
0.00.444.933 I llama_init_from_model: n_ctx         = 2048
0.00.444.933 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.444.934 I llama_init_from_model: n_batch       = 2048
0.00.444.934 I llama_init_from_model: n_ubatch      = 512
0.00.444.934 I llama_init_from_model: flash_attn    = 0
0.00.444.939 I llama_init_from_model: freq_base     = 10000.0
0.00.444.939 I llama_init_from_model: freq_scale    = 1
0.00.444.941 I ggml_metal_init: allocating
0.00.444.991 I ggml_metal_init: found device: Apple M4
0.00.445.006 I ggml_metal_init: picking default device: Apple M4
0.00.446.712 I ggml_metal_init: using embedded metal library
0.00.452.690 I ggml_metal_init: GPU name:   Apple M4
0.00.452.702 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.452.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.452.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.452.705 I ggml_metal_init: simdgroup reduction   = true
0.00.452.706 I ggml_metal_init: simdgroup matrix mul. = true
0.00.452.706 I ggml_metal_init: has residency sets    = true
0.00.452.706 I ggml_metal_init: has bfloat            = true
0.00.452.706 I ggml_metal_init: use bfloat            = true
0.00.452.709 I ggml_metal_init: hasUnifiedMemory      = true
0.00.452.712 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.473.414 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.535.086 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.535.094 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.535.131 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.539.475 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.539.476 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.539.477 I llama_init_from_model: graph nodes  = 967
0.00.539.477 I llama_init_from_model: graph splits = 2
0.00.539.482 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.539.610 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.539.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.599.170 I main: llama threadpool init, n_threads = 4
0.00.599.209 I 
0.00.599.234 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.599.238 I 
0.00.599.388 I sampler seed: 1234
0.00.599.392 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.599.435 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.599.436 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.599.436 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.152 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49964.81 tokens per second)
0.01.342.152 I llama_perf_context_print:        load time =     589.51 ms
0.01.342.153 I llama_perf_context_print: prompt eval time =      49.20 ms /     7 tokens (    7.03 ms per token,   142.28 tokens per second)
0.01.342.154 I llama_perf_context_print:        eval time =     690.54 ms /    63 runs   (   10.96 ms per token,    91.23 tokens per second)
0.01.342.154 I llama_perf_context_print:       total time =     743.89 ms /    70 tokens
0.01.342.377 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.111s
sys	0m0.190s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.877 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.314 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.317 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.317 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.210 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.971 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.973 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.974 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.974 I llama_model_loader: - type  f32:  194 tensors
0.00.025.975 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.975 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.975 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.976 I print_info: file format = GGUF V3 (latest)
0.00.025.976 I print_info: file type   = Q4_K - Medium
0.00.025.977 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.124 I load: special tokens cache size = 25
0.00.040.228 I load: token to piece cache size = 0.2984 MB
0.00.040.231 I print_info: arch             = gptneox
0.00.040.231 I print_info: vocab_only       = 0
0.00.040.231 I print_info: n_ctx_train      = 2048
0.00.040.231 I print_info: n_embd           = 2048
0.00.040.232 I print_info: n_layer          = 24
0.00.040.234 I print_info: n_head           = 16
0.00.040.235 I print_info: n_head_kv        = 16
0.00.040.237 I print_info: n_rot            = 32
0.00.040.237 I print_info: n_swa            = 0
0.00.040.237 I print_info: n_embd_head_k    = 128
0.00.040.238 I print_info: n_embd_head_v    = 128
0.00.040.238 I print_info: n_gqa            = 1
0.00.040.239 I print_info: n_embd_k_gqa     = 2048
0.00.040.240 I print_info: n_embd_v_gqa     = 2048
0.00.040.240 I print_info: f_norm_eps       = 1.0e-05
0.00.040.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.241 I print_info: f_logit_scale    = 0.0e+00
0.00.040.242 I print_info: n_ff             = 8192
0.00.040.242 I print_info: n_expert         = 0
0.00.040.242 I print_info: n_expert_used    = 0
0.00.040.242 I print_info: causal attn      = 1
0.00.040.244 I print_info: pooling type     = 0
0.00.040.244 I print_info: rope type        = 2
0.00.040.244 I print_info: rope scaling     = linear
0.00.040.244 I print_info: freq_base_train  = 10000.0
0.00.040.245 I print_info: freq_scale_train = 1
0.00.040.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.245 I print_info: rope_finetuned   = unknown
0.00.040.245 I print_info: ssm_d_conv       = 0
0.00.040.246 I print_info: ssm_d_inner      = 0
0.00.040.246 I print_info: ssm_d_state      = 0
0.00.040.246 I print_info: ssm_dt_rank      = 0
0.00.040.247 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.247 I print_info: model type       = 1.4B
0.00.040.248 I print_info: model params     = 1.41 B
0.00.040.248 I print_info: general.name     = 1.4B
0.00.040.248 I print_info: vocab type       = BPE
0.00.040.249 I print_info: n_vocab          = 50304
0.00.040.249 I print_info: n_merges         = 50009
0.00.040.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.249 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.250 I print_info: LF token         = 187 'Ċ'
0.00.040.251 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.251 I print_info: max token length = 1024
0.00.517.771 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.785 I load_tensors: offloading output layer to GPU
0.00.517.786 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.818 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.819 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.519.133 I llama_init_from_model: n_seq_max     = 1
0.00.519.138 I llama_init_from_model: n_ctx         = 2048
0.00.519.139 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.139 I llama_init_from_model: n_batch       = 2048
0.00.519.139 I llama_init_from_model: n_ubatch      = 512
0.00.519.140 I llama_init_from_model: flash_attn    = 0
0.00.519.142 I llama_init_from_model: freq_base     = 10000.0
0.00.519.146 I llama_init_from_model: freq_scale    = 1
0.00.519.148 I ggml_metal_init: allocating
0.00.519.193 I ggml_metal_init: found device: Apple M4
0.00.519.206 I ggml_metal_init: picking default device: Apple M4
0.00.520.982 I ggml_metal_init: using embedded metal library
0.00.527.676 I ggml_metal_init: GPU name:   Apple M4
0.00.527.683 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.527.684 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.527.685 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.527.685 I ggml_metal_init: simdgroup reduction   = true
0.00.527.686 I ggml_metal_init: simdgroup matrix mul. = true
0.00.527.686 I ggml_metal_init: has residency sets    = true
0.00.527.686 I ggml_metal_init: has bfloat            = true
0.00.527.686 I ggml_metal_init: use bfloat            = true
0.00.527.687 I ggml_metal_init: hasUnifiedMemory      = true
0.00.527.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.979 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.593.188 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.593.194 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.593.230 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.400 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.598.402 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.598.402 I llama_init_from_model: graph nodes  = 967
0.00.598.403 I llama_init_from_model: graph splits = 2
0.00.598.409 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.598.533 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.598.534 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.851 I main: llama threadpool init, n_threads = 4
0.00.657.892 I 
0.00.657.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.914 I 
0.00.658.067 I sampler seed: 1234
0.00.658.071 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.658.090 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.658.090 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.658.090 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.420.752 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46284.22 tokens per second)
0.01.420.753 I llama_perf_context_print:        load time =     647.02 ms
0.01.420.754 I llama_perf_context_print: prompt eval time =      56.20 ms /     7 tokens (    8.03 ms per token,   124.55 tokens per second)
0.01.420.754 I llama_perf_context_print:        eval time =     703.87 ms /    63 runs   (   11.17 ms per token,    89.50 tokens per second)
0.01.420.755 I llama_perf_context_print:       total time =     763.86 ms /    70 tokens
0.01.421.017 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.110s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.022 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.650 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.656 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.661 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.662 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.662 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.663 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.663 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.666 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.666 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.667 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.673 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.673 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.411 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.195 I llama_model_loader: - type  f32:  194 tensors
0.00.025.195 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.195 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.196 I print_info: file format = GGUF V3 (latest)
0.00.025.196 I print_info: file type   = Q5_K - Medium
0.00.025.197 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.333 I load: special tokens cache size = 25
0.00.039.314 I load: token to piece cache size = 0.2984 MB
0.00.039.317 I print_info: arch             = gptneox
0.00.039.317 I print_info: vocab_only       = 0
0.00.039.318 I print_info: n_ctx_train      = 2048
0.00.039.318 I print_info: n_embd           = 2048
0.00.039.318 I print_info: n_layer          = 24
0.00.039.321 I print_info: n_head           = 16
0.00.039.322 I print_info: n_head_kv        = 16
0.00.039.322 I print_info: n_rot            = 32
0.00.039.322 I print_info: n_swa            = 0
0.00.039.322 I print_info: n_embd_head_k    = 128
0.00.039.323 I print_info: n_embd_head_v    = 128
0.00.039.324 I print_info: n_gqa            = 1
0.00.039.324 I print_info: n_embd_k_gqa     = 2048
0.00.039.325 I print_info: n_embd_v_gqa     = 2048
0.00.039.326 I print_info: f_norm_eps       = 1.0e-05
0.00.039.326 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.327 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.327 I print_info: f_logit_scale    = 0.0e+00
0.00.039.327 I print_info: n_ff             = 8192
0.00.039.328 I print_info: n_expert         = 0
0.00.039.328 I print_info: n_expert_used    = 0
0.00.039.328 I print_info: causal attn      = 1
0.00.039.328 I print_info: pooling type     = 0
0.00.039.330 I print_info: rope type        = 2
0.00.039.331 I print_info: rope scaling     = linear
0.00.039.332 I print_info: freq_base_train  = 10000.0
0.00.039.332 I print_info: freq_scale_train = 1
0.00.039.332 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.332 I print_info: rope_finetuned   = unknown
0.00.039.333 I print_info: ssm_d_conv       = 0
0.00.039.333 I print_info: ssm_d_inner      = 0
0.00.039.333 I print_info: ssm_d_state      = 0
0.00.039.333 I print_info: ssm_dt_rank      = 0
0.00.039.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.333 I print_info: model type       = 1.4B
0.00.039.334 I print_info: model params     = 1.41 B
0.00.039.334 I print_info: general.name     = 1.4B
0.00.039.334 I print_info: vocab type       = BPE
0.00.039.335 I print_info: n_vocab          = 50304
0.00.039.335 I print_info: n_merges         = 50009
0.00.039.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.339 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.340 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.340 I print_info: LF token         = 187 'Ċ'
0.00.039.340 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.340 I print_info: max token length = 1024
0.00.589.054 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.059 I load_tensors: offloading output layer to GPU
0.00.589.060 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.084 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.085 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.590.434 I llama_init_from_model: n_seq_max     = 1
0.00.590.436 I llama_init_from_model: n_ctx         = 2048
0.00.590.437 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.590.437 I llama_init_from_model: n_batch       = 2048
0.00.590.438 I llama_init_from_model: n_ubatch      = 512
0.00.590.438 I llama_init_from_model: flash_attn    = 0
0.00.590.439 I llama_init_from_model: freq_base     = 10000.0
0.00.590.439 I llama_init_from_model: freq_scale    = 1
0.00.590.444 I ggml_metal_init: allocating
0.00.590.465 I ggml_metal_init: found device: Apple M4
0.00.590.473 I ggml_metal_init: picking default device: Apple M4
0.00.591.866 I ggml_metal_init: using embedded metal library
0.00.597.831 I ggml_metal_init: GPU name:   Apple M4
0.00.597.835 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.836 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.837 I ggml_metal_init: simdgroup reduction   = true
0.00.597.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.838 I ggml_metal_init: has residency sets    = true
0.00.597.838 I ggml_metal_init: has bfloat            = true
0.00.597.838 I ggml_metal_init: use bfloat            = true
0.00.597.839 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.840 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.679 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.671.832 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.671.837 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.671.871 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.538 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.676.540 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.676.541 I llama_init_from_model: graph nodes  = 967
0.00.676.541 I llama_init_from_model: graph splits = 2
0.00.676.547 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.676.682 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.676.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.876 I main: llama threadpool init, n_threads = 4
0.00.730.916 I 
0.00.730.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.939 I 
0.00.731.068 I sampler seed: 1234
0.00.731.072 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.089 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.089 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.089 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.578.714 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.578.714 I llama_perf_context_print:        load time =     720.88 ms
0.01.578.715 I llama_perf_context_print: prompt eval time =      51.21 ms /     7 tokens (    7.32 ms per token,   136.70 tokens per second)
0.01.578.716 I llama_perf_context_print:        eval time =     793.52 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.578.716 I llama_perf_context_print:       total time =     848.81 ms /    70 tokens
0.01.578.972 I ggml_metal_free: deallocating

real	0m1.596s
user	0m0.108s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.308 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.894 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.900 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.902 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.902 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.906 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.906 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.907 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.907 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.907 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.911 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.701 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.555 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.556 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.557 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.557 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.558 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.558 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.558 I llama_model_loader: - type  f32:  194 tensors
0.00.026.559 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.559 I print_info: file format = GGUF V3 (latest)
0.00.026.560 I print_info: file type   = Q6_K
0.00.026.560 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.642 I load: special tokens cache size = 25
0.00.040.459 I load: token to piece cache size = 0.2984 MB
0.00.040.462 I print_info: arch             = gptneox
0.00.040.462 I print_info: vocab_only       = 0
0.00.040.462 I print_info: n_ctx_train      = 2048
0.00.040.463 I print_info: n_embd           = 2048
0.00.040.463 I print_info: n_layer          = 24
0.00.040.465 I print_info: n_head           = 16
0.00.040.466 I print_info: n_head_kv        = 16
0.00.040.466 I print_info: n_rot            = 32
0.00.040.466 I print_info: n_swa            = 0
0.00.040.467 I print_info: n_embd_head_k    = 128
0.00.040.467 I print_info: n_embd_head_v    = 128
0.00.040.467 I print_info: n_gqa            = 1
0.00.040.468 I print_info: n_embd_k_gqa     = 2048
0.00.040.469 I print_info: n_embd_v_gqa     = 2048
0.00.040.470 I print_info: f_norm_eps       = 1.0e-05
0.00.040.470 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.470 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.470 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.472 I print_info: f_logit_scale    = 0.0e+00
0.00.040.473 I print_info: n_ff             = 8192
0.00.040.473 I print_info: n_expert         = 0
0.00.040.473 I print_info: n_expert_used    = 0
0.00.040.474 I print_info: causal attn      = 1
0.00.040.474 I print_info: pooling type     = 0
0.00.040.474 I print_info: rope type        = 2
0.00.040.475 I print_info: rope scaling     = linear
0.00.040.477 I print_info: freq_base_train  = 10000.0
0.00.040.477 I print_info: freq_scale_train = 1
0.00.040.477 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.477 I print_info: rope_finetuned   = unknown
0.00.040.477 I print_info: ssm_d_conv       = 0
0.00.040.478 I print_info: ssm_d_inner      = 0
0.00.040.478 I print_info: ssm_d_state      = 0
0.00.040.478 I print_info: ssm_dt_rank      = 0
0.00.040.478 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.478 I print_info: model type       = 1.4B
0.00.040.479 I print_info: model params     = 1.41 B
0.00.040.479 I print_info: general.name     = 1.4B
0.00.040.480 I print_info: vocab type       = BPE
0.00.040.480 I print_info: n_vocab          = 50304
0.00.040.480 I print_info: n_merges         = 50009
0.00.040.481 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.481 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.481 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.482 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.482 I print_info: LF token         = 187 'Ċ'
0.00.040.482 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.482 I print_info: max token length = 1024
0.00.656.544 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.549 I load_tensors: offloading output layer to GPU
0.00.656.550 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.585 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.656.589 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.657.829 I llama_init_from_model: n_seq_max     = 1
0.00.657.831 I llama_init_from_model: n_ctx         = 2048
0.00.657.832 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.832 I llama_init_from_model: n_batch       = 2048
0.00.657.833 I llama_init_from_model: n_ubatch      = 512
0.00.657.833 I llama_init_from_model: flash_attn    = 0
0.00.657.834 I llama_init_from_model: freq_base     = 10000.0
0.00.657.835 I llama_init_from_model: freq_scale    = 1
0.00.657.836 I ggml_metal_init: allocating
0.00.657.854 I ggml_metal_init: found device: Apple M4
0.00.657.866 I ggml_metal_init: picking default device: Apple M4
0.00.659.291 I ggml_metal_init: using embedded metal library
0.00.665.249 I ggml_metal_init: GPU name:   Apple M4
0.00.665.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.254 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.256 I ggml_metal_init: simdgroup reduction   = true
0.00.665.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.256 I ggml_metal_init: has residency sets    = true
0.00.665.256 I ggml_metal_init: has bfloat            = true
0.00.665.257 I ggml_metal_init: use bfloat            = true
0.00.665.257 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.262 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.681.954 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.733 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.735.750 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.790 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.325 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.327 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.327 I llama_init_from_model: graph nodes  = 967
0.00.740.327 I llama_init_from_model: graph splits = 2
0.00.740.332 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.839 I main: llama threadpool init, n_threads = 4
0.00.799.877 I 
0.00.799.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.903 I 
0.00.800.021 I sampler seed: 1234
0.00.800.025 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.800.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.800.062 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.800.062 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.679.414 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51449.28 tokens per second)
0.01.679.414 I llama_perf_context_print:        load time =     788.64 ms
0.01.679.415 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.48 tokens per second)
0.01.679.416 I llama_perf_context_print:        eval time =     821.86 ms /    63 runs   (   13.05 ms per token,    76.66 tokens per second)
0.01.679.416 I llama_perf_context_print:       total time =     880.47 ms /    70 tokens
0.01.679.641 I ggml_metal_free: deallocating

real	0m1.699s
user	0m0.108s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.524 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.848 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.250 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.262 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.268 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.269 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.272 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.272 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.274 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.276 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.282 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.282 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.283 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.875 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.582 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.300 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.303 I llama_model_loader: - type  f32:  194 tensors
0.00.054.303 I llama_model_loader: - type  f16:   98 tensors
0.00.054.304 I print_info: file format = GGUF V3 (latest)
0.00.054.305 I print_info: file type   = all F32 (guessed)
0.00.054.307 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.141 I load: special tokens cache size = 25
0.00.077.088 I load: token to piece cache size = 0.2984 MB
0.00.077.092 I print_info: arch             = gptneox
0.00.077.092 I print_info: vocab_only       = 0
0.00.077.093 I print_info: n_ctx_train      = 2048
0.00.077.093 I print_info: n_embd           = 2048
0.00.077.093 I print_info: n_layer          = 24
0.00.077.098 I print_info: n_head           = 16
0.00.077.099 I print_info: n_head_kv        = 16
0.00.077.102 I print_info: n_rot            = 32
0.00.077.102 I print_info: n_swa            = 0
0.00.077.102 I print_info: n_embd_head_k    = 128
0.00.077.102 I print_info: n_embd_head_v    = 128
0.00.077.103 I print_info: n_gqa            = 1
0.00.077.104 I print_info: n_embd_k_gqa     = 2048
0.00.077.105 I print_info: n_embd_v_gqa     = 2048
0.00.077.106 I print_info: f_norm_eps       = 1.0e-05
0.00.077.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.077.108 I print_info: f_clamp_kqv      = 0.0e+00
0.00.077.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.077.109 I print_info: f_logit_scale    = 0.0e+00
0.00.077.109 I print_info: n_ff             = 8192
0.00.077.110 I print_info: n_expert         = 0
0.00.077.110 I print_info: n_expert_used    = 0
0.00.077.110 I print_info: causal attn      = 1
0.00.077.110 I print_info: pooling type     = 0
0.00.077.110 I print_info: rope type        = 2
0.00.077.111 I print_info: rope scaling     = linear
0.00.077.111 I print_info: freq_base_train  = 10000.0
0.00.077.116 I print_info: freq_scale_train = 1
0.00.077.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.077.116 I print_info: rope_finetuned   = unknown
0.00.077.117 I print_info: ssm_d_conv       = 0
0.00.077.117 I print_info: ssm_d_inner      = 0
0.00.077.117 I print_info: ssm_d_state      = 0
0.00.077.117 I print_info: ssm_dt_rank      = 0
0.00.077.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.077.118 I print_info: model type       = 1.4B
0.00.077.118 I print_info: model params     = 1.41 B
0.00.077.119 I print_info: general.name     = 1.4B
0.00.077.119 I print_info: vocab type       = BPE
0.00.077.120 I print_info: n_vocab          = 50304
0.00.077.120 I print_info: n_merges         = 50009
0.00.077.120 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.077.120 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.077.121 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.077.121 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.077.121 I print_info: LF token         = 187 'Ċ'
0.00.077.121 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.077.122 I print_info: max token length = 1024
0.01.157.371 I load_tensors: offloading 24 repeating layers to GPU
0.01.157.376 I load_tensors: offloading output layer to GPU
0.01.157.376 I load_tensors: offloaded 25/25 layers to GPU
0.01.157.399 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.157.401 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.158.549 I llama_init_from_model: n_seq_max     = 1
0.01.158.551 I llama_init_from_model: n_ctx         = 128
0.01.158.551 I llama_init_from_model: n_ctx_per_seq = 128
0.01.158.551 I llama_init_from_model: n_batch       = 128
0.01.158.551 I llama_init_from_model: n_ubatch      = 128
0.01.158.552 I llama_init_from_model: flash_attn    = 0
0.01.158.552 I llama_init_from_model: freq_base     = 10000.0
0.01.158.552 I llama_init_from_model: freq_scale    = 1
0.01.158.553 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.158.554 I ggml_metal_init: allocating
0.01.158.617 I ggml_metal_init: found device: Apple M4
0.01.158.622 I ggml_metal_init: picking default device: Apple M4
0.01.159.617 I ggml_metal_init: using embedded metal library
0.01.163.828 I ggml_metal_init: GPU name:   Apple M4
0.01.163.830 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.163.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.163.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.163.832 I ggml_metal_init: simdgroup reduction   = true
0.01.163.832 I ggml_metal_init: simdgroup matrix mul. = true
0.01.163.832 I ggml_metal_init: has residency sets    = true
0.01.163.832 I ggml_metal_init: has bfloat            = true
0.01.163.833 I ggml_metal_init: use bfloat            = true
0.01.163.833 I ggml_metal_init: hasUnifiedMemory      = true
0.01.163.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.175.018 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.176.815 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.176.817 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.176.842 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.178.529 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.178.530 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.178.531 I llama_init_from_model: graph nodes  = 967
0.01.178.531 I llama_init_from_model: graph splits = 2
0.01.178.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.178.532 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.213.139 I 
0.01.213.183 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.213.187 I perplexity: tokenizing the input ..
0.01.218.410 I perplexity: tokenization took 5.222 ms
0.01.218.415 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.337.605 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.339.608 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.339.641 I llama_perf_context_print:        load time =    1189.28 ms
0.01.339.642 I llama_perf_context_print: prompt eval time =     118.92 ms /   128 tokens (    0.93 ms per token,  1076.33 tokens per second)
0.01.339.643 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.339.643 I llama_perf_context_print:       total time =     126.50 ms /   129 tokens
0.01.340.059 I ggml_metal_free: deallocating

real	0m1.529s
user	0m0.103s
sys	0m0.240s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.272 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.903 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.014 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.023 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.024 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.024 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.025 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.026 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.027 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.027 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.029 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.030 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.033 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.033 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.033 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.826 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.639 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.641 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.641 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.641 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.642 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.642 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.643 I llama_model_loader: - type  f32:  194 tensors
0.00.025.643 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.644 I print_info: file format = GGUF V3 (latest)
0.00.025.644 I print_info: file type   = Q8_0
0.00.025.645 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.785 I load: special tokens cache size = 25
0.00.039.839 I load: token to piece cache size = 0.2984 MB
0.00.039.843 I print_info: arch             = gptneox
0.00.039.843 I print_info: vocab_only       = 0
0.00.039.843 I print_info: n_ctx_train      = 2048
0.00.039.844 I print_info: n_embd           = 2048
0.00.039.844 I print_info: n_layer          = 24
0.00.039.848 I print_info: n_head           = 16
0.00.039.849 I print_info: n_head_kv        = 16
0.00.039.852 I print_info: n_rot            = 32
0.00.039.852 I print_info: n_swa            = 0
0.00.039.852 I print_info: n_embd_head_k    = 128
0.00.039.852 I print_info: n_embd_head_v    = 128
0.00.039.853 I print_info: n_gqa            = 1
0.00.039.854 I print_info: n_embd_k_gqa     = 2048
0.00.039.854 I print_info: n_embd_v_gqa     = 2048
0.00.039.858 I print_info: f_norm_eps       = 1.0e-05
0.00.039.859 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.859 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.859 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.859 I print_info: f_logit_scale    = 0.0e+00
0.00.039.860 I print_info: n_ff             = 8192
0.00.039.860 I print_info: n_expert         = 0
0.00.039.860 I print_info: n_expert_used    = 0
0.00.039.860 I print_info: causal attn      = 1
0.00.039.860 I print_info: pooling type     = 0
0.00.039.861 I print_info: rope type        = 2
0.00.039.861 I print_info: rope scaling     = linear
0.00.039.861 I print_info: freq_base_train  = 10000.0
0.00.039.863 I print_info: freq_scale_train = 1
0.00.039.863 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.863 I print_info: rope_finetuned   = unknown
0.00.039.863 I print_info: ssm_d_conv       = 0
0.00.039.863 I print_info: ssm_d_inner      = 0
0.00.039.864 I print_info: ssm_d_state      = 0
0.00.039.864 I print_info: ssm_dt_rank      = 0
0.00.039.864 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.864 I print_info: model type       = 1.4B
0.00.039.864 I print_info: model params     = 1.41 B
0.00.039.865 I print_info: general.name     = 1.4B
0.00.039.865 I print_info: vocab type       = BPE
0.00.039.865 I print_info: n_vocab          = 50304
0.00.039.865 I print_info: n_merges         = 50009
0.00.039.866 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.866 I print_info: LF token         = 187 'Ċ'
0.00.039.866 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.867 I print_info: max token length = 1024
0.00.906.961 I load_tensors: offloading 24 repeating layers to GPU
0.00.906.964 I load_tensors: offloading output layer to GPU
0.00.906.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.906.992 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.906.995 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.908.110 I llama_init_from_model: n_seq_max     = 1
0.00.908.112 I llama_init_from_model: n_ctx         = 128
0.00.908.112 I llama_init_from_model: n_ctx_per_seq = 128
0.00.908.112 I llama_init_from_model: n_batch       = 128
0.00.908.112 I llama_init_from_model: n_ubatch      = 128
0.00.908.113 I llama_init_from_model: flash_attn    = 0
0.00.908.113 I llama_init_from_model: freq_base     = 10000.0
0.00.908.114 I llama_init_from_model: freq_scale    = 1
0.00.908.114 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.908.115 I ggml_metal_init: allocating
0.00.908.140 I ggml_metal_init: found device: Apple M4
0.00.908.149 I ggml_metal_init: picking default device: Apple M4
0.00.909.352 I ggml_metal_init: using embedded metal library
0.00.914.709 I ggml_metal_init: GPU name:   Apple M4
0.00.914.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.914.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.914.714 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.914.714 I ggml_metal_init: simdgroup reduction   = true
0.00.914.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.914.715 I ggml_metal_init: has residency sets    = true
0.00.914.715 I ggml_metal_init: has bfloat            = true
0.00.914.715 I ggml_metal_init: use bfloat            = true
0.00.914.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.914.717 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.929.989 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.933.328 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.933.332 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.933.370 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.936.557 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.936.558 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.936.559 I llama_init_from_model: graph nodes  = 967
0.00.936.559 I llama_init_from_model: graph splits = 2
0.00.936.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.936.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.961.134 I 
0.00.961.209 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.961.216 I perplexity: tokenizing the input ..
0.00.968.273 I perplexity: tokenization took 7.053 ms
0.00.968.281 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.093.618 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.094.964 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.094.989 I llama_perf_context_print:        load time =     951.22 ms
0.01.094.990 I llama_perf_context_print: prompt eval time =     124.36 ms /   128 tokens (    0.97 ms per token,  1029.28 tokens per second)
0.01.094.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.094.991 I llama_perf_context_print:       total time =     133.86 ms /   129 tokens
0.01.095.378 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.076s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.271 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.107 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.097 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.103 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.109 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.110 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.114 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.114 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.808 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.905 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.850 I llama_model_loader: - type  f32:  194 tensors
0.00.025.850 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.851 I print_info: file format = GGUF V3 (latest)
0.00.025.852 I print_info: file type   = Q4_0
0.00.025.853 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.151 I load: special tokens cache size = 25
0.00.039.995 I load: token to piece cache size = 0.2984 MB
0.00.040.000 I print_info: arch             = gptneox
0.00.040.000 I print_info: vocab_only       = 0
0.00.040.000 I print_info: n_ctx_train      = 2048
0.00.040.000 I print_info: n_embd           = 2048
0.00.040.000 I print_info: n_layer          = 24
0.00.040.005 I print_info: n_head           = 16
0.00.040.006 I print_info: n_head_kv        = 16
0.00.040.006 I print_info: n_rot            = 32
0.00.040.008 I print_info: n_swa            = 0
0.00.040.008 I print_info: n_embd_head_k    = 128
0.00.040.008 I print_info: n_embd_head_v    = 128
0.00.040.009 I print_info: n_gqa            = 1
0.00.040.010 I print_info: n_embd_k_gqa     = 2048
0.00.040.010 I print_info: n_embd_v_gqa     = 2048
0.00.040.011 I print_info: f_norm_eps       = 1.0e-05
0.00.040.011 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.011 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.012 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.012 I print_info: f_logit_scale    = 0.0e+00
0.00.040.012 I print_info: n_ff             = 8192
0.00.040.013 I print_info: n_expert         = 0
0.00.040.015 I print_info: n_expert_used    = 0
0.00.040.015 I print_info: causal attn      = 1
0.00.040.015 I print_info: pooling type     = 0
0.00.040.016 I print_info: rope type        = 2
0.00.040.016 I print_info: rope scaling     = linear
0.00.040.016 I print_info: freq_base_train  = 10000.0
0.00.040.018 I print_info: freq_scale_train = 1
0.00.040.018 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.018 I print_info: rope_finetuned   = unknown
0.00.040.019 I print_info: ssm_d_conv       = 0
0.00.040.019 I print_info: ssm_d_inner      = 0
0.00.040.019 I print_info: ssm_d_state      = 0
0.00.040.019 I print_info: ssm_dt_rank      = 0
0.00.040.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.019 I print_info: model type       = 1.4B
0.00.040.020 I print_info: model params     = 1.41 B
0.00.040.020 I print_info: general.name     = 1.4B
0.00.040.021 I print_info: vocab type       = BPE
0.00.040.021 I print_info: n_vocab          = 50304
0.00.040.021 I print_info: n_merges         = 50009
0.00.040.021 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.022 I print_info: LF token         = 187 'Ċ'
0.00.040.022 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.022 I print_info: max token length = 1024
0.00.563.657 I load_tensors: offloading 24 repeating layers to GPU
0.00.563.672 I load_tensors: offloading output layer to GPU
0.00.563.672 I load_tensors: offloaded 25/25 layers to GPU
0.00.563.700 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.563.701 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.564.916 I llama_init_from_model: n_seq_max     = 1
0.00.564.921 I llama_init_from_model: n_ctx         = 128
0.00.564.922 I llama_init_from_model: n_ctx_per_seq = 128
0.00.564.923 I llama_init_from_model: n_batch       = 128
0.00.564.923 I llama_init_from_model: n_ubatch      = 128
0.00.564.924 I llama_init_from_model: flash_attn    = 0
0.00.564.926 I llama_init_from_model: freq_base     = 10000.0
0.00.564.927 I llama_init_from_model: freq_scale    = 1
0.00.564.928 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.564.933 I ggml_metal_init: allocating
0.00.565.001 I ggml_metal_init: found device: Apple M4
0.00.565.015 I ggml_metal_init: picking default device: Apple M4
0.00.566.643 I ggml_metal_init: using embedded metal library
0.00.572.052 I ggml_metal_init: GPU name:   Apple M4
0.00.572.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.572.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.572.070 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.572.070 I ggml_metal_init: simdgroup reduction   = true
0.00.572.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.572.071 I ggml_metal_init: has residency sets    = true
0.00.572.071 I ggml_metal_init: has bfloat            = true
0.00.572.071 I ggml_metal_init: use bfloat            = true
0.00.572.073 I ggml_metal_init: hasUnifiedMemory      = true
0.00.572.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.592.061 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.595.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.595.452 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.595.493 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.598.883 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.598.885 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.598.885 I llama_init_from_model: graph nodes  = 967
0.00.598.886 I llama_init_from_model: graph splits = 2
0.00.598.889 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.598.889 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.612 I 
0.00.627.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.707 I perplexity: tokenizing the input ..
0.00.635.433 I perplexity: tokenization took 7.717 ms
0.00.635.439 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.772.628 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.774.040 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.774.068 I llama_perf_context_print:        load time =     617.49 ms
0.00.774.069 I llama_perf_context_print: prompt eval time =     136.12 ms /   128 tokens (    1.06 ms per token,   940.34 tokens per second)
0.00.774.069 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.774.070 I llama_perf_context_print:       total time =     146.46 ms /   129 tokens
0.00.774.419 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.026 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.568 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.570 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.574 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.574 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.575 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.576 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.577 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.577 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.578 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.580 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.580 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.580 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.211 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.212 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.212 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.213 I llama_model_loader: - type  f32:  194 tensors
0.00.025.213 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.214 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.215 I print_info: file format = GGUF V3 (latest)
0.00.025.215 I print_info: file type   = Q4_1
0.00.025.216 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.394 I load: special tokens cache size = 25
0.00.040.315 I load: token to piece cache size = 0.2984 MB
0.00.040.321 I print_info: arch             = gptneox
0.00.040.321 I print_info: vocab_only       = 0
0.00.040.321 I print_info: n_ctx_train      = 2048
0.00.040.321 I print_info: n_embd           = 2048
0.00.040.322 I print_info: n_layer          = 24
0.00.040.325 I print_info: n_head           = 16
0.00.040.326 I print_info: n_head_kv        = 16
0.00.040.326 I print_info: n_rot            = 32
0.00.040.326 I print_info: n_swa            = 0
0.00.040.327 I print_info: n_embd_head_k    = 128
0.00.040.327 I print_info: n_embd_head_v    = 128
0.00.040.327 I print_info: n_gqa            = 1
0.00.040.331 I print_info: n_embd_k_gqa     = 2048
0.00.040.332 I print_info: n_embd_v_gqa     = 2048
0.00.040.332 I print_info: f_norm_eps       = 1.0e-05
0.00.040.333 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.333 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.333 I print_info: f_logit_scale    = 0.0e+00
0.00.040.334 I print_info: n_ff             = 8192
0.00.040.337 I print_info: n_expert         = 0
0.00.040.338 I print_info: n_expert_used    = 0
0.00.040.338 I print_info: causal attn      = 1
0.00.040.338 I print_info: pooling type     = 0
0.00.040.340 I print_info: rope type        = 2
0.00.040.340 I print_info: rope scaling     = linear
0.00.040.341 I print_info: freq_base_train  = 10000.0
0.00.040.341 I print_info: freq_scale_train = 1
0.00.040.341 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.341 I print_info: rope_finetuned   = unknown
0.00.040.363 I print_info: ssm_d_conv       = 0
0.00.040.365 I print_info: ssm_d_inner      = 0
0.00.040.365 I print_info: ssm_d_state      = 0
0.00.040.366 I print_info: ssm_dt_rank      = 0
0.00.040.366 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.367 I print_info: model type       = 1.4B
0.00.040.368 I print_info: model params     = 1.41 B
0.00.040.368 I print_info: general.name     = 1.4B
0.00.040.368 I print_info: vocab type       = BPE
0.00.040.368 I print_info: n_vocab          = 50304
0.00.040.368 I print_info: n_merges         = 50009
0.00.040.369 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.369 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.370 I print_info: LF token         = 187 'Ċ'
0.00.040.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: max token length = 1024
0.00.636.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.636.860 I load_tensors: offloading output layer to GPU
0.00.636.860 I load_tensors: offloaded 25/25 layers to GPU
0.00.636.903 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.636.904 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.638.482 I llama_init_from_model: n_seq_max     = 1
0.00.638.487 I llama_init_from_model: n_ctx         = 128
0.00.638.488 I llama_init_from_model: n_ctx_per_seq = 128
0.00.638.492 I llama_init_from_model: n_batch       = 128
0.00.638.493 I llama_init_from_model: n_ubatch      = 128
0.00.638.493 I llama_init_from_model: flash_attn    = 0
0.00.638.502 I llama_init_from_model: freq_base     = 10000.0
0.00.638.502 I llama_init_from_model: freq_scale    = 1
0.00.638.503 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.638.505 I ggml_metal_init: allocating
0.00.638.617 I ggml_metal_init: found device: Apple M4
0.00.638.631 I ggml_metal_init: picking default device: Apple M4
0.00.640.418 I ggml_metal_init: using embedded metal library
0.00.647.282 I ggml_metal_init: GPU name:   Apple M4
0.00.647.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.647.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.647.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.647.292 I ggml_metal_init: simdgroup reduction   = true
0.00.647.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.647.293 I ggml_metal_init: has residency sets    = true
0.00.647.293 I ggml_metal_init: has bfloat            = true
0.00.647.293 I ggml_metal_init: use bfloat            = true
0.00.647.294 I ggml_metal_init: hasUnifiedMemory      = true
0.00.647.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.665.360 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.668.846 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.668.850 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.668.912 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.672.252 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.672.255 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.672.255 I llama_init_from_model: graph nodes  = 967
0.00.672.256 I llama_init_from_model: graph splits = 2
0.00.672.258 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.672.259 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.659 I 
0.00.700.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.749 I perplexity: tokenizing the input ..
0.00.708.035 I perplexity: tokenization took 7.283 ms
0.00.708.042 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.938 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.846.263 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.846.285 I llama_perf_context_print:        load time =     691.62 ms
0.00.846.286 I llama_perf_context_print: prompt eval time =     135.97 ms /   128 tokens (    1.06 ms per token,   941.40 tokens per second)
0.00.846.286 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.287 I llama_perf_context_print:       total time =     145.63 ms /   129 tokens
0.00.846.642 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.081s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.019 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.409 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.414 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.415 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.417 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.141 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.168 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.878 I llama_model_loader: - type  f32:  194 tensors
0.00.027.878 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.879 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.879 I print_info: file format = GGUF V3 (latest)
0.00.027.880 I print_info: file type   = Q5_0
0.00.027.881 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.637 I load: special tokens cache size = 25
0.00.041.551 I load: token to piece cache size = 0.2984 MB
0.00.041.554 I print_info: arch             = gptneox
0.00.041.554 I print_info: vocab_only       = 0
0.00.041.554 I print_info: n_ctx_train      = 2048
0.00.041.555 I print_info: n_embd           = 2048
0.00.041.555 I print_info: n_layer          = 24
0.00.041.558 I print_info: n_head           = 16
0.00.041.558 I print_info: n_head_kv        = 16
0.00.041.559 I print_info: n_rot            = 32
0.00.041.561 I print_info: n_swa            = 0
0.00.041.561 I print_info: n_embd_head_k    = 128
0.00.041.561 I print_info: n_embd_head_v    = 128
0.00.041.562 I print_info: n_gqa            = 1
0.00.041.562 I print_info: n_embd_k_gqa     = 2048
0.00.041.563 I print_info: n_embd_v_gqa     = 2048
0.00.041.568 I print_info: f_norm_eps       = 1.0e-05
0.00.041.569 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.571 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.571 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.571 I print_info: f_logit_scale    = 0.0e+00
0.00.041.572 I print_info: n_ff             = 8192
0.00.041.572 I print_info: n_expert         = 0
0.00.041.572 I print_info: n_expert_used    = 0
0.00.041.572 I print_info: causal attn      = 1
0.00.041.572 I print_info: pooling type     = 0
0.00.041.572 I print_info: rope type        = 2
0.00.041.573 I print_info: rope scaling     = linear
0.00.041.573 I print_info: freq_base_train  = 10000.0
0.00.041.573 I print_info: freq_scale_train = 1
0.00.041.573 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.574 I print_info: rope_finetuned   = unknown
0.00.041.577 I print_info: ssm_d_conv       = 0
0.00.041.577 I print_info: ssm_d_inner      = 0
0.00.041.577 I print_info: ssm_d_state      = 0
0.00.041.577 I print_info: ssm_dt_rank      = 0
0.00.041.577 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.577 I print_info: model type       = 1.4B
0.00.041.578 I print_info: model params     = 1.41 B
0.00.041.578 I print_info: general.name     = 1.4B
0.00.041.578 I print_info: vocab type       = BPE
0.00.041.579 I print_info: n_vocab          = 50304
0.00.041.579 I print_info: n_merges         = 50009
0.00.041.579 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.579 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.580 I print_info: LF token         = 187 'Ċ'
0.00.041.580 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.580 I print_info: max token length = 1024
0.00.689.814 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.828 I load_tensors: offloading output layer to GPU
0.00.689.829 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.877 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.689.880 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.691.405 I llama_init_from_model: n_seq_max     = 1
0.00.691.409 I llama_init_from_model: n_ctx         = 128
0.00.691.410 I llama_init_from_model: n_ctx_per_seq = 128
0.00.691.410 I llama_init_from_model: n_batch       = 128
0.00.691.411 I llama_init_from_model: n_ubatch      = 128
0.00.691.411 I llama_init_from_model: flash_attn    = 0
0.00.691.414 I llama_init_from_model: freq_base     = 10000.0
0.00.691.414 I llama_init_from_model: freq_scale    = 1
0.00.691.415 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.691.416 I ggml_metal_init: allocating
0.00.691.482 I ggml_metal_init: found device: Apple M4
0.00.691.496 I ggml_metal_init: picking default device: Apple M4
0.00.693.143 I ggml_metal_init: using embedded metal library
0.00.699.703 I ggml_metal_init: GPU name:   Apple M4
0.00.699.707 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.710 I ggml_metal_init: simdgroup reduction   = true
0.00.699.710 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.710 I ggml_metal_init: has residency sets    = true
0.00.699.710 I ggml_metal_init: has bfloat            = true
0.00.699.711 I ggml_metal_init: use bfloat            = true
0.00.699.711 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.136 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.720.567 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.720.570 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.720.610 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.723.721 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.723.723 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.723.723 I llama_init_from_model: graph nodes  = 967
0.00.723.724 I llama_init_from_model: graph splits = 2
0.00.723.726 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.723.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.010 I 
0.00.756.099 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.106 I perplexity: tokenizing the input ..
0.00.763.452 I perplexity: tokenization took 7.344 ms
0.00.763.458 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.906.288 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.907.735 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.907.757 I llama_perf_context_print:        load time =     744.98 ms
0.00.907.758 I llama_perf_context_print: prompt eval time =     142.28 ms /   128 tokens (    1.11 ms per token,   899.63 tokens per second)
0.00.907.759 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.907.759 I llama_perf_context_print:       total time =     151.75 ms /   129 tokens
0.00.908.125 I ggml_metal_free: deallocating

real	0m0.924s
user	0m0.078s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.933 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.781 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.788 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.789 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.789 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.789 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.790 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.791 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.794 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.794 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.795 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.797 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.797 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.363 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.364 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.364 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.365 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.365 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.365 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.366 I llama_model_loader: - type  f32:  194 tensors
0.00.024.366 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.366 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.367 I print_info: file format = GGUF V3 (latest)
0.00.024.367 I print_info: file type   = Q5_1
0.00.024.368 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.322 I load: special tokens cache size = 25
0.00.038.306 I load: token to piece cache size = 0.2984 MB
0.00.038.309 I print_info: arch             = gptneox
0.00.038.310 I print_info: vocab_only       = 0
0.00.038.310 I print_info: n_ctx_train      = 2048
0.00.038.310 I print_info: n_embd           = 2048
0.00.038.310 I print_info: n_layer          = 24
0.00.038.313 I print_info: n_head           = 16
0.00.038.314 I print_info: n_head_kv        = 16
0.00.038.314 I print_info: n_rot            = 32
0.00.038.314 I print_info: n_swa            = 0
0.00.038.315 I print_info: n_embd_head_k    = 128
0.00.038.315 I print_info: n_embd_head_v    = 128
0.00.038.316 I print_info: n_gqa            = 1
0.00.038.316 I print_info: n_embd_k_gqa     = 2048
0.00.038.317 I print_info: n_embd_v_gqa     = 2048
0.00.038.318 I print_info: f_norm_eps       = 1.0e-05
0.00.038.318 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.318 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.319 I print_info: f_logit_scale    = 0.0e+00
0.00.038.319 I print_info: n_ff             = 8192
0.00.038.319 I print_info: n_expert         = 0
0.00.038.320 I print_info: n_expert_used    = 0
0.00.038.320 I print_info: causal attn      = 1
0.00.038.320 I print_info: pooling type     = 0
0.00.038.320 I print_info: rope type        = 2
0.00.038.320 I print_info: rope scaling     = linear
0.00.038.321 I print_info: freq_base_train  = 10000.0
0.00.038.321 I print_info: freq_scale_train = 1
0.00.038.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.322 I print_info: rope_finetuned   = unknown
0.00.038.322 I print_info: ssm_d_conv       = 0
0.00.038.324 I print_info: ssm_d_inner      = 0
0.00.038.324 I print_info: ssm_d_state      = 0
0.00.038.324 I print_info: ssm_dt_rank      = 0
0.00.038.324 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.324 I print_info: model type       = 1.4B
0.00.038.325 I print_info: model params     = 1.41 B
0.00.038.325 I print_info: general.name     = 1.4B
0.00.038.325 I print_info: vocab type       = BPE
0.00.038.326 I print_info: n_vocab          = 50304
0.00.038.326 I print_info: n_merges         = 50009
0.00.038.326 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.326 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.327 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.327 I print_info: LF token         = 187 'Ċ'
0.00.038.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.329 I print_info: max token length = 1024
0.00.648.372 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.386 I load_tensors: offloading output layer to GPU
0.00.648.387 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.421 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.648.422 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.649.925 I llama_init_from_model: n_seq_max     = 1
0.00.649.929 I llama_init_from_model: n_ctx         = 128
0.00.649.930 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.931 I llama_init_from_model: n_batch       = 128
0.00.649.931 I llama_init_from_model: n_ubatch      = 128
0.00.649.932 I llama_init_from_model: flash_attn    = 0
0.00.649.934 I llama_init_from_model: freq_base     = 10000.0
0.00.649.935 I llama_init_from_model: freq_scale    = 1
0.00.649.936 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.939 I ggml_metal_init: allocating
0.00.650.018 I ggml_metal_init: found device: Apple M4
0.00.650.032 I ggml_metal_init: picking default device: Apple M4
0.00.651.814 I ggml_metal_init: using embedded metal library
0.00.658.336 I ggml_metal_init: GPU name:   Apple M4
0.00.658.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.342 I ggml_metal_init: simdgroup reduction   = true
0.00.658.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.342 I ggml_metal_init: has residency sets    = true
0.00.658.343 I ggml_metal_init: has bfloat            = true
0.00.658.343 I ggml_metal_init: use bfloat            = true
0.00.658.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.353 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.624 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.077 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.679.081 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.679.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.401 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.682.403 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.682.404 I llama_init_from_model: graph nodes  = 967
0.00.682.404 I llama_init_from_model: graph splits = 2
0.00.682.407 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.682.407 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.716.245 I 
0.00.716.325 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.334 I perplexity: tokenizing the input ..
0.00.723.295 I perplexity: tokenization took 6.957 ms
0.00.723.302 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.872.145 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.873.484 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.873.518 I llama_perf_context_print:        load time =     707.30 ms
0.00.873.519 I llama_perf_context_print: prompt eval time =     147.93 ms /   128 tokens (    1.16 ms per token,   865.30 tokens per second)
0.00.873.519 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.873.519 I llama_perf_context_print:       total time =     157.28 ms /   129 tokens
0.00.873.923 I ggml_metal_free: deallocating

real	0m0.888s
user	0m0.079s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.511 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.428 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.428 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.432 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.435 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.436 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.156 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.856 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.857 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.857 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.857 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.858 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.858 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.858 I llama_model_loader: - type  f32:  194 tensors
0.00.027.859 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.859 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.859 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.859 I print_info: file format = GGUF V3 (latest)
0.00.027.860 I print_info: file type   = Q2_K - Medium
0.00.027.864 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.035.543 I load: special tokens cache size = 25
0.00.041.456 I load: token to piece cache size = 0.2984 MB
0.00.041.459 I print_info: arch             = gptneox
0.00.041.459 I print_info: vocab_only       = 0
0.00.041.459 I print_info: n_ctx_train      = 2048
0.00.041.460 I print_info: n_embd           = 2048
0.00.041.460 I print_info: n_layer          = 24
0.00.041.463 I print_info: n_head           = 16
0.00.041.464 I print_info: n_head_kv        = 16
0.00.041.464 I print_info: n_rot            = 32
0.00.041.464 I print_info: n_swa            = 0
0.00.041.464 I print_info: n_embd_head_k    = 128
0.00.041.464 I print_info: n_embd_head_v    = 128
0.00.041.465 I print_info: n_gqa            = 1
0.00.041.466 I print_info: n_embd_k_gqa     = 2048
0.00.041.467 I print_info: n_embd_v_gqa     = 2048
0.00.041.467 I print_info: f_norm_eps       = 1.0e-05
0.00.041.468 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.468 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.468 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.468 I print_info: f_logit_scale    = 0.0e+00
0.00.041.469 I print_info: n_ff             = 8192
0.00.041.469 I print_info: n_expert         = 0
0.00.041.469 I print_info: n_expert_used    = 0
0.00.041.469 I print_info: causal attn      = 1
0.00.041.469 I print_info: pooling type     = 0
0.00.041.470 I print_info: rope type        = 2
0.00.041.472 I print_info: rope scaling     = linear
0.00.041.473 I print_info: freq_base_train  = 10000.0
0.00.041.473 I print_info: freq_scale_train = 1
0.00.041.473 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.473 I print_info: rope_finetuned   = unknown
0.00.041.474 I print_info: ssm_d_conv       = 0
0.00.041.474 I print_info: ssm_d_inner      = 0
0.00.041.474 I print_info: ssm_d_state      = 0
0.00.041.474 I print_info: ssm_dt_rank      = 0
0.00.041.474 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.474 I print_info: model type       = 1.4B
0.00.041.475 I print_info: model params     = 1.41 B
0.00.041.475 I print_info: general.name     = 1.4B
0.00.041.475 I print_info: vocab type       = BPE
0.00.041.475 I print_info: n_vocab          = 50304
0.00.041.476 I print_info: n_merges         = 50009
0.00.041.479 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.479 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.479 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.481 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.481 I print_info: LF token         = 187 'Ċ'
0.00.041.481 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.481 I print_info: max token length = 1024
0.00.351.328 I load_tensors: offloading 24 repeating layers to GPU
0.00.351.337 I load_tensors: offloading output layer to GPU
0.00.351.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.351.371 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.351.373 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.352.916 I llama_init_from_model: n_seq_max     = 1
0.00.352.921 I llama_init_from_model: n_ctx         = 128
0.00.352.922 I llama_init_from_model: n_ctx_per_seq = 128
0.00.352.922 I llama_init_from_model: n_batch       = 128
0.00.352.923 I llama_init_from_model: n_ubatch      = 128
0.00.352.923 I llama_init_from_model: flash_attn    = 0
0.00.352.926 I llama_init_from_model: freq_base     = 10000.0
0.00.352.926 I llama_init_from_model: freq_scale    = 1
0.00.352.927 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.352.929 I ggml_metal_init: allocating
0.00.352.992 I ggml_metal_init: found device: Apple M4
0.00.353.005 I ggml_metal_init: picking default device: Apple M4
0.00.354.689 I ggml_metal_init: using embedded metal library
0.00.360.227 I ggml_metal_init: GPU name:   Apple M4
0.00.360.239 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.360.240 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.360.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.360.241 I ggml_metal_init: simdgroup reduction   = true
0.00.360.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.360.242 I ggml_metal_init: has residency sets    = true
0.00.360.242 I ggml_metal_init: has bfloat            = true
0.00.360.243 I ggml_metal_init: use bfloat            = true
0.00.360.247 I ggml_metal_init: hasUnifiedMemory      = true
0.00.360.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.381.693 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.385.401 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.385.408 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.385.463 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.388.844 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.388.846 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.388.847 I llama_init_from_model: graph nodes  = 967
0.00.388.848 I llama_init_from_model: graph splits = 2
0.00.388.851 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.388.851 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.819 I 
0.00.416.898 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.905 I perplexity: tokenizing the input ..
0.00.423.764 I perplexity: tokenization took 6.858 ms
0.00.423.769 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.555.406 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.556.761 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.556.789 I llama_perf_context_print:        load time =     405.30 ms
0.00.556.790 I llama_perf_context_print: prompt eval time =     131.40 ms /   128 tokens (    1.03 ms per token,   974.11 tokens per second)
0.00.556.791 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.556.791 I llama_perf_context_print:       total time =     139.97 ms /   129 tokens
0.00.557.160 I ggml_metal_free: deallocating

real	0m0.572s
user	0m0.079s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.210 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.237 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.244 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.246 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.247 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.249 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.249 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.254 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.045 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.723 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.724 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.724 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.725 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.725 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.726 I llama_model_loader: - type  f32:  194 tensors
0.00.024.726 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.727 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.727 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.727 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.728 I print_info: file format = GGUF V3 (latest)
0.00.024.728 I print_info: file type   = Q3_K - Medium
0.00.024.729 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.347 I load: special tokens cache size = 25
0.00.038.166 I load: token to piece cache size = 0.2984 MB
0.00.038.170 I print_info: arch             = gptneox
0.00.038.171 I print_info: vocab_only       = 0
0.00.038.171 I print_info: n_ctx_train      = 2048
0.00.038.171 I print_info: n_embd           = 2048
0.00.038.171 I print_info: n_layer          = 24
0.00.038.177 I print_info: n_head           = 16
0.00.038.178 I print_info: n_head_kv        = 16
0.00.038.178 I print_info: n_rot            = 32
0.00.038.179 I print_info: n_swa            = 0
0.00.038.180 I print_info: n_embd_head_k    = 128
0.00.038.180 I print_info: n_embd_head_v    = 128
0.00.038.180 I print_info: n_gqa            = 1
0.00.038.181 I print_info: n_embd_k_gqa     = 2048
0.00.038.182 I print_info: n_embd_v_gqa     = 2048
0.00.038.182 I print_info: f_norm_eps       = 1.0e-05
0.00.038.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.183 I print_info: f_logit_scale    = 0.0e+00
0.00.038.184 I print_info: n_ff             = 8192
0.00.038.184 I print_info: n_expert         = 0
0.00.038.184 I print_info: n_expert_used    = 0
0.00.038.184 I print_info: causal attn      = 1
0.00.038.184 I print_info: pooling type     = 0
0.00.038.185 I print_info: rope type        = 2
0.00.038.185 I print_info: rope scaling     = linear
0.00.038.185 I print_info: freq_base_train  = 10000.0
0.00.038.186 I print_info: freq_scale_train = 1
0.00.038.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.186 I print_info: rope_finetuned   = unknown
0.00.038.186 I print_info: ssm_d_conv       = 0
0.00.038.186 I print_info: ssm_d_inner      = 0
0.00.038.186 I print_info: ssm_d_state      = 0
0.00.038.186 I print_info: ssm_dt_rank      = 0
0.00.038.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.187 I print_info: model type       = 1.4B
0.00.038.187 I print_info: model params     = 1.41 B
0.00.038.187 I print_info: general.name     = 1.4B
0.00.038.188 I print_info: vocab type       = BPE
0.00.038.188 I print_info: n_vocab          = 50304
0.00.038.188 I print_info: n_merges         = 50009
0.00.038.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: LF token         = 187 'Ċ'
0.00.038.189 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.190 I print_info: max token length = 1024
0.00.431.322 I load_tensors: offloading 24 repeating layers to GPU
0.00.431.335 I load_tensors: offloading output layer to GPU
0.00.431.336 I load_tensors: offloaded 25/25 layers to GPU
0.00.431.371 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.431.373 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.432.613 I llama_init_from_model: n_seq_max     = 1
0.00.432.622 I llama_init_from_model: n_ctx         = 128
0.00.432.623 I llama_init_from_model: n_ctx_per_seq = 128
0.00.432.623 I llama_init_from_model: n_batch       = 128
0.00.432.623 I llama_init_from_model: n_ubatch      = 128
0.00.432.624 I llama_init_from_model: flash_attn    = 0
0.00.432.627 I llama_init_from_model: freq_base     = 10000.0
0.00.432.627 I llama_init_from_model: freq_scale    = 1
0.00.432.628 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.432.630 I ggml_metal_init: allocating
0.00.432.703 I ggml_metal_init: found device: Apple M4
0.00.432.716 I ggml_metal_init: picking default device: Apple M4
0.00.434.537 I ggml_metal_init: using embedded metal library
0.00.440.029 I ggml_metal_init: GPU name:   Apple M4
0.00.440.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.440.048 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.440.049 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.440.049 I ggml_metal_init: simdgroup reduction   = true
0.00.440.049 I ggml_metal_init: simdgroup matrix mul. = true
0.00.440.050 I ggml_metal_init: has residency sets    = true
0.00.440.050 I ggml_metal_init: has bfloat            = true
0.00.440.050 I ggml_metal_init: use bfloat            = true
0.00.440.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.440.056 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.460.323 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.463.850 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.463.860 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.463.917 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.467.165 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.467.167 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.467.168 I llama_init_from_model: graph nodes  = 967
0.00.467.168 I llama_init_from_model: graph splits = 2
0.00.467.171 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.467.171 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.493.420 I 
0.00.493.515 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.493.525 I perplexity: tokenizing the input ..
0.00.500.251 I perplexity: tokenization took 6.723 ms
0.00.500.258 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.633.574 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.635.010 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.635.033 I llama_perf_context_print:        load time =     484.20 ms
0.00.635.034 I llama_perf_context_print: prompt eval time =     132.38 ms /   128 tokens (    1.03 ms per token,   966.93 tokens per second)
0.00.635.034 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.035 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.635.382 I ggml_metal_free: deallocating

real	0m0.649s
user	0m0.078s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.094 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.193 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.193 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.194 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.196 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.653 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.655 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.656 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.656 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.657 I llama_model_loader: - type  f32:  194 tensors
0.00.025.657 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.658 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.658 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.659 I print_info: file format = GGUF V3 (latest)
0.00.025.659 I print_info: file type   = Q4_K - Medium
0.00.025.660 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.780 I load: special tokens cache size = 25
0.00.039.830 I load: token to piece cache size = 0.2984 MB
0.00.039.834 I print_info: arch             = gptneox
0.00.039.834 I print_info: vocab_only       = 0
0.00.039.835 I print_info: n_ctx_train      = 2048
0.00.039.835 I print_info: n_embd           = 2048
0.00.039.835 I print_info: n_layer          = 24
0.00.039.839 I print_info: n_head           = 16
0.00.039.839 I print_info: n_head_kv        = 16
0.00.039.840 I print_info: n_rot            = 32
0.00.039.840 I print_info: n_swa            = 0
0.00.039.843 I print_info: n_embd_head_k    = 128
0.00.039.843 I print_info: n_embd_head_v    = 128
0.00.039.844 I print_info: n_gqa            = 1
0.00.039.845 I print_info: n_embd_k_gqa     = 2048
0.00.039.846 I print_info: n_embd_v_gqa     = 2048
0.00.039.847 I print_info: f_norm_eps       = 1.0e-05
0.00.039.847 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.849 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.849 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.849 I print_info: f_logit_scale    = 0.0e+00
0.00.039.850 I print_info: n_ff             = 8192
0.00.039.850 I print_info: n_expert         = 0
0.00.039.850 I print_info: n_expert_used    = 0
0.00.039.851 I print_info: causal attn      = 1
0.00.039.851 I print_info: pooling type     = 0
0.00.039.851 I print_info: rope type        = 2
0.00.039.851 I print_info: rope scaling     = linear
0.00.039.851 I print_info: freq_base_train  = 10000.0
0.00.039.855 I print_info: freq_scale_train = 1
0.00.039.856 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.856 I print_info: rope_finetuned   = unknown
0.00.039.856 I print_info: ssm_d_conv       = 0
0.00.039.856 I print_info: ssm_d_inner      = 0
0.00.039.856 I print_info: ssm_d_state      = 0
0.00.039.856 I print_info: ssm_dt_rank      = 0
0.00.039.857 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.857 I print_info: model type       = 1.4B
0.00.039.858 I print_info: model params     = 1.41 B
0.00.039.859 I print_info: general.name     = 1.4B
0.00.039.859 I print_info: vocab type       = BPE
0.00.039.859 I print_info: n_vocab          = 50304
0.00.039.859 I print_info: n_merges         = 50009
0.00.039.860 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.860 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.861 I print_info: LF token         = 187 'Ċ'
0.00.039.861 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.861 I print_info: max token length = 1024
0.00.527.039 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.044 I load_tensors: offloading output layer to GPU
0.00.527.045 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.062 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.066 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.527.917 I llama_init_from_model: n_seq_max     = 1
0.00.527.923 I llama_init_from_model: n_ctx         = 128
0.00.527.923 I llama_init_from_model: n_ctx_per_seq = 128
0.00.527.924 I llama_init_from_model: n_batch       = 128
0.00.527.924 I llama_init_from_model: n_ubatch      = 128
0.00.527.924 I llama_init_from_model: flash_attn    = 0
0.00.527.925 I llama_init_from_model: freq_base     = 10000.0
0.00.527.926 I llama_init_from_model: freq_scale    = 1
0.00.527.926 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.527.928 I ggml_metal_init: allocating
0.00.527.962 I ggml_metal_init: found device: Apple M4
0.00.527.970 I ggml_metal_init: picking default device: Apple M4
0.00.528.941 I ggml_metal_init: using embedded metal library
0.00.533.096 I ggml_metal_init: GPU name:   Apple M4
0.00.533.105 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.107 I ggml_metal_init: simdgroup reduction   = true
0.00.533.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.108 I ggml_metal_init: has residency sets    = true
0.00.533.108 I ggml_metal_init: has bfloat            = true
0.00.533.108 I ggml_metal_init: use bfloat            = true
0.00.533.110 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.375 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.550.940 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.550.943 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.550.972 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.552.430 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.552.431 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.552.431 I llama_init_from_model: graph nodes  = 967
0.00.552.432 I llama_init_from_model: graph splits = 2
0.00.552.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.552.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.110 I 
0.00.575.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.575.141 I perplexity: tokenizing the input ..
0.00.579.344 I perplexity: tokenization took 4.202 ms
0.00.579.348 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.712.500 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.713.854 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.713.873 I llama_perf_context_print:        load time =     565.01 ms
0.00.713.874 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   963.01 tokens per second)
0.00.713.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.875 I llama_perf_context_print:       total time =     138.76 ms /   129 tokens
0.00.714.220 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.071s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.908 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.821 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.829 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.830 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.831 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.831 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.831 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.832 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.833 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.833 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.834 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.834 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.834 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.777 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.953 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.793 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.793 I llama_model_loader: - type  f32:  194 tensors
0.00.026.794 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.794 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.795 I print_info: file format = GGUF V3 (latest)
0.00.026.795 I print_info: file type   = Q5_K - Medium
0.00.026.796 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.942 I load: special tokens cache size = 25
0.00.040.983 I load: token to piece cache size = 0.2984 MB
0.00.040.989 I print_info: arch             = gptneox
0.00.040.989 I print_info: vocab_only       = 0
0.00.040.989 I print_info: n_ctx_train      = 2048
0.00.040.989 I print_info: n_embd           = 2048
0.00.040.989 I print_info: n_layer          = 24
0.00.040.994 I print_info: n_head           = 16
0.00.040.995 I print_info: n_head_kv        = 16
0.00.040.995 I print_info: n_rot            = 32
0.00.040.995 I print_info: n_swa            = 0
0.00.040.995 I print_info: n_embd_head_k    = 128
0.00.040.995 I print_info: n_embd_head_v    = 128
0.00.040.996 I print_info: n_gqa            = 1
0.00.040.996 I print_info: n_embd_k_gqa     = 2048
0.00.040.997 I print_info: n_embd_v_gqa     = 2048
0.00.040.997 I print_info: f_norm_eps       = 1.0e-05
0.00.040.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.998 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.998 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.998 I print_info: f_logit_scale    = 0.0e+00
0.00.040.999 I print_info: n_ff             = 8192
0.00.040.999 I print_info: n_expert         = 0
0.00.040.999 I print_info: n_expert_used    = 0
0.00.040.999 I print_info: causal attn      = 1
0.00.041.000 I print_info: pooling type     = 0
0.00.041.000 I print_info: rope type        = 2
0.00.041.000 I print_info: rope scaling     = linear
0.00.041.000 I print_info: freq_base_train  = 10000.0
0.00.041.000 I print_info: freq_scale_train = 1
0.00.041.001 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.001 I print_info: rope_finetuned   = unknown
0.00.041.001 I print_info: ssm_d_conv       = 0
0.00.041.001 I print_info: ssm_d_inner      = 0
0.00.041.001 I print_info: ssm_d_state      = 0
0.00.041.001 I print_info: ssm_dt_rank      = 0
0.00.041.001 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.001 I print_info: model type       = 1.4B
0.00.041.002 I print_info: model params     = 1.41 B
0.00.041.002 I print_info: general.name     = 1.4B
0.00.041.002 I print_info: vocab type       = BPE
0.00.041.003 I print_info: n_vocab          = 50304
0.00.041.003 I print_info: n_merges         = 50009
0.00.041.003 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.003 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.003 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.005 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.005 I print_info: LF token         = 187 'Ċ'
0.00.041.005 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.005 I print_info: max token length = 1024
0.00.610.407 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.413 I load_tensors: offloading output layer to GPU
0.00.610.413 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.429 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.610.430 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.611.102 I llama_init_from_model: n_seq_max     = 1
0.00.611.106 I llama_init_from_model: n_ctx         = 128
0.00.611.106 I llama_init_from_model: n_ctx_per_seq = 128
0.00.611.108 I llama_init_from_model: n_batch       = 128
0.00.611.108 I llama_init_from_model: n_ubatch      = 128
0.00.611.109 I llama_init_from_model: flash_attn    = 0
0.00.611.109 I llama_init_from_model: freq_base     = 10000.0
0.00.611.110 I llama_init_from_model: freq_scale    = 1
0.00.611.111 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.611.111 I ggml_metal_init: allocating
0.00.611.167 I ggml_metal_init: found device: Apple M4
0.00.611.180 I ggml_metal_init: picking default device: Apple M4
0.00.612.147 I ggml_metal_init: using embedded metal library
0.00.616.114 I ggml_metal_init: GPU name:   Apple M4
0.00.616.120 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.120 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.121 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.121 I ggml_metal_init: simdgroup reduction   = true
0.00.616.122 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.122 I ggml_metal_init: has residency sets    = true
0.00.616.122 I ggml_metal_init: has bfloat            = true
0.00.616.123 I ggml_metal_init: use bfloat            = true
0.00.616.124 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.126 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.631.766 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.383 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.386 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.413 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.873 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.874 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.875 I llama_init_from_model: graph nodes  = 967
0.00.634.875 I llama_init_from_model: graph splits = 2
0.00.634.876 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.669 I 
0.00.663.700 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.704 I perplexity: tokenizing the input ..
0.00.667.882 I perplexity: tokenization took 4.176 ms
0.00.667.887 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.801 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.382 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.405 I llama_perf_context_print:        load time =     654.76 ms
0.00.809.406 I llama_perf_context_print: prompt eval time =     139.68 ms /   128 tokens (    1.09 ms per token,   916.39 tokens per second)
0.00.809.406 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.406 I llama_perf_context_print:       total time =     145.74 ms /   129 tokens
0.00.809.769 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.071s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.995 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.061 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.066 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.068 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.069 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.069 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.069 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.071 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.071 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.072 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.072 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.072 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.073 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.075 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.075 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.075 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.959 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.959 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.960 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.960 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.961 I llama_model_loader: - type  f32:  194 tensors
0.00.026.962 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.962 I print_info: file format = GGUF V3 (latest)
0.00.026.963 I print_info: file type   = Q6_K
0.00.026.964 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.574 I load: special tokens cache size = 25
0.00.041.679 I load: token to piece cache size = 0.2984 MB
0.00.041.685 I print_info: arch             = gptneox
0.00.041.685 I print_info: vocab_only       = 0
0.00.041.685 I print_info: n_ctx_train      = 2048
0.00.041.687 I print_info: n_embd           = 2048
0.00.041.687 I print_info: n_layer          = 24
0.00.041.691 I print_info: n_head           = 16
0.00.041.692 I print_info: n_head_kv        = 16
0.00.041.692 I print_info: n_rot            = 32
0.00.041.692 I print_info: n_swa            = 0
0.00.041.692 I print_info: n_embd_head_k    = 128
0.00.041.693 I print_info: n_embd_head_v    = 128
0.00.041.693 I print_info: n_gqa            = 1
0.00.041.694 I print_info: n_embd_k_gqa     = 2048
0.00.041.694 I print_info: n_embd_v_gqa     = 2048
0.00.041.695 I print_info: f_norm_eps       = 1.0e-05
0.00.041.695 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.695 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.695 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.699 I print_info: f_logit_scale    = 0.0e+00
0.00.041.699 I print_info: n_ff             = 8192
0.00.041.700 I print_info: n_expert         = 0
0.00.041.700 I print_info: n_expert_used    = 0
0.00.041.700 I print_info: causal attn      = 1
0.00.041.700 I print_info: pooling type     = 0
0.00.041.700 I print_info: rope type        = 2
0.00.041.700 I print_info: rope scaling     = linear
0.00.041.701 I print_info: freq_base_train  = 10000.0
0.00.041.701 I print_info: freq_scale_train = 1
0.00.041.701 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.701 I print_info: rope_finetuned   = unknown
0.00.041.701 I print_info: ssm_d_conv       = 0
0.00.041.701 I print_info: ssm_d_inner      = 0
0.00.041.701 I print_info: ssm_d_state      = 0
0.00.041.703 I print_info: ssm_dt_rank      = 0
0.00.041.703 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.703 I print_info: model type       = 1.4B
0.00.041.704 I print_info: model params     = 1.41 B
0.00.041.704 I print_info: general.name     = 1.4B
0.00.041.704 I print_info: vocab type       = BPE
0.00.041.704 I print_info: n_vocab          = 50304
0.00.041.704 I print_info: n_merges         = 50009
0.00.041.705 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.706 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.706 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.706 I print_info: LF token         = 187 'Ċ'
0.00.041.707 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.707 I print_info: max token length = 1024
0.00.392.339 I load_tensors: offloading 24 repeating layers to GPU
0.00.392.345 I load_tensors: offloading output layer to GPU
0.00.392.346 I load_tensors: offloaded 25/25 layers to GPU
0.00.392.387 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.392.390 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.393.131 I llama_init_from_model: n_seq_max     = 1
0.00.393.133 I llama_init_from_model: n_ctx         = 128
0.00.393.133 I llama_init_from_model: n_ctx_per_seq = 128
0.00.393.134 I llama_init_from_model: n_batch       = 128
0.00.393.134 I llama_init_from_model: n_ubatch      = 128
0.00.393.134 I llama_init_from_model: flash_attn    = 0
0.00.393.135 I llama_init_from_model: freq_base     = 10000.0
0.00.393.136 I llama_init_from_model: freq_scale    = 1
0.00.393.136 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.393.138 I ggml_metal_init: allocating
0.00.393.174 I ggml_metal_init: found device: Apple M4
0.00.393.188 I ggml_metal_init: picking default device: Apple M4
0.00.394.480 I ggml_metal_init: using embedded metal library
0.00.400.094 I ggml_metal_init: GPU name:   Apple M4
0.00.400.098 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.400.098 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.400.099 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.400.102 I ggml_metal_init: simdgroup reduction   = true
0.00.400.103 I ggml_metal_init: simdgroup matrix mul. = true
0.00.400.103 I ggml_metal_init: has residency sets    = true
0.00.400.103 I ggml_metal_init: has bfloat            = true
0.00.400.103 I ggml_metal_init: use bfloat            = true
0.00.400.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.400.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.414.376 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.417.003 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.417.006 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.417.049 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.419.686 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.419.687 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.419.688 I llama_init_from_model: graph nodes  = 967
0.00.419.688 I llama_init_from_model: graph splits = 2
0.00.419.690 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.419.691 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.454.275 I 
0.00.454.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.454.316 I perplexity: tokenizing the input ..
0.00.458.288 I perplexity: tokenization took 3.971 ms
0.00.458.296 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.597.218 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.598.557 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.598.584 I llama_perf_context_print:        load time =     443.28 ms
0.00.598.585 I llama_perf_context_print: prompt eval time =     138.69 ms /   128 tokens (    1.08 ms per token,   922.91 tokens per second)
0.00.598.586 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.598.586 I llama_perf_context_print:       total time =     144.31 ms /   129 tokens
0.00.598.981 I ggml_metal_free: deallocating

real	0m0.614s
user	0m0.072s
sys	0m0.098s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.243 I build: 4620 (396856b4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.499 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.290 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.302 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.312 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.313 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.314 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.326 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.327 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.601 I llama_model_loader: - type  f32:  194 tensors
0.00.053.602 I llama_model_loader: - type  f16:   98 tensors
0.00.053.603 I print_info: file format = GGUF V3 (latest)
0.00.053.604 I print_info: file type   = all F32 (guessed)
0.00.053.606 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.155 I load: special tokens cache size = 25
0.00.076.106 I load: token to piece cache size = 0.2984 MB
0.00.076.110 I print_info: arch             = gptneox
0.00.076.111 I print_info: vocab_only       = 0
0.00.076.111 I print_info: n_ctx_train      = 2048
0.00.076.111 I print_info: n_embd           = 2048
0.00.076.111 I print_info: n_layer          = 24
0.00.076.115 I print_info: n_head           = 16
0.00.076.116 I print_info: n_head_kv        = 16
0.00.076.116 I print_info: n_rot            = 32
0.00.076.116 I print_info: n_swa            = 0
0.00.076.117 I print_info: n_embd_head_k    = 128
0.00.076.117 I print_info: n_embd_head_v    = 128
0.00.076.118 I print_info: n_gqa            = 1
0.00.076.119 I print_info: n_embd_k_gqa     = 2048
0.00.076.119 I print_info: n_embd_v_gqa     = 2048
0.00.076.122 I print_info: f_norm_eps       = 1.0e-05
0.00.076.123 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.123 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.123 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.125 I print_info: f_logit_scale    = 0.0e+00
0.00.076.126 I print_info: n_ff             = 8192
0.00.076.126 I print_info: n_expert         = 0
0.00.076.127 I print_info: n_expert_used    = 0
0.00.076.127 I print_info: causal attn      = 1
0.00.076.127 I print_info: pooling type     = 0
0.00.076.127 I print_info: rope type        = 2
0.00.076.127 I print_info: rope scaling     = linear
0.00.076.128 I print_info: freq_base_train  = 10000.0
0.00.076.128 I print_info: freq_scale_train = 1
0.00.076.128 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.129 I print_info: rope_finetuned   = unknown
0.00.076.129 I print_info: ssm_d_conv       = 0
0.00.076.129 I print_info: ssm_d_inner      = 0
0.00.076.129 I print_info: ssm_d_state      = 0
0.00.076.129 I print_info: ssm_dt_rank      = 0
0.00.076.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.130 I print_info: model type       = 1.4B
0.00.076.130 I print_info: model params     = 1.41 B
0.00.076.131 I print_info: general.name     = 1.4B
0.00.076.133 I print_info: vocab type       = BPE
0.00.076.133 I print_info: n_vocab          = 50304
0.00.076.133 I print_info: n_merges         = 50009
0.00.076.134 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.134 I print_info: LF token         = 187 'Ċ'
0.00.076.135 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.135 I print_info: max token length = 1024
0.01.384.610 I load_tensors: offloading 24 repeating layers to GPU
0.01.384.614 I load_tensors: offloading output layer to GPU
0.01.384.615 I load_tensors: offloaded 25/25 layers to GPU
0.01.384.643 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.384.645 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.385.741 I llama_init_from_model: n_seq_max     = 1
0.01.385.742 I llama_init_from_model: n_ctx         = 128
0.01.385.742 I llama_init_from_model: n_ctx_per_seq = 128
0.01.385.742 I llama_init_from_model: n_batch       = 128
0.01.385.743 I llama_init_from_model: n_ubatch      = 128
0.01.385.746 I llama_init_from_model: flash_attn    = 0
0.01.385.747 I llama_init_from_model: freq_base     = 10000.0
0.01.385.747 I llama_init_from_model: freq_scale    = 1
0.01.385.748 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.385.748 I ggml_metal_init: allocating
0.01.385.844 I ggml_metal_init: found device: Apple M4
0.01.385.849 I ggml_metal_init: picking default device: Apple M4
0.01.386.987 I ggml_metal_init: using embedded metal library
0.01.390.995 I ggml_metal_init: GPU name:   Apple M4
0.01.390.998 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.390.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.390.999 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.390.999 I ggml_metal_init: simdgroup reduction   = true
0.01.390.999 I ggml_metal_init: simdgroup matrix mul. = true
0.01.391.000 I ggml_metal_init: has residency sets    = true
0.01.391.000 I ggml_metal_init: has bfloat            = true
0.01.391.000 I ggml_metal_init: use bfloat            = true
0.01.391.000 I ggml_metal_init: hasUnifiedMemory      = true
0.01.391.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.402.173 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.403.946 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.403.949 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.403.975 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.405.618 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.405.620 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.405.620 I llama_init_from_model: graph nodes  = 967
0.01.405.620 I llama_init_from_model: graph splits = 2
0.01.405.622 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.405.622 I 
0.01.405.657 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.405.658 I compute_imatrix: tokenizing the input ..
0.01.409.827 I compute_imatrix: tokenization took 4.168 ms
0.01.409.829 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.673.028 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.675.421 I llama_perf_context_print:        load time =    1650.52 ms
0.01.675.421 I llama_perf_context_print: prompt eval time =     261.46 ms /   128 tokens (    2.04 ms per token,   489.56 tokens per second)
0.01.675.422 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.675.422 I llama_perf_context_print:       total time =    1652.91 ms /   129 tokens
0.01.675.964 I ggml_metal_free: deallocating

real	0m1.862s
user	0m0.128s
sys	0m0.272s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4620 (396856b4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116e07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116e085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116e08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116e09150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116e09700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116e09cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116e0a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116e0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116e0adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116e0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116e0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116e0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116e0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116e0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116e0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116e0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116e0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116e0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x116e0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x116e0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116e10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116e10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x116e11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x116e119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116e12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116e123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116e129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116e13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116e13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116e142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116e145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116e14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116e15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116e15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116e15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116e15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116e16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116e168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116e16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116e17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116e176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116e17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116e17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116e182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116e188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116e18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116e197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116e19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116e1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116e1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116e1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116e1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116e1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116e1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116e1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116e1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116e1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116e1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116e1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116e1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116e1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116e1ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116e1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116e1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x116e1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116e1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116e20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116e205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116e20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116e20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x116e213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116e21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x116e21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116e22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116e22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116e22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x116e23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116e23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116e23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116e242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116e24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116e24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116e252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116e25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x116e25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116e262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116e26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116e26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116e272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116e27810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116e27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116e282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116e28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116e28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116e292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116e297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116e194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116e29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116e2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116e2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116e2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116e2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116e2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116e2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116e2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116e2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116e2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116e2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116e2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116e2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116e2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116e2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116e2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116e2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116e2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116e30040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116e304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116e30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116e30e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116e312c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116e31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116e31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116e320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116e32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116e329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116e32e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116e33320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116e337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116e33c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116e34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116e345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116e34a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116e34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116e35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116e35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116e35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116e36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116e36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116e36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116e36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116e373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x116e37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116e37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116e381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116e38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116e38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116e38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116e39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116e398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116e39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116e3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116e3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116e3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116e3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116e3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116e3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116e3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116e3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116e3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116e3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116e3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116e3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116e3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116e3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116e3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116e3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x116e3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116e3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116e3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116e3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116e40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116e407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116e40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116e41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116e415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116e41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116e41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116e423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116e42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116e42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116e43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116e43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116e43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116e43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116e44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116e448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x116e44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116e451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116e45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116e45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116e46070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116e465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116e46b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116e47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116e47320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116e47930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116e47f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116e48550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116e48d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116e491e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116e494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116e49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116e4a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116e4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116e4ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116e4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116e4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116e4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116e4c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116e4c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116e4ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116e4d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116e4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116e4de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116e4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116e4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116e4ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116e4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116e4f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116e4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116e50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116e508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116e50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116e51340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116e51890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116e51de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116e52330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116e52880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116e52dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116e53320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116e53870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116e53dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116e54310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116e54860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116e54db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116e55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116e55850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116e55da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116e562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116e56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116e56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116e572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116e57830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x116e57d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116e582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116e58820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116e58d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116e592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116e59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116e59d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116e5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116e5a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116e5ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116e5b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116e5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116e5bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116e5c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116e5c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116e5cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116e5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116e5d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116e5dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116e5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116e5e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116e5ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116e5f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116e5f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116e5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116e5fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116e60380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116e60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116e60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116e61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116e61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116e61aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x116e61f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116e623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x116e62880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116e62d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116e63270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116e63990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116e640b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116e647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116e64ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116e651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116e659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x116e65c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116e66270 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.704.833 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107a056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107a05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107a05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107a06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107a06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107a06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107a07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107a075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107a07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107a08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107a08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107a093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107a09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107a0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107a0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107a0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107a0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107a0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107a0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107a0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107a0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107a0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107a0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107a0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107a0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107a0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107a0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107a0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107a0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107a0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107a10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107a106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107a10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107a10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107a11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107a118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107a11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107a12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107a12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107a12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107a12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107a13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107a137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107a13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107a14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107a14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107a14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107a15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107a156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107a15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107a15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107a16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107a16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107a17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107a17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107a17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107a18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107a184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107a18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107a18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107a19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107a19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107a19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107a19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107a1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107a1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107a1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107a1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107a1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107a1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107a1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107a1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107a1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107a1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107a1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107a1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107a1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107a1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107a1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107a1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107a1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107a1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107a1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107a20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107a20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107a209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107a20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107a212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107a21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107a21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107a22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107a22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107a228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107a22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107a231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107a23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107a23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107a23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107a24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107a24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107a250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107a25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107a259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107a25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107a262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107a26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107a26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107a26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107a27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107a278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107a27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107a281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107a28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107a28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107a28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107a29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107a297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107a29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107a2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107a2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107a2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107a2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107a2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107a2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107a2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107a2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107a2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107a2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107a2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107a2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107a2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107a2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107a2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107a2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107a2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107a2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107a2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107a2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107a306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107a30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107a30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107a31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107a31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107a31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107a32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107a325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107a32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107a32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107a33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107a337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107a33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107a34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107a344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107a34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107a35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107a35e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107a36130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107a363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107a36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107a36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107a37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107a375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107a37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107a37e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107a38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107a38770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107a38be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107a39050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107a394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107a39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107a39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107a3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107a3a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107a3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107a3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107a3b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107a3b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107a3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107a3c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107a3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107a3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107a3ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107a3d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107a3d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107a3dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107a3e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107a3e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107a3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107a3ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107a3f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107a3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107a3fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107a400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107a40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107a409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107a40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107a41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107a417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107a41cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107a42830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107a42af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107a430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107a43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107a43c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107a441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107a447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107a44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107a458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107a45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107a46470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107a46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107a46ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107a475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107a47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107a48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107a486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107a48cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107a49270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107a49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107a49df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107a4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107a4a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107a4af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107a4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107a4bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107a4c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107a4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107a4cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107a4d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107a4d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107a4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107a4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107a4ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107a4f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107a4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107a4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107a50570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107a510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107a516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107a51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107a52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107a527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107a52db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107a53370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107a53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107a53ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107a544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107a54a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107a55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107a555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107a55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107a56170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107a56730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107a56cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107a571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107a576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107a57bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107a580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107a585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107a58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107a58ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107a594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107a599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107a59ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107a5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107a5a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107a5adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107a5b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107a5b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107a5c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107a5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107a5d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107a5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107a5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107a5e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107a5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107a5eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x107a5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x107a4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x107a4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x107a483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x107a45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x107a552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x107a52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x107a50830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x107a4e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x107a46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x107a43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x107a48f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x107a4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x107a4f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x107a4c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x107a541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x107a47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x107a513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x107a4ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x107a4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x107a47870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x107a558b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x107a44a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x107a43370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x107a455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x107a55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x107a4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x107a53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x107a49530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x107a4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x107a4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x107a472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x107a50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x107a51970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x107a46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x107a54770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x107a51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x107a4da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x107a569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x107a45030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x107a56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x107a444b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x107a54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x107a4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x107a50df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x107a53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x107a524f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x107a4a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x107a41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x107a04880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x107a5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x107a0bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x107a5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x107a5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x107a5f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x107a5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x107a5fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x107a5fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x107a5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x107a60280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x107a60540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x107a60800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x107a60ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x107a60d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x107a61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x107a61300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x107a615c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x107a61880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x107a61b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x107a61e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x107a620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x107a62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x107a62640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x107a62900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x107a62bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x107a62e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x107a63140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x107a63400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x107a636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x107a63980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x107a63c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x107a63f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x107a641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x107a64480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x107a64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x107a64a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x107a64cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x107a64f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x107a65240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x107a65500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x107a657c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x107a65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x107a65d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x107a66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x107a662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x107a66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x107a66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x107a66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x107a66dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x107a67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x107a67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x107a67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x107a678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x107a67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x107a67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x107a68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x107a683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x107a68680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x107a68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x107a68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x107a68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x107a69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x107a69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x107a69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x107a699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x107a69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x107a69f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x107a6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x107a6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x107a6a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x107a6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x107a6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x107a6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x107a6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x107a6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x107a6b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x107a6bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x107a6bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x107a6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x107a6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x107a6c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x107a6c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x107a6cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x107a6ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x107a6d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x107a6d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x107a6d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x107a6d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x107a6dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x107a6de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x107a6e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x107a6e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x107a6e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x107a6e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x107a6ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x107a6ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x107a6f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x107a6f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x107a6f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x107a6fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x107a6fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x107a6ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x107a70240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x107a70500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x107a707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x107a70a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x107a70d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x107a71000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x107a712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x107a71580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x107a71840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x107a71b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x107a71dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x107a72080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x107a72340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x107a72600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x107a728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x107a72b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x107a72e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x107a73100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x107a733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x107a73680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x107a73940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x107a73c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x107a73ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x107a74180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x107a74440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x107a74700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x107a749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x107a74c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x107a74f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x107a75200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x107a754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x107a75780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x107a75a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x107a75d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x107a75fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x107a76280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x107a76540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x107a76800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x107a76ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x107a76d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x107a77040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x107a77300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x107a775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x107a77880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x107a77b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x107a77e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x107a780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x107a78380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x107a78640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x107a78900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x107a78bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x107a78e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x107a79140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x107a79400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x107a796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x107a79980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x107a79c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x107a79f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x107a7a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x107a7a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x107a7aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x107a7ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x107a7b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x107a7b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x107a7bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x107a7c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x107a7c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x107a7ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x107a7d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x107a7d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x107a7dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x107a7e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x107a7e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x107a7ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x107a7f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x107a7f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x107a7fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x107a80210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x107a80760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x107a80cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x107a81200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x107a81750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x107a81ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x107a821f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x107a82740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x107a82c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x107a831e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x107a83730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x107a83c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x107a841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x107a84720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x107a84c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x107a851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x107a85710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x107a85c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x107a861b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x107a86700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x107a86c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x107a871a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x107a876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x107a87c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x107a88190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x107a886e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x107a88c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x107a89180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x107a896d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x107a89c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x107a8a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x107a8a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x107a8ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x107a8b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x107a8b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x107a8bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x107a8c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x107a8c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x107a8c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x107a8cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x107a8cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x107a8d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x107a8d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x107a8dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x107a8e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x107a8e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x107a8e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x107a8edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x107a8f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x107a8f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x107a8fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x107a8ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x107a90420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x107a90890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x107a90d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x107a919f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x107a92110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x107a92830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x107a92af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x107a92f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x107a93560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x107a93b70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.760s
user	0m0.279s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4620 (396856b4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15ae0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15ae0c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15ae0c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15ae0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15ae0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15ae0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15ae0dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15ae0e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15ae0e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15ae0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15ae0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15ae0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15ae102d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15ae10a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15ae11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15ae119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15ae120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15ae127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15ae12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15ae136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15ae13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15ae14520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15ae14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15ae154e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15ae15c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15ae15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15ae164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15ae17140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15ae17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15ae17940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15ae17de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15ae180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15ae18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15ae18e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15ae19130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15ae195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15ae19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15ae19f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15ae1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15ae1a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15ae1acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15ae1b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15ae1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15ae1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15ae1bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15ae1c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15ae1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15ae1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15ae1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15ae1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15ae1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15ae1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15ae1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15ae1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15ae1ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15ae203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15ae20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15ae20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15ae21130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15ae21920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15ae21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15ae22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15ae22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15ae229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15ae22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15ae23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15ae237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15ae23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15ae240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15ae24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15ae24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15ae24ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15ae25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15ae258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15ae25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15ae26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15ae268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15ae26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15ae27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15ae27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15ae27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15ae28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15ae28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15ae28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15ae29320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15ae29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15ae29dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15ae2a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15ae2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15ae2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15ae2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15ae2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15ae2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15ae2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15ae2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15ae2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15ae2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15ae1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15ae2d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15ae2df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15ae2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15ae2e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15ae2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15ae2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15ae2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15ae2fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15ae30430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15ae30980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15ae30ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15ae31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15ae31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15ae31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15ae32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15ae328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15ae32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15ae331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15ae33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15ae33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15ae33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15ae34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15ae34910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15ae34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15ae35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15ae356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15ae35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15ae36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15ae364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15ae36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15ae36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15ae372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15ae37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15ae37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15ae38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15ae38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15ae389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15ae38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15ae39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15ae397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15ae39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15ae3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15ae3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15ae3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15ae3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15ae3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15ae3b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15ae3bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15ae3c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15ae3c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15ae3ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15ae3cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15ae3d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15ae3d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15ae3dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15ae3e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15ae3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15ae3eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15ae3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15ae3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15ae3f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15ae3fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15ae40210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15ae406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15ae40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15ae40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15ae41490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15ae41930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15ae41dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15ae42270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15ae42710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15ae42bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15ae43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15ae434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15ae43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15ae43e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15ae442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15ae44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15ae44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15ae450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15ae45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15ae459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15ae45e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15ae46330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15ae467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15ae46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15ae47110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15ae475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15ae47a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15ae47ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15ae48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15ae48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15ae48cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15ae49170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15ae49610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15ae49b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15ae4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15ae4a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15ae4ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15ae4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15ae4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15ae4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15ae4c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15ae4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15ae4ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15ae4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15ae4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15ae4dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15ae4e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15ae4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15ae4ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15ae4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15ae4f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15ae4fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15ae503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15ae50920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15ae50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15ae513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15ae51910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15ae51e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15ae523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15ae52900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15ae52e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15ae533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15ae538f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15ae53e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15ae54390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15ae548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15ae54e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15ae55380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15ae558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15ae55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15ae56370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15ae568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15ae56e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15ae57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15ae578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15ae57e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15ae58350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15ae588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15ae58df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15ae59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15ae59890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15ae59de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15ae5a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15ae5a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15ae5add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15ae5b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15ae5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15ae5bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15ae5c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15ae5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15ae5cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15ae5d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15ae5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15ae5dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15ae5e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15ae5e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15ae5ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15ae5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15ae5f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15ae5fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15ae602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15ae60820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15ae60d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15ae612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15ae61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15ae61d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15ae622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15ae62750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15ae62bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15ae63090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15ae63530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15ae639d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15ae63e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15ae64310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15ae647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15ae64c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15ae650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15ae65590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15ae65a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15ae65ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15ae66370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15ae66810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15ae66d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15ae67480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15ae67ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15ae682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15ae689e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15ae68ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15ae69490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15ae69750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15ae69d60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.096.604 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15c804b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15c805000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15c805470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15c8058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15c805d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15c8061c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15c806630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15c806aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15c806f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15c807380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15c8077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15c807ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15c808a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15c8091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15c8099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15c80a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15c80a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15c80af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15c80b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15c80bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15c80c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15c80cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15c80d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15c80d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15c80e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15c80e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15c80e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15c80eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15c80ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15c80f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15c80f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15c80fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15c8101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15c8104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15c810920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15c810d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15c811200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15c811670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15c811ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15c811f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15c8123c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15c812830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15c812ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15c813110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15c813580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15c8139f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15c813e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15c8142d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15c814740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15c814bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15c815020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15c815490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15c815900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15c815d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15c8161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15c816650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15c816bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15c8170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15c817530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15c8179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15c817e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15c818280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15c8186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15c818b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15c818fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15c819440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15c8198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15c819d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15c81a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15c81a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15c81aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15c81aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15c81b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15c81b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15c81bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15c81c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15c81c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15c81c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15c81cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15c81d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15c81d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15c81db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15c81dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15c81e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15c81e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15c81ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15c81f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15c81f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15c81fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15c81fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15c820330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15c8207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15c820c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15c821080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15c8214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15c821960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15c821dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15c822240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15c8226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15c822b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15c822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15c823400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15c823870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15c823ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15c824150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15c8245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15c824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15c824ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15c825310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15c825780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15c825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15c826060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15c8264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15c826940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15c826db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15c827220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15c827690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15c827b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15c827f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15c8283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15c828850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15c828cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15c829130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15c8295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15c829a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15c829e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15c82a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15c82a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15c82abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15c82b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15c82b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15c82b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15c82bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15c82c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15c82c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15c82cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15c82cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15c82d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15c82d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15c82dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15c82e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15c82e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15c82e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15c82ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15c82f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15c82f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15c82fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15c830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15c830490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15c830900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15c830d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15c8311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15c831650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15c831ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15c831f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15c8323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15c832810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15c832c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15c8330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15c833560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15c8339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15c833e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15c8342b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15c834720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15c834b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15c835000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15c835c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15c835ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15c8361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15c836620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15c836a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15c836f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15c837370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15c8377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15c837c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15c8380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15c838530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15c8389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15c838e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15c839280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15c8396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15c839b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15c839fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15c83a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15c83a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15c83ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15c83b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15c83b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15c83ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15c83bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15c83c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15c83c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15c83cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15c83d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15c83d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15c83d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15c83ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15c83e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15c83e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15c83eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15c83efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15c83f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15c83f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15c83fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15c840300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15c840770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15c840be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15c841050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15c841570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15c841a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15c8425f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15c8428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15c842e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15c843430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15c8439f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15c843fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15c844570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15c844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15c8450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15c8456b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15c845c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15c846230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15c8467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15c846db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15c847370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15c847930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15c847ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15c8484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15c848a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15c849030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15c8495f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15c849bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15c84a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15c84a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15c84acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15c84b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15c84b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15c84be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15c84c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15c84c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15c84cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15c84d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15c84daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15c84e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15c84e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15c84ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15c84f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15c84f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15c84fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15c850330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15c8508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15c850eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15c851470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15c851a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15c851ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15c8525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15c852b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15c853130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15c8536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15c853cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15c854270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15c854830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15c854df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15c8553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15c855970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15c855f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15c8564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15c856ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15c856fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15c8574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15c8579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15c857eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15c8583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15c8588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15c858db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15c8592b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15c8597b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15c859cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15c85a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15c85a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15c85abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15c85b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15c85b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15c85bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15c85c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15c85ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15c85d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15c85d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15c85dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15c85e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15c85e8a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15ae69a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15ae4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15ae4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15ae4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15ae1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15ae1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15ae20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15ae4d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15ae16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15ae1cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15ae1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15ae1dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15ae1c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15ae1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15ae15180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15ae213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15ae2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15ae68f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15ae18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15ae18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15ae4de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15ae4c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15ae16790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15ae16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15ae16d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15ae6a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15ae6a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15ae6a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15ae6aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15ae6acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15ae6af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15ae6b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15ae6b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15ae6b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15ae6ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15ae6bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15ae6c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15ae6c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15ae6c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15ae6c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15ae6cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15ae6cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15ae6d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15ae6d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15ae6d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15ae6d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15ae6db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15ae6de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15ae6e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15ae6e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15ae6e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15ae6e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15ae6ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15ae6eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15ae6f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15ae6f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15ae6f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15ae6f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15ae6fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15ae6ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15ae70200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15ae704c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15ae70780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15ae70a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15ae70d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15ae70fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15ae71280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15ae71540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15ae71800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15ae71ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15ae71d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15ae72040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15ae72300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15ae725c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x104404230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1044046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x104404b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x104404f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1044053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x104405860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x104405cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x104406140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1044065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x104406a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x104406e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x104407300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x104407770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x104407be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x104408050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1044084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x104408930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x104408da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x104409210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x104409680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x104409af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x104409f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10440a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10440a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10440acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10440b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10440b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10440ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10440be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10440c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10440cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10440d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10440d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10440dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10440e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10440e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10440ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10440f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10440f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10440ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x104410450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x104410950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x104410e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x104411350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x104411850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x104411d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x104412250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x104412750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x104412c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x104413150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x104413650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x104413b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x104414050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x104414550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x104414a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x104414f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x104415450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x104415950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x104415e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x104416350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x104416850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x104416d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x104417250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x104417750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x104417c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x104418150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x104418650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x104418b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x104419050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x104419550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x104419a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x104419f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10441a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10441a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10441ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10441b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10441b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10441bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10441c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10441c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10441cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10441d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10441d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10441db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10441e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10441e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10441ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10441ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10441f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10441f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10441fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x104420350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x104420850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x104420d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x104421250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x104421750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x104421c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x104422150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x104422650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x104422b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x104423050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x104423550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x104423a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x104423f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x104424450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x104424950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x104424e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x104425350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x104425850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x104425d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x104426250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x104426750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x104426c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x104427150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x104427650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x104427b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x104428050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x104428550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x104428a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x104428f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x104429500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x104429ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10442a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10442a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10442ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10442b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10442b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10442c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10442c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10442c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10442cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10442d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10442dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10442e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10442e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10442e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10442f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10442f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10442fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x104430120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x104430670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x104430bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x104431110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x104431660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x104431bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x104432100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x104432650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x104432ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1044330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x104433640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x104433b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1044340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x104434630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x104434b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1044350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x104435620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x104435b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1044360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x104436610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x104436b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1044370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x104437600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x104437b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1044380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1044385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x104438b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x104439090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1044395e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x104439b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10443a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10443a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10443ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10443b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10443b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10443bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10443c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10443c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10443cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10443d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10443d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10443daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10443e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10443e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10443eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10443f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10443f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10443fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x104440020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x104440570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x104440ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x104441010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x104441560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x104441ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x104441f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1044423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x104442890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x104442d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1044431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x104443670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x104443b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x104443fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x104444450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1044448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x104444d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x104445230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1044456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x104445b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x104446010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x104446560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x104446c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1044473a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x104447ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1044481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1044484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x104448c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x104448f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x104449560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.235s
sys	0m0.188s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
