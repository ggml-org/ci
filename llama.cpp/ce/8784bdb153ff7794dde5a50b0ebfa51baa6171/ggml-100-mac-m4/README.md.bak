### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.21 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.15 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.23 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.90 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.99 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.99 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 223.39 sec*proc (27 tests)

Total Test time (real) = 223.40 sec

real	3m43.430s
user	7m44.941s
sys	0m6.190s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.20 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.21 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.90 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.39 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.38 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.05 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.03 sec*proc (27 tests)

Total Test time (real) =  51.04 sec

real	0m51.053s
user	1m11.809s
sys	0m5.622s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.111 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.587 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.597 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.598 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.599 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.600 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.601 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.602 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.603 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.603 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.604 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.607 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.608 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.608 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.609 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.609 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.610 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.611 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.136 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.138 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.139 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.140 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.140 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.029.141 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.141 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.029.142 I llama_model_loader: - type  f32:  124 tensors
0.00.029.143 I llama_model_loader: - type  f16:   73 tensors
0.00.033.919 I llm_load_vocab: special tokens cache size = 5
0.00.036.311 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.036.314 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.036.315 I llm_load_print_meta: arch             = bert
0.00.036.315 I llm_load_print_meta: vocab type       = WPM
0.00.036.316 I llm_load_print_meta: n_vocab          = 30522
0.00.036.316 I llm_load_print_meta: n_merges         = 0
0.00.036.316 I llm_load_print_meta: vocab_only       = 0
0.00.036.316 I llm_load_print_meta: n_ctx_train      = 512
0.00.036.317 I llm_load_print_meta: n_embd           = 384
0.00.036.317 I llm_load_print_meta: n_layer          = 12
0.00.036.345 I llm_load_print_meta: n_head           = 12
0.00.036.346 I llm_load_print_meta: n_head_kv        = 12
0.00.036.347 I llm_load_print_meta: n_rot            = 32
0.00.036.347 I llm_load_print_meta: n_swa            = 0
0.00.036.347 I llm_load_print_meta: n_embd_head_k    = 32
0.00.036.347 I llm_load_print_meta: n_embd_head_v    = 32
0.00.036.348 I llm_load_print_meta: n_gqa            = 1
0.00.036.349 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.036.350 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.036.351 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.036.351 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.036.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.036.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.036.352 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.036.353 I llm_load_print_meta: n_ff             = 1536
0.00.036.353 I llm_load_print_meta: n_expert         = 0
0.00.036.353 I llm_load_print_meta: n_expert_used    = 0
0.00.036.353 I llm_load_print_meta: causal attn      = 0
0.00.036.354 I llm_load_print_meta: pooling type     = 2
0.00.036.354 I llm_load_print_meta: rope type        = 2
0.00.036.354 I llm_load_print_meta: rope scaling     = linear
0.00.036.363 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.036.365 I llm_load_print_meta: freq_scale_train = 1
0.00.036.366 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.036.366 I llm_load_print_meta: rope_finetuned   = unknown
0.00.036.366 I llm_load_print_meta: ssm_d_conv       = 0
0.00.036.366 I llm_load_print_meta: ssm_d_inner      = 0
0.00.036.368 I llm_load_print_meta: ssm_d_state      = 0
0.00.036.368 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.036.369 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.036.381 I llm_load_print_meta: model type       = 33M
0.00.036.382 I llm_load_print_meta: model ftype      = F16
0.00.036.382 I llm_load_print_meta: model params     = 33.21 M
0.00.036.383 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.036.384 I llm_load_print_meta: general.name     = Bge Small
0.00.036.385 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.036.386 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.036.386 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.036.386 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.036.387 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.036.387 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.036.387 I llm_load_print_meta: max token length = 21
0.00.038.446 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.038.446 I llm_load_tensors: offloading output layer to GPU
0.00.038.448 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.038.477 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.038.478 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.039.080 I llama_new_context_with_model: n_seq_max     = 1
0.00.039.082 I llama_new_context_with_model: n_ctx         = 512
0.00.039.082 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.039.082 I llama_new_context_with_model: n_batch       = 2048
0.00.039.083 I llama_new_context_with_model: n_ubatch      = 2048
0.00.039.083 I llama_new_context_with_model: flash_attn    = 0
0.00.039.083 I llama_new_context_with_model: freq_base     = 10000.0
0.00.039.084 I llama_new_context_with_model: freq_scale    = 1
0.00.039.084 I ggml_metal_init: allocating
0.00.039.091 I ggml_metal_init: found device: Apple M4
0.00.039.094 I ggml_metal_init: picking default device: Apple M4
0.00.039.936 I ggml_metal_init: using embedded metal library
0.00.044.314 I ggml_metal_init: GPU name:   Apple M4
0.00.044.317 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.044.317 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.044.318 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.044.318 I ggml_metal_init: simdgroup reduction   = true
0.00.044.318 I ggml_metal_init: simdgroup matrix mul. = true
0.00.044.319 I ggml_metal_init: has bfloat            = true
0.00.044.319 I ggml_metal_init: use bfloat            = true
0.00.044.319 I ggml_metal_init: hasUnifiedMemory      = true
0.00.044.320 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.811 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.057.814 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.057.815 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.058.680 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.058.682 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.058.682 I llama_new_context_with_model: graph nodes  = 429
0.00.058.682 I llama_new_context_with_model: graph splits = 2
0.00.058.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.518 I 
0.00.065.549 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.235 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.025 I llama_perf_context_print:        load time =      47.21 ms
0.00.071.027 I llama_perf_context_print: prompt eval time =       4.65 ms /     9 tokens (    0.52 ms per token,  1937.57 tokens per second)
0.00.071.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.028 I llama_perf_context_print:       total time =       5.51 ms /    10 tokens
0.00.071.161 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.050s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.032 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.066 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.291 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.296 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.297 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.298 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.299 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.299 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.300 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.300 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.301 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.301 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.301 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.303 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.304 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.304 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.304 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.305 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.305 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.305 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.831 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.487 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.488 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.488 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.489 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.489 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.489 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.490 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.490 I llama_model_loader: - type  f32:  124 tensors
0.00.014.490 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.051 I llm_load_vocab: special tokens cache size = 5
0.00.018.447 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.450 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.450 I llm_load_print_meta: arch             = bert
0.00.018.450 I llm_load_print_meta: vocab type       = WPM
0.00.018.450 I llm_load_print_meta: n_vocab          = 30522
0.00.018.451 I llm_load_print_meta: n_merges         = 0
0.00.018.451 I llm_load_print_meta: vocab_only       = 0
0.00.018.451 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.451 I llm_load_print_meta: n_embd           = 384
0.00.018.451 I llm_load_print_meta: n_layer          = 12
0.00.018.459 I llm_load_print_meta: n_head           = 12
0.00.018.460 I llm_load_print_meta: n_head_kv        = 12
0.00.018.460 I llm_load_print_meta: n_rot            = 32
0.00.018.460 I llm_load_print_meta: n_swa            = 0
0.00.018.462 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.462 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.463 I llm_load_print_meta: n_gqa            = 1
0.00.018.464 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.464 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.465 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.465 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.465 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.465 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.466 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.466 I llm_load_print_meta: n_ff             = 1536
0.00.018.466 I llm_load_print_meta: n_expert         = 0
0.00.018.466 I llm_load_print_meta: n_expert_used    = 0
0.00.018.467 I llm_load_print_meta: causal attn      = 0
0.00.018.467 I llm_load_print_meta: pooling type     = 2
0.00.018.467 I llm_load_print_meta: rope type        = 2
0.00.018.467 I llm_load_print_meta: rope scaling     = linear
0.00.018.467 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.468 I llm_load_print_meta: freq_scale_train = 1
0.00.018.468 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.468 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.468 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.468 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.469 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.469 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.469 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.473 I llm_load_print_meta: model type       = 33M
0.00.018.474 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.474 I llm_load_print_meta: model params     = 33.21 M
0.00.018.474 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.475 I llm_load_print_meta: general.name     = Bge Small
0.00.018.475 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.475 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.475 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.475 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.476 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.476 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.476 I llm_load_print_meta: max token length = 21
0.00.019.711 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.711 I llm_load_tensors: offloading output layer to GPU
0.00.019.714 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.721 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.722 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.152 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.153 I llama_new_context_with_model: n_ctx         = 512
0.00.020.153 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.153 I llama_new_context_with_model: n_batch       = 2048
0.00.020.153 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.153 I llama_new_context_with_model: flash_attn    = 0
0.00.020.154 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.154 I llama_new_context_with_model: freq_scale    = 1
0.00.020.155 I ggml_metal_init: allocating
0.00.020.162 I ggml_metal_init: found device: Apple M4
0.00.020.164 I ggml_metal_init: picking default device: Apple M4
0.00.020.820 I ggml_metal_init: using embedded metal library
0.00.023.448 I ggml_metal_init: GPU name:   Apple M4
0.00.023.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.451 I ggml_metal_init: simdgroup reduction   = true
0.00.023.452 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.452 I ggml_metal_init: has bfloat            = true
0.00.023.452 I ggml_metal_init: use bfloat            = true
0.00.023.453 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.277 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.283 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.285 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.927 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.928 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.928 I llama_new_context_with_model: graph nodes  = 429
0.00.034.928 I llama_new_context_with_model: graph splits = 2
0.00.034.940 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.021 I 
0.00.040.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.604 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.065 I llama_perf_context_print:        load time =      30.95 ms
0.00.045.066 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2079.96 tokens per second)
0.00.045.067 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.067 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.045.253 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.156 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.735 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.332 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.337 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.340 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.350 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.351 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.352 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.353 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.354 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.355 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.355 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.356 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.360 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.360 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.361 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.356 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.359 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.359 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.360 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.360 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.361 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.361 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.361 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.362 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.362 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.362 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.363 I llama_model_loader: - type  f32:   41 tensors
0.00.050.363 I llama_model_loader: - type  f16:   29 tensors
0.00.068.842 W llm_load_vocab: empty token at index 5
0.00.073.630 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.961 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.988 I llm_load_vocab: special tokens cache size = 5
0.00.340.867 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.340.874 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.340.874 I llm_load_print_meta: arch             = jina-bert-v2
0.00.340.875 I llm_load_print_meta: vocab type       = BPE
0.00.340.875 I llm_load_print_meta: n_vocab          = 61056
0.00.340.875 I llm_load_print_meta: n_merges         = 39382
0.00.340.876 I llm_load_print_meta: vocab_only       = 0
0.00.340.876 I llm_load_print_meta: n_ctx_train      = 8192
0.00.340.876 I llm_load_print_meta: n_embd           = 384
0.00.340.876 I llm_load_print_meta: n_layer          = 4
0.00.340.905 I llm_load_print_meta: n_head           = 12
0.00.340.906 I llm_load_print_meta: n_head_kv        = 12
0.00.340.906 I llm_load_print_meta: n_rot            = 32
0.00.340.906 I llm_load_print_meta: n_swa            = 0
0.00.340.906 I llm_load_print_meta: n_embd_head_k    = 32
0.00.340.906 I llm_load_print_meta: n_embd_head_v    = 32
0.00.340.907 I llm_load_print_meta: n_gqa            = 1
0.00.340.907 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.340.907 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.340.908 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.340.909 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.340.909 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.340.909 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.340.909 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.340.910 I llm_load_print_meta: n_ff             = 1536
0.00.340.912 I llm_load_print_meta: n_expert         = 0
0.00.340.912 I llm_load_print_meta: n_expert_used    = 0
0.00.340.912 I llm_load_print_meta: causal attn      = 0
0.00.340.912 I llm_load_print_meta: pooling type     = -1
0.00.340.912 I llm_load_print_meta: rope type        = -1
0.00.340.912 I llm_load_print_meta: rope scaling     = linear
0.00.340.913 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.340.913 I llm_load_print_meta: freq_scale_train = 1
0.00.340.913 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.340.913 I llm_load_print_meta: rope_finetuned   = unknown
0.00.340.914 I llm_load_print_meta: ssm_d_conv       = 0
0.00.340.914 I llm_load_print_meta: ssm_d_inner      = 0
0.00.340.914 I llm_load_print_meta: ssm_d_state      = 0
0.00.340.914 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.340.914 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.340.933 I llm_load_print_meta: model type       = 33M
0.00.340.934 I llm_load_print_meta: model ftype      = F16
0.00.340.934 I llm_load_print_meta: model params     = 32.90 M
0.00.340.934 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.340.935 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.340.935 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.340.935 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.340.935 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.340.935 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.340.936 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.340.936 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.340.936 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.340.936 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.340.936 I llm_load_print_meta: max token length = 45
0.00.341.949 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.341.949 I llm_load_tensors: offloading output layer to GPU
0.00.341.949 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.341.973 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.341.974 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.342.698 I llama_new_context_with_model: n_seq_max     = 1
0.00.342.700 I llama_new_context_with_model: n_ctx         = 8192
0.00.342.700 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.342.700 I llama_new_context_with_model: n_batch       = 2048
0.00.342.700 I llama_new_context_with_model: n_ubatch      = 2048
0.00.342.700 I llama_new_context_with_model: flash_attn    = 0
0.00.342.701 I llama_new_context_with_model: freq_base     = 10000.0
0.00.342.701 I llama_new_context_with_model: freq_scale    = 1
0.00.342.701 I ggml_metal_init: allocating
0.00.342.711 I ggml_metal_init: found device: Apple M4
0.00.342.714 I ggml_metal_init: picking default device: Apple M4
0.00.343.579 I ggml_metal_init: using embedded metal library
0.00.346.292 I ggml_metal_init: GPU name:   Apple M4
0.00.346.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.346.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.346.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.346.295 I ggml_metal_init: simdgroup reduction   = true
0.00.346.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.346.295 I ggml_metal_init: has bfloat            = true
0.00.346.295 I ggml_metal_init: use bfloat            = true
0.00.346.295 I ggml_metal_init: hasUnifiedMemory      = true
0.00.346.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.358.489 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.358.493 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.358.496 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.359.075 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.359.076 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.359.077 I llama_new_context_with_model: graph nodes  = 154
0.00.359.077 I llama_new_context_with_model: graph splits = 2
0.00.359.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.371.559 I 
0.00.371.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.371.831 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.371.832 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.371.835 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.371.835 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.371.839 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.371.839 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.372.411 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.376.074 I llama_perf_context_print:        load time =     346.81 ms
0.00.376.075 I llama_perf_context_print: prompt eval time =       3.65 ms /    62 tokens (    0.06 ms per token, 16967.71 tokens per second)
0.00.376.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.376.080 I llama_perf_context_print:       total time =       4.52 ms /    63 tokens
0.00.376.284 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.348s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.098 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.195 I main: llama backend init
0.00.000.200 I main: load the model and apply lora adapter, if any
0.00.029.202 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.837 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.863 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.863 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.864 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.864 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.865 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.867 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.867 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.869 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.869 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.870 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.876 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.877 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.570 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.775 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.780 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.781 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.782 I llama_model_loader: - type  f32:  194 tensors
0.00.059.783 I llama_model_loader: - type  f16:   98 tensors
0.00.092.796 I llm_load_vocab: special tokens cache size = 25
0.00.100.026 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.029 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.029 I llm_load_print_meta: arch             = gptneox
0.00.100.030 I llm_load_print_meta: vocab type       = BPE
0.00.100.030 I llm_load_print_meta: n_vocab          = 50304
0.00.100.030 I llm_load_print_meta: n_merges         = 50009
0.00.100.030 I llm_load_print_meta: vocab_only       = 0
0.00.100.030 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.030 I llm_load_print_meta: n_embd           = 2048
0.00.100.031 I llm_load_print_meta: n_layer          = 24
0.00.100.054 I llm_load_print_meta: n_head           = 16
0.00.100.055 I llm_load_print_meta: n_head_kv        = 16
0.00.100.055 I llm_load_print_meta: n_rot            = 32
0.00.100.056 I llm_load_print_meta: n_swa            = 0
0.00.100.056 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.056 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.057 I llm_load_print_meta: n_gqa            = 1
0.00.100.057 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.058 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.059 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.061 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.061 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.061 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.061 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.062 I llm_load_print_meta: n_ff             = 8192
0.00.100.062 I llm_load_print_meta: n_expert         = 0
0.00.100.062 I llm_load_print_meta: n_expert_used    = 0
0.00.100.062 I llm_load_print_meta: causal attn      = 1
0.00.100.062 I llm_load_print_meta: pooling type     = 0
0.00.100.063 I llm_load_print_meta: rope type        = 2
0.00.100.063 I llm_load_print_meta: rope scaling     = linear
0.00.100.063 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.063 I llm_load_print_meta: freq_scale_train = 1
0.00.100.064 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.064 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.064 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.064 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.064 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.064 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.074 I llm_load_print_meta: model type       = 1.4B
0.00.100.075 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.075 I llm_load_print_meta: model params     = 1.41 B
0.00.100.076 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.076 I llm_load_print_meta: general.name     = 1.4B
0.00.100.076 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.076 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.077 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.077 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.077 I llm_load_print_meta: LF token         = 128 ''
0.00.100.077 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.077 I llm_load_print_meta: max token length = 1024
0.00.102.891 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.891 I llm_load_tensors: offloading output layer to GPU
0.00.102.891 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.911 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.913 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.927 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.928 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.928 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.928 I llama_new_context_with_model: n_batch       = 2048
0.00.103.928 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.929 I llama_new_context_with_model: flash_attn    = 0
0.00.103.929 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.929 I llama_new_context_with_model: freq_scale    = 1
0.00.103.930 I ggml_metal_init: allocating
0.00.103.939 I ggml_metal_init: found device: Apple M4
0.00.103.944 I ggml_metal_init: picking default device: Apple M4
0.00.104.720 I ggml_metal_init: using embedded metal library
0.00.155.713 I ggml_metal_init: GPU name:   Apple M4
0.00.155.719 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.155.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.155.720 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.155.720 I ggml_metal_init: simdgroup reduction   = true
0.00.155.720 I ggml_metal_init: simdgroup matrix mul. = true
0.00.155.720 I ggml_metal_init: has bfloat            = true
0.00.155.720 I ggml_metal_init: use bfloat            = true
0.00.155.721 I ggml_metal_init: hasUnifiedMemory      = true
0.00.155.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.243.242 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.243.249 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.243.275 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.244.614 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.244.615 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.244.616 I llama_new_context_with_model: graph nodes  = 967
0.00.244.616 I llama_new_context_with_model: graph splits = 2
0.00.244.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.340.560 I main: llama threadpool init, n_threads = 4
0.00.340.594 I 
0.00.340.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.340.628 I 
0.00.340.713 I sampler seed: 1234
0.00.340.718 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.340.743 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.340.745 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.340.745 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.199.724 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54996.13 tokens per second)
0.02.199.725 I llama_perf_context_print:        load time =     311.35 ms
0.02.199.726 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.42 tokens per second)
0.02.199.728 I llama_perf_context_print:        eval time =    1812.51 ms /    63 runs   (   28.77 ms per token,    34.76 tokens per second)
0.02.199.728 I llama_perf_context_print:       total time =    1859.17 ms /    70 tokens
0.02.199.948 I ggml_metal_free: deallocating

real	0m2.481s
user	0m0.152s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.537 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.784 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.163 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.182 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.186 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.190 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.191 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.197 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.198 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.198 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.908 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.909 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.911 I llama_model_loader: - type  f32:  194 tensors
0.00.053.911 I llama_model_loader: - type  f16:   98 tensors
0.00.084.393 I llm_load_vocab: special tokens cache size = 25
0.00.091.352 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.355 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.355 I llm_load_print_meta: arch             = gptneox
0.00.091.355 I llm_load_print_meta: vocab type       = BPE
0.00.091.356 I llm_load_print_meta: n_vocab          = 50304
0.00.091.356 I llm_load_print_meta: n_merges         = 50009
0.00.091.356 I llm_load_print_meta: vocab_only       = 0
0.00.091.356 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.356 I llm_load_print_meta: n_embd           = 2048
0.00.091.356 I llm_load_print_meta: n_layer          = 24
0.00.091.370 I llm_load_print_meta: n_head           = 16
0.00.091.371 I llm_load_print_meta: n_head_kv        = 16
0.00.091.371 I llm_load_print_meta: n_rot            = 32
0.00.091.371 I llm_load_print_meta: n_swa            = 0
0.00.091.373 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.373 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.374 I llm_load_print_meta: n_gqa            = 1
0.00.091.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.376 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.376 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.376 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.377 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.377 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.377 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.378 I llm_load_print_meta: n_ff             = 8192
0.00.091.378 I llm_load_print_meta: n_expert         = 0
0.00.091.378 I llm_load_print_meta: n_expert_used    = 0
0.00.091.378 I llm_load_print_meta: causal attn      = 1
0.00.091.378 I llm_load_print_meta: pooling type     = 0
0.00.091.379 I llm_load_print_meta: rope type        = 2
0.00.091.379 I llm_load_print_meta: rope scaling     = linear
0.00.091.379 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.380 I llm_load_print_meta: freq_scale_train = 1
0.00.091.380 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.380 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.380 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.380 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.380 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.380 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.381 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.390 I llm_load_print_meta: model type       = 1.4B
0.00.091.391 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.391 I llm_load_print_meta: model params     = 1.41 B
0.00.091.392 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.393 I llm_load_print_meta: general.name     = 1.4B
0.00.091.393 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.393 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.393 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.393 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.394 I llm_load_print_meta: LF token         = 128 ''
0.00.091.395 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.395 I llm_load_print_meta: max token length = 1024
0.00.093.965 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.965 I llm_load_tensors: offloading output layer to GPU
0.00.093.965 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.976 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.977 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.947 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.947 I llama_new_context_with_model: n_ctx         = 128
0.00.094.948 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.948 I llama_new_context_with_model: n_batch       = 128
0.00.094.948 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.948 I llama_new_context_with_model: flash_attn    = 0
0.00.094.949 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.949 I llama_new_context_with_model: freq_scale    = 1
0.00.094.949 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.950 I ggml_metal_init: allocating
0.00.094.957 I ggml_metal_init: found device: Apple M4
0.00.094.959 I ggml_metal_init: picking default device: Apple M4
0.00.095.575 I ggml_metal_init: using embedded metal library
0.00.098.162 I ggml_metal_init: GPU name:   Apple M4
0.00.098.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.164 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.165 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.165 I ggml_metal_init: simdgroup reduction   = true
0.00.098.165 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.165 I ggml_metal_init: has bfloat            = true
0.00.098.165 I ggml_metal_init: use bfloat            = true
0.00.098.166 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.166 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.582 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.585 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.599 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.531 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.532 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.532 I llama_new_context_with_model: graph nodes  = 967
0.00.109.532 I llama_new_context_with_model: graph splits = 2
0.00.109.544 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.123.116 I 
0.01.123.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.123.188 I perplexity: tokenizing the input ..
0.01.135.961 I perplexity: tokenization took 12.77 ms
0.01.135.991 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.256.969 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.258.815 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.258.840 I llama_perf_context_print:        load time =    1100.32 ms
0.01.258.842 I llama_perf_context_print: prompt eval time =     120.59 ms /   128 tokens (    0.94 ms per token,  1061.47 tokens per second)
0.01.258.843 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.258.844 I llama_perf_context_print:       total time =     135.73 ms /   129 tokens
0.01.259.602 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.125s
sys	0m0.215s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.704 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.388 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.403 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.403 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.403 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.342 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.416 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.424 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.424 I llama_model_loader: - type  f32:  194 tensors
0.00.034.425 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.205 I llm_load_vocab: special tokens cache size = 25
0.00.062.388 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.392 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.393 I llm_load_print_meta: arch             = gptneox
0.00.062.393 I llm_load_print_meta: vocab type       = BPE
0.00.062.393 I llm_load_print_meta: n_vocab          = 50304
0.00.062.393 I llm_load_print_meta: n_merges         = 50009
0.00.062.394 I llm_load_print_meta: vocab_only       = 0
0.00.062.394 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.394 I llm_load_print_meta: n_embd           = 2048
0.00.062.394 I llm_load_print_meta: n_layer          = 24
0.00.062.412 I llm_load_print_meta: n_head           = 16
0.00.062.414 I llm_load_print_meta: n_head_kv        = 16
0.00.062.414 I llm_load_print_meta: n_rot            = 32
0.00.062.414 I llm_load_print_meta: n_swa            = 0
0.00.062.414 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.415 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.415 I llm_load_print_meta: n_gqa            = 1
0.00.062.416 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.416 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.417 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.417 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.418 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.418 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.420 I llm_load_print_meta: n_ff             = 8192
0.00.062.420 I llm_load_print_meta: n_expert         = 0
0.00.062.420 I llm_load_print_meta: n_expert_used    = 0
0.00.062.421 I llm_load_print_meta: causal attn      = 1
0.00.062.421 I llm_load_print_meta: pooling type     = 0
0.00.062.421 I llm_load_print_meta: rope type        = 2
0.00.062.421 I llm_load_print_meta: rope scaling     = linear
0.00.062.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.421 I llm_load_print_meta: freq_scale_train = 1
0.00.062.422 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.422 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.422 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.423 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.423 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.423 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.423 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.433 I llm_load_print_meta: model type       = 1.4B
0.00.062.434 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.434 I llm_load_print_meta: model params     = 1.41 B
0.00.062.436 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.436 I llm_load_print_meta: general.name     = 1.4B
0.00.062.437 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.437 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.437 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.437 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.438 I llm_load_print_meta: LF token         = 128 ''
0.00.062.438 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.438 I llm_load_print_meta: max token length = 1024
0.00.064.807 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.807 I llm_load_tensors: offloading output layer to GPU
0.00.064.808 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.819 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.821 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.065.797 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.798 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.798 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.798 I llama_new_context_with_model: n_batch       = 2048
0.00.065.799 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.799 I llama_new_context_with_model: flash_attn    = 0
0.00.065.799 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.799 I llama_new_context_with_model: freq_scale    = 1
0.00.065.800 I ggml_metal_init: allocating
0.00.065.803 I ggml_metal_init: found device: Apple M4
0.00.065.805 I ggml_metal_init: picking default device: Apple M4
0.00.066.543 I ggml_metal_init: using embedded metal library
0.00.069.026 I ggml_metal_init: GPU name:   Apple M4
0.00.069.028 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.028 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.029 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.029 I ggml_metal_init: simdgroup reduction   = true
0.00.069.029 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.029 I ggml_metal_init: has bfloat            = true
0.00.069.029 I ggml_metal_init: use bfloat            = true
0.00.069.030 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.258 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.268 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.292 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.552 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.554 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.555 I llama_new_context_with_model: graph nodes  = 967
0.00.105.555 I llama_new_context_with_model: graph splits = 2
0.00.105.572 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.450.976 I main: llama threadpool init, n_threads = 4
0.01.451.023 I 
0.01.451.074 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.451.075 I 
0.01.451.421 I sampler seed: 1234
0.01.451.429 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.451.456 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.451.458 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.451.458 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.552.837 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.02.552.838 I llama_perf_context_print:        load time =    1441.27 ms
0.02.552.838 I llama_perf_context_print: prompt eval time =      51.02 ms /     7 tokens (    7.29 ms per token,   137.20 tokens per second)
0.02.552.839 I llama_perf_context_print:        eval time =    1047.36 ms /    63 runs   (   16.62 ms per token,    60.15 tokens per second)
0.02.552.840 I llama_perf_context_print:       total time =    1101.86 ms /    70 tokens
0.02.553.041 I ggml_metal_free: deallocating

real	0m2.571s
user	0m0.121s
sys	0m0.251s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.161 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.719 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.409 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.410 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.410 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.411 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.412 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.412 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.413 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.415 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.991 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.654 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.689 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.691 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.691 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.692 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.692 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.693 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.694 I llama_model_loader: - type  f32:  194 tensors
0.00.039.694 I llama_model_loader: - type q8_0:   98 tensors
0.00.067.005 I llm_load_vocab: special tokens cache size = 25
0.00.073.323 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.327 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.327 I llm_load_print_meta: arch             = gptneox
0.00.073.327 I llm_load_print_meta: vocab type       = BPE
0.00.073.328 I llm_load_print_meta: n_vocab          = 50304
0.00.073.328 I llm_load_print_meta: n_merges         = 50009
0.00.073.328 I llm_load_print_meta: vocab_only       = 0
0.00.073.328 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.328 I llm_load_print_meta: n_embd           = 2048
0.00.073.328 I llm_load_print_meta: n_layer          = 24
0.00.073.344 I llm_load_print_meta: n_head           = 16
0.00.073.345 I llm_load_print_meta: n_head_kv        = 16
0.00.073.345 I llm_load_print_meta: n_rot            = 32
0.00.073.345 I llm_load_print_meta: n_swa            = 0
0.00.073.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.346 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.346 I llm_load_print_meta: n_gqa            = 1
0.00.073.347 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.348 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.348 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.349 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.350 I llm_load_print_meta: n_ff             = 8192
0.00.073.350 I llm_load_print_meta: n_expert         = 0
0.00.073.350 I llm_load_print_meta: n_expert_used    = 0
0.00.073.350 I llm_load_print_meta: causal attn      = 1
0.00.073.350 I llm_load_print_meta: pooling type     = 0
0.00.073.350 I llm_load_print_meta: rope type        = 2
0.00.073.351 I llm_load_print_meta: rope scaling     = linear
0.00.073.351 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.352 I llm_load_print_meta: freq_scale_train = 1
0.00.073.352 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.352 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.352 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.352 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.352 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.352 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.353 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.363 I llm_load_print_meta: model type       = 1.4B
0.00.073.363 I llm_load_print_meta: model ftype      = Q8_0
0.00.073.363 I llm_load_print_meta: model params     = 1.41 B
0.00.073.364 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.073.364 I llm_load_print_meta: general.name     = 1.4B
0.00.073.364 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.365 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.365 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.365 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.365 I llm_load_print_meta: LF token         = 128 ''
0.00.073.365 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.365 I llm_load_print_meta: max token length = 1024
0.00.075.763 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.763 I llm_load_tensors: offloading output layer to GPU
0.00.075.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.775 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.075.776 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.076.740 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.740 I llama_new_context_with_model: n_ctx         = 128
0.00.076.741 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.076.741 I llama_new_context_with_model: n_batch       = 128
0.00.076.741 I llama_new_context_with_model: n_ubatch      = 128
0.00.076.741 I llama_new_context_with_model: flash_attn    = 0
0.00.076.742 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.742 I llama_new_context_with_model: freq_scale    = 1
0.00.076.742 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.743 I ggml_metal_init: allocating
0.00.076.746 I ggml_metal_init: found device: Apple M4
0.00.076.750 I ggml_metal_init: picking default device: Apple M4
0.00.077.440 I ggml_metal_init: using embedded metal library
0.00.080.009 I ggml_metal_init: GPU name:   Apple M4
0.00.080.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.012 I ggml_metal_init: simdgroup reduction   = true
0.00.080.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.013 I ggml_metal_init: has bfloat            = true
0.00.080.013 I ggml_metal_init: use bfloat            = true
0.00.080.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.716 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.091.719 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.091.737 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.770 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.092.771 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.092.772 I llama_new_context_with_model: graph nodes  = 967
0.00.092.772 I llama_new_context_with_model: graph splits = 2
0.00.092.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.420 I 
0.00.933.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.482 I perplexity: tokenizing the input ..
0.00.940.867 I perplexity: tokenization took 7.383 ms
0.00.940.877 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.064.981 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.066.130 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.066.152 I llama_perf_context_print:        load time =     919.70 ms
0.01.066.153 I llama_perf_context_print: prompt eval time =     123.88 ms /   128 tokens (    0.97 ms per token,  1033.27 tokens per second)
0.01.066.154 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.066.154 I llama_perf_context_print:       total time =     132.73 ms /   129 tokens
0.01.066.612 I ggml_metal_free: deallocating

real	0m1.087s
user	0m0.102s
sys	0m0.151s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.014.015 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.605 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.615 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.617 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.617 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.618 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.618 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.621 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.621 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.152 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.971 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.039.972 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.973 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.973 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.973 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.973 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.974 I llama_model_loader: - type  f32:  194 tensors
0.00.039.974 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.975 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.506 I llm_load_vocab: special tokens cache size = 25
0.00.076.611 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.615 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.615 I llm_load_print_meta: arch             = gptneox
0.00.076.615 I llm_load_print_meta: vocab type       = BPE
0.00.076.616 I llm_load_print_meta: n_vocab          = 50304
0.00.076.616 I llm_load_print_meta: n_merges         = 50009
0.00.076.616 I llm_load_print_meta: vocab_only       = 0
0.00.076.616 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.616 I llm_load_print_meta: n_embd           = 2048
0.00.076.616 I llm_load_print_meta: n_layer          = 24
0.00.076.632 I llm_load_print_meta: n_head           = 16
0.00.076.633 I llm_load_print_meta: n_head_kv        = 16
0.00.076.633 I llm_load_print_meta: n_rot            = 32
0.00.076.634 I llm_load_print_meta: n_swa            = 0
0.00.076.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.635 I llm_load_print_meta: n_gqa            = 1
0.00.076.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.637 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.638 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.638 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.639 I llm_load_print_meta: n_ff             = 8192
0.00.076.639 I llm_load_print_meta: n_expert         = 0
0.00.076.640 I llm_load_print_meta: n_expert_used    = 0
0.00.076.640 I llm_load_print_meta: causal attn      = 1
0.00.076.640 I llm_load_print_meta: pooling type     = 0
0.00.076.640 I llm_load_print_meta: rope type        = 2
0.00.076.640 I llm_load_print_meta: rope scaling     = linear
0.00.076.641 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.641 I llm_load_print_meta: freq_scale_train = 1
0.00.076.641 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.641 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.642 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.642 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.642 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.642 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.642 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.652 I llm_load_print_meta: model type       = 1.4B
0.00.076.653 I llm_load_print_meta: model ftype      = Q4_0
0.00.076.653 I llm_load_print_meta: model params     = 1.41 B
0.00.076.654 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.076.657 I llm_load_print_meta: general.name     = 1.4B
0.00.076.657 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.657 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.657 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.658 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.658 I llm_load_print_meta: LF token         = 128 ''
0.00.076.658 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.658 I llm_load_print_meta: max token length = 1024
0.00.079.313 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.314 I llm_load_tensors: offloading output layer to GPU
0.00.079.314 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.326 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.079.327 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.080.516 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.517 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.517 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.517 I llama_new_context_with_model: n_batch       = 2048
0.00.080.518 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.518 I llama_new_context_with_model: flash_attn    = 0
0.00.080.518 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.519 I llama_new_context_with_model: freq_scale    = 1
0.00.080.519 I ggml_metal_init: allocating
0.00.080.522 I ggml_metal_init: found device: Apple M4
0.00.080.525 I ggml_metal_init: picking default device: Apple M4
0.00.081.375 I ggml_metal_init: using embedded metal library
0.00.085.177 I ggml_metal_init: GPU name:   Apple M4
0.00.085.179 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.180 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.180 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.181 I ggml_metal_init: simdgroup reduction   = true
0.00.085.181 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.181 I ggml_metal_init: has bfloat            = true
0.00.085.181 I ggml_metal_init: use bfloat            = true
0.00.085.182 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.183 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.122.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.406 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.428 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.564 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.566 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.566 I llama_new_context_with_model: graph nodes  = 967
0.00.123.566 I llama_new_context_with_model: graph splits = 2
0.00.123.577 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.565 I main: llama threadpool init, n_threads = 4
0.00.808.618 I 
0.00.808.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.808.662 I 
0.00.808.894 I sampler seed: 1234
0.00.808.901 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.808.951 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.808.956 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.808.956 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.497.410 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.497.411 I llama_perf_context_print:        load time =     794.55 ms
0.01.497.412 I llama_perf_context_print: prompt eval time =      46.19 ms /     7 tokens (    6.60 ms per token,   151.56 tokens per second)
0.01.497.413 I llama_perf_context_print:        eval time =     639.24 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.497.413 I llama_perf_context_print:       total time =     688.85 ms /    70 tokens
0.01.497.616 I ggml_metal_free: deallocating

real	0m1.519s
user	0m0.128s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.058 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.683 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.684 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.560 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.605 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.505 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.507 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.508 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.508 I llama_model_loader: - type  f32:  194 tensors
0.00.023.509 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.509 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.412 I llm_load_vocab: special tokens cache size = 25
0.00.050.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.395 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.395 I llm_load_print_meta: arch             = gptneox
0.00.050.396 I llm_load_print_meta: vocab type       = BPE
0.00.050.396 I llm_load_print_meta: n_vocab          = 50304
0.00.050.396 I llm_load_print_meta: n_merges         = 50009
0.00.050.396 I llm_load_print_meta: vocab_only       = 0
0.00.050.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.396 I llm_load_print_meta: n_embd           = 2048
0.00.050.397 I llm_load_print_meta: n_layer          = 24
0.00.050.411 I llm_load_print_meta: n_head           = 16
0.00.050.412 I llm_load_print_meta: n_head_kv        = 16
0.00.050.412 I llm_load_print_meta: n_rot            = 32
0.00.050.412 I llm_load_print_meta: n_swa            = 0
0.00.050.412 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.412 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.413 I llm_load_print_meta: n_gqa            = 1
0.00.050.415 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.417 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.418 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.418 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.418 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.418 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.419 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.419 I llm_load_print_meta: n_ff             = 8192
0.00.050.419 I llm_load_print_meta: n_expert         = 0
0.00.050.420 I llm_load_print_meta: n_expert_used    = 0
0.00.050.421 I llm_load_print_meta: causal attn      = 1
0.00.050.421 I llm_load_print_meta: pooling type     = 0
0.00.050.421 I llm_load_print_meta: rope type        = 2
0.00.050.421 I llm_load_print_meta: rope scaling     = linear
0.00.050.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.425 I llm_load_print_meta: freq_scale_train = 1
0.00.050.425 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.426 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.427 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.427 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.427 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.427 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.427 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.436 I llm_load_print_meta: model type       = 1.4B
0.00.050.437 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.437 I llm_load_print_meta: model params     = 1.41 B
0.00.050.438 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.438 I llm_load_print_meta: general.name     = 1.4B
0.00.050.438 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.438 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.439 I llm_load_print_meta: LF token         = 128 ''
0.00.050.439 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.439 I llm_load_print_meta: max token length = 1024
0.00.052.388 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.388 I llm_load_tensors: offloading output layer to GPU
0.00.052.388 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.399 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.400 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.323 I llama_new_context_with_model: n_ctx         = 128
0.00.053.324 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.324 I llama_new_context_with_model: n_batch       = 128
0.00.053.324 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.324 I llama_new_context_with_model: flash_attn    = 0
0.00.053.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.325 I llama_new_context_with_model: freq_scale    = 1
0.00.053.325 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.326 I ggml_metal_init: allocating
0.00.053.332 I ggml_metal_init: found device: Apple M4
0.00.053.334 I ggml_metal_init: picking default device: Apple M4
0.00.053.897 I ggml_metal_init: using embedded metal library
0.00.056.220 I ggml_metal_init: GPU name:   Apple M4
0.00.056.222 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.223 I ggml_metal_init: simdgroup reduction   = true
0.00.056.223 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.223 I ggml_metal_init: has bfloat            = true
0.00.056.223 I ggml_metal_init: use bfloat            = true
0.00.056.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.088 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.092 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.106 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.029 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.031 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.031 I llama_new_context_with_model: graph nodes  = 967
0.00.068.031 I llama_new_context_with_model: graph splits = 2
0.00.068.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.383 I 
0.00.628.427 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.628.435 I perplexity: tokenizing the input ..
0.00.636.280 I perplexity: tokenization took 7.844 ms
0.00.636.290 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.934 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.759.337 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.759.351 I llama_perf_context_print:        load time =     619.32 ms
0.00.759.354 I llama_perf_context_print: prompt eval time =     121.42 ms /   128 tokens (    0.95 ms per token,  1054.18 tokens per second)
0.00.759.355 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.759.355 I llama_perf_context_print:       total time =     130.97 ms /   129 tokens
0.00.759.727 I ggml_metal_free: deallocating

real	0m0.775s
user	0m0.079s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.596 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.142 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.143 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.143 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.148 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.148 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.157 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.974 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.861 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.035.862 I llama_model_loader: - type  f32:  194 tensors
0.00.035.862 I llama_model_loader: - type q4_1:   97 tensors
0.00.035.863 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.681 I llm_load_vocab: special tokens cache size = 25
0.00.066.345 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.348 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.348 I llm_load_print_meta: arch             = gptneox
0.00.066.349 I llm_load_print_meta: vocab type       = BPE
0.00.066.349 I llm_load_print_meta: n_vocab          = 50304
0.00.066.349 I llm_load_print_meta: n_merges         = 50009
0.00.066.349 I llm_load_print_meta: vocab_only       = 0
0.00.066.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.349 I llm_load_print_meta: n_embd           = 2048
0.00.066.349 I llm_load_print_meta: n_layer          = 24
0.00.066.363 I llm_load_print_meta: n_head           = 16
0.00.066.365 I llm_load_print_meta: n_head_kv        = 16
0.00.066.365 I llm_load_print_meta: n_rot            = 32
0.00.066.365 I llm_load_print_meta: n_swa            = 0
0.00.066.365 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.365 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.366 I llm_load_print_meta: n_gqa            = 1
0.00.066.367 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.367 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.368 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.368 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.368 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.368 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.368 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.369 I llm_load_print_meta: n_ff             = 8192
0.00.066.369 I llm_load_print_meta: n_expert         = 0
0.00.066.369 I llm_load_print_meta: n_expert_used    = 0
0.00.066.370 I llm_load_print_meta: causal attn      = 1
0.00.066.371 I llm_load_print_meta: pooling type     = 0
0.00.066.371 I llm_load_print_meta: rope type        = 2
0.00.066.371 I llm_load_print_meta: rope scaling     = linear
0.00.066.371 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.372 I llm_load_print_meta: freq_scale_train = 1
0.00.066.372 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.372 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.372 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.372 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.372 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.372 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.372 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.382 I llm_load_print_meta: model type       = 1.4B
0.00.066.382 I llm_load_print_meta: model ftype      = Q4_1
0.00.066.382 I llm_load_print_meta: model params     = 1.41 B
0.00.066.383 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.066.383 I llm_load_print_meta: general.name     = 1.4B
0.00.066.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.383 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.383 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.385 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.385 I llm_load_print_meta: LF token         = 128 ''
0.00.066.386 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.386 I llm_load_print_meta: max token length = 1024
0.00.068.446 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.446 I llm_load_tensors: offloading output layer to GPU
0.00.068.447 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.457 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.068.458 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.069.409 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.410 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.410 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.069.410 I llama_new_context_with_model: n_batch       = 2048
0.00.069.411 I llama_new_context_with_model: n_ubatch      = 512
0.00.069.411 I llama_new_context_with_model: flash_attn    = 0
0.00.069.411 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.412 I llama_new_context_with_model: freq_scale    = 1
0.00.069.412 I ggml_metal_init: allocating
0.00.069.419 I ggml_metal_init: found device: Apple M4
0.00.069.421 I ggml_metal_init: picking default device: Apple M4
0.00.070.079 I ggml_metal_init: using embedded metal library
0.00.072.721 I ggml_metal_init: GPU name:   Apple M4
0.00.072.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.724 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.724 I ggml_metal_init: simdgroup reduction   = true
0.00.072.724 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.724 I ggml_metal_init: has bfloat            = true
0.00.072.726 I ggml_metal_init: use bfloat            = true
0.00.072.726 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.727 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.430 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.437 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.456 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.553 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.554 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.554 I llama_new_context_with_model: graph nodes  = 967
0.00.104.555 I llama_new_context_with_model: graph splits = 2
0.00.104.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.920.192 I main: llama threadpool init, n_threads = 4
0.00.920.232 I 
0.00.920.262 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.920.262 I 
0.00.920.496 I sampler seed: 1234
0.00.920.500 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.920.511 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.920.511 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.920.512 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.653.378 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.01.653.379 I llama_perf_context_print:        load time =     911.59 ms
0.01.653.381 I llama_perf_context_print: prompt eval time =      44.03 ms /     7 tokens (    6.29 ms per token,   158.99 tokens per second)
0.01.653.381 I llama_perf_context_print:        eval time =     686.33 ms /    63 runs   (   10.89 ms per token,    91.79 tokens per second)
0.01.653.382 I llama_perf_context_print:       total time =     733.19 ms /    70 tokens
0.01.653.598 I ggml_metal_free: deallocating

real	0m1.670s
user	0m0.113s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.364 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.218 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.228 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.229 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.232 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.232 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.144 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.112 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.114 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.115 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.116 I llama_model_loader: - type  f32:  194 tensors
0.00.023.117 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.117 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.974 I llm_load_vocab: special tokens cache size = 25
0.00.050.864 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.868 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.868 I llm_load_print_meta: arch             = gptneox
0.00.050.868 I llm_load_print_meta: vocab type       = BPE
0.00.050.869 I llm_load_print_meta: n_vocab          = 50304
0.00.050.869 I llm_load_print_meta: n_merges         = 50009
0.00.050.869 I llm_load_print_meta: vocab_only       = 0
0.00.050.869 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.869 I llm_load_print_meta: n_embd           = 2048
0.00.050.870 I llm_load_print_meta: n_layer          = 24
0.00.050.885 I llm_load_print_meta: n_head           = 16
0.00.050.887 I llm_load_print_meta: n_head_kv        = 16
0.00.050.887 I llm_load_print_meta: n_rot            = 32
0.00.050.887 I llm_load_print_meta: n_swa            = 0
0.00.050.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.888 I llm_load_print_meta: n_gqa            = 1
0.00.050.889 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.890 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.890 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.892 I llm_load_print_meta: n_ff             = 8192
0.00.050.892 I llm_load_print_meta: n_expert         = 0
0.00.050.892 I llm_load_print_meta: n_expert_used    = 0
0.00.050.892 I llm_load_print_meta: causal attn      = 1
0.00.050.892 I llm_load_print_meta: pooling type     = 0
0.00.050.892 I llm_load_print_meta: rope type        = 2
0.00.050.892 I llm_load_print_meta: rope scaling     = linear
0.00.050.893 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.893 I llm_load_print_meta: freq_scale_train = 1
0.00.050.893 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.893 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.894 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.894 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.894 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.894 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.904 I llm_load_print_meta: model type       = 1.4B
0.00.050.904 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.904 I llm_load_print_meta: model params     = 1.41 B
0.00.050.905 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.905 I llm_load_print_meta: general.name     = 1.4B
0.00.050.905 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.906 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.906 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.906 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.906 I llm_load_print_meta: LF token         = 128 ''
0.00.050.906 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.906 I llm_load_print_meta: max token length = 1024
0.00.052.900 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.900 I llm_load_tensors: offloading output layer to GPU
0.00.052.900 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.911 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.912 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.824 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.825 I llama_new_context_with_model: n_ctx         = 128
0.00.053.825 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.825 I llama_new_context_with_model: n_batch       = 128
0.00.053.826 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.826 I llama_new_context_with_model: flash_attn    = 0
0.00.053.826 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.827 I llama_new_context_with_model: freq_scale    = 1
0.00.053.827 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.827 I ggml_metal_init: allocating
0.00.053.832 I ggml_metal_init: found device: Apple M4
0.00.053.834 I ggml_metal_init: picking default device: Apple M4
0.00.054.409 I ggml_metal_init: using embedded metal library
0.00.056.724 I ggml_metal_init: GPU name:   Apple M4
0.00.056.725 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.726 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.727 I ggml_metal_init: simdgroup reduction   = true
0.00.056.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.727 I ggml_metal_init: has bfloat            = true
0.00.056.727 I ggml_metal_init: use bfloat            = true
0.00.056.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.144 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.160 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.072 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.073 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.074 I llama_new_context_with_model: graph nodes  = 967
0.00.069.074 I llama_new_context_with_model: graph splits = 2
0.00.069.087 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.941 I 
0.00.685.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.983 I perplexity: tokenizing the input ..
0.00.693.403 I perplexity: tokenization took 7.418 ms
0.00.693.413 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.816.178 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.817.357 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.817.381 I llama_perf_context_print:        load time =     677.57 ms
0.00.817.383 I llama_perf_context_print: prompt eval time =     122.53 ms /   128 tokens (    0.96 ms per token,  1044.60 tokens per second)
0.00.817.384 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.385 I llama_perf_context_print:       total time =     131.44 ms /   129 tokens
0.00.817.907 I ggml_metal_free: deallocating

real	0m0.833s
user	0m0.080s
sys	0m0.120s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.096 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.339 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.344 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.351 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.352 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.352 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.354 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.355 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.355 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.356 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.359 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.223 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.996 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.996 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.997 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.997 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.998 I llama_model_loader: - type  f32:  194 tensors
0.00.023.998 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.999 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.292 I llm_load_vocab: special tokens cache size = 25
0.00.050.284 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.287 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.287 I llm_load_print_meta: arch             = gptneox
0.00.050.288 I llm_load_print_meta: vocab type       = BPE
0.00.050.288 I llm_load_print_meta: n_vocab          = 50304
0.00.050.288 I llm_load_print_meta: n_merges         = 50009
0.00.050.288 I llm_load_print_meta: vocab_only       = 0
0.00.050.289 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.289 I llm_load_print_meta: n_embd           = 2048
0.00.050.289 I llm_load_print_meta: n_layer          = 24
0.00.050.298 I llm_load_print_meta: n_head           = 16
0.00.050.299 I llm_load_print_meta: n_head_kv        = 16
0.00.050.299 I llm_load_print_meta: n_rot            = 32
0.00.050.299 I llm_load_print_meta: n_swa            = 0
0.00.050.300 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.300 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.300 I llm_load_print_meta: n_gqa            = 1
0.00.050.301 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.302 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.302 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.303 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.303 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.303 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.303 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.304 I llm_load_print_meta: n_ff             = 8192
0.00.050.304 I llm_load_print_meta: n_expert         = 0
0.00.050.304 I llm_load_print_meta: n_expert_used    = 0
0.00.050.304 I llm_load_print_meta: causal attn      = 1
0.00.050.304 I llm_load_print_meta: pooling type     = 0
0.00.050.305 I llm_load_print_meta: rope type        = 2
0.00.050.305 I llm_load_print_meta: rope scaling     = linear
0.00.050.305 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.305 I llm_load_print_meta: freq_scale_train = 1
0.00.050.306 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.306 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.306 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.306 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.306 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.306 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.306 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.311 I llm_load_print_meta: model type       = 1.4B
0.00.050.311 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.312 I llm_load_print_meta: model params     = 1.41 B
0.00.050.312 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.312 I llm_load_print_meta: general.name     = 1.4B
0.00.050.312 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.313 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.313 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.313 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.313 I llm_load_print_meta: LF token         = 128 ''
0.00.050.313 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.314 I llm_load_print_meta: max token length = 1024
0.00.052.074 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.074 I llm_load_tensors: offloading output layer to GPU
0.00.052.074 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.080 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.082 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.035 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.035 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.036 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.036 I llama_new_context_with_model: n_batch       = 2048
0.00.053.036 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.036 I llama_new_context_with_model: flash_attn    = 0
0.00.053.036 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.037 I llama_new_context_with_model: freq_scale    = 1
0.00.053.037 I ggml_metal_init: allocating
0.00.053.041 I ggml_metal_init: found device: Apple M4
0.00.053.043 I ggml_metal_init: picking default device: Apple M4
0.00.053.639 I ggml_metal_init: using embedded metal library
0.00.055.978 I ggml_metal_init: GPU name:   Apple M4
0.00.055.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.980 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.980 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.980 I ggml_metal_init: simdgroup reduction   = true
0.00.055.981 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.981 I ggml_metal_init: has bfloat            = true
0.00.055.981 I ggml_metal_init: use bfloat            = true
0.00.055.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.983 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.471 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.477 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.498 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.491 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.493 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.493 I llama_new_context_with_model: graph nodes  = 967
0.00.086.493 I llama_new_context_with_model: graph splits = 2
0.00.086.507 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.768.296 I main: llama threadpool init, n_threads = 4
0.00.768.336 I 
0.00.768.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.768.360 I 
0.00.768.585 I sampler seed: 1234
0.00.768.589 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.629 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.633 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.633 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.561.259 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.01.561.260 I llama_perf_context_print:        load time =     759.20 ms
0.01.561.260 I llama_perf_context_print: prompt eval time =      46.56 ms /     7 tokens (    6.65 ms per token,   150.34 tokens per second)
0.01.561.261 I llama_perf_context_print:        eval time =     743.08 ms /    63 runs   (   11.79 ms per token,    84.78 tokens per second)
0.01.561.261 I llama_perf_context_print:       total time =     792.96 ms /    70 tokens
0.01.561.446 I ggml_metal_free: deallocating

real	0m1.579s
user	0m0.109s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.838 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.273 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.277 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.282 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.283 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.283 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.283 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.284 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.287 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.288 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.290 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.290 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.242 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.138 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.139 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.140 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.141 I llama_model_loader: - type  f32:  194 tensors
0.00.024.141 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.142 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.290 I llm_load_vocab: special tokens cache size = 25
0.00.050.256 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.259 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.259 I llm_load_print_meta: arch             = gptneox
0.00.050.259 I llm_load_print_meta: vocab type       = BPE
0.00.050.259 I llm_load_print_meta: n_vocab          = 50304
0.00.050.260 I llm_load_print_meta: n_merges         = 50009
0.00.050.260 I llm_load_print_meta: vocab_only       = 0
0.00.050.260 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.260 I llm_load_print_meta: n_embd           = 2048
0.00.050.260 I llm_load_print_meta: n_layer          = 24
0.00.050.275 I llm_load_print_meta: n_head           = 16
0.00.050.276 I llm_load_print_meta: n_head_kv        = 16
0.00.050.276 I llm_load_print_meta: n_rot            = 32
0.00.050.276 I llm_load_print_meta: n_swa            = 0
0.00.050.276 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.276 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.277 I llm_load_print_meta: n_gqa            = 1
0.00.050.278 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.278 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.280 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.280 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.280 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.281 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.281 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.281 I llm_load_print_meta: n_ff             = 8192
0.00.050.281 I llm_load_print_meta: n_expert         = 0
0.00.050.282 I llm_load_print_meta: n_expert_used    = 0
0.00.050.282 I llm_load_print_meta: causal attn      = 1
0.00.050.282 I llm_load_print_meta: pooling type     = 0
0.00.050.282 I llm_load_print_meta: rope type        = 2
0.00.050.282 I llm_load_print_meta: rope scaling     = linear
0.00.050.283 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.283 I llm_load_print_meta: freq_scale_train = 1
0.00.050.283 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.283 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.283 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.284 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.284 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.284 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.284 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.293 I llm_load_print_meta: model type       = 1.4B
0.00.050.294 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.294 I llm_load_print_meta: model params     = 1.41 B
0.00.050.296 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.296 I llm_load_print_meta: general.name     = 1.4B
0.00.050.296 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.296 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.297 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.298 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.298 I llm_load_print_meta: LF token         = 128 ''
0.00.050.298 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.299 I llm_load_print_meta: max token length = 1024
0.00.052.219 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.219 I llm_load_tensors: offloading output layer to GPU
0.00.052.219 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.230 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.231 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.096 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.097 I llama_new_context_with_model: n_ctx         = 128
0.00.053.097 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.097 I llama_new_context_with_model: n_batch       = 128
0.00.053.097 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.098 I llama_new_context_with_model: flash_attn    = 0
0.00.053.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.098 I llama_new_context_with_model: freq_scale    = 1
0.00.053.099 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.099 I ggml_metal_init: allocating
0.00.053.104 I ggml_metal_init: found device: Apple M4
0.00.053.107 I ggml_metal_init: picking default device: Apple M4
0.00.053.694 I ggml_metal_init: using embedded metal library
0.00.056.038 I ggml_metal_init: GPU name:   Apple M4
0.00.056.040 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.041 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.041 I ggml_metal_init: simdgroup reduction   = true
0.00.056.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.041 I ggml_metal_init: has bfloat            = true
0.00.056.041 I ggml_metal_init: use bfloat            = true
0.00.056.042 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.758 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.767 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.785 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.674 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.675 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.676 I llama_new_context_with_model: graph nodes  = 967
0.00.067.676 I llama_new_context_with_model: graph splits = 2
0.00.067.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.165 I 
0.00.724.200 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.208 I perplexity: tokenizing the input ..
0.00.731.683 I perplexity: tokenization took 7.474 ms
0.00.731.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.866.333 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.867.492 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.867.507 I llama_perf_context_print:        load time =     714.32 ms
0.00.867.508 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.28 tokens per second)
0.00.867.509 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.867.509 I llama_perf_context_print:       total time =     143.34 ms /   129 tokens
0.00.867.908 I ggml_metal_free: deallocating

real	0m0.882s
user	0m0.077s
sys	0m0.118s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.559 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.627 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.631 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.641 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.619 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.663 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.580 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.581 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.581 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.582 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.582 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.582 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.583 I llama_model_loader: - type  f32:  194 tensors
0.00.024.583 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.583 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.093 I llm_load_vocab: special tokens cache size = 25
0.00.051.177 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.180 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.180 I llm_load_print_meta: arch             = gptneox
0.00.051.181 I llm_load_print_meta: vocab type       = BPE
0.00.051.181 I llm_load_print_meta: n_vocab          = 50304
0.00.051.181 I llm_load_print_meta: n_merges         = 50009
0.00.051.181 I llm_load_print_meta: vocab_only       = 0
0.00.051.181 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.182 I llm_load_print_meta: n_embd           = 2048
0.00.051.182 I llm_load_print_meta: n_layer          = 24
0.00.051.197 I llm_load_print_meta: n_head           = 16
0.00.051.199 I llm_load_print_meta: n_head_kv        = 16
0.00.051.199 I llm_load_print_meta: n_rot            = 32
0.00.051.199 I llm_load_print_meta: n_swa            = 0
0.00.051.199 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.199 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.200 I llm_load_print_meta: n_gqa            = 1
0.00.051.201 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.204 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.206 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.206 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.206 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.206 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.206 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.207 I llm_load_print_meta: n_ff             = 8192
0.00.051.207 I llm_load_print_meta: n_expert         = 0
0.00.051.207 I llm_load_print_meta: n_expert_used    = 0
0.00.051.207 I llm_load_print_meta: causal attn      = 1
0.00.051.208 I llm_load_print_meta: pooling type     = 0
0.00.051.208 I llm_load_print_meta: rope type        = 2
0.00.051.208 I llm_load_print_meta: rope scaling     = linear
0.00.051.208 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.209 I llm_load_print_meta: freq_scale_train = 1
0.00.051.209 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.210 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.211 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.211 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.211 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.211 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.221 I llm_load_print_meta: model type       = 1.4B
0.00.051.221 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.221 I llm_load_print_meta: model params     = 1.41 B
0.00.051.222 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.222 I llm_load_print_meta: general.name     = 1.4B
0.00.051.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.223 I llm_load_print_meta: LF token         = 128 ''
0.00.051.224 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.224 I llm_load_print_meta: max token length = 1024
0.00.053.241 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.241 I llm_load_tensors: offloading output layer to GPU
0.00.053.241 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.252 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.253 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.231 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.232 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.232 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.232 I llama_new_context_with_model: n_batch       = 2048
0.00.054.232 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.232 I llama_new_context_with_model: flash_attn    = 0
0.00.054.233 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.233 I llama_new_context_with_model: freq_scale    = 1
0.00.054.234 I ggml_metal_init: allocating
0.00.054.240 I ggml_metal_init: found device: Apple M4
0.00.054.243 I ggml_metal_init: picking default device: Apple M4
0.00.054.870 I ggml_metal_init: using embedded metal library
0.00.057.212 I ggml_metal_init: GPU name:   Apple M4
0.00.057.214 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.214 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.215 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.215 I ggml_metal_init: simdgroup reduction   = true
0.00.057.215 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.215 I ggml_metal_init: has bfloat            = true
0.00.057.215 I ggml_metal_init: use bfloat            = true
0.00.057.216 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.216 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.361 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.366 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.384 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.383 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.384 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.384 I llama_new_context_with_model: graph nodes  = 967
0.00.087.384 I llama_new_context_with_model: graph splits = 2
0.00.087.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.777.723 I main: llama threadpool init, n_threads = 4
0.00.777.760 I 
0.00.777.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.777.792 I 
0.00.777.912 I sampler seed: 1234
0.00.777.917 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.940 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.941 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.941 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.623.471 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59463.99 tokens per second)
0.01.623.472 I llama_perf_context_print:        load time =     769.16 ms
0.01.623.472 I llama_perf_context_print: prompt eval time =      48.49 ms /     7 tokens (    6.93 ms per token,   144.35 tokens per second)
0.01.623.473 I llama_perf_context_print:        eval time =     794.10 ms /    63 runs   (   12.60 ms per token,    79.34 tokens per second)
0.01.623.474 I llama_perf_context_print:       total time =     845.75 ms /    70 tokens
0.01.623.658 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.109s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.695 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.484 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.273 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.128 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.128 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.129 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.129 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.129 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.130 I llama_model_loader: - type  f32:  194 tensors
0.00.023.130 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.130 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.254 I llm_load_vocab: special tokens cache size = 25
0.00.049.322 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.324 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.325 I llm_load_print_meta: arch             = gptneox
0.00.049.325 I llm_load_print_meta: vocab type       = BPE
0.00.049.325 I llm_load_print_meta: n_vocab          = 50304
0.00.049.326 I llm_load_print_meta: n_merges         = 50009
0.00.049.326 I llm_load_print_meta: vocab_only       = 0
0.00.049.326 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.326 I llm_load_print_meta: n_embd           = 2048
0.00.049.326 I llm_load_print_meta: n_layer          = 24
0.00.049.341 I llm_load_print_meta: n_head           = 16
0.00.049.342 I llm_load_print_meta: n_head_kv        = 16
0.00.049.342 I llm_load_print_meta: n_rot            = 32
0.00.049.342 I llm_load_print_meta: n_swa            = 0
0.00.049.342 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.342 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.343 I llm_load_print_meta: n_gqa            = 1
0.00.049.344 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.345 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.345 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.346 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.348 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.348 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.348 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.349 I llm_load_print_meta: n_ff             = 8192
0.00.049.349 I llm_load_print_meta: n_expert         = 0
0.00.049.349 I llm_load_print_meta: n_expert_used    = 0
0.00.049.349 I llm_load_print_meta: causal attn      = 1
0.00.049.350 I llm_load_print_meta: pooling type     = 0
0.00.049.350 I llm_load_print_meta: rope type        = 2
0.00.049.350 I llm_load_print_meta: rope scaling     = linear
0.00.049.350 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.351 I llm_load_print_meta: freq_scale_train = 1
0.00.049.351 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.351 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.352 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.352 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.352 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.352 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.352 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.362 I llm_load_print_meta: model type       = 1.4B
0.00.049.362 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.363 I llm_load_print_meta: model params     = 1.41 B
0.00.049.363 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.363 I llm_load_print_meta: general.name     = 1.4B
0.00.049.363 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.364 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.364 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.364 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.364 I llm_load_print_meta: LF token         = 128 ''
0.00.049.364 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.364 I llm_load_print_meta: max token length = 1024
0.00.051.318 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.318 I llm_load_tensors: offloading output layer to GPU
0.00.051.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.329 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.330 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.254 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.255 I llama_new_context_with_model: n_ctx         = 128
0.00.052.255 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.256 I llama_new_context_with_model: n_batch       = 128
0.00.052.256 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.256 I llama_new_context_with_model: flash_attn    = 0
0.00.052.256 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.257 I llama_new_context_with_model: freq_scale    = 1
0.00.052.257 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.257 I ggml_metal_init: allocating
0.00.052.263 I ggml_metal_init: found device: Apple M4
0.00.052.265 I ggml_metal_init: picking default device: Apple M4
0.00.052.829 I ggml_metal_init: using embedded metal library
0.00.055.164 I ggml_metal_init: GPU name:   Apple M4
0.00.055.165 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.166 I ggml_metal_init: simdgroup reduction   = true
0.00.055.166 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.166 I ggml_metal_init: has bfloat            = true
0.00.055.166 I ggml_metal_init: use bfloat            = true
0.00.055.167 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.870 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.875 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.889 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.769 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.770 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.770 I llama_new_context_with_model: graph nodes  = 967
0.00.066.771 I llama_new_context_with_model: graph splits = 2
0.00.066.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.710 I 
0.00.737.760 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.776 I perplexity: tokenizing the input ..
0.00.745.600 I perplexity: tokenization took 7.823 ms
0.00.745.612 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.879.861 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.881.035 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.881.045 I llama_perf_context_print:        load time =     729.01 ms
0.00.881.046 I llama_perf_context_print: prompt eval time =     134.02 ms /   128 tokens (    1.05 ms per token,   955.05 tokens per second)
0.00.881.047 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.881.047 I llama_perf_context_print:       total time =     143.34 ms /   129 tokens
0.00.881.479 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.078s
sys	0m0.128s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.367 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.058 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.063 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.066 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.067 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.067 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.068 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.070 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.071 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.937 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.954 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.954 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.955 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.956 I llama_model_loader: - type  f32:  194 tensors
0.00.023.956 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.956 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.956 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.460 I llm_load_vocab: special tokens cache size = 25
0.00.050.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.484 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.484 I llm_load_print_meta: arch             = gptneox
0.00.050.485 I llm_load_print_meta: vocab type       = BPE
0.00.050.485 I llm_load_print_meta: n_vocab          = 50304
0.00.050.486 I llm_load_print_meta: n_merges         = 50009
0.00.050.487 I llm_load_print_meta: vocab_only       = 0
0.00.050.491 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.491 I llm_load_print_meta: n_embd           = 2048
0.00.050.493 I llm_load_print_meta: n_layer          = 24
0.00.050.507 I llm_load_print_meta: n_head           = 16
0.00.050.509 I llm_load_print_meta: n_head_kv        = 16
0.00.050.510 I llm_load_print_meta: n_rot            = 32
0.00.050.510 I llm_load_print_meta: n_swa            = 0
0.00.050.510 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.510 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.511 I llm_load_print_meta: n_gqa            = 1
0.00.050.511 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.512 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.512 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.513 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.513 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.513 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.513 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.514 I llm_load_print_meta: n_ff             = 8192
0.00.050.514 I llm_load_print_meta: n_expert         = 0
0.00.050.514 I llm_load_print_meta: n_expert_used    = 0
0.00.050.515 I llm_load_print_meta: causal attn      = 1
0.00.050.515 I llm_load_print_meta: pooling type     = 0
0.00.050.515 I llm_load_print_meta: rope type        = 2
0.00.050.515 I llm_load_print_meta: rope scaling     = linear
0.00.050.515 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.516 I llm_load_print_meta: freq_scale_train = 1
0.00.050.516 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.516 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.516 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.516 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.516 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.516 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.526 I llm_load_print_meta: model type       = 1.4B
0.00.050.526 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.527 I llm_load_print_meta: model params     = 1.41 B
0.00.050.527 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.527 I llm_load_print_meta: general.name     = 1.4B
0.00.050.528 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.528 I llm_load_print_meta: LF token         = 128 ''
0.00.050.529 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.529 I llm_load_print_meta: max token length = 1024
0.00.052.407 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.408 I llm_load_tensors: offloading output layer to GPU
0.00.052.408 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.419 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.420 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.303 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.304 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.304 I llama_new_context_with_model: n_batch       = 2048
0.00.053.304 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.304 I llama_new_context_with_model: flash_attn    = 0
0.00.053.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.305 I llama_new_context_with_model: freq_scale    = 1
0.00.053.305 I ggml_metal_init: allocating
0.00.053.308 I ggml_metal_init: found device: Apple M4
0.00.053.310 I ggml_metal_init: picking default device: Apple M4
0.00.053.903 I ggml_metal_init: using embedded metal library
0.00.056.233 I ggml_metal_init: GPU name:   Apple M4
0.00.056.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.235 I ggml_metal_init: simdgroup reduction   = true
0.00.056.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.236 I ggml_metal_init: has bfloat            = true
0.00.056.236 I ggml_metal_init: use bfloat            = true
0.00.056.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.293 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.301 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.321 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.337 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.338 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.339 I llama_new_context_with_model: graph nodes  = 967
0.00.086.339 I llama_new_context_with_model: graph splits = 2
0.00.086.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.140 I main: llama threadpool init, n_threads = 4
0.00.451.179 I 
0.00.451.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.208 I 
0.00.451.425 I sampler seed: 1234
0.00.451.430 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.451.460 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.451.462 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.451.462 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.135.156 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.135.157 I llama_perf_context_print:        load time =     441.77 ms
0.01.135.158 I llama_perf_context_print: prompt eval time =      39.68 ms /     7 tokens (    5.67 ms per token,   176.42 tokens per second)
0.01.135.159 I llama_perf_context_print:        eval time =     641.02 ms /    63 runs   (   10.17 ms per token,    98.28 tokens per second)
0.01.135.160 I llama_perf_context_print:       total time =     684.02 ms /    70 tokens
0.01.135.361 I ggml_metal_free: deallocating

real	0m1.152s
user	0m0.109s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.863 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.339 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.249 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.093 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.094 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.095 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.096 I llama_model_loader: - type  f32:  194 tensors
0.00.024.097 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.097 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.097 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.148 I llm_load_vocab: special tokens cache size = 25
0.00.050.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.085 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.086 I llm_load_print_meta: arch             = gptneox
0.00.050.086 I llm_load_print_meta: vocab type       = BPE
0.00.050.086 I llm_load_print_meta: n_vocab          = 50304
0.00.050.086 I llm_load_print_meta: n_merges         = 50009
0.00.050.087 I llm_load_print_meta: vocab_only       = 0
0.00.050.087 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.087 I llm_load_print_meta: n_embd           = 2048
0.00.050.087 I llm_load_print_meta: n_layer          = 24
0.00.050.101 I llm_load_print_meta: n_head           = 16
0.00.050.102 I llm_load_print_meta: n_head_kv        = 16
0.00.050.102 I llm_load_print_meta: n_rot            = 32
0.00.050.102 I llm_load_print_meta: n_swa            = 0
0.00.050.103 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.103 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.104 I llm_load_print_meta: n_gqa            = 1
0.00.050.104 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.105 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.106 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.106 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.106 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.107 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.107 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.108 I llm_load_print_meta: n_ff             = 8192
0.00.050.108 I llm_load_print_meta: n_expert         = 0
0.00.050.108 I llm_load_print_meta: n_expert_used    = 0
0.00.050.108 I llm_load_print_meta: causal attn      = 1
0.00.050.108 I llm_load_print_meta: pooling type     = 0
0.00.050.109 I llm_load_print_meta: rope type        = 2
0.00.050.109 I llm_load_print_meta: rope scaling     = linear
0.00.050.109 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.109 I llm_load_print_meta: freq_scale_train = 1
0.00.050.110 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.110 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.110 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.110 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.110 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.110 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.110 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.120 I llm_load_print_meta: model type       = 1.4B
0.00.050.120 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.121 I llm_load_print_meta: model params     = 1.41 B
0.00.050.121 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.121 I llm_load_print_meta: general.name     = 1.4B
0.00.050.121 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.122 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.123 I llm_load_print_meta: LF token         = 128 ''
0.00.050.124 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.124 I llm_load_print_meta: max token length = 1024
0.00.051.954 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.954 I llm_load_tensors: offloading output layer to GPU
0.00.051.954 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.965 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.966 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.855 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.856 I llama_new_context_with_model: n_ctx         = 128
0.00.052.856 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.856 I llama_new_context_with_model: n_batch       = 128
0.00.052.856 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.857 I llama_new_context_with_model: flash_attn    = 0
0.00.052.857 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.857 I llama_new_context_with_model: freq_scale    = 1
0.00.052.857 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.858 I ggml_metal_init: allocating
0.00.052.861 I ggml_metal_init: found device: Apple M4
0.00.052.863 I ggml_metal_init: picking default device: Apple M4
0.00.053.428 I ggml_metal_init: using embedded metal library
0.00.055.708 I ggml_metal_init: GPU name:   Apple M4
0.00.055.709 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.709 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.710 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.710 I ggml_metal_init: simdgroup reduction   = true
0.00.055.710 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.710 I ggml_metal_init: has bfloat            = true
0.00.055.710 I ggml_metal_init: use bfloat            = true
0.00.055.711 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.711 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.430 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.435 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.448 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.347 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.348 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.348 I llama_new_context_with_model: graph nodes  = 967
0.00.067.349 I llama_new_context_with_model: graph splits = 2
0.00.067.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.395.415 I 
0.00.395.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.395.496 I perplexity: tokenizing the input ..
0.00.403.123 I perplexity: tokenization took 7.625 ms
0.00.403.134 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.535.825 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.537.075 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.537.094 I llama_perf_context_print:        load time =     385.55 ms
0.00.537.094 I llama_perf_context_print: prompt eval time =     132.46 ms /   128 tokens (    1.03 ms per token,   966.32 tokens per second)
0.00.537.095 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.537.096 I llama_perf_context_print:       total time =     141.68 ms /   129 tokens
0.00.537.629 I ggml_metal_free: deallocating

real	0m0.553s
user	0m0.077s
sys	0m0.076s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.646 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.400 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.405 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.412 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.417 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.489 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.491 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.492 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.492 I llama_model_loader: - type  f32:  194 tensors
0.00.024.492 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.493 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.493 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.493 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.466 I llm_load_vocab: special tokens cache size = 25
0.00.051.515 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.518 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.518 I llm_load_print_meta: arch             = gptneox
0.00.051.518 I llm_load_print_meta: vocab type       = BPE
0.00.051.519 I llm_load_print_meta: n_vocab          = 50304
0.00.051.519 I llm_load_print_meta: n_merges         = 50009
0.00.051.519 I llm_load_print_meta: vocab_only       = 0
0.00.051.519 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.519 I llm_load_print_meta: n_embd           = 2048
0.00.051.520 I llm_load_print_meta: n_layer          = 24
0.00.051.534 I llm_load_print_meta: n_head           = 16
0.00.051.536 I llm_load_print_meta: n_head_kv        = 16
0.00.051.536 I llm_load_print_meta: n_rot            = 32
0.00.051.536 I llm_load_print_meta: n_swa            = 0
0.00.051.536 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.536 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.537 I llm_load_print_meta: n_gqa            = 1
0.00.051.538 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.538 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.539 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.539 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.540 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.540 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.540 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.540 I llm_load_print_meta: n_ff             = 8192
0.00.051.541 I llm_load_print_meta: n_expert         = 0
0.00.051.541 I llm_load_print_meta: n_expert_used    = 0
0.00.051.541 I llm_load_print_meta: causal attn      = 1
0.00.051.541 I llm_load_print_meta: pooling type     = 0
0.00.051.541 I llm_load_print_meta: rope type        = 2
0.00.051.541 I llm_load_print_meta: rope scaling     = linear
0.00.051.542 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.542 I llm_load_print_meta: freq_scale_train = 1
0.00.051.542 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.542 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.542 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.543 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.543 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.543 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.545 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.555 I llm_load_print_meta: model type       = 1.4B
0.00.051.556 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.556 I llm_load_print_meta: model params     = 1.41 B
0.00.051.557 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.558 I llm_load_print_meta: general.name     = 1.4B
0.00.051.558 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.558 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.559 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.559 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.559 I llm_load_print_meta: LF token         = 128 ''
0.00.051.559 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.559 I llm_load_print_meta: max token length = 1024
0.00.053.548 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.548 I llm_load_tensors: offloading output layer to GPU
0.00.053.548 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.558 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.560 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.448 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.448 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.449 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.449 I llama_new_context_with_model: n_batch       = 2048
0.00.054.449 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.449 I llama_new_context_with_model: flash_attn    = 0
0.00.054.450 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.450 I llama_new_context_with_model: freq_scale    = 1
0.00.054.450 I ggml_metal_init: allocating
0.00.054.453 I ggml_metal_init: found device: Apple M4
0.00.054.455 I ggml_metal_init: picking default device: Apple M4
0.00.055.049 I ggml_metal_init: using embedded metal library
0.00.057.499 I ggml_metal_init: GPU name:   Apple M4
0.00.057.500 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.501 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.501 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.501 I ggml_metal_init: simdgroup reduction   = true
0.00.057.501 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.502 I ggml_metal_init: has bfloat            = true
0.00.057.502 I ggml_metal_init: use bfloat            = true
0.00.057.502 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.503 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.944 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.962 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.009 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.010 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.010 I llama_new_context_with_model: graph nodes  = 967
0.00.088.011 I llama_new_context_with_model: graph splits = 2
0.00.088.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.549.394 I main: llama threadpool init, n_threads = 4
0.00.549.431 I 
0.00.549.463 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.549.464 I 
0.00.549.693 I sampler seed: 1234
0.00.549.699 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.549.710 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.549.710 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.549.711 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.296.089 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.296.090 I llama_perf_context_print:        load time =     540.74 ms
0.01.296.091 I llama_perf_context_print: prompt eval time =      40.50 ms /     7 tokens (    5.79 ms per token,   172.84 tokens per second)
0.01.296.091 I llama_perf_context_print:        eval time =     702.89 ms /    63 runs   (   11.16 ms per token,    89.63 tokens per second)
0.01.296.092 I llama_perf_context_print:       total time =     746.70 ms /    70 tokens
0.01.296.318 I ggml_metal_free: deallocating

real	0m1.313s
user	0m0.111s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.651 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.636 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.640 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.645 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.645 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.646 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.646 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.648 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.649 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.649 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.649 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.652 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.654 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.655 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.556 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.623 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.623 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.624 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.624 I llama_model_loader: - type  f32:  194 tensors
0.00.023.625 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.625 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.625 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.625 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.606 I llm_load_vocab: special tokens cache size = 25
0.00.050.612 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.614 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.615 I llm_load_print_meta: arch             = gptneox
0.00.050.615 I llm_load_print_meta: vocab type       = BPE
0.00.050.615 I llm_load_print_meta: n_vocab          = 50304
0.00.050.615 I llm_load_print_meta: n_merges         = 50009
0.00.050.616 I llm_load_print_meta: vocab_only       = 0
0.00.050.616 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.616 I llm_load_print_meta: n_embd           = 2048
0.00.050.616 I llm_load_print_meta: n_layer          = 24
0.00.050.630 I llm_load_print_meta: n_head           = 16
0.00.050.631 I llm_load_print_meta: n_head_kv        = 16
0.00.050.631 I llm_load_print_meta: n_rot            = 32
0.00.050.631 I llm_load_print_meta: n_swa            = 0
0.00.050.633 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.635 I llm_load_print_meta: n_gqa            = 1
0.00.050.635 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.636 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.638 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.638 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.639 I llm_load_print_meta: n_ff             = 8192
0.00.050.639 I llm_load_print_meta: n_expert         = 0
0.00.050.640 I llm_load_print_meta: n_expert_used    = 0
0.00.050.640 I llm_load_print_meta: causal attn      = 1
0.00.050.640 I llm_load_print_meta: pooling type     = 0
0.00.050.641 I llm_load_print_meta: rope type        = 2
0.00.050.642 I llm_load_print_meta: rope scaling     = linear
0.00.050.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.643 I llm_load_print_meta: freq_scale_train = 1
0.00.050.643 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.643 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.643 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.644 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.644 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.644 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.644 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.655 I llm_load_print_meta: model type       = 1.4B
0.00.050.656 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.657 I llm_load_print_meta: model params     = 1.41 B
0.00.050.658 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.658 I llm_load_print_meta: general.name     = 1.4B
0.00.050.658 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.659 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.659 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.659 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.659 I llm_load_print_meta: LF token         = 128 ''
0.00.050.659 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.659 I llm_load_print_meta: max token length = 1024
0.00.052.617 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.617 I llm_load_tensors: offloading output layer to GPU
0.00.052.617 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.628 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.629 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.580 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.581 I llama_new_context_with_model: n_ctx         = 128
0.00.053.581 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.581 I llama_new_context_with_model: n_batch       = 128
0.00.053.582 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.582 I llama_new_context_with_model: flash_attn    = 0
0.00.053.582 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.582 I llama_new_context_with_model: freq_scale    = 1
0.00.053.583 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.583 I ggml_metal_init: allocating
0.00.053.587 I ggml_metal_init: found device: Apple M4
0.00.053.589 I ggml_metal_init: picking default device: Apple M4
0.00.054.196 I ggml_metal_init: using embedded metal library
0.00.056.515 I ggml_metal_init: GPU name:   Apple M4
0.00.056.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.518 I ggml_metal_init: simdgroup reduction   = true
0.00.056.519 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.519 I ggml_metal_init: has bfloat            = true
0.00.056.519 I ggml_metal_init: use bfloat            = true
0.00.056.519 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.520 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.608 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.610 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.625 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.570 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.571 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.571 I llama_new_context_with_model: graph nodes  = 967
0.00.068.571 I llama_new_context_with_model: graph splits = 2
0.00.068.584 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.220 I 
0.00.487.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.263 I perplexity: tokenizing the input ..
0.00.494.951 I perplexity: tokenization took 7.686 ms
0.00.494.961 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.093 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.319 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.336 I llama_perf_context_print:        load time =     478.56 ms
0.00.628.337 I llama_perf_context_print: prompt eval time =     131.91 ms /   128 tokens (    1.03 ms per token,   970.39 tokens per second)
0.00.628.337 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.338 I llama_perf_context_print:       total time =     141.12 ms /   129 tokens
0.00.628.800 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.079s
sys	0m0.085s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.615 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.618 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.627 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.628 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.629 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.630 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.630 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.630 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.632 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.633 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.633 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.600 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.680 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.682 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.689 I llama_model_loader: - type  f32:  194 tensors
0.00.024.690 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.690 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.690 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.618 I llm_load_vocab: special tokens cache size = 25
0.00.052.667 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.673 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.678 I llm_load_print_meta: arch             = gptneox
0.00.052.678 I llm_load_print_meta: vocab type       = BPE
0.00.052.679 I llm_load_print_meta: n_vocab          = 50304
0.00.052.679 I llm_load_print_meta: n_merges         = 50009
0.00.052.679 I llm_load_print_meta: vocab_only       = 0
0.00.052.679 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.680 I llm_load_print_meta: n_embd           = 2048
0.00.052.680 I llm_load_print_meta: n_layer          = 24
0.00.052.698 I llm_load_print_meta: n_head           = 16
0.00.052.700 I llm_load_print_meta: n_head_kv        = 16
0.00.052.700 I llm_load_print_meta: n_rot            = 32
0.00.052.700 I llm_load_print_meta: n_swa            = 0
0.00.052.700 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.700 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.701 I llm_load_print_meta: n_gqa            = 1
0.00.052.701 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.702 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.702 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.703 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.703 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.703 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.703 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.704 I llm_load_print_meta: n_ff             = 8192
0.00.052.704 I llm_load_print_meta: n_expert         = 0
0.00.052.704 I llm_load_print_meta: n_expert_used    = 0
0.00.052.704 I llm_load_print_meta: causal attn      = 1
0.00.052.704 I llm_load_print_meta: pooling type     = 0
0.00.052.704 I llm_load_print_meta: rope type        = 2
0.00.052.705 I llm_load_print_meta: rope scaling     = linear
0.00.052.705 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.705 I llm_load_print_meta: freq_scale_train = 1
0.00.052.705 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.706 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.706 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.706 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.706 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.716 I llm_load_print_meta: model type       = 1.4B
0.00.052.717 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.717 I llm_load_print_meta: model params     = 1.41 B
0.00.052.717 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.718 I llm_load_print_meta: general.name     = 1.4B
0.00.052.718 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.718 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.718 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.718 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.719 I llm_load_print_meta: LF token         = 128 ''
0.00.052.719 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.719 I llm_load_print_meta: max token length = 1024
0.00.054.623 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.623 I llm_load_tensors: offloading output layer to GPU
0.00.054.623 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.635 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.636 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.564 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.565 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.565 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.566 I llama_new_context_with_model: n_batch       = 2048
0.00.055.566 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.566 I llama_new_context_with_model: flash_attn    = 0
0.00.055.567 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.567 I llama_new_context_with_model: freq_scale    = 1
0.00.055.567 I ggml_metal_init: allocating
0.00.055.573 I ggml_metal_init: found device: Apple M4
0.00.055.575 I ggml_metal_init: picking default device: Apple M4
0.00.056.220 I ggml_metal_init: using embedded metal library
0.00.058.627 I ggml_metal_init: GPU name:   Apple M4
0.00.058.629 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.629 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.630 I ggml_metal_init: simdgroup reduction   = true
0.00.058.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.630 I ggml_metal_init: has bfloat            = true
0.00.058.630 I ggml_metal_init: use bfloat            = true
0.00.058.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.676 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.683 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.701 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.602 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.603 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.603 I llama_new_context_with_model: graph nodes  = 967
0.00.088.604 I llama_new_context_with_model: graph splits = 2
0.00.088.618 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.947 I main: llama threadpool init, n_threads = 4
0.00.610.994 I 
0.00.611.021 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.611.021 I 
0.00.611.230 I sampler seed: 1234
0.00.611.234 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.245 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.245 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.245 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.437 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48135.59 tokens per second)
0.01.376.438 I llama_perf_context_print:        load time =     602.33 ms
0.01.376.439 I llama_perf_context_print: prompt eval time =      47.30 ms /     7 tokens (    6.76 ms per token,   147.99 tokens per second)
0.01.376.440 I llama_perf_context_print:        eval time =     715.19 ms /    63 runs   (   11.35 ms per token,    88.09 tokens per second)
0.01.376.441 I llama_perf_context_print:       total time =     765.49 ms /    70 tokens
0.01.376.655 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.111s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.435 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.210 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.215 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.217 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.218 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.218 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.218 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.219 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.220 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.220 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.221 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.221 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.223 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.968 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.064 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.920 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.921 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.923 I llama_model_loader: - type  f32:  194 tensors
0.00.024.923 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.923 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.924 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.057 I llm_load_vocab: special tokens cache size = 25
0.00.051.072 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.079 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.080 I llm_load_print_meta: arch             = gptneox
0.00.051.080 I llm_load_print_meta: vocab type       = BPE
0.00.051.080 I llm_load_print_meta: n_vocab          = 50304
0.00.051.080 I llm_load_print_meta: n_merges         = 50009
0.00.051.083 I llm_load_print_meta: vocab_only       = 0
0.00.051.083 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.083 I llm_load_print_meta: n_embd           = 2048
0.00.051.083 I llm_load_print_meta: n_layer          = 24
0.00.051.093 I llm_load_print_meta: n_head           = 16
0.00.051.093 I llm_load_print_meta: n_head_kv        = 16
0.00.051.099 I llm_load_print_meta: n_rot            = 32
0.00.051.099 I llm_load_print_meta: n_swa            = 0
0.00.051.099 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.100 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.108 I llm_load_print_meta: n_gqa            = 1
0.00.051.109 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.110 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.113 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.113 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.114 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.114 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.115 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.116 I llm_load_print_meta: n_ff             = 8192
0.00.051.117 I llm_load_print_meta: n_expert         = 0
0.00.051.117 I llm_load_print_meta: n_expert_used    = 0
0.00.051.117 I llm_load_print_meta: causal attn      = 1
0.00.051.117 I llm_load_print_meta: pooling type     = 0
0.00.051.117 I llm_load_print_meta: rope type        = 2
0.00.051.117 I llm_load_print_meta: rope scaling     = linear
0.00.051.118 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.118 I llm_load_print_meta: freq_scale_train = 1
0.00.051.118 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.118 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.119 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.119 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.119 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.119 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.119 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.124 I llm_load_print_meta: model type       = 1.4B
0.00.051.124 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.125 I llm_load_print_meta: model params     = 1.41 B
0.00.051.125 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.125 I llm_load_print_meta: general.name     = 1.4B
0.00.051.126 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.126 I llm_load_print_meta: LF token         = 128 ''
0.00.051.127 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.127 I llm_load_print_meta: max token length = 1024
0.00.053.064 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.064 I llm_load_tensors: offloading output layer to GPU
0.00.053.064 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.075 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.076 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.056 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.057 I llama_new_context_with_model: n_ctx         = 128
0.00.054.057 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.057 I llama_new_context_with_model: n_batch       = 128
0.00.054.057 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.057 I llama_new_context_with_model: flash_attn    = 0
0.00.054.058 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.058 I llama_new_context_with_model: freq_scale    = 1
0.00.054.059 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.059 I ggml_metal_init: allocating
0.00.054.063 I ggml_metal_init: found device: Apple M4
0.00.054.065 I ggml_metal_init: picking default device: Apple M4
0.00.054.666 I ggml_metal_init: using embedded metal library
0.00.056.958 I ggml_metal_init: GPU name:   Apple M4
0.00.056.959 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.959 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.960 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.960 I ggml_metal_init: simdgroup reduction   = true
0.00.056.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.960 I ggml_metal_init: has bfloat            = true
0.00.056.960 I ggml_metal_init: use bfloat            = true
0.00.056.961 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.697 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.699 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.713 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.648 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.649 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.649 I llama_new_context_with_model: graph nodes  = 967
0.00.068.649 I llama_new_context_with_model: graph splits = 2
0.00.068.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.567.498 I 
0.00.567.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.567.556 I perplexity: tokenizing the input ..
0.00.575.178 I perplexity: tokenization took 7.621 ms
0.00.575.189 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.303 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.710.465 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.710.484 I llama_perf_context_print:        load time =     557.06 ms
0.00.710.485 I llama_perf_context_print: prompt eval time =     133.89 ms /   128 tokens (    1.05 ms per token,   956.01 tokens per second)
0.00.710.489 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.710.490 I llama_perf_context_print:       total time =     142.99 ms /   129 tokens
0.00.710.971 I ggml_metal_free: deallocating

real	0m0.725s
user	0m0.077s
sys	0m0.102s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.009.533 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.108 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.113 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.115 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.115 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.116 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.117 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.118 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.119 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.119 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.120 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.120 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.038 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.018 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.020 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.020 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.021 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.021 I llama_model_loader: - type  f32:  194 tensors
0.00.025.022 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.022 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.283 I llm_load_vocab: special tokens cache size = 25
0.00.051.155 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.157 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.157 I llm_load_print_meta: arch             = gptneox
0.00.051.158 I llm_load_print_meta: vocab type       = BPE
0.00.051.158 I llm_load_print_meta: n_vocab          = 50304
0.00.051.158 I llm_load_print_meta: n_merges         = 50009
0.00.051.158 I llm_load_print_meta: vocab_only       = 0
0.00.051.159 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.159 I llm_load_print_meta: n_embd           = 2048
0.00.051.159 I llm_load_print_meta: n_layer          = 24
0.00.051.173 I llm_load_print_meta: n_head           = 16
0.00.051.174 I llm_load_print_meta: n_head_kv        = 16
0.00.051.174 I llm_load_print_meta: n_rot            = 32
0.00.051.174 I llm_load_print_meta: n_swa            = 0
0.00.051.174 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.175 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.177 I llm_load_print_meta: n_gqa            = 1
0.00.051.178 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.179 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.179 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.180 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.180 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.180 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.180 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.181 I llm_load_print_meta: n_ff             = 8192
0.00.051.181 I llm_load_print_meta: n_expert         = 0
0.00.051.181 I llm_load_print_meta: n_expert_used    = 0
0.00.051.183 I llm_load_print_meta: causal attn      = 1
0.00.051.184 I llm_load_print_meta: pooling type     = 0
0.00.051.184 I llm_load_print_meta: rope type        = 2
0.00.051.184 I llm_load_print_meta: rope scaling     = linear
0.00.051.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.184 I llm_load_print_meta: freq_scale_train = 1
0.00.051.185 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.186 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.186 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.186 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.186 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.186 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.186 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.195 I llm_load_print_meta: model type       = 1.4B
0.00.051.196 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.196 I llm_load_print_meta: model params     = 1.41 B
0.00.051.197 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.198 I llm_load_print_meta: general.name     = 1.4B
0.00.051.198 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: LF token         = 128 ''
0.00.051.199 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: max token length = 1024
0.00.053.205 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.205 I llm_load_tensors: offloading output layer to GPU
0.00.053.205 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.216 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.217 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.108 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.109 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.110 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.110 I llama_new_context_with_model: n_batch       = 2048
0.00.054.110 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.110 I llama_new_context_with_model: flash_attn    = 0
0.00.054.110 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.111 I llama_new_context_with_model: freq_scale    = 1
0.00.054.111 I ggml_metal_init: allocating
0.00.054.114 I ggml_metal_init: found device: Apple M4
0.00.054.116 I ggml_metal_init: picking default device: Apple M4
0.00.054.726 I ggml_metal_init: using embedded metal library
0.00.056.998 I ggml_metal_init: GPU name:   Apple M4
0.00.056.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.002 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.002 I ggml_metal_init: simdgroup reduction   = true
0.00.057.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.002 I ggml_metal_init: has bfloat            = true
0.00.057.002 I ggml_metal_init: use bfloat            = true
0.00.057.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.311 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.316 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.454 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.456 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.456 I llama_new_context_with_model: graph nodes  = 967
0.00.086.456 I llama_new_context_with_model: graph splits = 2
0.00.086.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.939 I main: llama threadpool init, n_threads = 4
0.00.711.976 I 
0.00.712.004 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.007 I 
0.00.712.247 I sampler seed: 1234
0.00.712.253 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.264 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.264 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.264 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.578.834 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58872.31 tokens per second)
0.01.578.835 I llama_perf_context_print:        load time =     702.40 ms
0.01.578.835 I llama_perf_context_print: prompt eval time =      51.67 ms /     7 tokens (    7.38 ms per token,   135.48 tokens per second)
0.01.578.836 I llama_perf_context_print:        eval time =     811.96 ms /    63 runs   (   12.89 ms per token,    77.59 tokens per second)
0.01.578.836 I llama_perf_context_print:       total time =     866.90 ms /    70 tokens
0.01.579.053 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.596 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.600 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.603 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.603 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.603 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.604 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.605 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.606 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.607 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.607 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.607 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.608 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.610 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.513 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.574 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.498 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.499 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.500 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.501 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.501 I llama_model_loader: - type  f32:  194 tensors
0.00.024.502 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.502 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.432 I llm_load_vocab: special tokens cache size = 25
0.00.051.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.516 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.516 I llm_load_print_meta: arch             = gptneox
0.00.051.516 I llm_load_print_meta: vocab type       = BPE
0.00.051.516 I llm_load_print_meta: n_vocab          = 50304
0.00.051.517 I llm_load_print_meta: n_merges         = 50009
0.00.051.517 I llm_load_print_meta: vocab_only       = 0
0.00.051.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.517 I llm_load_print_meta: n_embd           = 2048
0.00.051.517 I llm_load_print_meta: n_layer          = 24
0.00.051.530 I llm_load_print_meta: n_head           = 16
0.00.051.531 I llm_load_print_meta: n_head_kv        = 16
0.00.051.531 I llm_load_print_meta: n_rot            = 32
0.00.051.531 I llm_load_print_meta: n_swa            = 0
0.00.051.532 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.532 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.532 I llm_load_print_meta: n_gqa            = 1
0.00.051.533 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.534 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.534 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.535 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.535 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.535 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.535 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.536 I llm_load_print_meta: n_ff             = 8192
0.00.051.536 I llm_load_print_meta: n_expert         = 0
0.00.051.536 I llm_load_print_meta: n_expert_used    = 0
0.00.051.537 I llm_load_print_meta: causal attn      = 1
0.00.051.537 I llm_load_print_meta: pooling type     = 0
0.00.051.537 I llm_load_print_meta: rope type        = 2
0.00.051.537 I llm_load_print_meta: rope scaling     = linear
0.00.051.537 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.538 I llm_load_print_meta: freq_scale_train = 1
0.00.051.538 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.538 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.538 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.538 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.538 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.539 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.547 I llm_load_print_meta: model type       = 1.4B
0.00.051.548 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.548 I llm_load_print_meta: model params     = 1.41 B
0.00.051.548 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.549 I llm_load_print_meta: general.name     = 1.4B
0.00.051.549 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.549 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.549 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.550 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.550 I llm_load_print_meta: LF token         = 128 ''
0.00.051.550 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.550 I llm_load_print_meta: max token length = 1024
0.00.053.109 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.109 I llm_load_tensors: offloading output layer to GPU
0.00.053.109 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.119 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.120 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.980 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.981 I llama_new_context_with_model: n_ctx         = 128
0.00.053.981 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.981 I llama_new_context_with_model: n_batch       = 128
0.00.053.982 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.982 I llama_new_context_with_model: flash_attn    = 0
0.00.053.982 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.982 I llama_new_context_with_model: freq_scale    = 1
0.00.053.983 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.983 I ggml_metal_init: allocating
0.00.053.986 I ggml_metal_init: found device: Apple M4
0.00.053.989 I ggml_metal_init: picking default device: Apple M4
0.00.054.556 I ggml_metal_init: using embedded metal library
0.00.056.909 I ggml_metal_init: GPU name:   Apple M4
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.910 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.911 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.911 I ggml_metal_init: simdgroup reduction   = true
0.00.056.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.911 I ggml_metal_init: has bfloat            = true
0.00.056.912 I ggml_metal_init: use bfloat            = true
0.00.056.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.913 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.039 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.041 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.054 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.980 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.981 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.982 I llama_new_context_with_model: graph nodes  = 967
0.00.068.982 I llama_new_context_with_model: graph splits = 2
0.00.068.994 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.641 I 
0.00.657.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.697 I perplexity: tokenizing the input ..
0.00.665.494 I perplexity: tokenization took 7.796 ms
0.00.665.509 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.269 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.807.462 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.807.476 I llama_perf_context_print:        load time =     647.84 ms
0.00.807.477 I llama_perf_context_print: prompt eval time =     140.53 ms /   128 tokens (    1.10 ms per token,   910.82 tokens per second)
0.00.807.477 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.478 I llama_perf_context_print:       total time =     149.84 ms /   129 tokens
0.00.807.876 I ggml_metal_free: deallocating

real	0m0.823s
user	0m0.079s
sys	0m0.125s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.514 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.285 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.286 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.286 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.286 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.287 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.288 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.288 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.288 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.289 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.289 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.291 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.224 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.368 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.369 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.369 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.369 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.369 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.370 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.370 I llama_model_loader: - type  f32:  194 tensors
0.00.024.370 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.104 I llm_load_vocab: special tokens cache size = 25
0.00.051.151 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.154 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.155 I llm_load_print_meta: arch             = gptneox
0.00.051.155 I llm_load_print_meta: vocab type       = BPE
0.00.051.155 I llm_load_print_meta: n_vocab          = 50304
0.00.051.155 I llm_load_print_meta: n_merges         = 50009
0.00.051.155 I llm_load_print_meta: vocab_only       = 0
0.00.051.156 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.156 I llm_load_print_meta: n_embd           = 2048
0.00.051.156 I llm_load_print_meta: n_layer          = 24
0.00.051.170 I llm_load_print_meta: n_head           = 16
0.00.051.172 I llm_load_print_meta: n_head_kv        = 16
0.00.051.172 I llm_load_print_meta: n_rot            = 32
0.00.051.172 I llm_load_print_meta: n_swa            = 0
0.00.051.172 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.172 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.173 I llm_load_print_meta: n_gqa            = 1
0.00.051.175 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.175 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.176 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.176 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.176 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.176 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.176 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.177 I llm_load_print_meta: n_ff             = 8192
0.00.051.177 I llm_load_print_meta: n_expert         = 0
0.00.051.177 I llm_load_print_meta: n_expert_used    = 0
0.00.051.178 I llm_load_print_meta: causal attn      = 1
0.00.051.178 I llm_load_print_meta: pooling type     = 0
0.00.051.179 I llm_load_print_meta: rope type        = 2
0.00.051.179 I llm_load_print_meta: rope scaling     = linear
0.00.051.180 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.180 I llm_load_print_meta: freq_scale_train = 1
0.00.051.180 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.180 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.181 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.181 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.181 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.181 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.190 I llm_load_print_meta: model type       = 1.4B
0.00.051.190 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.191 I llm_load_print_meta: model params     = 1.41 B
0.00.051.191 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.191 I llm_load_print_meta: general.name     = 1.4B
0.00.051.192 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.192 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.192 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.192 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.193 I llm_load_print_meta: LF token         = 128 ''
0.00.051.193 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.193 I llm_load_print_meta: max token length = 1024
0.00.052.849 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.849 I llm_load_tensors: offloading output layer to GPU
0.00.052.849 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.859 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.860 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.701 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.702 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.702 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.702 I llama_new_context_with_model: n_batch       = 2048
0.00.053.702 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.702 I llama_new_context_with_model: flash_attn    = 0
0.00.053.703 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.703 I llama_new_context_with_model: freq_scale    = 1
0.00.053.703 I ggml_metal_init: allocating
0.00.053.707 I ggml_metal_init: found device: Apple M4
0.00.053.708 I ggml_metal_init: picking default device: Apple M4
0.00.054.301 I ggml_metal_init: using embedded metal library
0.00.056.831 I ggml_metal_init: GPU name:   Apple M4
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.833 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.833 I ggml_metal_init: simdgroup reduction   = true
0.00.056.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.833 I ggml_metal_init: has bfloat            = true
0.00.056.833 I ggml_metal_init: use bfloat            = true
0.00.056.834 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.458 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.463 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.479 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.568 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.570 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.570 I llama_new_context_with_model: graph nodes  = 967
0.00.088.571 I llama_new_context_with_model: graph splits = 2
0.00.088.584 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.019 I main: llama threadpool init, n_threads = 4
0.00.786.066 I 
0.00.786.095 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.097 I 
0.00.786.337 I sampler seed: 1234
0.00.786.341 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.352 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.352 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.353 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.666.885 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.666.886 I llama_perf_context_print:        load time =     777.50 ms
0.01.666.886 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.41 tokens per second)
0.01.666.887 I llama_perf_context_print:        eval time =     823.01 ms /    63 runs   (   13.06 ms per token,    76.55 tokens per second)
0.01.666.887 I llama_perf_context_print:       total time =     880.87 ms /    70 tokens
0.01.667.081 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.110s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4291 (ce8784bd) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.647 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.390 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.394 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.396 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.396 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.397 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.397 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.397 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.398 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.399 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.399 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.399 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.400 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.400 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.400 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.404 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.405 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.405 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.237 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.146 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.147 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.148 I llama_model_loader: - type  f32:  194 tensors
0.00.023.148 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.308 I llm_load_vocab: special tokens cache size = 25
0.00.049.331 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.334 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.334 I llm_load_print_meta: arch             = gptneox
0.00.049.335 I llm_load_print_meta: vocab type       = BPE
0.00.049.335 I llm_load_print_meta: n_vocab          = 50304
0.00.049.335 I llm_load_print_meta: n_merges         = 50009
0.00.049.335 I llm_load_print_meta: vocab_only       = 0
0.00.049.336 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.336 I llm_load_print_meta: n_embd           = 2048
0.00.049.336 I llm_load_print_meta: n_layer          = 24
0.00.049.352 I llm_load_print_meta: n_head           = 16
0.00.049.353 I llm_load_print_meta: n_head_kv        = 16
0.00.049.353 I llm_load_print_meta: n_rot            = 32
0.00.049.353 I llm_load_print_meta: n_swa            = 0
0.00.049.354 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.354 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.354 I llm_load_print_meta: n_gqa            = 1
0.00.049.355 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.356 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.356 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.357 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.357 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.357 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.357 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.358 I llm_load_print_meta: n_ff             = 8192
0.00.049.358 I llm_load_print_meta: n_expert         = 0
0.00.049.358 I llm_load_print_meta: n_expert_used    = 0
0.00.049.358 I llm_load_print_meta: causal attn      = 1
0.00.049.359 I llm_load_print_meta: pooling type     = 0
0.00.049.359 I llm_load_print_meta: rope type        = 2
0.00.049.359 I llm_load_print_meta: rope scaling     = linear
0.00.049.359 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.360 I llm_load_print_meta: freq_scale_train = 1
0.00.049.360 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.361 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.361 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.361 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.361 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.361 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.362 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.371 I llm_load_print_meta: model type       = 1.4B
0.00.049.371 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.372 I llm_load_print_meta: model params     = 1.41 B
0.00.049.372 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.372 I llm_load_print_meta: general.name     = 1.4B
0.00.049.372 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.373 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.373 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.373 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.373 I llm_load_print_meta: LF token         = 128 ''
0.00.049.373 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.373 I llm_load_print_meta: max token length = 1024
0.00.051.324 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.325 I llm_load_tensors: offloading output layer to GPU
0.00.051.325 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.335 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.336 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.208 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.209 I llama_new_context_with_model: n_ctx         = 128
0.00.052.209 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.209 I llama_new_context_with_model: n_batch       = 128
0.00.052.210 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.210 I llama_new_context_with_model: flash_attn    = 0
0.00.052.210 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.210 I llama_new_context_with_model: freq_scale    = 1
0.00.052.211 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.211 I ggml_metal_init: allocating
0.00.052.214 I ggml_metal_init: found device: Apple M4
0.00.052.216 I ggml_metal_init: picking default device: Apple M4
0.00.052.787 I ggml_metal_init: using embedded metal library
0.00.055.070 I ggml_metal_init: GPU name:   Apple M4
0.00.055.072 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.073 I ggml_metal_init: simdgroup reduction   = true
0.00.055.073 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.073 I ggml_metal_init: has bfloat            = true
0.00.055.073 I ggml_metal_init: use bfloat            = true
0.00.055.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.074 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.704 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.706 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.719 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.625 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.626 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.626 I llama_new_context_with_model: graph nodes  = 967
0.00.066.627 I llama_new_context_with_model: graph splits = 2
0.00.066.639 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.386.536 I 
0.00.386.564 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.386.572 I perplexity: tokenizing the input ..
0.00.394.063 I perplexity: tokenization took 7.49 ms
0.00.394.074 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.534.349 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.535.520 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.535.538 I llama_perf_context_print:        load time =     377.88 ms
0.00.535.539 I llama_perf_context_print: prompt eval time =     140.04 ms /   128 tokens (    1.09 ms per token,   914.00 tokens per second)
0.00.535.540 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.535.540 I llama_perf_context_print:       total time =     149.00 ms /   129 tokens
0.00.536.012 I ggml_metal_free: deallocating

real	0m0.550s
user	0m0.077s
sys	0m0.085s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4291 (ce8784bd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e0a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e0a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e0af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e0b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e0c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e0cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e0d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e0d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e0e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e0f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e0fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e10220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e10940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e11060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e11780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e11f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e12670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e14470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e14d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e159b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e15ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e16650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e176e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e179a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e182e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e19560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e19a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e19ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e1a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e1a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e1c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e1c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e1d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e1d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e1e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e1f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e1f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e20450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e20d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e21230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e21b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e23290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e23bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e24120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e24670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e24bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e25110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e25bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e26650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e26ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e27640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e28630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e29b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e2a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e2b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e2b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e2bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e1b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e2bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e2c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e2ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e2d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e2d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e2e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e2eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e2f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e2f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e2fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e30c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e315c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e31f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e323a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e32840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e32ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e33620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e33ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e33f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e34400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e34d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e35680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e35fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e36460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e36da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e37240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e37b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e38020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e38960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e38e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e39740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e3a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e3a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e3a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e3ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e3b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e3bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e3c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e3ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e3cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e3d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e3dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e3f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e3f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e3fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e40640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e40ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e40f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e426a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e43480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e43920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e43dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e44700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e44ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e45040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e454e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e45980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e45e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e46760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e46c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e47540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e479e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e48e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e493c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e49c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e4a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e4a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e4be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e4c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e4cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e4d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e4d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e4e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e4e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e4f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e4fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e50180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e50c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e52160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e526b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e53150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e536a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e53bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e54690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e54be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e55130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e55680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e55bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e56120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e56670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e56bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e57110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e57660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e58100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e58650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e58ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e59640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e59b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e5a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e5a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e5ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e5b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e5b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e5bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e5c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e5c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e5cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e5d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e5d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e5db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e5e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e5e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e5eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e5f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e5f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e5fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e60080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e60b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e61460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e61900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e61da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e62240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e62b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e63020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e63960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e63e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e642a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e64be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e65080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e65cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e66410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e66b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e67250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e67510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e67d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e67fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e685d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.121.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137f07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137f077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137f07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137f08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137f09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137f09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137f0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137f0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137f0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137f0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137f0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137f0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137f0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137f0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137f0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137f0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137f0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137f0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137f0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137f0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137f10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137f104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137f10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137f11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137f11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137f123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137f12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137f13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137f13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137f13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137f14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137f15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x137f154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137f15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x137f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137f16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137f16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x137f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137f17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137f17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x137f18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137f18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x137f18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137f19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137f1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137f1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x137f1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x137f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x137f1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x137f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x137f1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x137f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x137f1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x137f1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x137f1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x137f1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x137f1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x137f1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x137f1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x137f1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x137f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x137f1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x137f1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x137f1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x137f1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x137f20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x137f207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x137f20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137f21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x137f21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137f21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137f21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137f22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x137f226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137f22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137f22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137f23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137f23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137f23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137f24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x137f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137f24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x137f24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137f25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137f25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137f25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137f26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x137f264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137f26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137f26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137f27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137f276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137f27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137f27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x137f283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137f28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137f28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137f29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x137f295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137f29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137f29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x137f2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137f2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137f2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x137f2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x137f2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x137f2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x137f2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x137f2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x137f2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x137f2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x137f2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x137f2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x137f2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x137f2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x137f2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x137f2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x137f2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x137f2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x137f2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x137f2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x137f2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x137f30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x137f304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x137f30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x137f30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x137f311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137f31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137f31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137f31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x137f323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137f32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137f32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x137f33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137f33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137f339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137f33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x137f342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137f34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137f34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137f35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137f35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137f358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137f35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x137f361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137f36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137f36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137f36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137f37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x137f37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137f37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x137f380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137f38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137f389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137f38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137f392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137f39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137f39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x137f39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137f3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137f3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137f3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x137f3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x137f3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x137f3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x137f3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x137f3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x137f3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x137f3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x137f3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x137f3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x137f3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x137f3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x137f3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x137f3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x137f3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x137f3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x137f3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x137f3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x137f3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x137f40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x137f40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x137f40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x137f41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137f41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x137f41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137f42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137f42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137f429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137f42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137f43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x137f43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137f43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x137f43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137f44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137f448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137f44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137f451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137f45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137f45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x137f45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137f46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x137f467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137f46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x137f470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137f47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137f47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137f47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137f48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x137f486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137f48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x137f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137f49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137f498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137f49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137f4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x137f4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137f4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x137f4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x137f4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x137f4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x137f4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x137f4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x137f4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x137f4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x137f4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x137f4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x137f4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x137f4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x137f4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x137f4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x137f4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x137f4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x137f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x137f4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x137f4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x137f4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x137f50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x137f50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x137f50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137f51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x137f514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137f51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137f51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137f52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137f526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137f52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137f52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x137f533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137f53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137f53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137f54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x137f545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137f54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137f54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x137f55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137f55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137f56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137f56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137f57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137f57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137f57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x137f582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137f588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x137f58ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136e24ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136e25150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136e255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136e25a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136e25ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136e26310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136e26780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136e26bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136e27060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136e274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136e27940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136e27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136e28810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136e28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136e29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136e29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136e2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136e2ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136e2b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136e2bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136e2c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136e2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136e2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136e2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136e2df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136e2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136e2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136e2ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136e2f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136e2f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136e2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136e2fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136e302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136e305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136e30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136e30e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136e312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136e31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136e31bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136e32040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136e324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136e32920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136e32d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136e33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136e33670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136e33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136e33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136e343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136e34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136e34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136e35110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136e35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136e359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136e35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136e362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136e36740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136e36bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136e37020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136e37490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136e37900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136e37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136e381e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136e38650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136e38ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136e38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136e393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136e39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136e39c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136e3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136e3a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136e3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136e3ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136e3b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136e3b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136e3bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136e3c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136e3c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136e3c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136e3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136e3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136e3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136e3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136e3df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136e3e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136e3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136e3ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136e3f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136e3f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136e3f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136e3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136e40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136e40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136e40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136e40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136e41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136e418c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136e41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136e421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136e42610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136e42a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136e42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136e43360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136e437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136e43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136e440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136e44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136e44990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136e44e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136e45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136e456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136e45b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136e45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136e46430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136e468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136e46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136e47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136e475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136e47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136e47ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136e48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136e487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136e48c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136e49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136e49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136e49970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136e49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136e4a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136e4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136e4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136e4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136e4b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136e4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x136e4bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136e4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136e4c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136e4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136e4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136e4d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136e4d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136e4dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136e4e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136e4e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136e4e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x136e4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136e4f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x136e4f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136e4fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136e4ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136e503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136e50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136e50cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136e51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136e515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136e51a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136e51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136e52300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136e52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136e52be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136e53050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136e534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136e53930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136e53da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136e54210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136e54680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136e54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136e54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136e55840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136e55cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136e56120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136e56590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136e56a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136e56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136e572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136e57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136e57bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136e58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136e584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136e58910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136e58d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136e591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136e59660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136e59ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136e59f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136e5a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136e5a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136e5ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136e5b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136e5b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136e5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136e5be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136e5c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136e5c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136e5cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136e5d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136e5d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136e5d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136e5dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136e5e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136e5e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136e5eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136e5ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136e5f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136e5f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136e5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136e600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136e60550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136e609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136e60e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136e612a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136e61a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136e61e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136e62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136e62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136e62be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136e63050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136e634c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136e63da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136e64210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136e64680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136e64af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136e64f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136e653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136e65840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136e65cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136e66120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136e66590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136e66a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136e66e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136e672e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136e67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136e67bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136e68030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136e684a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136e0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136e0ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136e09870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136e0a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136e17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136e17cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136e18160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136e185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136e18a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136e18eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136e19320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136e19790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136e19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136e1a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136e1a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136e1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136e1adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136e1b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x136e1b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136e1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136e1bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136e1c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136e1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136e1ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136e1d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136e1d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136e1da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136e1de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136e1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136e1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136e1ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136e1f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136e1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136e1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136e1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136e20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136e20680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136e20af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136e20f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136e213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136e21840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136e21cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136e22120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136e22590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136e22a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136e22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136e232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136e23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136e23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136e24530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136e16310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136e16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136e16e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136e0d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136e0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136e0dec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.779s
user	0m0.274s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4291 (ce8784bd)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a7102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a7109d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a710f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a711530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a711ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a712090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a712bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a7131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a7136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a713ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a7140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a714bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a715370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a715b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a7162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a7169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a7170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a717800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a7186f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a718e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a719dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a71a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a71a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a71adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a71ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a71bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a71c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a71c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a71c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a71d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a71d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a71da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a71dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a71e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a71e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a71eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a71f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a71f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a71fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a71ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a7203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a720c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a7212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a721bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a7221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a7227e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a722df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a723400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a723a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a724810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a724cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a725150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a725a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a726210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a7264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a726970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a726e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a7272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a727750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a727bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a728090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a728530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a7289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a728e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a729310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a7297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a729c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a72a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15a72a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a72ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a72b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a72b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15a72bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a72c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a72c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15a72cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a72d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a72d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a72dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a72e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a72e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15a72ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15a72f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15a72f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a72fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a730140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a730690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a730be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a731130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a731680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a731bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a7218b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15a732040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15a7327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a732d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a733290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a7337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a733d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a734280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15a7347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15a734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15a735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15a7357c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a735d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15a736260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a7367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15a736d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a7371a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a737640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a737ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a737f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a738420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a7388c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a738d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a739200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a7396a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a739b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a739fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a73a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a73a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a73adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a73b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a73b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a73bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a73c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a73c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a73c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a73ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a73d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a73d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a73dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a73e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a73e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a73e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a73ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a73f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a73f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a73fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a740100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a7405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a740a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a740ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a741380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a741820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a741cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a742160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a742600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a742aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a742f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a7433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a743d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a7441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a744660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a744b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a744fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a745440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a7458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a745d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a746220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a7466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a746b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a747000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a7474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a747de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a748280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a748bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a749060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a749500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a7499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a749e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a74a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a74a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a74ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a74b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a74b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a74ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a74bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a74c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a74c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a74cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a74d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a74d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a74da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a74df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a74e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a74e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a74eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a74f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a74f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a74fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a750320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a750930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a751120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a7515c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a751e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a7524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a753130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a7535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a753a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a754220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a754770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a754cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a755210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a755760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a755cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a756200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a756750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a756ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a7571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a757740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a757c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a7581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a758730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a758c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a7591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a759720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a759c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a75a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a75a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a75ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a75b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a75b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a75bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a75c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a75c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a75cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a75d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a75d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a75dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a75e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a75e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a75ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a75f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a75f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a75fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a760160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a7606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a760c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a761150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a7616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a761bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a762140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a762690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a762be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a763130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a763680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a763bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a764120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a764670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a764bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a765110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a765660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a765bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a766100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a766650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a766ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a767040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a767980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a767e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a7682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a768760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a768c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a7690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a769540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a7699e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a769e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a76a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a76a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a76ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a76b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a76b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a76bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a76c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a76cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a76d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a76d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a76dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a76e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a76e650 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a605bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a606020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a606490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a606900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a606d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a6071e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a607650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a607ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a607f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a6083a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a608810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a608e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a609990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a60a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a60a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a60b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a60b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a60beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a60c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a60cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a60d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a60dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a60e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a60ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a60f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a60f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a60f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a60fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a60ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a610410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a610db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a611220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a6114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a611950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a6126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a612b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a612f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a6133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a613860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a613cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a614140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a6145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a614a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a615770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a615be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a616050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a6164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a616930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a616da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a617680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a617bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a6180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a618560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a6189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a618e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a6192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a619720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a619b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a61a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a61a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a61a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a61ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a61b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a61b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a61baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a61bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a61c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a61c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15a61cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a61d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15a61d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15a61d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15a61de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a61e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a61e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15a61eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15a61efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a61f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15a61f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15a61fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15a6201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15a620610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15a620a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15a620ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15a621360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a6217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15a621c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15a6220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15a622e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15a6236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15a623b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a623fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a624430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a6248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15a624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15a6255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15a625a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15a625ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15a626340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a6267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15a626c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a627090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15a627500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a627970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a627de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a6286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a629410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a629880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a629cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a62a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a62a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a62aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a62aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a62b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a62b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a62bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a62c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a62c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a62c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a62cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a62d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a62d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a62db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a62df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a62e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a62e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a62ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a62f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a62f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a62fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a62fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a630300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a630be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a631050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a6314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a631930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a631da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a632210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a632680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a632af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a632f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a6333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a633840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a633cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a634120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a634590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a634a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a634e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a6352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a635bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a6364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a636910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a636d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a6371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a637660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a637ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a637f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a6383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a638820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a638c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a639100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a639570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a6399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a639e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a63a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a63a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a63aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a63b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a63b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a63b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a63bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a63c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a63c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a63cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a63cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a63d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a63d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a63dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a63e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a63e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a63e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a63f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a63f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a63fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a63fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15a640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a6408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a640d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a6411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a641620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a641bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a642020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a642490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a642fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a6432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a643560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a6439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a643e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a6442b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a644720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a645000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a645470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a6458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a6461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a646630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a646aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a646f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a647380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a6477f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a647c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a6480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a648540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a6489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a648e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a649290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a649700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a649b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a649fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a64a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a64a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a64ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a64b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a64b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a64ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a64bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a64c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a64c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a64cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a64d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a64d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a64d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a64de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a64e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a64e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a64eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a64efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a64f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a64f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a64fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a650180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a6505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a650a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a650ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a6517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a651c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a652090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a652500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15a652970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15a652de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a653250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a6536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a653b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a653fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a654410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a654880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a654cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a655160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a6555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a655a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a655eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a656320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a656790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a657670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a657d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a6584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a658bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a658e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15a659300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a659900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a659f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15b8044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15b804950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15b804dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15b805230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15b8056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15b805b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15b805f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15b8063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15b806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15b806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15b807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15b807810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15b808330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15b808ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15b8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15b809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15b80a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15b80a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15b80af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15b80b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15b80be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15b80c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15b80cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15b80d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15b80dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15b80dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15b80e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15b80e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15b80e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15b80edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15b80f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15b80f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15b80fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15b80fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15b8102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15b810760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15b810bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15b811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15b8114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15b811920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15b811d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15b812200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15b812670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15b812ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15b812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15b8133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15b813830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15b813ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15b814110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15b814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15b8149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15b814e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15b8152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15b815740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15b815bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15b816020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15b816590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15b816a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15b816f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15b817370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15b8177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15b817c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15b8180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15b818530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15b8189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15b818e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15b819280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15b8196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15b819b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15b819fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15b81a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15b81a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15b81ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15b81b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15b81b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15b81ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15b81bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15b81c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15b81c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15b81cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15b81d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15b81d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15b81d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15b81ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15b81e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15b81e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15b81eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15b81efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15b81f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15b81f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15b81fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15b820170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15b8205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15b820a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15b820ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15b821330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15b8217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15b821c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15b822080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15b8224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15b822960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15b822dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15b823240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15b8236b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15b823b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15b823f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15b824400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15b824870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15b824ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15b825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15b8255c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15b825a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15b825ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15b826310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15b826780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15b826bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15b827060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15b8274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15b827940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15b827db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15b828220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15b828690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15b828b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15b828f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15b8293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15b829850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15b829cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15b82a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15b82a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15b82aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15b82ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15b82b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15b82b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15b82bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15b82c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15b82c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15b82c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15b82cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15b82d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15b82d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15b82dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15b82df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15b82e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15b82e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15b82eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15b82f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15b82f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15b82f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15b82fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15b8302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15b830740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15b830bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15b831020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15b831490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15b831900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15b831d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15b8321e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15b832650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15b832ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15b832f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15b8333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15b833810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15b833c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15b8340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15b834560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15b8349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15b834e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15b8352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15b835720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15b835b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15b836000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15b836470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15b8368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15b836d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15b8371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15b837630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15b837aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15b837f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15b838380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15b8387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15b838c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15b8390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15b839540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15b8399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15b839e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15b83a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15b83a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15b83ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15b83afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15b83b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15b83b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15b83bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15b83c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15b83c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15b83ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15b83cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15b83d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15b83d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15b83dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15b83e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15b83e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15b83e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15b83ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15b83f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15b83f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15b83fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15b83ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15b840550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15b8409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15b840e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15b841980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15b841c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15b841f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15b842370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15b8427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15b842c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15b8430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15b843530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15b8439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15b843e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15b844280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15b8446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15b844b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15b844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15b845440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15b8458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15b845d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15b846190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15b846600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15b846a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15b846ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15b847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15b8477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15b847c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15b8480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15b848510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15b848980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15b848df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15b849260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15b8496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15b849b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15b849fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15b84a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15b84a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15b84b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15b84b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15b84b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15b84bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15b84c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15b84c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15b84caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15b84cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15b84d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15b84d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15b84dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15b84e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15b84e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15b84ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15b84ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15b84f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15b84f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15b84fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15b850030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15b8504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15b850910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15b850d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15b8511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15b851660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15b851ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15b851f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15b8523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15b852820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15b852c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15b853100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15b853570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15b8539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15b853e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15b8542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15b854730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15b854ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15b855010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15b855480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15b8558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15b856360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15b856a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15b8571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15b8578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15b857b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15b857ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15b8585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15b858c00 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.945s
user	0m0.244s
sys	0m0.155s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.25 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.51 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.52 real         0.15 user         0.04 sys
```
