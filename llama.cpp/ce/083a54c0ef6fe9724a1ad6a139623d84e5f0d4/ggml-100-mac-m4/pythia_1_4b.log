Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.686s
user	0m0.711s
sys	0m1.013s
++ nproc
+ make -j10
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  7%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  7%] Built target build_info
[  7%] Built target sha256
[  7%] Built target sha1
[  7%] Built target xxhash
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 16%] Linking C shared library libggml-metal.dylib
[ 16%] Built target ggml-metal
[ 17%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Linking CXX shared library libllama.dylib
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 22%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple-chat
[ 28%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Built target llava
[ 31%] Linking CXX static library libcommon.a
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Built target llama-simple
[ 32%] Built target llama-simple-chat
[ 32%] Built target test-c
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llava_static
[ 32%] Built target common
[ 32%] Built target llava_shared
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-tokenizer-0
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Linking CXX executable ../bin/test-sampling
[ 44%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-arg-parser
[ 47%] Built target test-tokenizer-1-bpe
[ 47%] Built target test-tokenizer-0
[ 47%] Built target test-tokenizer-1-spm
[ 47%] Built target test-json-schema-to-grammar
[ 47%] Built target test-llama-grammar
[ 47%] Built target test-sampling
[ 48%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Built target test-log
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 49%] Built target test-arg-parser
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-backend-ops
[ 53%] Linking CXX executable ../bin/test-chat-template
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Linking CXX executable ../bin/test-model-load-cancel
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 58%] Linking CXX executable ../../bin/llama-batched-bench
[ 59%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 59%] Built target test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Built target test-chat-template
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Built target test-autorelease
[ 61%] Built target test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-rope
[ 61%] Linking CXX executable ../../bin/llama-batched
[ 61%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 61%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 61%] Built target test-barrier
[ 61%] Built target llama-batched-bench
[ 62%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 62%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-eval-callback
[ 63%] Built target test-quantize-perf
[ 64%] Linking CXX executable ../../bin/llama-embedding
[ 64%] Built target test-quantize-fns
[ 65%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Built target test-rope
[ 65%] Built target llama-batched
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 67%] Linking CXX executable ../../bin/llama-gguf-split
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-embedding
[ 71%] Linking CXX executable ../../bin/llama-bench
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 71%] Built target llama-gguf-split
[ 71%] Built target llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-lookahead
[ 71%] Linking CXX executable ../../bin/llama-lookup
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Built target llama-bench
[ 75%] Built target llama-infill
[ 75%] Linking CXX executable ../../bin/llama-lookup-merge
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Linking CXX executable ../../bin/llama-lookup-create
[ 75%] Built target llama-lookahead
[ 75%] Built target llama-lookup
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 76%] Linking CXX executable ../../bin/llama-cli
[ 76%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 76%] Generating loading.html.hpp
[ 76%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-parallel
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Generating index.html.gz.hpp
[ 80%] Built target llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-cli
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-parallel
[ 82%] Built target llama-passkey
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Built target llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Built target llama-perplexity
[ 86%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-run
[ 90%] Built target llama-speculative
[ 90%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tts
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Built target llama-cvector-generator
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.823s
user	0m6.032s
sys	0m9.925s

main: quantize time =  2753.84 ms
main:    total time =  2753.84 ms

main: quantize time =  1267.13 ms
main:    total time =  1267.13 ms

main: quantize time =  1114.49 ms
main:    total time =  1114.49 ms

main: quantize time =  1219.74 ms
main:    total time =  1219.74 ms

main: quantize time =  1793.48 ms
main:    total time =  1793.48 ms

main: quantize time =  4833.66 ms
main:    total time =  4833.66 ms

main: quantize time =  5866.33 ms
main:    total time =  5866.33 ms

main: quantize time =  6800.74 ms
main:    total time =  6800.74 ms

main: quantize time =  5822.34 ms
main:    total time =  5822.34 ms

main: quantize time =  4602.46 ms
main:    total time =  4602.46 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.108 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.217 I main: llama backend init
0.00.000.223 I main: load the model and apply lora adapter, if any
0.00.062.520 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.075.472 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.075.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.075.498 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.075.498 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.075.499 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.075.499 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.075.500 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.075.502 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.075.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.075.504 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.075.505 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.075.505 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.075.506 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.075.507 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.075.513 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.075.513 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.075.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.082.686 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.084.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.092.219 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.092.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.092.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.092.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.092.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.092.227 I llama_model_loader: - type  f32:  194 tensors
0.00.092.227 I llama_model_loader: - type  f16:   98 tensors
0.00.131.958 I llm_load_vocab: special tokens cache size = 25
0.00.139.477 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.139.481 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.139.481 I llm_load_print_meta: arch             = gptneox
0.00.139.482 I llm_load_print_meta: vocab type       = BPE
0.00.139.482 I llm_load_print_meta: n_vocab          = 50304
0.00.139.482 I llm_load_print_meta: n_merges         = 50009
0.00.139.482 I llm_load_print_meta: vocab_only       = 0
0.00.139.482 I llm_load_print_meta: n_ctx_train      = 2048
0.00.139.483 I llm_load_print_meta: n_embd           = 2048
0.00.139.483 I llm_load_print_meta: n_layer          = 24
0.00.139.487 I llm_load_print_meta: n_head           = 16
0.00.139.487 I llm_load_print_meta: n_head_kv        = 16
0.00.139.488 I llm_load_print_meta: n_rot            = 32
0.00.139.488 I llm_load_print_meta: n_swa            = 0
0.00.139.488 I llm_load_print_meta: n_embd_head_k    = 128
0.00.139.488 I llm_load_print_meta: n_embd_head_v    = 128
0.00.139.489 I llm_load_print_meta: n_gqa            = 1
0.00.139.490 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.139.490 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.139.491 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.139.491 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.139.492 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.139.492 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.139.492 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.139.493 I llm_load_print_meta: n_ff             = 8192
0.00.139.493 I llm_load_print_meta: n_expert         = 0
0.00.139.493 I llm_load_print_meta: n_expert_used    = 0
0.00.139.493 I llm_load_print_meta: causal attn      = 1
0.00.139.493 I llm_load_print_meta: pooling type     = 0
0.00.139.493 I llm_load_print_meta: rope type        = 2
0.00.139.494 I llm_load_print_meta: rope scaling     = linear
0.00.139.494 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.139.494 I llm_load_print_meta: freq_scale_train = 1
0.00.139.495 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.139.495 I llm_load_print_meta: rope_finetuned   = unknown
0.00.139.495 I llm_load_print_meta: ssm_d_conv       = 0
0.00.139.495 I llm_load_print_meta: ssm_d_inner      = 0
0.00.139.495 I llm_load_print_meta: ssm_d_state      = 0
0.00.139.495 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.139.495 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.139.496 I llm_load_print_meta: model type       = 1.4B
0.00.139.497 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.139.497 I llm_load_print_meta: model params     = 1.41 B
0.00.139.498 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.139.498 I llm_load_print_meta: general.name     = 1.4B
0.00.139.500 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.139.500 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.139.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.139.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.139.501 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.139.501 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.139.501 I llm_load_print_meta: max token length = 1024
0.00.141.700 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.141.701 I llm_load_tensors: offloading output layer to GPU
0.00.141.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.141.715 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.141.716 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.142.705 I llama_new_context_with_model: n_seq_max     = 1
0.00.142.706 I llama_new_context_with_model: n_ctx         = 2048
0.00.142.707 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.142.707 I llama_new_context_with_model: n_batch       = 2048
0.00.142.707 I llama_new_context_with_model: n_ubatch      = 512
0.00.142.707 I llama_new_context_with_model: flash_attn    = 0
0.00.142.708 I llama_new_context_with_model: freq_base     = 10000.0
0.00.142.708 I llama_new_context_with_model: freq_scale    = 1
0.00.142.708 I ggml_metal_init: allocating
0.00.142.712 I ggml_metal_init: found device: Apple M4
0.00.142.714 I ggml_metal_init: picking default device: Apple M4
0.00.143.433 I ggml_metal_init: using embedded metal library
0.00.156.246 I ggml_metal_init: GPU name:   Apple M4
0.00.156.249 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.156.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.156.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.156.250 I ggml_metal_init: simdgroup reduction   = true
0.00.156.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.156.250 I ggml_metal_init: has bfloat            = true
0.00.156.250 I ggml_metal_init: use bfloat            = true
0.00.156.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.156.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.182.038 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.203.836 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.203.844 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.203.862 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.204.889 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.204.890 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.204.890 I llama_new_context_with_model: graph nodes  = 967
0.00.204.891 I llama_new_context_with_model: graph splits = 2
0.00.204.914 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.205.054 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.205.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.286.883 I main: llama threadpool init, n_threads = 4
0.00.286.915 I 
0.00.286.954 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.286.955 I 
0.00.287.027 I sampler seed: 1234
0.00.287.032 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.287.065 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.287.067 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.287.067 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.139.368 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55773.76 tokens per second)
0.02.139.369 I llama_perf_context_print:        load time =     224.35 ms
0.02.139.370 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.85 tokens per second)
0.02.139.370 I llama_perf_context_print:        eval time =    1805.58 ms /    63 runs   (   28.66 ms per token,    34.89 tokens per second)
0.02.139.371 I llama_perf_context_print:       total time =    1852.49 ms /    70 tokens
0.02.139.545 I ggml_metal_free: deallocating

real	0m2.532s
user	0m0.154s
sys	0m0.107s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.685 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.236 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.243 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.247 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.247 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.251 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.252 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.380 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.365 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.367 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.368 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.368 I llama_model_loader: - type  f32:  194 tensors
0.00.039.369 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.760 I llm_load_vocab: special tokens cache size = 25
0.00.071.215 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.219 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.220 I llm_load_print_meta: arch             = gptneox
0.00.071.220 I llm_load_print_meta: vocab type       = BPE
0.00.071.220 I llm_load_print_meta: n_vocab          = 50304
0.00.071.220 I llm_load_print_meta: n_merges         = 50009
0.00.071.221 I llm_load_print_meta: vocab_only       = 0
0.00.071.222 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.224 I llm_load_print_meta: n_embd           = 2048
0.00.071.224 I llm_load_print_meta: n_layer          = 24
0.00.071.230 I llm_load_print_meta: n_head           = 16
0.00.071.231 I llm_load_print_meta: n_head_kv        = 16
0.00.071.231 I llm_load_print_meta: n_rot            = 32
0.00.071.231 I llm_load_print_meta: n_swa            = 0
0.00.071.231 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.232 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.232 I llm_load_print_meta: n_gqa            = 1
0.00.071.234 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.235 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.236 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.236 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.236 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.236 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.238 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.238 I llm_load_print_meta: n_ff             = 8192
0.00.071.239 I llm_load_print_meta: n_expert         = 0
0.00.071.239 I llm_load_print_meta: n_expert_used    = 0
0.00.071.239 I llm_load_print_meta: causal attn      = 1
0.00.071.239 I llm_load_print_meta: pooling type     = 0
0.00.071.239 I llm_load_print_meta: rope type        = 2
0.00.071.241 I llm_load_print_meta: rope scaling     = linear
0.00.071.241 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.241 I llm_load_print_meta: freq_scale_train = 1
0.00.071.241 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.242 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.242 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.242 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.242 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.242 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.242 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.243 I llm_load_print_meta: model type       = 1.4B
0.00.071.243 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.244 I llm_load_print_meta: model params     = 1.41 B
0.00.071.244 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.244 I llm_load_print_meta: general.name     = 1.4B
0.00.071.244 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.245 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.245 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.245 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.245 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.246 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.246 I llm_load_print_meta: max token length = 1024
0.00.073.868 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.869 I llm_load_tensors: offloading output layer to GPU
0.00.073.869 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.881 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.882 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.946 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.947 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.948 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.948 I llama_new_context_with_model: n_batch       = 2048
0.00.074.948 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.948 I llama_new_context_with_model: flash_attn    = 0
0.00.074.949 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.949 I llama_new_context_with_model: freq_scale    = 1
0.00.074.949 I ggml_metal_init: allocating
0.00.074.952 I ggml_metal_init: found device: Apple M4
0.00.074.955 I ggml_metal_init: picking default device: Apple M4
0.00.075.760 I ggml_metal_init: using embedded metal library
0.00.078.775 I ggml_metal_init: GPU name:   Apple M4
0.00.078.777 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.778 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.778 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.778 I ggml_metal_init: simdgroup reduction   = true
0.00.078.778 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.779 I ggml_metal_init: has bfloat            = true
0.00.078.779 I ggml_metal_init: use bfloat            = true
0.00.078.779 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.780 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.132 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.116.607 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.615 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.638 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.117.816 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.117.818 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.117.819 I llama_new_context_with_model: graph nodes  = 967
0.00.117.819 I llama_new_context_with_model: graph splits = 2
0.00.117.838 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.117.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.117.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.282.437 I main: llama threadpool init, n_threads = 4
0.01.282.474 I 
0.01.282.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.282.506 I 
0.01.282.731 I sampler seed: 1234
0.01.282.738 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.282.783 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.282.787 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.282.788 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.382.617 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 46988.75 tokens per second)
0.02.382.618 I llama_perf_context_print:        load time =    1272.75 ms
0.02.382.618 I llama_perf_context_print: prompt eval time =      49.93 ms /     7 tokens (    7.13 ms per token,   140.20 tokens per second)
0.02.382.619 I llama_perf_context_print:        eval time =    1046.70 ms /    63 runs   (   16.61 ms per token,    60.19 tokens per second)
0.02.382.619 I llama_perf_context_print:       total time =    1100.18 ms /    70 tokens
0.02.382.832 I ggml_metal_free: deallocating

real	0m2.403s
user	0m0.122s
sys	0m0.223s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.017.147 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.495 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.504 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.506 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.507 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.508 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.509 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.512 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.512 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.512 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.913 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.876 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.877 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.878 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.878 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.878 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.879 I llama_model_loader: - type  f32:  194 tensors
0.00.035.880 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.880 I llama_model_loader: - type q6_K:    1 tensors
0.00.066.103 I llm_load_vocab: special tokens cache size = 25
0.00.076.331 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.076.335 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.076.335 I llm_load_print_meta: arch             = gptneox
0.00.076.336 I llm_load_print_meta: vocab type       = BPE
0.00.076.336 I llm_load_print_meta: n_vocab          = 50304
0.00.076.336 I llm_load_print_meta: n_merges         = 50009
0.00.076.337 I llm_load_print_meta: vocab_only       = 0
0.00.076.337 I llm_load_print_meta: n_ctx_train      = 2048
0.00.076.337 I llm_load_print_meta: n_embd           = 2048
0.00.076.337 I llm_load_print_meta: n_layer          = 24
0.00.076.342 I llm_load_print_meta: n_head           = 16
0.00.076.343 I llm_load_print_meta: n_head_kv        = 16
0.00.076.343 I llm_load_print_meta: n_rot            = 32
0.00.076.343 I llm_load_print_meta: n_swa            = 0
0.00.076.343 I llm_load_print_meta: n_embd_head_k    = 128
0.00.076.344 I llm_load_print_meta: n_embd_head_v    = 128
0.00.076.345 I llm_load_print_meta: n_gqa            = 1
0.00.076.346 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.076.347 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.076.347 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.076.348 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.076.348 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.076.349 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.076.349 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.076.350 I llm_load_print_meta: n_ff             = 8192
0.00.076.350 I llm_load_print_meta: n_expert         = 0
0.00.076.350 I llm_load_print_meta: n_expert_used    = 0
0.00.076.350 I llm_load_print_meta: causal attn      = 1
0.00.076.350 I llm_load_print_meta: pooling type     = 0
0.00.076.351 I llm_load_print_meta: rope type        = 2
0.00.076.351 I llm_load_print_meta: rope scaling     = linear
0.00.076.351 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.076.352 I llm_load_print_meta: freq_scale_train = 1
0.00.076.352 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.076.352 I llm_load_print_meta: rope_finetuned   = unknown
0.00.076.354 I llm_load_print_meta: ssm_d_conv       = 0
0.00.076.354 I llm_load_print_meta: ssm_d_inner      = 0
0.00.076.354 I llm_load_print_meta: ssm_d_state      = 0
0.00.076.355 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.076.355 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.076.355 I llm_load_print_meta: model type       = 1.4B
0.00.076.355 I llm_load_print_meta: model ftype      = Q4_0
0.00.076.356 I llm_load_print_meta: model params     = 1.41 B
0.00.076.357 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.076.357 I llm_load_print_meta: general.name     = 1.4B
0.00.076.357 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.076.360 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.076.360 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.076.360 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.076.361 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.076.361 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.076.361 I llm_load_print_meta: max token length = 1024
0.00.079.257 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.259 I llm_load_tensors: offloading output layer to GPU
0.00.079.259 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.271 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.079.273 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.080.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.080.628 I llama_new_context_with_model: n_ctx         = 2048
0.00.080.629 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.080.629 I llama_new_context_with_model: n_batch       = 2048
0.00.080.629 I llama_new_context_with_model: n_ubatch      = 512
0.00.080.629 I llama_new_context_with_model: flash_attn    = 0
0.00.080.630 I llama_new_context_with_model: freq_base     = 10000.0
0.00.080.630 I llama_new_context_with_model: freq_scale    = 1
0.00.080.631 I ggml_metal_init: allocating
0.00.080.635 I ggml_metal_init: found device: Apple M4
0.00.080.638 I ggml_metal_init: picking default device: Apple M4
0.00.081.584 I ggml_metal_init: using embedded metal library
0.00.085.266 I ggml_metal_init: GPU name:   Apple M4
0.00.085.269 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.270 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.270 I ggml_metal_init: simdgroup reduction   = true
0.00.085.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.270 I ggml_metal_init: has bfloat            = true
0.00.085.271 I ggml_metal_init: use bfloat            = true
0.00.085.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.272 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.032 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.121.986 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.995 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.019 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.123.008 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.123.010 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.123.011 I llama_new_context_with_model: graph nodes  = 967
0.00.123.011 I llama_new_context_with_model: graph splits = 2
0.00.123.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.151 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.757.352 I main: llama threadpool init, n_threads = 4
0.00.757.402 I 
0.00.757.445 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.757.446 I 
0.00.757.719 I sampler seed: 1234
0.00.757.724 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.757.757 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.757.759 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.757.759 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.437.097 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57258.06 tokens per second)
0.01.437.097 I llama_perf_context_print:        load time =     740.19 ms
0.01.437.098 I llama_perf_context_print: prompt eval time =      44.91 ms /     7 tokens (    6.42 ms per token,   155.87 tokens per second)
0.01.437.099 I llama_perf_context_print:        eval time =     631.37 ms /    63 runs   (   10.02 ms per token,    99.78 tokens per second)
0.01.437.099 I llama_perf_context_print:       total time =     679.75 ms /    70 tokens
0.01.437.238 I ggml_metal_free: deallocating

real	0m1.467s
user	0m0.133s
sys	0m0.177s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.917 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.670 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.670 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.671 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.674 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.674 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.675 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.675 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.676 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.678 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.678 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.537 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.409 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.409 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.410 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.411 I llama_model_loader: - type  f32:  194 tensors
0.00.023.411 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.411 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.506 I llm_load_vocab: special tokens cache size = 25
0.00.050.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.484 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.484 I llm_load_print_meta: arch             = gptneox
0.00.050.485 I llm_load_print_meta: vocab type       = BPE
0.00.050.485 I llm_load_print_meta: n_vocab          = 50304
0.00.050.485 I llm_load_print_meta: n_merges         = 50009
0.00.050.485 I llm_load_print_meta: vocab_only       = 0
0.00.050.490 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.490 I llm_load_print_meta: n_embd           = 2048
0.00.050.491 I llm_load_print_meta: n_layer          = 24
0.00.050.493 I llm_load_print_meta: n_head           = 16
0.00.050.494 I llm_load_print_meta: n_head_kv        = 16
0.00.050.494 I llm_load_print_meta: n_rot            = 32
0.00.050.495 I llm_load_print_meta: n_swa            = 0
0.00.050.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.496 I llm_load_print_meta: n_gqa            = 1
0.00.050.496 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.498 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.498 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.499 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.499 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.500 I llm_load_print_meta: n_ff             = 8192
0.00.050.500 I llm_load_print_meta: n_expert         = 0
0.00.050.500 I llm_load_print_meta: n_expert_used    = 0
0.00.050.501 I llm_load_print_meta: causal attn      = 1
0.00.050.501 I llm_load_print_meta: pooling type     = 0
0.00.050.501 I llm_load_print_meta: rope type        = 2
0.00.050.501 I llm_load_print_meta: rope scaling     = linear
0.00.050.501 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.502 I llm_load_print_meta: freq_scale_train = 1
0.00.050.502 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.502 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.502 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.502 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.504 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.504 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.504 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.504 I llm_load_print_meta: model type       = 1.4B
0.00.050.505 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.505 I llm_load_print_meta: model params     = 1.41 B
0.00.050.506 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.506 I llm_load_print_meta: general.name     = 1.4B
0.00.050.506 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.507 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.507 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.507 I llm_load_print_meta: max token length = 1024
0.00.052.573 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.574 I llm_load_tensors: offloading output layer to GPU
0.00.052.574 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.584 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.585 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.543 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.543 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.543 I llama_new_context_with_model: n_batch       = 2048
0.00.053.544 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.544 I llama_new_context_with_model: flash_attn    = 0
0.00.053.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.544 I llama_new_context_with_model: freq_scale    = 1
0.00.053.545 I ggml_metal_init: allocating
0.00.053.548 I ggml_metal_init: found device: Apple M4
0.00.053.550 I ggml_metal_init: picking default device: Apple M4
0.00.054.159 I ggml_metal_init: using embedded metal library
0.00.056.519 I ggml_metal_init: GPU name:   Apple M4
0.00.056.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.521 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.522 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.522 I ggml_metal_init: simdgroup reduction   = true
0.00.056.522 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.522 I ggml_metal_init: has bfloat            = true
0.00.056.522 I ggml_metal_init: use bfloat            = true
0.00.056.523 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.524 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.520 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.096 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.101 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.122 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.091 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.092 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.092 I llama_new_context_with_model: graph nodes  = 967
0.00.087.092 I llama_new_context_with_model: graph splits = 2
0.00.087.108 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.237 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.238 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.714.861 I main: llama threadpool init, n_threads = 4
0.00.714.902 I 
0.00.714.943 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.714.944 I 
0.00.715.170 I sampler seed: 1234
0.00.715.175 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.217 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.230 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.447.238 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66108.01 tokens per second)
0.01.447.239 I llama_perf_context_print:        load time =     705.94 ms
0.01.447.240 I llama_perf_context_print: prompt eval time =      45.78 ms /     7 tokens (    6.54 ms per token,   152.90 tokens per second)
0.01.447.240 I llama_perf_context_print:        eval time =     683.40 ms /    63 runs   (   10.85 ms per token,    92.19 tokens per second)
0.01.447.241 I llama_perf_context_print:       total time =     732.38 ms /    70 tokens
0.01.447.432 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.111s
sys	0m0.146s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.337 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.179 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.180 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.183 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.184 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.184 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.187 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.188 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.188 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.884 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.885 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.886 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.887 I llama_model_loader: - type  f32:  194 tensors
0.00.024.887 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.998 I llm_load_vocab: special tokens cache size = 25
0.00.052.049 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.052 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.052 I llm_load_print_meta: arch             = gptneox
0.00.052.053 I llm_load_print_meta: vocab type       = BPE
0.00.052.053 I llm_load_print_meta: n_vocab          = 50304
0.00.052.053 I llm_load_print_meta: n_merges         = 50009
0.00.052.053 I llm_load_print_meta: vocab_only       = 0
0.00.052.054 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.054 I llm_load_print_meta: n_embd           = 2048
0.00.052.054 I llm_load_print_meta: n_layer          = 24
0.00.052.057 I llm_load_print_meta: n_head           = 16
0.00.052.057 I llm_load_print_meta: n_head_kv        = 16
0.00.052.058 I llm_load_print_meta: n_rot            = 32
0.00.052.059 I llm_load_print_meta: n_swa            = 0
0.00.052.059 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.061 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.062 I llm_load_print_meta: n_gqa            = 1
0.00.052.062 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.063 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.064 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.066 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.066 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.066 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.066 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.067 I llm_load_print_meta: n_ff             = 8192
0.00.052.067 I llm_load_print_meta: n_expert         = 0
0.00.052.067 I llm_load_print_meta: n_expert_used    = 0
0.00.052.069 I llm_load_print_meta: causal attn      = 1
0.00.052.069 I llm_load_print_meta: pooling type     = 0
0.00.052.069 I llm_load_print_meta: rope type        = 2
0.00.052.069 I llm_load_print_meta: rope scaling     = linear
0.00.052.069 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.070 I llm_load_print_meta: freq_scale_train = 1
0.00.052.070 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.074 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.074 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.074 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.074 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.075 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.075 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.075 I llm_load_print_meta: model type       = 1.4B
0.00.052.076 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.077 I llm_load_print_meta: model params     = 1.41 B
0.00.052.078 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.078 I llm_load_print_meta: general.name     = 1.4B
0.00.052.078 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.078 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.080 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.080 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.080 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.080 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.080 I llm_load_print_meta: max token length = 1024
0.00.054.122 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.123 I llm_load_tensors: offloading output layer to GPU
0.00.054.123 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.134 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.135 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.039 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.039 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.040 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.040 I llama_new_context_with_model: n_batch       = 2048
0.00.055.040 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.040 I llama_new_context_with_model: flash_attn    = 0
0.00.055.041 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.041 I llama_new_context_with_model: freq_scale    = 1
0.00.055.041 I ggml_metal_init: allocating
0.00.055.047 I ggml_metal_init: found device: Apple M4
0.00.055.049 I ggml_metal_init: picking default device: Apple M4
0.00.055.641 I ggml_metal_init: using embedded metal library
0.00.057.968 I ggml_metal_init: GPU name:   Apple M4
0.00.057.970 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.970 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.971 I ggml_metal_init: simdgroup reduction   = true
0.00.057.971 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.971 I ggml_metal_init: has bfloat            = true
0.00.057.971 I ggml_metal_init: use bfloat            = true
0.00.057.972 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.974 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.894 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.600 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.609 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.628 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.605 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.606 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.606 I llama_new_context_with_model: graph nodes  = 967
0.00.087.607 I llama_new_context_with_model: graph splits = 2
0.00.087.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.764 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.765 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.517 I main: llama threadpool init, n_threads = 4
0.00.767.557 I 
0.00.767.593 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.594 I 
0.00.767.828 I sampler seed: 1234
0.00.767.832 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.848 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.848 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.848 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.556.355 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61739.13 tokens per second)
0.01.556.356 I llama_perf_context_print:        load time =     758.18 ms
0.01.556.356 I llama_perf_context_print: prompt eval time =      43.19 ms /     7 tokens (    6.17 ms per token,   162.08 tokens per second)
0.01.556.357 I llama_perf_context_print:        eval time =     742.44 ms /    63 runs   (   11.78 ms per token,    84.86 tokens per second)
0.01.556.357 I llama_perf_context_print:       total time =     788.84 ms /    70 tokens
0.01.556.578 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.079 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.976 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.980 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.982 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.988 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.989 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.989 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.989 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.990 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.991 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.991 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.991 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.992 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.992 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.992 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.994 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.995 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.982 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.105 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.947 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.947 I llama_model_loader: - type  f32:  194 tensors
0.00.024.947 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.948 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.149 I llm_load_vocab: special tokens cache size = 25
0.00.052.076 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.079 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.079 I llm_load_print_meta: arch             = gptneox
0.00.052.079 I llm_load_print_meta: vocab type       = BPE
0.00.052.080 I llm_load_print_meta: n_vocab          = 50304
0.00.052.080 I llm_load_print_meta: n_merges         = 50009
0.00.052.080 I llm_load_print_meta: vocab_only       = 0
0.00.052.080 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.080 I llm_load_print_meta: n_embd           = 2048
0.00.052.080 I llm_load_print_meta: n_layer          = 24
0.00.052.084 I llm_load_print_meta: n_head           = 16
0.00.052.085 I llm_load_print_meta: n_head_kv        = 16
0.00.052.085 I llm_load_print_meta: n_rot            = 32
0.00.052.086 I llm_load_print_meta: n_swa            = 0
0.00.052.086 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.086 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.087 I llm_load_print_meta: n_gqa            = 1
0.00.052.088 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.088 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.089 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.091 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.091 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.092 I llm_load_print_meta: n_ff             = 8192
0.00.052.092 I llm_load_print_meta: n_expert         = 0
0.00.052.093 I llm_load_print_meta: n_expert_used    = 0
0.00.052.093 I llm_load_print_meta: causal attn      = 1
0.00.052.093 I llm_load_print_meta: pooling type     = 0
0.00.052.093 I llm_load_print_meta: rope type        = 2
0.00.052.093 I llm_load_print_meta: rope scaling     = linear
0.00.052.094 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.094 I llm_load_print_meta: freq_scale_train = 1
0.00.052.094 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.094 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.094 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.094 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.095 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.095 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.095 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.095 I llm_load_print_meta: model type       = 1.4B
0.00.052.095 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.096 I llm_load_print_meta: model params     = 1.41 B
0.00.052.096 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.096 I llm_load_print_meta: general.name     = 1.4B
0.00.052.097 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.097 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.097 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.097 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.098 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.098 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.098 I llm_load_print_meta: max token length = 1024
0.00.054.198 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.199 I llm_load_tensors: offloading output layer to GPU
0.00.054.199 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.210 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.211 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.096 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.097 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.097 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.097 I llama_new_context_with_model: n_batch       = 2048
0.00.055.097 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.097 I llama_new_context_with_model: flash_attn    = 0
0.00.055.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.098 I llama_new_context_with_model: freq_scale    = 1
0.00.055.099 I ggml_metal_init: allocating
0.00.055.105 I ggml_metal_init: found device: Apple M4
0.00.055.108 I ggml_metal_init: picking default device: Apple M4
0.00.055.688 I ggml_metal_init: using embedded metal library
0.00.058.024 I ggml_metal_init: GPU name:   Apple M4
0.00.058.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.027 I ggml_metal_init: simdgroup reduction   = true
0.00.058.027 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.027 I ggml_metal_init: has bfloat            = true
0.00.058.027 I ggml_metal_init: use bfloat            = true
0.00.058.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.594 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.420 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.426 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.445 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.466 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.468 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.468 I llama_new_context_with_model: graph nodes  = 967
0.00.088.468 I llama_new_context_with_model: graph splits = 2
0.00.088.484 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.612 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.612 I main: llama threadpool init, n_threads = 4
0.00.699.648 I 
0.00.699.675 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.677 I 
0.00.699.815 I sampler seed: 1234
0.00.699.819 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.836 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.836 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.546.627 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50426.14 tokens per second)
0.01.546.628 I llama_perf_context_print:        load time =     690.53 ms
0.01.546.631 I llama_perf_context_print: prompt eval time =      46.07 ms /     7 tokens (    6.58 ms per token,   151.94 tokens per second)
0.01.546.631 I llama_perf_context_print:        eval time =     798.21 ms /    63 runs   (   12.67 ms per token,    78.93 tokens per second)
0.01.546.632 I llama_perf_context_print:       total time =     847.02 ms /    70 tokens
0.01.546.851 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.440 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.077 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.079 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.079 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.081 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.081 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.082 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.082 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.083 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.085 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.085 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.085 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.884 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.843 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.844 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.844 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.845 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.846 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.846 I llama_model_loader: - type  f32:  194 tensors
0.00.024.847 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.847 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.847 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.455 I llm_load_vocab: special tokens cache size = 25
0.00.051.316 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.319 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.320 I llm_load_print_meta: arch             = gptneox
0.00.051.320 I llm_load_print_meta: vocab type       = BPE
0.00.051.320 I llm_load_print_meta: n_vocab          = 50304
0.00.051.322 I llm_load_print_meta: n_merges         = 50009
0.00.051.322 I llm_load_print_meta: vocab_only       = 0
0.00.051.322 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.323 I llm_load_print_meta: n_embd           = 2048
0.00.051.323 I llm_load_print_meta: n_layer          = 24
0.00.051.326 I llm_load_print_meta: n_head           = 16
0.00.051.327 I llm_load_print_meta: n_head_kv        = 16
0.00.051.327 I llm_load_print_meta: n_rot            = 32
0.00.051.327 I llm_load_print_meta: n_swa            = 0
0.00.051.327 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.328 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.328 I llm_load_print_meta: n_gqa            = 1
0.00.051.331 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.331 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.333 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.334 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.334 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.334 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.334 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.335 I llm_load_print_meta: n_ff             = 8192
0.00.051.335 I llm_load_print_meta: n_expert         = 0
0.00.051.335 I llm_load_print_meta: n_expert_used    = 0
0.00.051.335 I llm_load_print_meta: causal attn      = 1
0.00.051.335 I llm_load_print_meta: pooling type     = 0
0.00.051.336 I llm_load_print_meta: rope type        = 2
0.00.051.336 I llm_load_print_meta: rope scaling     = linear
0.00.051.336 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.336 I llm_load_print_meta: freq_scale_train = 1
0.00.051.337 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.337 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.337 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.338 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.338 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.338 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.338 I llm_load_print_meta: model type       = 1.4B
0.00.051.338 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.339 I llm_load_print_meta: model params     = 1.41 B
0.00.051.340 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.340 I llm_load_print_meta: general.name     = 1.4B
0.00.051.340 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.340 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.341 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.341 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.342 I llm_load_print_meta: max token length = 1024
0.00.053.193 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.194 I llm_load_tensors: offloading output layer to GPU
0.00.053.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.199 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.200 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.187 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.188 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.188 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.188 I llama_new_context_with_model: n_batch       = 2048
0.00.054.188 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.189 I llama_new_context_with_model: flash_attn    = 0
0.00.054.189 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.189 I llama_new_context_with_model: freq_scale    = 1
0.00.054.190 I ggml_metal_init: allocating
0.00.054.193 I ggml_metal_init: found device: Apple M4
0.00.054.195 I ggml_metal_init: picking default device: Apple M4
0.00.054.813 I ggml_metal_init: using embedded metal library
0.00.057.192 I ggml_metal_init: GPU name:   Apple M4
0.00.057.193 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.194 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.194 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.194 I ggml_metal_init: simdgroup reduction   = true
0.00.057.195 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.195 I ggml_metal_init: has bfloat            = true
0.00.057.195 I ggml_metal_init: use bfloat            = true
0.00.057.195 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.196 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.676 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.411 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.416 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.437 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.416 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.418 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.418 I llama_new_context_with_model: graph nodes  = 967
0.00.088.418 I llama_new_context_with_model: graph splits = 2
0.00.088.435 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.442.776 I main: llama threadpool init, n_threads = 4
0.00.442.811 I 
0.00.442.844 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.442.844 I 
0.00.443.053 I sampler seed: 1234
0.00.443.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.443.093 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.443.095 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.443.095 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.122.989 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62226.12 tokens per second)
0.01.122.990 I llama_perf_context_print:        load time =     432.33 ms
0.01.122.991 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.68 tokens per second)
0.01.122.991 I llama_perf_context_print:        eval time =     641.19 ms /    63 runs   (   10.18 ms per token,    98.25 tokens per second)
0.01.122.991 I llama_perf_context_print:       total time =     680.22 ms /    70 tokens
0.01.123.187 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.839 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.344 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.345 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.346 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.346 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.347 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.347 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.347 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.348 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.348 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.350 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.350 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.067 I llama_model_loader: - type  f32:  194 tensors
0.00.024.067 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.068 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.068 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.068 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.389 I llm_load_vocab: special tokens cache size = 25
0.00.050.377 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.379 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.380 I llm_load_print_meta: arch             = gptneox
0.00.050.380 I llm_load_print_meta: vocab type       = BPE
0.00.050.380 I llm_load_print_meta: n_vocab          = 50304
0.00.050.380 I llm_load_print_meta: n_merges         = 50009
0.00.050.380 I llm_load_print_meta: vocab_only       = 0
0.00.050.381 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.381 I llm_load_print_meta: n_embd           = 2048
0.00.050.381 I llm_load_print_meta: n_layer          = 24
0.00.050.383 I llm_load_print_meta: n_head           = 16
0.00.050.384 I llm_load_print_meta: n_head_kv        = 16
0.00.050.384 I llm_load_print_meta: n_rot            = 32
0.00.050.387 I llm_load_print_meta: n_swa            = 0
0.00.050.387 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.387 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.388 I llm_load_print_meta: n_gqa            = 1
0.00.050.388 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.389 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.390 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.390 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.390 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.390 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.391 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.391 I llm_load_print_meta: n_ff             = 8192
0.00.050.391 I llm_load_print_meta: n_expert         = 0
0.00.050.392 I llm_load_print_meta: n_expert_used    = 0
0.00.050.392 I llm_load_print_meta: causal attn      = 1
0.00.050.396 I llm_load_print_meta: pooling type     = 0
0.00.050.396 I llm_load_print_meta: rope type        = 2
0.00.050.397 I llm_load_print_meta: rope scaling     = linear
0.00.050.397 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.397 I llm_load_print_meta: freq_scale_train = 1
0.00.050.398 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.398 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.398 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.398 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.398 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.398 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.398 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.399 I llm_load_print_meta: model type       = 1.4B
0.00.050.399 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.399 I llm_load_print_meta: model params     = 1.41 B
0.00.050.403 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.403 I llm_load_print_meta: general.name     = 1.4B
0.00.050.403 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.404 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.404 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.404 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.405 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.405 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.405 I llm_load_print_meta: max token length = 1024
0.00.052.334 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.334 I llm_load_tensors: offloading output layer to GPU
0.00.052.334 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.345 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.346 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.227 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.227 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.228 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.228 I llama_new_context_with_model: n_batch       = 2048
0.00.053.228 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.228 I llama_new_context_with_model: flash_attn    = 0
0.00.053.229 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.229 I llama_new_context_with_model: freq_scale    = 1
0.00.053.229 I ggml_metal_init: allocating
0.00.053.232 I ggml_metal_init: found device: Apple M4
0.00.053.234 I ggml_metal_init: picking default device: Apple M4
0.00.053.818 I ggml_metal_init: using embedded metal library
0.00.056.151 I ggml_metal_init: GPU name:   Apple M4
0.00.056.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.153 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.153 I ggml_metal_init: simdgroup reduction   = true
0.00.056.153 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.154 I ggml_metal_init: has bfloat            = true
0.00.056.154 I ggml_metal_init: use bfloat            = true
0.00.056.154 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.155 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.931 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.461 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.467 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.486 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.497 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.498 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.499 I llama_new_context_with_model: graph nodes  = 967
0.00.086.499 I llama_new_context_with_model: graph splits = 2
0.00.086.514 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.682 I main: llama threadpool init, n_threads = 4
0.00.533.730 I 
0.00.533.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.769 I 
0.00.534.010 I sampler seed: 1234
0.00.534.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.534.059 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.534.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.534.079 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.284.475 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56170.89 tokens per second)
0.01.284.475 I llama_perf_context_print:        load time =     523.84 ms
0.01.284.476 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.61 tokens per second)
0.01.284.477 I llama_perf_context_print:        eval time =     702.79 ms /    63 runs   (   11.16 ms per token,    89.64 tokens per second)
0.01.284.478 I llama_perf_context_print:       total time =     750.80 ms /    70 tokens
0.01.284.672 I ggml_metal_free: deallocating

real	0m1.302s
user	0m0.110s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.283 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.288 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.297 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.298 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.146 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.148 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.148 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.148 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.149 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.149 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.149 I llama_model_loader: - type  f32:  194 tensors
0.00.024.150 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.150 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.150 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.377 I llm_load_vocab: special tokens cache size = 25
0.00.051.563 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.566 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.566 I llm_load_print_meta: arch             = gptneox
0.00.051.566 I llm_load_print_meta: vocab type       = BPE
0.00.051.567 I llm_load_print_meta: n_vocab          = 50304
0.00.051.567 I llm_load_print_meta: n_merges         = 50009
0.00.051.567 I llm_load_print_meta: vocab_only       = 0
0.00.051.567 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.567 I llm_load_print_meta: n_embd           = 2048
0.00.051.567 I llm_load_print_meta: n_layer          = 24
0.00.051.570 I llm_load_print_meta: n_head           = 16
0.00.051.571 I llm_load_print_meta: n_head_kv        = 16
0.00.051.571 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.572 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.572 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.573 I llm_load_print_meta: n_gqa            = 1
0.00.051.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.575 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.575 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.577 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.577 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.577 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.577 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.578 I llm_load_print_meta: n_ff             = 8192
0.00.051.578 I llm_load_print_meta: n_expert         = 0
0.00.051.578 I llm_load_print_meta: n_expert_used    = 0
0.00.051.579 I llm_load_print_meta: causal attn      = 1
0.00.051.579 I llm_load_print_meta: pooling type     = 0
0.00.051.579 I llm_load_print_meta: rope type        = 2
0.00.051.579 I llm_load_print_meta: rope scaling     = linear
0.00.051.579 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.581 I llm_load_print_meta: freq_scale_train = 1
0.00.051.581 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.582 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.582 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.582 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.582 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.582 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.582 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.582 I llm_load_print_meta: model type       = 1.4B
0.00.051.583 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.583 I llm_load_print_meta: model params     = 1.41 B
0.00.051.584 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.584 I llm_load_print_meta: general.name     = 1.4B
0.00.051.584 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.588 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.588 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.588 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.589 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.589 I llm_load_print_meta: max token length = 1024
0.00.053.141 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.141 I llm_load_tensors: offloading output layer to GPU
0.00.053.142 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.151 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.152 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.971 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.971 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.971 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.972 I llama_new_context_with_model: n_batch       = 2048
0.00.053.972 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.972 I llama_new_context_with_model: flash_attn    = 0
0.00.053.972 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.973 I llama_new_context_with_model: freq_scale    = 1
0.00.053.973 I ggml_metal_init: allocating
0.00.053.976 I ggml_metal_init: found device: Apple M4
0.00.053.978 I ggml_metal_init: picking default device: Apple M4
0.00.054.533 I ggml_metal_init: using embedded metal library
0.00.056.853 I ggml_metal_init: GPU name:   Apple M4
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.856 I ggml_metal_init: simdgroup reduction   = true
0.00.056.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.856 I ggml_metal_init: has bfloat            = true
0.00.056.856 I ggml_metal_init: use bfloat            = true
0.00.056.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.444 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.259 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.277 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.257 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.258 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.259 I llama_new_context_with_model: graph nodes  = 967
0.00.087.259 I llama_new_context_with_model: graph splits = 2
0.00.087.274 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.418 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.493 I main: llama threadpool init, n_threads = 4
0.00.615.541 I 
0.00.615.575 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.576 I 
0.00.615.803 I sampler seed: 1234
0.00.615.808 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.615.845 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.615.847 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.615.847 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.376.233 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.376.234 I llama_perf_context_print:        load time =     605.67 ms
0.01.376.234 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.55 tokens per second)
0.01.376.235 I llama_perf_context_print:        eval time =     710.09 ms /    63 runs   (   11.27 ms per token,    88.72 tokens per second)
0.01.376.235 I llama_perf_context_print:       total time =     760.74 ms /    70 tokens
0.01.376.432 I ggml_metal_free: deallocating

real	0m1.394s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.008.880 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.037 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.041 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.043 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.043 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.046 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.047 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.047 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.047 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.048 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.914 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.769 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.771 I llama_model_loader: - type  f32:  194 tensors
0.00.024.771 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.772 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.187 I llm_load_vocab: special tokens cache size = 25
0.00.051.056 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.059 I llm_load_print_meta: arch             = gptneox
0.00.051.059 I llm_load_print_meta: vocab type       = BPE
0.00.051.059 I llm_load_print_meta: n_vocab          = 50304
0.00.051.059 I llm_load_print_meta: n_merges         = 50009
0.00.051.059 I llm_load_print_meta: vocab_only       = 0
0.00.051.060 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.060 I llm_load_print_meta: n_embd           = 2048
0.00.051.060 I llm_load_print_meta: n_layer          = 24
0.00.051.063 I llm_load_print_meta: n_head           = 16
0.00.051.063 I llm_load_print_meta: n_head_kv        = 16
0.00.051.064 I llm_load_print_meta: n_rot            = 32
0.00.051.064 I llm_load_print_meta: n_swa            = 0
0.00.051.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.065 I llm_load_print_meta: n_gqa            = 1
0.00.051.066 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.068 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.068 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.073 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.073 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.074 I llm_load_print_meta: n_ff             = 8192
0.00.051.076 I llm_load_print_meta: n_expert         = 0
0.00.051.076 I llm_load_print_meta: n_expert_used    = 0
0.00.051.077 I llm_load_print_meta: causal attn      = 1
0.00.051.078 I llm_load_print_meta: pooling type     = 0
0.00.051.078 I llm_load_print_meta: rope type        = 2
0.00.051.078 I llm_load_print_meta: rope scaling     = linear
0.00.051.079 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.080 I llm_load_print_meta: freq_scale_train = 1
0.00.051.080 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.080 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.082 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.082 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.082 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.082 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.082 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.082 I llm_load_print_meta: model type       = 1.4B
0.00.051.083 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.083 I llm_load_print_meta: model params     = 1.41 B
0.00.051.084 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.084 I llm_load_print_meta: general.name     = 1.4B
0.00.051.084 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.085 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.085 I llm_load_print_meta: max token length = 1024
0.00.053.075 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.076 I llm_load_tensors: offloading output layer to GPU
0.00.053.076 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.087 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.088 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.971 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.972 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.972 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.972 I llama_new_context_with_model: n_batch       = 2048
0.00.053.972 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.972 I llama_new_context_with_model: flash_attn    = 0
0.00.053.973 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.973 I llama_new_context_with_model: freq_scale    = 1
0.00.053.974 I ggml_metal_init: allocating
0.00.053.977 I ggml_metal_init: found device: Apple M4
0.00.053.979 I ggml_metal_init: picking default device: Apple M4
0.00.054.581 I ggml_metal_init: using embedded metal library
0.00.056.877 I ggml_metal_init: GPU name:   Apple M4
0.00.056.879 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.880 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.881 I ggml_metal_init: simdgroup reduction   = true
0.00.056.881 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.881 I ggml_metal_init: has bfloat            = true
0.00.056.881 I ggml_metal_init: use bfloat            = true
0.00.056.882 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.639 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.743 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.748 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.818 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.819 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.820 I llama_new_context_with_model: graph nodes  = 967
0.00.087.820 I llama_new_context_with_model: graph splits = 2
0.00.087.835 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.978 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.979 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.727 I main: llama threadpool init, n_threads = 4
0.00.683.762 I 
0.00.683.789 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.789 I 
0.00.683.940 I sampler seed: 1234
0.00.683.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.683.979 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.683.983 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.683.984 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.533.331 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.533.332 I llama_perf_context_print:        load time =     674.84 ms
0.01.533.332 I llama_perf_context_print: prompt eval time =      51.51 ms /     7 tokens (    7.36 ms per token,   135.89 tokens per second)
0.01.533.335 I llama_perf_context_print:        eval time =     795.18 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.533.336 I llama_perf_context_print:       total time =     849.61 ms /    70 tokens
0.01.533.575 I ggml_metal_free: deallocating

real	0m1.551s
user	0m0.109s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.356 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.649 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.653 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.655 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.660 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.660 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.661 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.661 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.662 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.663 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.666 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.550 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.550 I llama_model_loader: - type  f32:  194 tensors
0.00.026.550 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.907 I llm_load_vocab: special tokens cache size = 25
0.00.053.810 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.813 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.813 I llm_load_print_meta: arch             = gptneox
0.00.053.813 I llm_load_print_meta: vocab type       = BPE
0.00.053.814 I llm_load_print_meta: n_vocab          = 50304
0.00.053.814 I llm_load_print_meta: n_merges         = 50009
0.00.053.814 I llm_load_print_meta: vocab_only       = 0
0.00.053.814 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.814 I llm_load_print_meta: n_embd           = 2048
0.00.053.815 I llm_load_print_meta: n_layer          = 24
0.00.053.817 I llm_load_print_meta: n_head           = 16
0.00.053.818 I llm_load_print_meta: n_head_kv        = 16
0.00.053.818 I llm_load_print_meta: n_rot            = 32
0.00.053.819 I llm_load_print_meta: n_swa            = 0
0.00.053.819 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.819 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.820 I llm_load_print_meta: n_gqa            = 1
0.00.053.821 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.821 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.822 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.822 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.822 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.822 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.823 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.823 I llm_load_print_meta: n_ff             = 8192
0.00.053.823 I llm_load_print_meta: n_expert         = 0
0.00.053.823 I llm_load_print_meta: n_expert_used    = 0
0.00.053.826 I llm_load_print_meta: causal attn      = 1
0.00.053.826 I llm_load_print_meta: pooling type     = 0
0.00.053.826 I llm_load_print_meta: rope type        = 2
0.00.053.827 I llm_load_print_meta: rope scaling     = linear
0.00.053.827 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.827 I llm_load_print_meta: freq_scale_train = 1
0.00.053.827 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.828 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.828 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.828 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.828 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.828 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.829 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.829 I llm_load_print_meta: model type       = 1.4B
0.00.053.829 I llm_load_print_meta: model ftype      = Q6_K
0.00.053.830 I llm_load_print_meta: model params     = 1.41 B
0.00.053.830 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.053.830 I llm_load_print_meta: general.name     = 1.4B
0.00.053.831 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.831 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.831 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.831 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.831 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.832 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.832 I llm_load_print_meta: max token length = 1024
0.00.055.953 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.954 I llm_load_tensors: offloading output layer to GPU
0.00.055.954 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.965 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.055.966 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.056.912 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.913 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.913 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.913 I llama_new_context_with_model: n_batch       = 2048
0.00.056.914 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.914 I llama_new_context_with_model: flash_attn    = 0
0.00.056.914 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.914 I llama_new_context_with_model: freq_scale    = 1
0.00.056.915 I ggml_metal_init: allocating
0.00.056.918 I ggml_metal_init: found device: Apple M4
0.00.056.920 I ggml_metal_init: picking default device: Apple M4
0.00.057.522 I ggml_metal_init: using embedded metal library
0.00.059.916 I ggml_metal_init: GPU name:   Apple M4
0.00.059.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.918 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.918 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.918 I ggml_metal_init: simdgroup reduction   = true
0.00.059.918 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.919 I ggml_metal_init: has bfloat            = true
0.00.059.919 I ggml_metal_init: use bfloat            = true
0.00.059.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.945 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.649 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.657 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.676 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.725 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.726 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.726 I llama_new_context_with_model: graph nodes  = 967
0.00.090.727 I llama_new_context_with_model: graph splits = 2
0.00.090.742 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.883 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.884 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.748 I main: llama threadpool init, n_threads = 4
0.00.755.784 I 
0.00.755.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.755.812 I 
0.00.756.021 I sampler seed: 1234
0.00.756.025 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.756.066 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.756.066 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.756.066 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.638.199 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54239.88 tokens per second)
0.01.638.199 I llama_perf_context_print:        load time =     745.39 ms
0.01.638.200 I llama_perf_context_print: prompt eval time =      54.37 ms /     7 tokens (    7.77 ms per token,   128.76 tokens per second)
0.01.638.200 I llama_perf_context_print:        eval time =     824.59 ms /    63 runs   (   13.09 ms per token,    76.40 tokens per second)
0.01.638.201 I llama_perf_context_print:       total time =     882.45 ms /    70 tokens
0.01.638.381 I ggml_metal_free: deallocating

real	0m1.657s
user	0m0.111s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.584 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.385 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.035 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.051 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.055 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.056 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.061 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.061 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.062 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.065 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.071 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.071 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.072 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.460 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.018 I llama_model_loader: - type  f32:  194 tensors
0.00.053.018 I llama_model_loader: - type  f16:   98 tensors
0.00.084.932 I llm_load_vocab: special tokens cache size = 25
0.00.092.061 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.064 I llm_load_print_meta: arch             = gptneox
0.00.092.064 I llm_load_print_meta: vocab type       = BPE
0.00.092.064 I llm_load_print_meta: n_vocab          = 50304
0.00.092.065 I llm_load_print_meta: n_merges         = 50009
0.00.092.065 I llm_load_print_meta: vocab_only       = 0
0.00.092.065 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.065 I llm_load_print_meta: n_embd           = 2048
0.00.092.065 I llm_load_print_meta: n_layer          = 24
0.00.092.068 I llm_load_print_meta: n_head           = 16
0.00.092.069 I llm_load_print_meta: n_head_kv        = 16
0.00.092.069 I llm_load_print_meta: n_rot            = 32
0.00.092.069 I llm_load_print_meta: n_swa            = 0
0.00.092.069 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.069 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.070 I llm_load_print_meta: n_gqa            = 1
0.00.092.071 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.071 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.072 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.074 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.075 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.075 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.075 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.075 I llm_load_print_meta: n_ff             = 8192
0.00.092.076 I llm_load_print_meta: n_expert         = 0
0.00.092.076 I llm_load_print_meta: n_expert_used    = 0
0.00.092.076 I llm_load_print_meta: causal attn      = 1
0.00.092.076 I llm_load_print_meta: pooling type     = 0
0.00.092.076 I llm_load_print_meta: rope type        = 2
0.00.092.076 I llm_load_print_meta: rope scaling     = linear
0.00.092.077 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.077 I llm_load_print_meta: freq_scale_train = 1
0.00.092.077 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.078 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.078 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.078 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.078 I llm_load_print_meta: model type       = 1.4B
0.00.092.080 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.081 I llm_load_print_meta: model params     = 1.41 B
0.00.092.081 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.082 I llm_load_print_meta: general.name     = 1.4B
0.00.092.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.083 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.084 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.084 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.084 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.092.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.084 I llm_load_print_meta: max token length = 1024
0.00.094.698 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.699 I llm_load_tensors: offloading output layer to GPU
0.00.094.699 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.710 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.711 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.653 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.653 I llama_new_context_with_model: n_ctx         = 128
0.00.095.654 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.654 I llama_new_context_with_model: n_batch       = 128
0.00.095.654 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.654 I llama_new_context_with_model: flash_attn    = 0
0.00.095.655 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.655 I llama_new_context_with_model: freq_scale    = 1
0.00.095.655 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.656 I ggml_metal_init: allocating
0.00.095.658 I ggml_metal_init: found device: Apple M4
0.00.095.661 I ggml_metal_init: picking default device: Apple M4
0.00.096.255 I ggml_metal_init: using embedded metal library
0.00.098.870 I ggml_metal_init: GPU name:   Apple M4
0.00.098.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.873 I ggml_metal_init: simdgroup reduction   = true
0.00.098.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.873 I ggml_metal_init: has bfloat            = true
0.00.098.873 I ggml_metal_init: use bfloat            = true
0.00.098.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.277 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.109.560 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.576 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.448 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.449 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.449 I llama_new_context_with_model: graph nodes  = 967
0.00.110.450 I llama_new_context_with_model: graph splits = 2
0.00.110.462 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.110.463 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.072.813 I 
0.01.072.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.072.899 I perplexity: tokenizing the input ..
0.01.085.641 I perplexity: tokenization took 12.734 ms
0.01.085.648 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.208.052 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.209.861 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.209.882 I llama_perf_context_print:        load time =    1050.41 ms
0.01.209.884 I llama_perf_context_print: prompt eval time =     121.45 ms /   128 tokens (    0.95 ms per token,  1053.91 tokens per second)
0.01.209.885 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.209.886 I llama_perf_context_print:       total time =     137.07 ms /   129 tokens
0.01.210.584 I ggml_metal_free: deallocating

real	0m1.407s
user	0m0.128s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.146 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.746 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.749 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.750 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.319 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.149 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.574 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.576 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.576 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.577 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.577 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.578 I llama_model_loader: - type  f32:  194 tensors
0.00.039.579 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.223 I llm_load_vocab: special tokens cache size = 25
0.00.074.865 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.868 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.868 I llm_load_print_meta: arch             = gptneox
0.00.074.868 I llm_load_print_meta: vocab type       = BPE
0.00.074.868 I llm_load_print_meta: n_vocab          = 50304
0.00.074.869 I llm_load_print_meta: n_merges         = 50009
0.00.074.869 I llm_load_print_meta: vocab_only       = 0
0.00.074.869 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.869 I llm_load_print_meta: n_embd           = 2048
0.00.074.869 I llm_load_print_meta: n_layer          = 24
0.00.074.872 I llm_load_print_meta: n_head           = 16
0.00.074.872 I llm_load_print_meta: n_head_kv        = 16
0.00.074.874 I llm_load_print_meta: n_rot            = 32
0.00.074.874 I llm_load_print_meta: n_swa            = 0
0.00.074.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.874 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.875 I llm_load_print_meta: n_gqa            = 1
0.00.074.876 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.876 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.877 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.877 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.877 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.880 I llm_load_print_meta: n_ff             = 8192
0.00.074.880 I llm_load_print_meta: n_expert         = 0
0.00.074.880 I llm_load_print_meta: n_expert_used    = 0
0.00.074.880 I llm_load_print_meta: causal attn      = 1
0.00.074.881 I llm_load_print_meta: pooling type     = 0
0.00.074.881 I llm_load_print_meta: rope type        = 2
0.00.074.881 I llm_load_print_meta: rope scaling     = linear
0.00.074.887 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.889 I llm_load_print_meta: freq_scale_train = 1
0.00.074.890 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.891 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.891 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.891 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.891 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.891 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.891 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.892 I llm_load_print_meta: model type       = 1.4B
0.00.074.892 I llm_load_print_meta: model ftype      = Q8_0
0.00.074.892 I llm_load_print_meta: model params     = 1.41 B
0.00.074.893 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.074.893 I llm_load_print_meta: general.name     = 1.4B
0.00.074.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.894 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.894 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.895 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.074.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.895 I llm_load_print_meta: max token length = 1024
0.00.076.997 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.998 I llm_load_tensors: offloading output layer to GPU
0.00.076.998 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.004 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.077.005 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.078.024 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.025 I llama_new_context_with_model: n_ctx         = 128
0.00.078.025 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.078.025 I llama_new_context_with_model: n_batch       = 128
0.00.078.025 I llama_new_context_with_model: n_ubatch      = 128
0.00.078.025 I llama_new_context_with_model: flash_attn    = 0
0.00.078.026 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.026 I llama_new_context_with_model: freq_scale    = 1
0.00.078.026 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.078.027 I ggml_metal_init: allocating
0.00.078.032 I ggml_metal_init: found device: Apple M4
0.00.078.034 I ggml_metal_init: picking default device: Apple M4
0.00.078.670 I ggml_metal_init: using embedded metal library
0.00.081.288 I ggml_metal_init: GPU name:   Apple M4
0.00.081.290 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.290 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.291 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.291 I ggml_metal_init: simdgroup reduction   = true
0.00.081.291 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.291 I ggml_metal_init: has bfloat            = true
0.00.081.291 I ggml_metal_init: use bfloat            = true
0.00.081.292 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.091 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.092.419 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.092.423 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.092.438 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.361 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.093.362 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.093.363 I llama_new_context_with_model: graph nodes  = 967
0.00.093.363 I llama_new_context_with_model: graph splits = 2
0.00.093.371 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.093.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.934.424 I 
0.00.934.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.934.514 I perplexity: tokenizing the input ..
0.00.943.594 I perplexity: tokenization took 9.078 ms
0.00.943.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.069.006 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.070.162 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.070.180 I llama_perf_context_print:        load time =     919.67 ms
0.01.070.181 I llama_perf_context_print: prompt eval time =     125.19 ms /   128 tokens (    0.98 ms per token,  1022.48 tokens per second)
0.01.070.182 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.070.182 I llama_perf_context_print:       total time =     135.76 ms /   129 tokens
0.01.070.676 I ggml_metal_free: deallocating

real	0m1.094s
user	0m0.103s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.483 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.030 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.030 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.031 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.032 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.033 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.033 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.033 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.034 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.035 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.036 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.036 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.867 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.880 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.638 I llama_model_loader: - type  f32:  194 tensors
0.00.023.638 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.686 I llm_load_vocab: special tokens cache size = 25
0.00.050.701 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.704 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.704 I llm_load_print_meta: arch             = gptneox
0.00.050.705 I llm_load_print_meta: vocab type       = BPE
0.00.050.705 I llm_load_print_meta: n_vocab          = 50304
0.00.050.705 I llm_load_print_meta: n_merges         = 50009
0.00.050.705 I llm_load_print_meta: vocab_only       = 0
0.00.050.705 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.706 I llm_load_print_meta: n_embd           = 2048
0.00.050.706 I llm_load_print_meta: n_layer          = 24
0.00.050.709 I llm_load_print_meta: n_head           = 16
0.00.050.709 I llm_load_print_meta: n_head_kv        = 16
0.00.050.710 I llm_load_print_meta: n_rot            = 32
0.00.050.710 I llm_load_print_meta: n_swa            = 0
0.00.050.710 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.710 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.711 I llm_load_print_meta: n_gqa            = 1
0.00.050.712 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.712 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.713 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.713 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.715 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.716 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.716 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.716 I llm_load_print_meta: n_ff             = 8192
0.00.050.717 I llm_load_print_meta: n_expert         = 0
0.00.050.717 I llm_load_print_meta: n_expert_used    = 0
0.00.050.717 I llm_load_print_meta: causal attn      = 1
0.00.050.717 I llm_load_print_meta: pooling type     = 0
0.00.050.717 I llm_load_print_meta: rope type        = 2
0.00.050.717 I llm_load_print_meta: rope scaling     = linear
0.00.050.720 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.720 I llm_load_print_meta: freq_scale_train = 1
0.00.050.720 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.720 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.720 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.720 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.721 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.721 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.721 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.721 I llm_load_print_meta: model type       = 1.4B
0.00.050.721 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.722 I llm_load_print_meta: model params     = 1.41 B
0.00.050.722 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.722 I llm_load_print_meta: general.name     = 1.4B
0.00.050.723 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.723 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.727 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.727 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.728 I llm_load_print_meta: max token length = 1024
0.00.052.703 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.703 I llm_load_tensors: offloading output layer to GPU
0.00.052.704 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.715 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.716 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.620 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.621 I llama_new_context_with_model: n_ctx         = 128
0.00.053.621 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.621 I llama_new_context_with_model: n_batch       = 128
0.00.053.621 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.621 I llama_new_context_with_model: flash_attn    = 0
0.00.053.622 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.622 I llama_new_context_with_model: freq_scale    = 1
0.00.053.622 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.623 I ggml_metal_init: allocating
0.00.053.629 I ggml_metal_init: found device: Apple M4
0.00.053.631 I ggml_metal_init: picking default device: Apple M4
0.00.054.196 I ggml_metal_init: using embedded metal library
0.00.056.523 I ggml_metal_init: GPU name:   Apple M4
0.00.056.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.525 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.525 I ggml_metal_init: simdgroup reduction   = true
0.00.056.525 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.526 I ggml_metal_init: has bfloat            = true
0.00.056.526 I ggml_metal_init: use bfloat            = true
0.00.056.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.757 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.033 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.047 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.905 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.907 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.907 I llama_new_context_with_model: graph nodes  = 967
0.00.068.907 I llama_new_context_with_model: graph splits = 2
0.00.068.920 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.920 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.154 I 
0.00.629.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.629.224 I perplexity: tokenizing the input ..
0.00.637.427 I perplexity: tokenization took 8.201 ms
0.00.637.431 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.759.840 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.761.032 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.761.048 I llama_perf_context_print:        load time =     619.66 ms
0.00.761.049 I llama_perf_context_print: prompt eval time =     122.18 ms /   128 tokens (    0.95 ms per token,  1047.61 tokens per second)
0.00.761.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.050 I llama_perf_context_print:       total time =     131.90 ms /   129 tokens
0.00.761.500 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.079s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.772 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.349 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.358 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.359 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.360 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.360 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.361 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.361 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.362 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.362 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.363 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.365 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.365 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.190 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.191 I llama_model_loader: - type  f32:  194 tensors
0.00.023.191 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.192 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.430 I llm_load_vocab: special tokens cache size = 25
0.00.049.488 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.490 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.491 I llm_load_print_meta: arch             = gptneox
0.00.049.491 I llm_load_print_meta: vocab type       = BPE
0.00.049.491 I llm_load_print_meta: n_vocab          = 50304
0.00.049.491 I llm_load_print_meta: n_merges         = 50009
0.00.049.492 I llm_load_print_meta: vocab_only       = 0
0.00.049.492 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.492 I llm_load_print_meta: n_embd           = 2048
0.00.049.492 I llm_load_print_meta: n_layer          = 24
0.00.049.495 I llm_load_print_meta: n_head           = 16
0.00.049.495 I llm_load_print_meta: n_head_kv        = 16
0.00.049.496 I llm_load_print_meta: n_rot            = 32
0.00.049.496 I llm_load_print_meta: n_swa            = 0
0.00.049.496 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.496 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.497 I llm_load_print_meta: n_gqa            = 1
0.00.049.498 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.500 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.500 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.501 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.504 I llm_load_print_meta: n_ff             = 8192
0.00.049.504 I llm_load_print_meta: n_expert         = 0
0.00.049.504 I llm_load_print_meta: n_expert_used    = 0
0.00.049.504 I llm_load_print_meta: causal attn      = 1
0.00.049.504 I llm_load_print_meta: pooling type     = 0
0.00.049.504 I llm_load_print_meta: rope type        = 2
0.00.049.505 I llm_load_print_meta: rope scaling     = linear
0.00.049.505 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.505 I llm_load_print_meta: freq_scale_train = 1
0.00.049.506 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.507 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.508 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.508 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.508 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.508 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.508 I llm_load_print_meta: model type       = 1.4B
0.00.049.508 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.509 I llm_load_print_meta: model params     = 1.41 B
0.00.049.510 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.510 I llm_load_print_meta: general.name     = 1.4B
0.00.049.510 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.510 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.510 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.511 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.511 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.511 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.511 I llm_load_print_meta: max token length = 1024
0.00.051.519 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.519 I llm_load_tensors: offloading output layer to GPU
0.00.051.519 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.530 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.531 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.459 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.460 I llama_new_context_with_model: n_ctx         = 128
0.00.052.460 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.460 I llama_new_context_with_model: n_batch       = 128
0.00.052.460 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.460 I llama_new_context_with_model: flash_attn    = 0
0.00.052.461 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.461 I llama_new_context_with_model: freq_scale    = 1
0.00.052.462 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.462 I ggml_metal_init: allocating
0.00.052.469 I ggml_metal_init: found device: Apple M4
0.00.052.471 I ggml_metal_init: picking default device: Apple M4
0.00.053.058 I ggml_metal_init: using embedded metal library
0.00.055.385 I ggml_metal_init: GPU name:   Apple M4
0.00.055.387 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.388 I ggml_metal_init: simdgroup reduction   = true
0.00.055.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.388 I ggml_metal_init: has bfloat            = true
0.00.055.388 I ggml_metal_init: use bfloat            = true
0.00.055.388 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.389 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.006 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.246 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.250 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.087 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.088 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.089 I llama_new_context_with_model: graph nodes  = 967
0.00.067.089 I llama_new_context_with_model: graph splits = 2
0.00.067.101 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.102 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.211 I 
0.00.657.262 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.290 I perplexity: tokenizing the input ..
0.00.665.243 I perplexity: tokenization took 7.952 ms
0.00.665.246 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.962 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.789.276 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.789.298 I llama_perf_context_print:        load time =     648.43 ms
0.00.789.299 I llama_perf_context_print: prompt eval time =     122.46 ms /   128 tokens (    0.96 ms per token,  1045.24 tokens per second)
0.00.789.300 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.300 I llama_perf_context_print:       total time =     132.09 ms /   129 tokens
0.00.789.709 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.077s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.677 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.491 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.495 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.498 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.313 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.429 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.202 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.203 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.203 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.204 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.204 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.204 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.205 I llama_model_loader: - type  f32:  194 tensors
0.00.024.205 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.205 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.533 I llm_load_vocab: special tokens cache size = 25
0.00.050.475 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.477 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.478 I llm_load_print_meta: arch             = gptneox
0.00.050.478 I llm_load_print_meta: vocab type       = BPE
0.00.050.478 I llm_load_print_meta: n_vocab          = 50304
0.00.050.478 I llm_load_print_meta: n_merges         = 50009
0.00.050.479 I llm_load_print_meta: vocab_only       = 0
0.00.050.479 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.479 I llm_load_print_meta: n_embd           = 2048
0.00.050.479 I llm_load_print_meta: n_layer          = 24
0.00.050.482 I llm_load_print_meta: n_head           = 16
0.00.050.483 I llm_load_print_meta: n_head_kv        = 16
0.00.050.483 I llm_load_print_meta: n_rot            = 32
0.00.050.483 I llm_load_print_meta: n_swa            = 0
0.00.050.483 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.483 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.484 I llm_load_print_meta: n_gqa            = 1
0.00.050.485 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.485 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.486 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.486 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.487 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.487 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.487 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.487 I llm_load_print_meta: n_ff             = 8192
0.00.050.488 I llm_load_print_meta: n_expert         = 0
0.00.050.488 I llm_load_print_meta: n_expert_used    = 0
0.00.050.488 I llm_load_print_meta: causal attn      = 1
0.00.050.488 I llm_load_print_meta: pooling type     = 0
0.00.050.488 I llm_load_print_meta: rope type        = 2
0.00.050.488 I llm_load_print_meta: rope scaling     = linear
0.00.050.489 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.489 I llm_load_print_meta: freq_scale_train = 1
0.00.050.489 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.490 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.490 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.490 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.491 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.491 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.491 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.491 I llm_load_print_meta: model type       = 1.4B
0.00.050.492 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.492 I llm_load_print_meta: model params     = 1.41 B
0.00.050.493 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.493 I llm_load_print_meta: general.name     = 1.4B
0.00.050.493 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.494 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.494 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.494 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.496 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.496 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.496 I llm_load_print_meta: max token length = 1024
0.00.052.495 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.495 I llm_load_tensors: offloading output layer to GPU
0.00.052.496 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.501 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.501 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.468 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.469 I llama_new_context_with_model: n_ctx         = 128
0.00.053.469 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.469 I llama_new_context_with_model: n_batch       = 128
0.00.053.470 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.470 I llama_new_context_with_model: flash_attn    = 0
0.00.053.470 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.470 I llama_new_context_with_model: freq_scale    = 1
0.00.053.471 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.471 I ggml_metal_init: allocating
0.00.053.477 I ggml_metal_init: found device: Apple M4
0.00.053.480 I ggml_metal_init: picking default device: Apple M4
0.00.054.061 I ggml_metal_init: using embedded metal library
0.00.056.352 I ggml_metal_init: GPU name:   Apple M4
0.00.056.353 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.354 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.354 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.355 I ggml_metal_init: simdgroup reduction   = true
0.00.056.355 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.355 I ggml_metal_init: has bfloat            = true
0.00.056.355 I ggml_metal_init: use bfloat            = true
0.00.056.356 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.356 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.230 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.658 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.675 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.552 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.553 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.553 I llama_new_context_with_model: graph nodes  = 967
0.00.068.553 I llama_new_context_with_model: graph splits = 2
0.00.068.561 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.085 I 
0.00.703.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.136 I perplexity: tokenizing the input ..
0.00.710.866 I perplexity: tokenization took 7.729 ms
0.00.710.871 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.015 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.846.524 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.846.536 I llama_perf_context_print:        load time =     693.40 ms
0.00.846.537 I llama_perf_context_print: prompt eval time =     133.90 ms /   128 tokens (    1.05 ms per token,   955.94 tokens per second)
0.00.846.538 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.539 I llama_perf_context_print:       total time =     143.45 ms /   129 tokens
0.00.846.888 I ggml_metal_free: deallocating

real	0m0.862s
user	0m0.078s
sys	0m0.107s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.369 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.640 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.645 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.652 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.652 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.653 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.653 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.653 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.654 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.654 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.655 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.655 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.657 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.658 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.658 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.661 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.640 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.527 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.529 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.529 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.529 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.530 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.530 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.531 I llama_model_loader: - type  f32:  194 tensors
0.00.024.531 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.531 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.448 I llm_load_vocab: special tokens cache size = 25
0.00.052.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.619 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.619 I llm_load_print_meta: arch             = gptneox
0.00.052.619 I llm_load_print_meta: vocab type       = BPE
0.00.052.619 I llm_load_print_meta: n_vocab          = 50304
0.00.052.620 I llm_load_print_meta: n_merges         = 50009
0.00.052.620 I llm_load_print_meta: vocab_only       = 0
0.00.052.620 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.620 I llm_load_print_meta: n_embd           = 2048
0.00.052.620 I llm_load_print_meta: n_layer          = 24
0.00.052.624 I llm_load_print_meta: n_head           = 16
0.00.052.625 I llm_load_print_meta: n_head_kv        = 16
0.00.052.625 I llm_load_print_meta: n_rot            = 32
0.00.052.625 I llm_load_print_meta: n_swa            = 0
0.00.052.625 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.625 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.626 I llm_load_print_meta: n_gqa            = 1
0.00.052.626 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.629 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.630 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.630 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.630 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.630 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.631 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.631 I llm_load_print_meta: n_ff             = 8192
0.00.052.631 I llm_load_print_meta: n_expert         = 0
0.00.052.631 I llm_load_print_meta: n_expert_used    = 0
0.00.052.632 I llm_load_print_meta: causal attn      = 1
0.00.052.632 I llm_load_print_meta: pooling type     = 0
0.00.052.632 I llm_load_print_meta: rope type        = 2
0.00.052.632 I llm_load_print_meta: rope scaling     = linear
0.00.052.632 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.633 I llm_load_print_meta: freq_scale_train = 1
0.00.052.633 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.633 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.636 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.637 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.637 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.637 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.637 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.637 I llm_load_print_meta: model type       = 1.4B
0.00.052.638 I llm_load_print_meta: model ftype      = Q5_1
0.00.052.638 I llm_load_print_meta: model params     = 1.41 B
0.00.052.639 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.052.639 I llm_load_print_meta: general.name     = 1.4B
0.00.052.639 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.639 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.640 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.640 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.640 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.644 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.644 I llm_load_print_meta: max token length = 1024
0.00.054.585 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.586 I llm_load_tensors: offloading output layer to GPU
0.00.054.586 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.597 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.599 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.055.588 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.589 I llama_new_context_with_model: n_ctx         = 128
0.00.055.589 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.589 I llama_new_context_with_model: n_batch       = 128
0.00.055.590 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.590 I llama_new_context_with_model: flash_attn    = 0
0.00.055.590 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.591 I llama_new_context_with_model: freq_scale    = 1
0.00.055.591 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.592 I ggml_metal_init: allocating
0.00.055.596 I ggml_metal_init: found device: Apple M4
0.00.055.598 I ggml_metal_init: picking default device: Apple M4
0.00.056.200 I ggml_metal_init: using embedded metal library
0.00.058.829 I ggml_metal_init: GPU name:   Apple M4
0.00.058.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.831 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.832 I ggml_metal_init: simdgroup reduction   = true
0.00.058.832 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.832 I ggml_metal_init: has bfloat            = true
0.00.058.832 I ggml_metal_init: use bfloat            = true
0.00.058.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.138 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.521 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.524 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.539 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.488 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.490 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.490 I llama_new_context_with_model: graph nodes  = 967
0.00.070.490 I llama_new_context_with_model: graph splits = 2
0.00.070.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.873 I 
0.00.600.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.919 I perplexity: tokenizing the input ..
0.00.608.211 I perplexity: tokenization took 7.291 ms
0.00.608.215 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.742.264 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.743.692 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.743.712 I llama_perf_context_print:        load time =     591.50 ms
0.00.743.712 I llama_perf_context_print: prompt eval time =     133.82 ms /   128 tokens (    1.05 ms per token,   956.54 tokens per second)
0.00.743.716 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.743.716 I llama_perf_context_print:       total time =     142.84 ms /   129 tokens
0.00.744.091 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.080s
sys	0m0.091s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.729 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.341 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.343 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.343 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.343 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.344 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.345 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.346 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.347 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.347 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.347 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.011 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.058 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.809 I llama_model_loader: - type  f32:  194 tensors
0.00.023.809 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.809 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.810 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.173 I llm_load_vocab: special tokens cache size = 25
0.00.050.288 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.290 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.291 I llm_load_print_meta: arch             = gptneox
0.00.050.291 I llm_load_print_meta: vocab type       = BPE
0.00.050.291 I llm_load_print_meta: n_vocab          = 50304
0.00.050.291 I llm_load_print_meta: n_merges         = 50009
0.00.050.292 I llm_load_print_meta: vocab_only       = 0
0.00.050.292 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.292 I llm_load_print_meta: n_embd           = 2048
0.00.050.292 I llm_load_print_meta: n_layer          = 24
0.00.050.295 I llm_load_print_meta: n_head           = 16
0.00.050.295 I llm_load_print_meta: n_head_kv        = 16
0.00.050.296 I llm_load_print_meta: n_rot            = 32
0.00.050.296 I llm_load_print_meta: n_swa            = 0
0.00.050.296 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.296 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.297 I llm_load_print_meta: n_gqa            = 1
0.00.050.298 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.298 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.299 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.299 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.300 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.301 I llm_load_print_meta: n_ff             = 8192
0.00.050.301 I llm_load_print_meta: n_expert         = 0
0.00.050.301 I llm_load_print_meta: n_expert_used    = 0
0.00.050.301 I llm_load_print_meta: causal attn      = 1
0.00.050.301 I llm_load_print_meta: pooling type     = 0
0.00.050.301 I llm_load_print_meta: rope type        = 2
0.00.050.302 I llm_load_print_meta: rope scaling     = linear
0.00.050.302 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.302 I llm_load_print_meta: freq_scale_train = 1
0.00.050.303 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.303 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.305 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.305 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.305 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.305 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.305 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.306 I llm_load_print_meta: model type       = 1.4B
0.00.050.306 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.306 I llm_load_print_meta: model params     = 1.41 B
0.00.050.307 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.307 I llm_load_print_meta: general.name     = 1.4B
0.00.050.307 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.307 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.308 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.308 I llm_load_print_meta: max token length = 1024
0.00.052.270 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.271 I llm_load_tensors: offloading output layer to GPU
0.00.052.271 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.281 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.282 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.208 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.209 I llama_new_context_with_model: n_ctx         = 128
0.00.053.209 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.209 I llama_new_context_with_model: n_batch       = 128
0.00.053.209 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.210 I llama_new_context_with_model: flash_attn    = 0
0.00.053.210 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.210 I llama_new_context_with_model: freq_scale    = 1
0.00.053.211 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.211 I ggml_metal_init: allocating
0.00.053.214 I ggml_metal_init: found device: Apple M4
0.00.053.216 I ggml_metal_init: picking default device: Apple M4
0.00.053.787 I ggml_metal_init: using embedded metal library
0.00.056.134 I ggml_metal_init: GPU name:   Apple M4
0.00.056.135 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.136 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.136 I ggml_metal_init: simdgroup reduction   = true
0.00.056.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.137 I ggml_metal_init: has bfloat            = true
0.00.056.137 I ggml_metal_init: use bfloat            = true
0.00.056.137 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.138 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.190 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.494 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.498 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.514 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.363 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.364 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.365 I llama_new_context_with_model: graph nodes  = 967
0.00.068.365 I llama_new_context_with_model: graph splits = 2
0.00.068.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.387.305 I 
0.00.387.345 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.387.358 I perplexity: tokenizing the input ..
0.00.395.709 I perplexity: tokenization took 8.349 ms
0.00.395.713 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.528.044 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.529.225 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.529.237 I llama_perf_context_print:        load time =     377.57 ms
0.00.529.238 I llama_perf_context_print: prompt eval time =     132.10 ms /   128 tokens (    1.03 ms per token,   968.93 tokens per second)
0.00.529.239 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.529.240 I llama_perf_context_print:       total time =     141.93 ms /   129 tokens
0.00.529.635 I ggml_metal_free: deallocating

real	0m0.546s
user	0m0.079s
sys	0m0.070s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.678 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.178 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.183 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.184 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.185 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.185 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.185 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.186 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.187 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.187 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.187 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.189 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.025 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.935 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.936 I llama_model_loader: - type  f32:  194 tensors
0.00.022.936 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.937 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.937 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.937 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.174 I llm_load_vocab: special tokens cache size = 25
0.00.049.207 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.210 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.210 I llm_load_print_meta: arch             = gptneox
0.00.049.211 I llm_load_print_meta: vocab type       = BPE
0.00.049.211 I llm_load_print_meta: n_vocab          = 50304
0.00.049.211 I llm_load_print_meta: n_merges         = 50009
0.00.049.211 I llm_load_print_meta: vocab_only       = 0
0.00.049.211 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.211 I llm_load_print_meta: n_embd           = 2048
0.00.049.212 I llm_load_print_meta: n_layer          = 24
0.00.049.215 I llm_load_print_meta: n_head           = 16
0.00.049.215 I llm_load_print_meta: n_head_kv        = 16
0.00.049.216 I llm_load_print_meta: n_rot            = 32
0.00.049.217 I llm_load_print_meta: n_swa            = 0
0.00.049.217 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.217 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.218 I llm_load_print_meta: n_gqa            = 1
0.00.049.219 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.219 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.220 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.220 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.221 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.221 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.221 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.221 I llm_load_print_meta: n_ff             = 8192
0.00.049.222 I llm_load_print_meta: n_expert         = 0
0.00.049.222 I llm_load_print_meta: n_expert_used    = 0
0.00.049.222 I llm_load_print_meta: causal attn      = 1
0.00.049.222 I llm_load_print_meta: pooling type     = 0
0.00.049.222 I llm_load_print_meta: rope type        = 2
0.00.049.223 I llm_load_print_meta: rope scaling     = linear
0.00.049.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.223 I llm_load_print_meta: freq_scale_train = 1
0.00.049.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.224 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.226 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.226 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.226 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.226 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.226 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.226 I llm_load_print_meta: model type       = 1.4B
0.00.049.227 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.227 I llm_load_print_meta: model params     = 1.41 B
0.00.049.228 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.228 I llm_load_print_meta: general.name     = 1.4B
0.00.049.228 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.228 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.232 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.232 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.233 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.233 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.233 I llm_load_print_meta: max token length = 1024
0.00.051.220 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.221 I llm_load_tensors: offloading output layer to GPU
0.00.051.221 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.231 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.232 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.125 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.126 I llama_new_context_with_model: n_ctx         = 128
0.00.052.126 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.126 I llama_new_context_with_model: n_batch       = 128
0.00.052.126 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.126 I llama_new_context_with_model: flash_attn    = 0
0.00.052.127 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.127 I llama_new_context_with_model: freq_scale    = 1
0.00.052.127 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.128 I ggml_metal_init: allocating
0.00.052.133 I ggml_metal_init: found device: Apple M4
0.00.052.136 I ggml_metal_init: picking default device: Apple M4
0.00.052.718 I ggml_metal_init: using embedded metal library
0.00.055.002 I ggml_metal_init: GPU name:   Apple M4
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.004 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.005 I ggml_metal_init: simdgroup reduction   = true
0.00.055.005 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.005 I ggml_metal_init: has bfloat            = true
0.00.055.005 I ggml_metal_init: use bfloat            = true
0.00.055.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.006 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.639 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.915 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.917 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.932 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.859 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.860 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.861 I llama_new_context_with_model: graph nodes  = 967
0.00.066.861 I llama_new_context_with_model: graph splits = 2
0.00.066.873 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.874 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.496.070 I 
0.00.496.121 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.496.139 I perplexity: tokenizing the input ..
0.00.504.300 I perplexity: tokenization took 8.159 ms
0.00.504.307 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.636.606 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.637.786 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.637.804 I llama_perf_context_print:        load time =     487.39 ms
0.00.637.805 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.21 tokens per second)
0.00.637.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.637.806 I llama_perf_context_print:       total time =     141.74 ms /   129 tokens
0.00.638.241 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.078s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.803 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.459 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.461 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.461 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.465 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.206 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.231 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.022 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.023 I llama_model_loader: - type  f32:  194 tensors
0.00.023.024 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.024 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.024 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.201 I llm_load_vocab: special tokens cache size = 25
0.00.050.209 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.212 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.212 I llm_load_print_meta: arch             = gptneox
0.00.050.213 I llm_load_print_meta: vocab type       = BPE
0.00.050.213 I llm_load_print_meta: n_vocab          = 50304
0.00.050.213 I llm_load_print_meta: n_merges         = 50009
0.00.050.213 I llm_load_print_meta: vocab_only       = 0
0.00.050.213 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.214 I llm_load_print_meta: n_embd           = 2048
0.00.050.214 I llm_load_print_meta: n_layer          = 24
0.00.050.217 I llm_load_print_meta: n_head           = 16
0.00.050.217 I llm_load_print_meta: n_head_kv        = 16
0.00.050.217 I llm_load_print_meta: n_rot            = 32
0.00.050.218 I llm_load_print_meta: n_swa            = 0
0.00.050.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.219 I llm_load_print_meta: n_gqa            = 1
0.00.050.220 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.220 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.221 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.221 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.222 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.222 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.222 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.223 I llm_load_print_meta: n_ff             = 8192
0.00.050.223 I llm_load_print_meta: n_expert         = 0
0.00.050.223 I llm_load_print_meta: n_expert_used    = 0
0.00.050.223 I llm_load_print_meta: causal attn      = 1
0.00.050.223 I llm_load_print_meta: pooling type     = 0
0.00.050.223 I llm_load_print_meta: rope type        = 2
0.00.050.224 I llm_load_print_meta: rope scaling     = linear
0.00.050.224 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.224 I llm_load_print_meta: freq_scale_train = 1
0.00.050.225 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.225 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.225 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.225 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.225 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.226 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.226 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.226 I llm_load_print_meta: model type       = 1.4B
0.00.050.226 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.227 I llm_load_print_meta: model params     = 1.41 B
0.00.050.227 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.228 I llm_load_print_meta: general.name     = 1.4B
0.00.050.229 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.229 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.229 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.229 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.230 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.230 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.230 I llm_load_print_meta: max token length = 1024
0.00.052.243 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.244 I llm_load_tensors: offloading output layer to GPU
0.00.052.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.254 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.255 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.177 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.178 I llama_new_context_with_model: n_ctx         = 128
0.00.053.178 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.179 I llama_new_context_with_model: n_batch       = 128
0.00.053.179 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.179 I llama_new_context_with_model: flash_attn    = 0
0.00.053.179 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.180 I llama_new_context_with_model: freq_scale    = 1
0.00.053.180 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.181 I ggml_metal_init: allocating
0.00.053.187 I ggml_metal_init: found device: Apple M4
0.00.053.189 I ggml_metal_init: picking default device: Apple M4
0.00.053.736 I ggml_metal_init: using embedded metal library
0.00.056.092 I ggml_metal_init: GPU name:   Apple M4
0.00.056.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.094 I ggml_metal_init: simdgroup reduction   = true
0.00.056.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.094 I ggml_metal_init: has bfloat            = true
0.00.056.094 I ggml_metal_init: use bfloat            = true
0.00.056.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.421 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.812 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.815 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.828 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.778 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.779 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.779 I llama_new_context_with_model: graph nodes  = 967
0.00.067.779 I llama_new_context_with_model: graph splits = 2
0.00.067.791 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.792 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.551.444 I 
0.00.551.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.551.492 I perplexity: tokenizing the input ..
0.00.559.115 I perplexity: tokenization took 7.621 ms
0.00.559.118 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.730 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.695.010 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.695.047 I llama_perf_context_print:        load time =     542.64 ms
0.00.695.048 I llama_perf_context_print: prompt eval time =     134.38 ms /   128 tokens (    1.05 ms per token,   952.49 tokens per second)
0.00.695.049 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.695.049 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.695.559 I ggml_metal_free: deallocating

real	0m0.709s
user	0m0.078s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.271 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.083 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.088 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.089 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.089 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.090 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.090 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.092 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.093 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.930 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.973 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.886 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.887 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.888 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.888 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.888 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.888 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.889 I llama_model_loader: - type  f32:  194 tensors
0.00.023.889 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.889 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.215 I llm_load_vocab: special tokens cache size = 25
0.00.050.194 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.197 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.197 I llm_load_print_meta: arch             = gptneox
0.00.050.198 I llm_load_print_meta: vocab type       = BPE
0.00.050.198 I llm_load_print_meta: n_vocab          = 50304
0.00.050.198 I llm_load_print_meta: n_merges         = 50009
0.00.050.198 I llm_load_print_meta: vocab_only       = 0
0.00.050.198 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.198 I llm_load_print_meta: n_embd           = 2048
0.00.050.199 I llm_load_print_meta: n_layer          = 24
0.00.050.201 I llm_load_print_meta: n_head           = 16
0.00.050.202 I llm_load_print_meta: n_head_kv        = 16
0.00.050.202 I llm_load_print_meta: n_rot            = 32
0.00.050.203 I llm_load_print_meta: n_swa            = 0
0.00.050.203 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.203 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.204 I llm_load_print_meta: n_gqa            = 1
0.00.050.205 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.205 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.206 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.206 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.206 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.206 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.207 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.207 I llm_load_print_meta: n_ff             = 8192
0.00.050.207 I llm_load_print_meta: n_expert         = 0
0.00.050.208 I llm_load_print_meta: n_expert_used    = 0
0.00.050.208 I llm_load_print_meta: causal attn      = 1
0.00.050.208 I llm_load_print_meta: pooling type     = 0
0.00.050.208 I llm_load_print_meta: rope type        = 2
0.00.050.208 I llm_load_print_meta: rope scaling     = linear
0.00.050.209 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.210 I llm_load_print_meta: freq_scale_train = 1
0.00.050.210 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.210 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.210 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.210 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.211 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.211 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.211 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.211 I llm_load_print_meta: model type       = 1.4B
0.00.050.211 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.212 I llm_load_print_meta: model params     = 1.41 B
0.00.050.212 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.213 I llm_load_print_meta: general.name     = 1.4B
0.00.050.213 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.213 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.213 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.214 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.214 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.214 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.214 I llm_load_print_meta: max token length = 1024
0.00.052.284 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.284 I llm_load_tensors: offloading output layer to GPU
0.00.052.285 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.295 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.296 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.230 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.231 I llama_new_context_with_model: n_ctx         = 128
0.00.053.231 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.231 I llama_new_context_with_model: n_batch       = 128
0.00.053.231 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.231 I llama_new_context_with_model: flash_attn    = 0
0.00.053.232 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.232 I llama_new_context_with_model: freq_scale    = 1
0.00.053.232 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.233 I ggml_metal_init: allocating
0.00.053.236 I ggml_metal_init: found device: Apple M4
0.00.053.238 I ggml_metal_init: picking default device: Apple M4
0.00.053.791 I ggml_metal_init: using embedded metal library
0.00.056.135 I ggml_metal_init: GPU name:   Apple M4
0.00.056.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.136 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.137 I ggml_metal_init: simdgroup reduction   = true
0.00.056.137 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.137 I ggml_metal_init: has bfloat            = true
0.00.056.138 I ggml_metal_init: use bfloat            = true
0.00.056.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.802 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.142 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.165 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.028 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.029 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.029 I llama_new_context_with_model: graph nodes  = 967
0.00.068.029 I llama_new_context_with_model: graph splits = 2
0.00.068.042 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.042 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.171 I 
0.00.637.203 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.218 I perplexity: tokenizing the input ..
0.00.645.056 I perplexity: tokenization took 7.836 ms
0.00.645.060 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.697 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.786.876 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.786.892 I llama_perf_context_print:        load time =     627.90 ms
0.00.786.893 I llama_perf_context_print: prompt eval time =     140.41 ms /   128 tokens (    1.10 ms per token,   911.60 tokens per second)
0.00.786.894 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.895 I llama_perf_context_print:       total time =     149.72 ms /   129 tokens
0.00.787.352 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.078s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.837 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.566 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.395 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.422 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.238 I llama_model_loader: - type  f32:  194 tensors
0.00.023.238 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.297 I llm_load_vocab: special tokens cache size = 25
0.00.049.280 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.282 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.283 I llm_load_print_meta: arch             = gptneox
0.00.049.283 I llm_load_print_meta: vocab type       = BPE
0.00.049.283 I llm_load_print_meta: n_vocab          = 50304
0.00.049.283 I llm_load_print_meta: n_merges         = 50009
0.00.049.283 I llm_load_print_meta: vocab_only       = 0
0.00.049.284 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.284 I llm_load_print_meta: n_embd           = 2048
0.00.049.284 I llm_load_print_meta: n_layer          = 24
0.00.049.287 I llm_load_print_meta: n_head           = 16
0.00.049.288 I llm_load_print_meta: n_head_kv        = 16
0.00.049.288 I llm_load_print_meta: n_rot            = 32
0.00.049.288 I llm_load_print_meta: n_swa            = 0
0.00.049.288 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.288 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.289 I llm_load_print_meta: n_gqa            = 1
0.00.049.290 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.290 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.291 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.291 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.292 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.292 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.292 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.293 I llm_load_print_meta: n_ff             = 8192
0.00.049.293 I llm_load_print_meta: n_expert         = 0
0.00.049.293 I llm_load_print_meta: n_expert_used    = 0
0.00.049.293 I llm_load_print_meta: causal attn      = 1
0.00.049.294 I llm_load_print_meta: pooling type     = 0
0.00.049.294 I llm_load_print_meta: rope type        = 2
0.00.049.295 I llm_load_print_meta: rope scaling     = linear
0.00.049.295 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.295 I llm_load_print_meta: freq_scale_train = 1
0.00.049.296 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.296 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.296 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.298 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.298 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.298 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.298 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.298 I llm_load_print_meta: model type       = 1.4B
0.00.049.299 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.299 I llm_load_print_meta: model params     = 1.41 B
0.00.049.299 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.300 I llm_load_print_meta: general.name     = 1.4B
0.00.049.300 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.300 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.304 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.304 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.304 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.305 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.305 I llm_load_print_meta: max token length = 1024
0.00.050.879 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.880 I llm_load_tensors: offloading output layer to GPU
0.00.050.880 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.890 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.891 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.051.713 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.714 I llama_new_context_with_model: n_ctx         = 128
0.00.051.714 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.715 I llama_new_context_with_model: n_batch       = 128
0.00.051.715 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.715 I llama_new_context_with_model: flash_attn    = 0
0.00.051.715 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.716 I llama_new_context_with_model: freq_scale    = 1
0.00.051.716 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.717 I ggml_metal_init: allocating
0.00.051.722 I ggml_metal_init: found device: Apple M4
0.00.051.725 I ggml_metal_init: picking default device: Apple M4
0.00.052.283 I ggml_metal_init: using embedded metal library
0.00.054.603 I ggml_metal_init: GPU name:   Apple M4
0.00.054.605 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.605 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.606 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.606 I ggml_metal_init: simdgroup reduction   = true
0.00.054.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.606 I ggml_metal_init: has bfloat            = true
0.00.054.606 I ggml_metal_init: use bfloat            = true
0.00.054.607 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.607 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.014 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.260 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.263 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.277 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.174 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.175 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.175 I llama_new_context_with_model: graph nodes  = 967
0.00.066.176 I llama_new_context_with_model: graph splits = 2
0.00.066.188 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.189 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.919 I 
0.00.357.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.003 I perplexity: tokenizing the input ..
0.00.366.148 I perplexity: tokenization took 8.142 ms
0.00.366.156 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.505.998 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.507.167 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.507.181 I llama_perf_context_print:        load time =     349.07 ms
0.00.507.182 I llama_perf_context_print: prompt eval time =     139.62 ms /   128 tokens (    1.09 ms per token,   916.79 tokens per second)
0.00.507.183 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.507.184 I llama_perf_context_print:       total time =     149.27 ms /   129 tokens
0.00.507.620 I ggml_metal_free: deallocating

real	0m0.522s
user	0m0.077s
sys	0m0.080s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.257 I build: 4380 (ce083a54) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.520 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.568 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.582 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.583 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.585 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.667 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.673 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.674 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.674 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.675 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.676 I llama_model_loader: - type  f32:  194 tensors
0.00.050.676 I llama_model_loader: - type  f16:   98 tensors
0.00.080.105 I llm_load_vocab: special tokens cache size = 25
0.00.086.929 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.086.932 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.086.933 I llm_load_print_meta: arch             = gptneox
0.00.086.933 I llm_load_print_meta: vocab type       = BPE
0.00.086.933 I llm_load_print_meta: n_vocab          = 50304
0.00.086.933 I llm_load_print_meta: n_merges         = 50009
0.00.086.933 I llm_load_print_meta: vocab_only       = 0
0.00.086.934 I llm_load_print_meta: n_ctx_train      = 2048
0.00.086.934 I llm_load_print_meta: n_embd           = 2048
0.00.086.934 I llm_load_print_meta: n_layer          = 24
0.00.086.937 I llm_load_print_meta: n_head           = 16
0.00.086.938 I llm_load_print_meta: n_head_kv        = 16
0.00.086.940 I llm_load_print_meta: n_rot            = 32
0.00.086.940 I llm_load_print_meta: n_swa            = 0
0.00.086.940 I llm_load_print_meta: n_embd_head_k    = 128
0.00.086.940 I llm_load_print_meta: n_embd_head_v    = 128
0.00.086.941 I llm_load_print_meta: n_gqa            = 1
0.00.086.942 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.086.942 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.086.943 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.086.943 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.086.943 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.086.943 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.086.943 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.086.944 I llm_load_print_meta: n_ff             = 8192
0.00.086.945 I llm_load_print_meta: n_expert         = 0
0.00.086.946 I llm_load_print_meta: n_expert_used    = 0
0.00.086.946 I llm_load_print_meta: causal attn      = 1
0.00.086.946 I llm_load_print_meta: pooling type     = 0
0.00.086.946 I llm_load_print_meta: rope type        = 2
0.00.086.946 I llm_load_print_meta: rope scaling     = linear
0.00.086.946 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.086.947 I llm_load_print_meta: freq_scale_train = 1
0.00.086.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.086.947 I llm_load_print_meta: rope_finetuned   = unknown
0.00.086.947 I llm_load_print_meta: ssm_d_conv       = 0
0.00.086.947 I llm_load_print_meta: ssm_d_inner      = 0
0.00.086.947 I llm_load_print_meta: ssm_d_state      = 0
0.00.086.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.086.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.086.948 I llm_load_print_meta: model type       = 1.4B
0.00.086.948 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.086.948 I llm_load_print_meta: model params     = 1.41 B
0.00.086.949 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.086.952 I llm_load_print_meta: general.name     = 1.4B
0.00.086.952 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.086.952 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.086.953 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.086.953 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.086.953 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.086.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.086.954 I llm_load_print_meta: max token length = 1024
0.00.089.496 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.089.497 I llm_load_tensors: offloading output layer to GPU
0.00.089.497 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.089.507 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.089.508 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.090.491 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.492 I llama_new_context_with_model: n_ctx         = 128
0.00.090.492 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.090.493 I llama_new_context_with_model: n_batch       = 128
0.00.090.493 I llama_new_context_with_model: n_ubatch      = 128
0.00.090.493 I llama_new_context_with_model: flash_attn    = 0
0.00.090.493 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.494 I llama_new_context_with_model: freq_scale    = 1
0.00.090.494 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.494 I ggml_metal_init: allocating
0.00.090.497 I ggml_metal_init: found device: Apple M4
0.00.090.499 I ggml_metal_init: picking default device: Apple M4
0.00.091.109 I ggml_metal_init: using embedded metal library
0.00.093.638 I ggml_metal_init: GPU name:   Apple M4
0.00.093.640 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.640 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.641 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.641 I ggml_metal_init: simdgroup reduction   = true
0.00.093.641 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.641 I ggml_metal_init: has bfloat            = true
0.00.093.641 I ggml_metal_init: use bfloat            = true
0.00.093.642 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.642 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.897 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.104.215 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.220 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.237 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.135 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.105.136 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.105.136 I llama_new_context_with_model: graph nodes  = 967
0.00.105.136 I llama_new_context_with_model: graph splits = 2
0.00.105.149 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.149 I 
0.00.105.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.185 I compute_imatrix: tokenizing the input ..
0.00.112.014 I compute_imatrix: tokenization took 6.828 ms
0.00.112.016 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.628.260 I compute_imatrix: 1.52 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.631.507 I llama_perf_context_print:        load time =    1605.73 ms
0.01.631.511 I llama_perf_context_print: prompt eval time =    1515.61 ms /   128 tokens (   11.84 ms per token,    84.45 tokens per second)
0.01.631.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.631.513 I llama_perf_context_print:       total time =    1608.97 ms /   129 tokens
0.01.632.371 I ggml_metal_free: deallocating

real	0m1.819s
user	0m0.171s
sys	0m0.252s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4380 (ce083a54)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14b20a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14b20a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14b20aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14b20b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14b20ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14b20bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14b20c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14b20cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14b20d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14b20d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14b20daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14b20dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14b20eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14b20f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14b20fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14b2101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14b210910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14b211030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14b211750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14b211f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14b212640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14b212d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14b213480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14b213d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14b214440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14b214700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14b214d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14b215980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14b215ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14b216180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14b216620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14b2168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14b217170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14b2176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14b217970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14b217e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14b2182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14b218750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14b218bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14b219090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14b219530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14b2199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14b219e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14b21a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14b21a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14b21abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14b21b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14b21bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14b21c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14b21c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14b21cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14b21d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14b21d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14b21df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14b21e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14b21ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14b21f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14b21f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14b21f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14b220160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14b220420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14b2208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14b220d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14b221200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14b2216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14b221b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14b221fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14b222480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14b222920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14b222dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14b223260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14b223700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14b223ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14b2240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14b224640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14b224b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14b2250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14b225630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14b225b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14b2260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14b226620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14b226b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14b2270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14b227610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14b227b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14b2280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14b228600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14b228b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14b2290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14b2295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14b229b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14b22a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14b22a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14b22ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14b22b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14b22b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14b22bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14b21b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14b22bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14b22c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14b22cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14b22d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14b22d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14b22dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14b22e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14b22e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14b22ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14b22f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14b22f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14b22fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14b2301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14b230700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14b230c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14b2310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14b231590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14b231a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14b231ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14b232370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14b232810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14b232cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14b233150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14b2335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14b233a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14b233f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14b2343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14b234870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14b234d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14b2351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14b235650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14b235af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14b235f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14b236430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14b2368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14b236d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14b237210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14b2376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14b237b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14b237ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14b238490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14b238930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14b238dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14b239270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14b239710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14b239bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14b23a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14b23a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14b23a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14b23ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14b23b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14b23b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14b23bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14b23c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14b23c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14b23c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14b23ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14b23d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14b23d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14b23dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14b23e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14b23e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14b23ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14b23eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14b23f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14b23f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14b23fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14b240170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14b240610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14b240ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14b240f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14b2413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14b241890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14b241d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14b2421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14b242670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14b242b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14b242fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14b243450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14b2438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14b243d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14b244230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14b2446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14b244b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14b245010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14b2454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14b245950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14b245df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14b246290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14b246730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14b246bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14b247070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14b247510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14b2479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14b247e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14b2483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14b2488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14b248e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14b249390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14b249650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14b249c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14b24a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14b24a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14b24b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14b24b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14b24b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14b24bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14b24c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14b24cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14b24d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14b24d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14b24d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14b24e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14b24e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14b24ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14b24f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14b24f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14b24fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14b250150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14b2506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14b250bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14b251140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14b251690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14b251be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14b252130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14b252680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14b252bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14b253120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14b253670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14b253bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14b254110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14b254660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14b254bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14b255100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14b255650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14b255ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14b2560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14b256640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14b256b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14b2570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14b257630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14b257b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14b2580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14b258620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14b258b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14b2590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14b259610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14b259b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14b25a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14b25a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14b25ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14b25b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14b25b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14b25bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14b25c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14b25c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14b25cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14b25d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14b25d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14b25db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14b25e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14b25e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14b25eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14b25f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14b25f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14b25fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14b260050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14b2605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14b260af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14b260f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14b261430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14b2618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14b261d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14b262210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14b2626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14b262b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14b262ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14b263490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14b263930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14b263dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14b264270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14b264710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14b264bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14b265050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14b2655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14b265cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14b2663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14b266b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14b267220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14b2674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14b267cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14b267f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14b2685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.148.347 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.148.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff42540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff43290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff43700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff43b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff43fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff451a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff45ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff46c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff47990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff47e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff48270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff48b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff49430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff4a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff4aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff4b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff4bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff4c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff4c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff4cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff4d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff4d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff4db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff4e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff4e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff4ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff4f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff4fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff50790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff50c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff51950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff51dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff526a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff53860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff53cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff54140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff54a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff55300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff55be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff56650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff57bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff582e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff58ef0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff35640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff35f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff36390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff36c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff37550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff37e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff38710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff38b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff39d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff3aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff3b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff3c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff3d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff3d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff3db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff3e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff3e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff3f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff3f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff3fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff3fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff40350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff40c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff56000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff56de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff57db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff58220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff58690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.765s
user	0m0.305s
sys	0m0.257s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4380 (ce083a54)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14560b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14560bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14560c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14560c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14560ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14560d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14560d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14560dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14560e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14560e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14560ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14560f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14560fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145610560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1456122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1456129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1456131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1456138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145614000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1456156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1456159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1456178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1456190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1456199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14561a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14561a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14561ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14561b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14561b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14561b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14561be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14561c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14561cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14561d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14561d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14561dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14561e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14561ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14561f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14561fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14561fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145620600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1456216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145621b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1456224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145622de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1456249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1456258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1456268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1456278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1456288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1456298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14562a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14562a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14562ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14562b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14562b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14562bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14562c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14562c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14562cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14561caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14562d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14562d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14562df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14562e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14562e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14562ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14562f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14562f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14562ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1456309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145630f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1456319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1456343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1456351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1456368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1456376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1456384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14563a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14563a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14563a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14563ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14563b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14563b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14563bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14563c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14563c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14563ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14563ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14563d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14563d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14563dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14563e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14563e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14563ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14563ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14563f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14563f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14563fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1456418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1456421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1456446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1456454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1456462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1456479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1456487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1456490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14564a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14564a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14564a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14564af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14564b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14564bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14564c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14564c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14564ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14564d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14564d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14564de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14564e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14564e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14564ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14564f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14564f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14564feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145650ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1456513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1456523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145652930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145652e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1456533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145653920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145653e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1456543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1456553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145655e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1456563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1456568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1456578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145658380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1456588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145658e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145659370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1456598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14565a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14565a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14565ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14565b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14565b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14565bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14565c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14565c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14565cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14565d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14565d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14565ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14565e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14565e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14565edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14565f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14565f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14565fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145660850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1456612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145661840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1456626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145662b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1456634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145663950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1456659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1456662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1456684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145669840 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1468075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146807a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146807e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1468044b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146804770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146804be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146805050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1468084b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146808920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146809310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146809ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14680a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14680a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14680b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14680b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14680bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14680c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14680d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14680d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14680de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14680e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14680eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14680ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14680f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14680fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146810190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146810980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146810e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1468110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146811970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146811eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146812170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146812610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146812ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146812f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1468133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146813890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146813d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1468141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146814670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146814b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146814dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1468153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1468159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146816000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146816610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146816c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146817230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146817840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146817e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146818460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146818c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1468190f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146819590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146819850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146819e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14681a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14681aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14681af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14681b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14681b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14681bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14681c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14681c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14681cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14681cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14681d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14681d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14681ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14681e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14681e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14681ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14681f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14681f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14681fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146820250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1468207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146820cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146821240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146821790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146821ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146822230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146822780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146822cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146823770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146824210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146824760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146824cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146825200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146825750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1468261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146826740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146826c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1468271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146827730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146827c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1468281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146828720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146828c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1468291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146829710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146829c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14682a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14682a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14682ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14682b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14682b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14682bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14682c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14682c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14682c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14682ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14682d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14682d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14682dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14682e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14682e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14682e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14682ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14682f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14682f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14682fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1468300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146830590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146830a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146830ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146831370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146831810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146831cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146832150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1468325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146832a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146832f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1468333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146833870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146833d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1468341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146834650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146834af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146834f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146835430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1468358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146835d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146836210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1468366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146836b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146836ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146837490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146837930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146837dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146838270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146838710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146838bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146839050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1468394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146839990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146839e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14683a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14683a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14683ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14683b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14683b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14683b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14683be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14683c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14683c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14683cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14683d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14683d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14683da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14683def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14683e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14570a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14570a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14570ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14570b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14570b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14570ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14570bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14570c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14570c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14570cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14570d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14570d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14570ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14570e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14570e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14570ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14570f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14570f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14570fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1457103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1457109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145711000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145711610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145711e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1457122a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145712560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145713180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145713970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145713e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1457142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145714750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145714f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145715450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1457159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145715ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145716440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145716990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145716ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145717430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145717980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145717ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145718420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145718970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145718ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145719410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145719960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145719eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14571a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14571a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14571aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14571b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14571b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14571be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14571c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14571c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14571ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14571d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14571d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14571de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14571e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14571e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14571ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14571f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14571f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14571fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1457203a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1457208f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145720e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145721390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1457218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145721e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145722380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1457228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145722e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1457238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145723e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145724360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1457248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145724e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145725350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1457258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145725df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145726340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145726890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145726de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145727330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145727880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145727d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1457281c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145728660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145728fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145729440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1457298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145729d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14572a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14572a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14572ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14572b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14572b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14572b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14572bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14572c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14572ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14572d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14572d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14572dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14572e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14572ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14572ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14572f330 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146806860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146806cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146807140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1468075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146807a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146807e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146808300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146808770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146808be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146809050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1468094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146809930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14680a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14680a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14680b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14680b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14680bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14680c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14680cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14680d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14680ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14680e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14680eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14680f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14680f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14680fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146810250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1468106c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146810b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146810fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146811410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146811880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146811cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146811fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146812420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146812890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146812d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146813170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1468135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146813a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146813ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146814330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1468147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146814c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146815080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1468154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146815960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146815dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146816240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1468166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146816b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146816f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146817400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146817870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146817ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146818150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1468185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146818a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146818ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146819310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146819780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146819bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14681a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14681a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14681a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14681adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14681b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14681b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14681bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14681bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14681c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14681c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14681ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14681d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14681d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14681da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14681de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14681e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14681e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14681ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14681f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14681f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14681f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14681fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146820200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146820670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146820ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146820f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1468213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146821830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146821ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146822110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146822580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1468229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146822e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1468232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146823740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146823bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146824020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146824490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146824900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146824d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1468251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146825650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146825ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146825f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1468263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146826810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146826c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1468270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146827560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1468279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146827e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1468282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146828720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146828b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146829000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146829470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1468298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146829d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14682a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14682a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14682aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14682af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14682b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14682b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14682bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14682c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14682c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14682c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14682ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14682d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14682d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14682db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14682dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14682e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14682e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14682ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14682f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14682f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14682fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14682fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146830360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1468307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146830c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1468310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146831520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146831990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146831e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146832270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1468326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146832b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146832fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146833430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1468338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146833d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146834180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1468345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146834a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146834ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146835340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1468357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146835c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146836090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146836500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146836970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146836de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146837250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1468376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146837b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146837fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146838410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146838880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146838cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146839160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1468395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146839a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146839eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14683a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14683a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14683ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14683b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14683b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14683b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14683bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14683c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14683c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14683cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14683cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14683d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14683d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14683dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14683e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146f04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146f044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146f04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146f04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146f05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146f056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146f05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146f05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146f06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146f06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146f06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146f07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146f075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146f07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146f07ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146f08310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146f08780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146f09320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146f095e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146f098a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146f09d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146f0a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146f0a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146f0aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146f0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146f0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146f0b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146f0bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146f0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146f0c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146f0c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146f0cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146f0d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146f0d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146f0db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146f0dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146f0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146f0e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146f0ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146f0f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146f0f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146f0fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146f0feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146f10320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146f10790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146f10c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146f11070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146f114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146f11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146f11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146f12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146f12ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146f12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146f132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146f13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146f13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146f14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146f14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146f14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146f14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146f151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146f15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146f15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146f15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146f163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146f16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146f16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146f170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146f17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146f179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146f17e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146f182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146f18720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146f18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146f19000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146f19470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146f198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146f19d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146f1a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146f1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146f1aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146f1af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146f1b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146f1b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146f1bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146f1c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146f1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146f1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146f1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146f1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146f1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146f1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146f1eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146f1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146f1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146f1f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146f1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146f205a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.929s
user	0m0.243s
sys	0m0.134s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
