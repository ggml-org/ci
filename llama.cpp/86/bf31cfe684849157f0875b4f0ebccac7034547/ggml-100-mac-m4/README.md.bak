### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.42 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.20 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.46 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.97 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.91 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.04 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.38 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 222.07 sec*proc (28 tests)

Total Test time (real) = 222.08 sec

real	3m42.219s
user	7m42.785s
sys	0m6.212s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.19 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.22 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.14 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.35 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.02 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.08 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.22 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.07 sec*proc (28 tests)

Total Test time (real) =  51.08 sec

real	0m51.094s
user	1m11.026s
sys	0m5.339s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.102 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.046 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.019 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.031 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.027.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.032 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.027.033 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.027.034 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.027.036 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.027.041 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.027.042 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.027.042 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.027.043 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.027.047 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.048 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.049 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.027.049 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.027.054 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.027.055 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.027.056 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.032.696 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.034.125 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.127 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.034.128 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.034.128 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.034.129 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.034.129 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.034.130 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.034.130 I llama_model_loader: - type  f32:  124 tensors
0.00.034.131 I llama_model_loader: - type  f16:   73 tensors
0.00.038.737 I llm_load_vocab: special tokens cache size = 5
0.00.041.137 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.041.166 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.041.168 I llm_load_print_meta: arch             = bert
0.00.041.169 I llm_load_print_meta: vocab type       = WPM
0.00.041.169 I llm_load_print_meta: n_vocab          = 30522
0.00.041.169 I llm_load_print_meta: n_merges         = 0
0.00.041.170 I llm_load_print_meta: vocab_only       = 0
0.00.041.170 I llm_load_print_meta: n_ctx_train      = 512
0.00.041.170 I llm_load_print_meta: n_embd           = 384
0.00.041.170 I llm_load_print_meta: n_layer          = 12
0.00.041.174 I llm_load_print_meta: n_head           = 12
0.00.041.175 I llm_load_print_meta: n_head_kv        = 12
0.00.041.175 I llm_load_print_meta: n_rot            = 32
0.00.041.175 I llm_load_print_meta: n_swa            = 0
0.00.041.175 I llm_load_print_meta: n_embd_head_k    = 32
0.00.041.176 I llm_load_print_meta: n_embd_head_v    = 32
0.00.041.177 I llm_load_print_meta: n_gqa            = 1
0.00.041.178 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.041.179 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.041.180 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.041.180 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.041.180 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.041.181 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.041.181 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.041.182 I llm_load_print_meta: n_ff             = 1536
0.00.041.182 I llm_load_print_meta: n_expert         = 0
0.00.041.183 I llm_load_print_meta: n_expert_used    = 0
0.00.041.183 I llm_load_print_meta: causal attn      = 0
0.00.041.183 I llm_load_print_meta: pooling type     = 2
0.00.041.183 I llm_load_print_meta: rope type        = 2
0.00.041.184 I llm_load_print_meta: rope scaling     = linear
0.00.041.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.041.185 I llm_load_print_meta: freq_scale_train = 1
0.00.041.185 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.041.185 I llm_load_print_meta: rope_finetuned   = unknown
0.00.041.186 I llm_load_print_meta: ssm_d_conv       = 0
0.00.041.186 I llm_load_print_meta: ssm_d_inner      = 0
0.00.041.186 I llm_load_print_meta: ssm_d_state      = 0
0.00.041.186 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.041.189 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.041.189 I llm_load_print_meta: model type       = 33M
0.00.041.190 I llm_load_print_meta: model ftype      = F16
0.00.041.190 I llm_load_print_meta: model params     = 33.21 M
0.00.041.191 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.041.193 I llm_load_print_meta: general.name     = Bge Small
0.00.041.194 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.041.194 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.041.195 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.041.195 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.041.195 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.041.195 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.041.196 I llm_load_print_meta: max token length = 21
0.00.043.377 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.043.379 I llm_load_tensors: offloading output layer to GPU
0.00.043.380 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.043.409 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.043.411 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.044.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.044.085 I llama_new_context_with_model: n_ctx         = 512
0.00.044.085 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.044.085 I llama_new_context_with_model: n_batch       = 2048
0.00.044.085 I llama_new_context_with_model: n_ubatch      = 2048
0.00.044.086 I llama_new_context_with_model: flash_attn    = 0
0.00.044.086 I llama_new_context_with_model: freq_base     = 10000.0
0.00.044.087 I llama_new_context_with_model: freq_scale    = 1
0.00.044.088 I ggml_metal_init: allocating
0.00.044.095 I ggml_metal_init: found device: Apple M4
0.00.044.099 I ggml_metal_init: picking default device: Apple M4
0.00.045.081 I ggml_metal_init: using embedded metal library
0.00.049.536 I ggml_metal_init: GPU name:   Apple M4
0.00.049.539 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.540 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.540 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.541 I ggml_metal_init: simdgroup reduction   = true
0.00.049.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.541 I ggml_metal_init: has bfloat            = true
0.00.049.541 I ggml_metal_init: use bfloat            = true
0.00.049.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.252 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.062.961 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.062.963 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.062.965 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.063.797 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.063.799 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.063.799 I llama_new_context_with_model: graph nodes  = 429
0.00.063.799 I llama_new_context_with_model: graph splits = 2
0.00.063.820 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.063.821 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.447 I 
0.00.070.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.071.190 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.076.281 I llama_perf_context_print:        load time =      48.39 ms
0.00.076.282 I llama_perf_context_print: prompt eval time =       4.93 ms /     9 tokens (    0.55 ms per token,  1824.08 tokens per second)
0.00.076.283 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.076.284 I llama_perf_context_print:       total time =       5.84 ms /    10 tokens
0.00.076.435 I ggml_metal_free: deallocating

real	0m0.265s
user	0m0.053s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.035 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.400 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.469 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.474 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.475 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.475 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.475 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.476 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.476 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.477 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.477 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.477 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.479 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.480 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.480 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.480 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.481 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.483 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.483 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.935 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.600 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.601 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.601 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.602 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.602 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.602 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.603 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.603 I llama_model_loader: - type  f32:  124 tensors
0.00.014.603 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.033 I llm_load_vocab: special tokens cache size = 5
0.00.018.347 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.357 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.358 I llm_load_print_meta: arch             = bert
0.00.018.358 I llm_load_print_meta: vocab type       = WPM
0.00.018.358 I llm_load_print_meta: n_vocab          = 30522
0.00.018.358 I llm_load_print_meta: n_merges         = 0
0.00.018.358 I llm_load_print_meta: vocab_only       = 0
0.00.018.359 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.359 I llm_load_print_meta: n_embd           = 384
0.00.018.359 I llm_load_print_meta: n_layer          = 12
0.00.018.361 I llm_load_print_meta: n_head           = 12
0.00.018.362 I llm_load_print_meta: n_head_kv        = 12
0.00.018.362 I llm_load_print_meta: n_rot            = 32
0.00.018.362 I llm_load_print_meta: n_swa            = 0
0.00.018.362 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.362 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.363 I llm_load_print_meta: n_gqa            = 1
0.00.018.364 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.364 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.365 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.365 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.365 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.365 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.365 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.366 I llm_load_print_meta: n_ff             = 1536
0.00.018.366 I llm_load_print_meta: n_expert         = 0
0.00.018.366 I llm_load_print_meta: n_expert_used    = 0
0.00.018.366 I llm_load_print_meta: causal attn      = 0
0.00.018.367 I llm_load_print_meta: pooling type     = 2
0.00.018.367 I llm_load_print_meta: rope type        = 2
0.00.018.367 I llm_load_print_meta: rope scaling     = linear
0.00.018.367 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.368 I llm_load_print_meta: freq_scale_train = 1
0.00.018.369 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.369 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.369 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.369 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.369 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.369 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.369 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.370 I llm_load_print_meta: model type       = 33M
0.00.018.370 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.370 I llm_load_print_meta: model params     = 33.21 M
0.00.018.371 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.371 I llm_load_print_meta: general.name     = Bge Small
0.00.018.371 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.372 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.372 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.372 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.372 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.372 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.372 I llm_load_print_meta: max token length = 21
0.00.019.665 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.665 I llm_load_tensors: offloading output layer to GPU
0.00.019.666 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.675 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.675 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.033 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.034 I llama_new_context_with_model: n_ctx         = 512
0.00.020.034 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.034 I llama_new_context_with_model: n_batch       = 2048
0.00.020.034 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.034 I llama_new_context_with_model: flash_attn    = 0
0.00.020.035 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.035 I llama_new_context_with_model: freq_scale    = 1
0.00.020.035 I ggml_metal_init: allocating
0.00.020.039 I ggml_metal_init: found device: Apple M4
0.00.020.041 I ggml_metal_init: picking default device: Apple M4
0.00.020.648 I ggml_metal_init: using embedded metal library
0.00.023.142 I ggml_metal_init: GPU name:   Apple M4
0.00.023.144 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.144 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.145 I ggml_metal_init: simdgroup reduction   = true
0.00.023.145 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.145 I ggml_metal_init: has bfloat            = true
0.00.023.146 I ggml_metal_init: use bfloat            = true
0.00.023.146 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.352 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.033.825 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.828 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.829 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.404 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.405 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.405 I llama_new_context_with_model: graph nodes  = 429
0.00.034.405 I llama_new_context_with_model: graph splits = 2
0.00.034.418 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.420 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.963 I 
0.00.038.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.532 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.033 I llama_perf_context_print:        load time =      29.56 ms
0.00.044.034 I llama_perf_context_print: prompt eval time =       4.37 ms /     9 tokens (    0.49 ms per token,  2059.97 tokens per second)
0.00.044.034 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.035 I llama_perf_context_print:       total time =       5.07 ms /    10 tokens
0.00.044.225 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.154 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.724 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.144 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.027.147 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.148 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.027.149 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.149 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.027.150 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.027.150 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.027.152 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.027.152 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.027.152 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.027.153 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.027.153 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.027.156 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.027.156 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.027.158 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.027.159 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.159 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.035.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.037.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.029 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.043.031 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.043.032 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.043.032 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.043.032 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.043.033 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.043.033 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.043.034 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.043.034 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.043.034 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.043.035 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.043.035 I llama_model_loader: - type  f32:   40 tensors
0.00.043.036 I llama_model_loader: - type  f16:   30 tensors
0.00.062.041 W llm_load_vocab: empty token at index 5
0.00.066.648 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.067.956 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.067.989 I llm_load_vocab: special tokens cache size = 5
0.00.335.436 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.335.445 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.335.446 I llm_load_print_meta: arch             = jina-bert-v2
0.00.335.446 I llm_load_print_meta: vocab type       = BPE
0.00.335.447 I llm_load_print_meta: n_vocab          = 61056
0.00.335.448 I llm_load_print_meta: n_merges         = 39382
0.00.335.449 I llm_load_print_meta: vocab_only       = 0
0.00.335.453 I llm_load_print_meta: n_ctx_train      = 8192
0.00.335.453 I llm_load_print_meta: n_embd           = 384
0.00.335.453 I llm_load_print_meta: n_layer          = 4
0.00.335.458 I llm_load_print_meta: n_head           = 12
0.00.335.458 I llm_load_print_meta: n_head_kv        = 12
0.00.335.460 I llm_load_print_meta: n_rot            = 32
0.00.335.460 I llm_load_print_meta: n_swa            = 0
0.00.335.460 I llm_load_print_meta: n_embd_head_k    = 32
0.00.335.460 I llm_load_print_meta: n_embd_head_v    = 32
0.00.335.460 I llm_load_print_meta: n_gqa            = 1
0.00.335.461 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.335.462 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.335.462 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.335.465 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.335.465 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.335.466 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.335.467 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.335.467 I llm_load_print_meta: n_ff             = 1536
0.00.335.468 I llm_load_print_meta: n_expert         = 0
0.00.335.468 I llm_load_print_meta: n_expert_used    = 0
0.00.335.468 I llm_load_print_meta: causal attn      = 0
0.00.335.469 I llm_load_print_meta: pooling type     = -1
0.00.335.469 I llm_load_print_meta: rope type        = -1
0.00.335.469 I llm_load_print_meta: rope scaling     = linear
0.00.335.470 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.335.470 I llm_load_print_meta: freq_scale_train = 1
0.00.335.470 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.335.470 I llm_load_print_meta: rope_finetuned   = unknown
0.00.335.470 I llm_load_print_meta: ssm_d_conv       = 0
0.00.335.470 I llm_load_print_meta: ssm_d_inner      = 0
0.00.335.471 I llm_load_print_meta: ssm_d_state      = 0
0.00.335.471 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.335.471 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.335.471 I llm_load_print_meta: model type       = 33M
0.00.335.472 I llm_load_print_meta: model ftype      = F16
0.00.335.472 I llm_load_print_meta: model params     = 32.90 M
0.00.335.473 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.335.473 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.335.473 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.335.475 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.335.475 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.335.475 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.335.475 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.335.475 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.335.476 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.335.476 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.335.476 I llm_load_print_meta: max token length = 45
0.00.336.306 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.336.306 I llm_load_tensors: offloading output layer to GPU
0.00.336.306 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.336.326 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.336.327 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.024 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.025 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.025 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.025 I llama_new_context_with_model: n_batch       = 2048
0.00.337.025 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.026 I llama_new_context_with_model: flash_attn    = 0
0.00.337.026 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.026 I llama_new_context_with_model: freq_scale    = 1
0.00.337.027 I ggml_metal_init: allocating
0.00.337.030 I ggml_metal_init: found device: Apple M4
0.00.337.032 I ggml_metal_init: picking default device: Apple M4
0.00.337.670 I ggml_metal_init: using embedded metal library
0.00.340.153 I ggml_metal_init: GPU name:   Apple M4
0.00.340.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.340.155 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.340.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.340.156 I ggml_metal_init: simdgroup reduction   = true
0.00.340.156 I ggml_metal_init: simdgroup matrix mul. = true
0.00.340.156 I ggml_metal_init: has bfloat            = true
0.00.340.156 I ggml_metal_init: use bfloat            = true
0.00.340.157 I ggml_metal_init: hasUnifiedMemory      = true
0.00.340.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.350.264 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.352.708 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.352.710 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.352.712 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.353.269 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.353.270 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.353.270 I llama_new_context_with_model: graph nodes  = 154
0.00.353.270 I llama_new_context_with_model: graph splits = 2
0.00.353.289 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.353.290 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.936 I 
0.00.363.972 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.364.114 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.364.115 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.364.118 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.364.118 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.364.121 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.364.121 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.364.631 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.368.082 I llama_perf_context_print:        load time =     342.21 ms
0.00.368.083 I llama_perf_context_print: prompt eval time =       3.44 ms /    62 tokens (    0.06 ms per token, 18012.78 tokens per second)
0.00.368.084 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.368.085 I llama_perf_context_print:       total time =       4.15 ms /    63 tokens
0.00.368.294 I ggml_metal_free: deallocating

real	0m1.116s
user	0m0.347s
sys	0m0.041s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.106 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.216 I main: llama backend init
0.00.000.223 I main: load the model and apply lora adapter, if any
0.00.071.666 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.082.626 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.082.638 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.082.642 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.082.643 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.082.644 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.082.644 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.082.645 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.082.647 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.082.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.082.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.082.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.082.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.082.651 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.082.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.082.660 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.082.660 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.082.661 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.089.563 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.091.779 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.098.734 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.098.740 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.098.741 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.098.742 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.098.742 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.098.743 I llama_model_loader: - type  f32:  194 tensors
0.00.098.744 I llama_model_loader: - type  f16:   98 tensors
0.00.138.113 I llm_load_vocab: special tokens cache size = 25
0.00.146.083 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.146.087 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.146.087 I llm_load_print_meta: arch             = gptneox
0.00.146.087 I llm_load_print_meta: vocab type       = BPE
0.00.146.088 I llm_load_print_meta: n_vocab          = 50304
0.00.146.088 I llm_load_print_meta: n_merges         = 50009
0.00.146.088 I llm_load_print_meta: vocab_only       = 0
0.00.146.088 I llm_load_print_meta: n_ctx_train      = 2048
0.00.146.089 I llm_load_print_meta: n_embd           = 2048
0.00.146.090 I llm_load_print_meta: n_layer          = 24
0.00.146.094 I llm_load_print_meta: n_head           = 16
0.00.146.096 I llm_load_print_meta: n_head_kv        = 16
0.00.146.096 I llm_load_print_meta: n_rot            = 32
0.00.146.096 I llm_load_print_meta: n_swa            = 0
0.00.146.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.146.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.146.098 I llm_load_print_meta: n_gqa            = 1
0.00.146.098 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.146.099 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.146.100 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.146.100 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.146.100 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.146.100 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.146.101 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.146.101 I llm_load_print_meta: n_ff             = 8192
0.00.146.101 I llm_load_print_meta: n_expert         = 0
0.00.146.102 I llm_load_print_meta: n_expert_used    = 0
0.00.146.102 I llm_load_print_meta: causal attn      = 1
0.00.146.102 I llm_load_print_meta: pooling type     = 0
0.00.146.102 I llm_load_print_meta: rope type        = 2
0.00.146.102 I llm_load_print_meta: rope scaling     = linear
0.00.146.104 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.146.105 I llm_load_print_meta: freq_scale_train = 1
0.00.146.105 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.146.105 I llm_load_print_meta: rope_finetuned   = unknown
0.00.146.105 I llm_load_print_meta: ssm_d_conv       = 0
0.00.146.105 I llm_load_print_meta: ssm_d_inner      = 0
0.00.146.105 I llm_load_print_meta: ssm_d_state      = 0
0.00.146.105 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.146.105 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.146.106 I llm_load_print_meta: model type       = 1.4B
0.00.146.106 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.146.107 I llm_load_print_meta: model params     = 1.41 B
0.00.146.107 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.146.107 I llm_load_print_meta: general.name     = 1.4B
0.00.146.107 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.146.108 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.146.108 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.146.112 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.146.113 I llm_load_print_meta: LF token         = 128 ''
0.00.146.113 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.146.113 I llm_load_print_meta: max token length = 1024
0.00.148.871 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.148.871 I llm_load_tensors: offloading output layer to GPU
0.00.148.872 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.148.891 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.148.892 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.149.966 I llama_new_context_with_model: n_seq_max     = 1
0.00.149.967 I llama_new_context_with_model: n_ctx         = 2048
0.00.149.967 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.149.967 I llama_new_context_with_model: n_batch       = 2048
0.00.149.967 I llama_new_context_with_model: n_ubatch      = 512
0.00.149.967 I llama_new_context_with_model: flash_attn    = 0
0.00.149.968 I llama_new_context_with_model: freq_base     = 10000.0
0.00.149.968 I llama_new_context_with_model: freq_scale    = 1
0.00.149.969 I ggml_metal_init: allocating
0.00.149.972 I ggml_metal_init: found device: Apple M4
0.00.149.974 I ggml_metal_init: picking default device: Apple M4
0.00.150.676 I ggml_metal_init: using embedded metal library
0.00.280.315 I ggml_metal_init: GPU name:   Apple M4
0.00.280.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.280.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.280.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.280.330 I ggml_metal_init: simdgroup reduction   = true
0.00.280.331 I ggml_metal_init: simdgroup matrix mul. = true
0.00.280.331 I ggml_metal_init: has bfloat            = true
0.00.280.331 I ggml_metal_init: use bfloat            = true
0.00.280.333 I ggml_metal_init: hasUnifiedMemory      = true
0.00.280.338 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.313.897 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.340.284 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.340.296 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.340.333 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.342.018 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.342.021 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.342.022 I llama_new_context_with_model: graph nodes  = 967
0.00.342.022 I llama_new_context_with_model: graph splits = 2
0.00.342.052 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.342.320 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.342.321 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.425.965 I main: llama threadpool init, n_threads = 4
0.00.426.000 I 
0.00.426.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.038 I 
0.00.426.111 I sampler seed: 1234
0.00.426.116 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.426.140 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.426.141 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.426.141 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.267.372 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.02.267.373 I llama_perf_context_print:        load time =     354.29 ms
0.02.267.374 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.64 tokens per second)
0.02.267.375 I llama_perf_context_print:        eval time =    1794.42 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.267.375 I llama_perf_context_print:       total time =    1841.41 ms /    70 tokens
0.02.267.531 I ggml_metal_free: deallocating

real	0m2.597s
user	0m0.163s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.220 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.031.052 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.936 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.950 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.950 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.951 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.952 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.952 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.953 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.955 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.956 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.956 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.058.222 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.223 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.223 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.224 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.225 I llama_model_loader: - type  f32:  194 tensors
0.00.058.225 I llama_model_loader: - type  f16:   98 tensors
0.00.086.075 I llm_load_vocab: special tokens cache size = 25
0.00.092.687 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.690 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.690 I llm_load_print_meta: arch             = gptneox
0.00.092.690 I llm_load_print_meta: vocab type       = BPE
0.00.092.691 I llm_load_print_meta: n_vocab          = 50304
0.00.092.691 I llm_load_print_meta: n_merges         = 50009
0.00.092.691 I llm_load_print_meta: vocab_only       = 0
0.00.092.691 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.691 I llm_load_print_meta: n_embd           = 2048
0.00.092.691 I llm_load_print_meta: n_layer          = 24
0.00.092.694 I llm_load_print_meta: n_head           = 16
0.00.092.695 I llm_load_print_meta: n_head_kv        = 16
0.00.092.695 I llm_load_print_meta: n_rot            = 32
0.00.092.697 I llm_load_print_meta: n_swa            = 0
0.00.092.698 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.698 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.699 I llm_load_print_meta: n_gqa            = 1
0.00.092.699 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.700 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.700 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.701 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.701 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.701 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.701 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.702 I llm_load_print_meta: n_ff             = 8192
0.00.092.702 I llm_load_print_meta: n_expert         = 0
0.00.092.702 I llm_load_print_meta: n_expert_used    = 0
0.00.092.703 I llm_load_print_meta: causal attn      = 1
0.00.092.703 I llm_load_print_meta: pooling type     = 0
0.00.092.703 I llm_load_print_meta: rope type        = 2
0.00.092.703 I llm_load_print_meta: rope scaling     = linear
0.00.092.703 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.703 I llm_load_print_meta: freq_scale_train = 1
0.00.092.704 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.704 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.704 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.704 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.705 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.706 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.706 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.706 I llm_load_print_meta: model type       = 1.4B
0.00.092.707 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.707 I llm_load_print_meta: model params     = 1.41 B
0.00.092.707 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.708 I llm_load_print_meta: general.name     = 1.4B
0.00.092.708 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.708 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.709 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.709 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.710 I llm_load_print_meta: LF token         = 128 ''
0.00.092.710 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.710 I llm_load_print_meta: max token length = 1024
0.00.095.209 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.095.209 I llm_load_tensors: offloading output layer to GPU
0.00.095.209 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.095.220 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.095.221 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.096.208 I llama_new_context_with_model: n_seq_max     = 1
0.00.096.209 I llama_new_context_with_model: n_ctx         = 128
0.00.096.209 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.096.209 I llama_new_context_with_model: n_batch       = 128
0.00.096.210 I llama_new_context_with_model: n_ubatch      = 128
0.00.096.210 I llama_new_context_with_model: flash_attn    = 0
0.00.096.210 I llama_new_context_with_model: freq_base     = 10000.0
0.00.096.210 I llama_new_context_with_model: freq_scale    = 1
0.00.096.211 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.096.211 I ggml_metal_init: allocating
0.00.096.214 I ggml_metal_init: found device: Apple M4
0.00.096.216 I ggml_metal_init: picking default device: Apple M4
0.00.096.798 I ggml_metal_init: using embedded metal library
0.00.099.244 I ggml_metal_init: GPU name:   Apple M4
0.00.099.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.247 I ggml_metal_init: simdgroup reduction   = true
0.00.099.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.247 I ggml_metal_init: has bfloat            = true
0.00.099.247 I ggml_metal_init: use bfloat            = true
0.00.099.248 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.248 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.200 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.111.609 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.111.611 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.111.626 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.514 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.112.515 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.112.516 I llama_new_context_with_model: graph nodes  = 967
0.00.112.516 I llama_new_context_with_model: graph splits = 2
0.00.112.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.112.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.954.830 I 
0.00.954.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.954.966 I perplexity: tokenizing the input ..
0.00.968.171 I perplexity: tokenization took 13.203 ms
0.00.968.176 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.100.005 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.101.443 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.101.463 I llama_perf_context_print:        load time =     923.76 ms
0.01.101.464 I llama_perf_context_print: prompt eval time =     131.29 ms /   128 tokens (    1.03 ms per token,   974.93 tokens per second)
0.01.101.465 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.101.467 I llama_perf_context_print:       total time =     146.64 ms /   129 tokens
0.01.102.232 I ggml_metal_free: deallocating

real	0m1.296s
user	0m0.118s
sys	0m0.195s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.751 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.480 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.482 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.483 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.483 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.483 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.484 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.485 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.486 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.548 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.493 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.038.494 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.495 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.495 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.496 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.496 I llama_model_loader: - type  f32:  194 tensors
0.00.038.497 I llama_model_loader: - type q8_0:   98 tensors
0.00.063.682 I llm_load_vocab: special tokens cache size = 25
0.00.070.396 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.070.400 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.070.400 I llm_load_print_meta: arch             = gptneox
0.00.070.400 I llm_load_print_meta: vocab type       = BPE
0.00.070.401 I llm_load_print_meta: n_vocab          = 50304
0.00.070.401 I llm_load_print_meta: n_merges         = 50009
0.00.070.401 I llm_load_print_meta: vocab_only       = 0
0.00.070.401 I llm_load_print_meta: n_ctx_train      = 2048
0.00.070.401 I llm_load_print_meta: n_embd           = 2048
0.00.070.401 I llm_load_print_meta: n_layer          = 24
0.00.070.407 I llm_load_print_meta: n_head           = 16
0.00.070.408 I llm_load_print_meta: n_head_kv        = 16
0.00.070.408 I llm_load_print_meta: n_rot            = 32
0.00.070.408 I llm_load_print_meta: n_swa            = 0
0.00.070.408 I llm_load_print_meta: n_embd_head_k    = 128
0.00.070.408 I llm_load_print_meta: n_embd_head_v    = 128
0.00.070.409 I llm_load_print_meta: n_gqa            = 1
0.00.070.410 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.070.410 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.070.411 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.070.411 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.070.411 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.070.411 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.070.412 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.070.412 I llm_load_print_meta: n_ff             = 8192
0.00.070.412 I llm_load_print_meta: n_expert         = 0
0.00.070.412 I llm_load_print_meta: n_expert_used    = 0
0.00.070.413 I llm_load_print_meta: causal attn      = 1
0.00.070.413 I llm_load_print_meta: pooling type     = 0
0.00.070.413 I llm_load_print_meta: rope type        = 2
0.00.070.413 I llm_load_print_meta: rope scaling     = linear
0.00.070.413 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.070.414 I llm_load_print_meta: freq_scale_train = 1
0.00.070.414 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.070.414 I llm_load_print_meta: rope_finetuned   = unknown
0.00.070.414 I llm_load_print_meta: ssm_d_conv       = 0
0.00.070.415 I llm_load_print_meta: ssm_d_inner      = 0
0.00.070.415 I llm_load_print_meta: ssm_d_state      = 0
0.00.070.418 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.070.418 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.070.419 I llm_load_print_meta: model type       = 1.4B
0.00.070.419 I llm_load_print_meta: model ftype      = Q8_0
0.00.070.419 I llm_load_print_meta: model params     = 1.41 B
0.00.070.420 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.070.420 I llm_load_print_meta: general.name     = 1.4B
0.00.070.420 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.070.420 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.070.421 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.070.421 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.070.421 I llm_load_print_meta: LF token         = 128 ''
0.00.070.421 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.070.422 I llm_load_print_meta: max token length = 1024
0.00.072.941 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.072.942 I llm_load_tensors: offloading output layer to GPU
0.00.072.942 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.072.954 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.072.955 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.073.986 I llama_new_context_with_model: n_seq_max     = 1
0.00.073.987 I llama_new_context_with_model: n_ctx         = 2048
0.00.073.987 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.073.987 I llama_new_context_with_model: n_batch       = 2048
0.00.073.988 I llama_new_context_with_model: n_ubatch      = 512
0.00.073.988 I llama_new_context_with_model: flash_attn    = 0
0.00.073.988 I llama_new_context_with_model: freq_base     = 10000.0
0.00.073.988 I llama_new_context_with_model: freq_scale    = 1
0.00.073.989 I ggml_metal_init: allocating
0.00.073.992 I ggml_metal_init: found device: Apple M4
0.00.073.998 I ggml_metal_init: picking default device: Apple M4
0.00.074.788 I ggml_metal_init: using embedded metal library
0.00.077.745 I ggml_metal_init: GPU name:   Apple M4
0.00.077.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.747 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.748 I ggml_metal_init: simdgroup reduction   = true
0.00.077.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.748 I ggml_metal_init: has bfloat            = true
0.00.077.748 I ggml_metal_init: use bfloat            = true
0.00.077.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.540 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.114.860 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.870 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.894 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.965 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.967 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.967 I llama_new_context_with_model: graph nodes  = 967
0.00.115.968 I llama_new_context_with_model: graph splits = 2
0.00.115.985 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.984.713 I main: llama threadpool init, n_threads = 4
0.01.984.759 I 
0.01.984.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.984.798 I 
0.01.985.080 I sampler seed: 1234
0.01.985.094 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.985.117 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.985.119 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.985.119 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.03.083.091 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48365.12 tokens per second)
0.03.083.092 I llama_perf_context_print:        load time =    1974.96 ms
0.03.083.093 I llama_perf_context_print: prompt eval time =      49.60 ms /     7 tokens (    7.09 ms per token,   141.13 tokens per second)
0.03.083.093 I llama_perf_context_print:        eval time =    1045.32 ms /    63 runs   (   16.59 ms per token,    60.27 tokens per second)
0.03.083.095 I llama_perf_context_print:       total time =    1098.38 ms /    70 tokens
0.03.083.312 I ggml_metal_free: deallocating

real	0m3.103s
user	0m0.128s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.121 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.599 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.524 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.529 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.531 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.532 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.532 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.533 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.534 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.535 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.536 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.019 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.093 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.093 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.094 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.094 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.095 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.095 I llama_model_loader: - type  f32:  194 tensors
0.00.031.096 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.787 I llm_load_vocab: special tokens cache size = 25
0.00.062.030 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.034 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.034 I llm_load_print_meta: arch             = gptneox
0.00.062.035 I llm_load_print_meta: vocab type       = BPE
0.00.062.035 I llm_load_print_meta: n_vocab          = 50304
0.00.062.035 I llm_load_print_meta: n_merges         = 50009
0.00.062.035 I llm_load_print_meta: vocab_only       = 0
0.00.062.035 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.035 I llm_load_print_meta: n_embd           = 2048
0.00.062.036 I llm_load_print_meta: n_layer          = 24
0.00.062.041 I llm_load_print_meta: n_head           = 16
0.00.062.042 I llm_load_print_meta: n_head_kv        = 16
0.00.062.042 I llm_load_print_meta: n_rot            = 32
0.00.062.042 I llm_load_print_meta: n_swa            = 0
0.00.062.042 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.042 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.043 I llm_load_print_meta: n_gqa            = 1
0.00.062.044 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.044 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.045 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.045 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.045 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.046 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.046 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.046 I llm_load_print_meta: n_ff             = 8192
0.00.062.047 I llm_load_print_meta: n_expert         = 0
0.00.062.047 I llm_load_print_meta: n_expert_used    = 0
0.00.062.047 I llm_load_print_meta: causal attn      = 1
0.00.062.047 I llm_load_print_meta: pooling type     = 0
0.00.062.048 I llm_load_print_meta: rope type        = 2
0.00.062.048 I llm_load_print_meta: rope scaling     = linear
0.00.062.048 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.048 I llm_load_print_meta: freq_scale_train = 1
0.00.062.049 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.049 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.049 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.049 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.049 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.050 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.050 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.050 I llm_load_print_meta: model type       = 1.4B
0.00.062.050 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.051 I llm_load_print_meta: model params     = 1.41 B
0.00.062.051 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.052 I llm_load_print_meta: general.name     = 1.4B
0.00.062.052 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.052 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.052 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.053 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.054 I llm_load_print_meta: LF token         = 128 ''
0.00.062.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.054 I llm_load_print_meta: max token length = 1024
0.00.063.895 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.895 I llm_load_tensors: offloading output layer to GPU
0.00.063.895 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.906 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.907 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.769 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.770 I llama_new_context_with_model: n_ctx         = 128
0.00.064.770 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.770 I llama_new_context_with_model: n_batch       = 128
0.00.064.771 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.771 I llama_new_context_with_model: flash_attn    = 0
0.00.064.771 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.771 I llama_new_context_with_model: freq_scale    = 1
0.00.064.772 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.772 I ggml_metal_init: allocating
0.00.064.779 I ggml_metal_init: found device: Apple M4
0.00.064.782 I ggml_metal_init: picking default device: Apple M4
0.00.065.461 I ggml_metal_init: using embedded metal library
0.00.067.970 I ggml_metal_init: GPU name:   Apple M4
0.00.067.972 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.972 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.973 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.973 I ggml_metal_init: simdgroup reduction   = true
0.00.067.973 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.973 I ggml_metal_init: has bfloat            = true
0.00.067.973 I ggml_metal_init: use bfloat            = true
0.00.067.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.359 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.079.823 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.826 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.843 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.080.957 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.080.959 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.080.959 I llama_new_context_with_model: graph nodes  = 967
0.00.080.959 I llama_new_context_with_model: graph splits = 2
0.00.080.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.080.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.003 I 
0.00.933.051 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.070 I perplexity: tokenizing the input ..
0.00.941.125 I perplexity: tokenization took 8.054 ms
0.00.941.132 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.065.377 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.066.541 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.066.558 I llama_perf_context_print:        load time =     921.40 ms
0.01.066.559 I llama_perf_context_print: prompt eval time =     124.02 ms /   128 tokens (    0.97 ms per token,  1032.09 tokens per second)
0.01.066.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.066.561 I llama_perf_context_print:       total time =     133.56 ms /   129 tokens
0.01.066.960 I ggml_metal_free: deallocating

real	0m1.084s
user	0m0.090s
sys	0m0.161s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.012.886 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.020.222 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.226 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.227 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.227 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.227 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.229 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.229 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.230 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.230 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.230 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.231 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.231 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.233 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.234 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.234 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.117 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.944 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.945 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.945 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.946 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.946 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.947 I llama_model_loader: - type  f32:  194 tensors
0.00.028.947 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.947 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.231 I llm_load_vocab: special tokens cache size = 25
0.00.055.012 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.014 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.015 I llm_load_print_meta: arch             = gptneox
0.00.055.015 I llm_load_print_meta: vocab type       = BPE
0.00.055.016 I llm_load_print_meta: n_vocab          = 50304
0.00.055.016 I llm_load_print_meta: n_merges         = 50009
0.00.055.016 I llm_load_print_meta: vocab_only       = 0
0.00.055.016 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.016 I llm_load_print_meta: n_embd           = 2048
0.00.055.017 I llm_load_print_meta: n_layer          = 24
0.00.055.021 I llm_load_print_meta: n_head           = 16
0.00.055.022 I llm_load_print_meta: n_head_kv        = 16
0.00.055.022 I llm_load_print_meta: n_rot            = 32
0.00.055.022 I llm_load_print_meta: n_swa            = 0
0.00.055.022 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.024 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.024 I llm_load_print_meta: n_gqa            = 1
0.00.055.025 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.028 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.030 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.030 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.030 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.031 I llm_load_print_meta: n_ff             = 8192
0.00.055.031 I llm_load_print_meta: n_expert         = 0
0.00.055.031 I llm_load_print_meta: n_expert_used    = 0
0.00.055.031 I llm_load_print_meta: causal attn      = 1
0.00.055.031 I llm_load_print_meta: pooling type     = 0
0.00.055.032 I llm_load_print_meta: rope type        = 2
0.00.055.032 I llm_load_print_meta: rope scaling     = linear
0.00.055.032 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.032 I llm_load_print_meta: freq_scale_train = 1
0.00.055.033 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.033 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.033 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.033 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.034 I llm_load_print_meta: model type       = 1.4B
0.00.055.034 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.035 I llm_load_print_meta: model params     = 1.41 B
0.00.055.043 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.044 I llm_load_print_meta: general.name     = 1.4B
0.00.055.045 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.045 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.045 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.045 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.045 I llm_load_print_meta: LF token         = 128 ''
0.00.055.046 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.046 I llm_load_print_meta: max token length = 1024
0.00.057.214 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.214 I llm_load_tensors: offloading output layer to GPU
0.00.057.215 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.226 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.228 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.210 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.211 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.211 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.211 I llama_new_context_with_model: n_batch       = 2048
0.00.058.211 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.211 I llama_new_context_with_model: flash_attn    = 0
0.00.058.212 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.212 I llama_new_context_with_model: freq_scale    = 1
0.00.058.213 I ggml_metal_init: allocating
0.00.058.216 I ggml_metal_init: found device: Apple M4
0.00.058.219 I ggml_metal_init: picking default device: Apple M4
0.00.058.957 I ggml_metal_init: using embedded metal library
0.00.061.510 I ggml_metal_init: GPU name:   Apple M4
0.00.061.512 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.513 I ggml_metal_init: simdgroup reduction   = true
0.00.061.513 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.513 I ggml_metal_init: has bfloat            = true
0.00.061.513 I ggml_metal_init: use bfloat            = true
0.00.061.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.514 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.028 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.102.961 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.974 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.999 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.201 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.203 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.204 I llama_new_context_with_model: graph nodes  = 967
0.00.104.204 I llama_new_context_with_model: graph splits = 2
0.00.104.223 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.742 I main: llama threadpool init, n_threads = 4
0.00.724.781 I 
0.00.724.815 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.815 I 
0.00.725.051 I sampler seed: 1234
0.00.725.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.725.066 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.725.066 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.725.066 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.401.697 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.401.698 I llama_perf_context_print:        load time =     711.85 ms
0.01.401.699 I llama_perf_context_print: prompt eval time =      43.70 ms /     7 tokens (    6.24 ms per token,   160.20 tokens per second)
0.01.401.699 I llama_perf_context_print:        eval time =     629.84 ms /    63 runs   (   10.00 ms per token,   100.03 tokens per second)
0.01.401.700 I llama_perf_context_print:       total time =     676.96 ms /    70 tokens
0.01.401.887 I ggml_metal_free: deallocating

real	0m1.423s
user	0m0.110s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.406 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.947 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.948 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.948 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.949 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.764 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.805 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.669 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.672 I llama_model_loader: - type  f32:  194 tensors
0.00.024.673 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.673 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.716 I llm_load_vocab: special tokens cache size = 25
0.00.050.695 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.698 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.698 I llm_load_print_meta: arch             = gptneox
0.00.050.699 I llm_load_print_meta: vocab type       = BPE
0.00.050.699 I llm_load_print_meta: n_vocab          = 50304
0.00.050.699 I llm_load_print_meta: n_merges         = 50009
0.00.050.699 I llm_load_print_meta: vocab_only       = 0
0.00.050.699 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.700 I llm_load_print_meta: n_embd           = 2048
0.00.050.700 I llm_load_print_meta: n_layer          = 24
0.00.050.703 I llm_load_print_meta: n_head           = 16
0.00.050.703 I llm_load_print_meta: n_head_kv        = 16
0.00.050.703 I llm_load_print_meta: n_rot            = 32
0.00.050.704 I llm_load_print_meta: n_swa            = 0
0.00.050.704 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.704 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.705 I llm_load_print_meta: n_gqa            = 1
0.00.050.706 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.706 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.707 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.707 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.707 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.708 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.708 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.709 I llm_load_print_meta: n_ff             = 8192
0.00.050.709 I llm_load_print_meta: n_expert         = 0
0.00.050.712 I llm_load_print_meta: n_expert_used    = 0
0.00.050.712 I llm_load_print_meta: causal attn      = 1
0.00.050.712 I llm_load_print_meta: pooling type     = 0
0.00.050.712 I llm_load_print_meta: rope type        = 2
0.00.050.712 I llm_load_print_meta: rope scaling     = linear
0.00.050.713 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.713 I llm_load_print_meta: freq_scale_train = 1
0.00.050.713 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.713 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.713 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.714 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.714 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.714 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.714 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.714 I llm_load_print_meta: model type       = 1.4B
0.00.050.714 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.715 I llm_load_print_meta: model params     = 1.41 B
0.00.050.715 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.715 I llm_load_print_meta: general.name     = 1.4B
0.00.050.716 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.716 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.716 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.716 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.717 I llm_load_print_meta: LF token         = 128 ''
0.00.050.717 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.717 I llm_load_print_meta: max token length = 1024
0.00.052.653 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.653 I llm_load_tensors: offloading output layer to GPU
0.00.052.653 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.664 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.665 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.562 I llama_new_context_with_model: n_ctx         = 128
0.00.053.562 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.562 I llama_new_context_with_model: n_batch       = 128
0.00.053.562 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.562 I llama_new_context_with_model: flash_attn    = 0
0.00.053.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.563 I llama_new_context_with_model: freq_scale    = 1
0.00.053.563 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.564 I ggml_metal_init: allocating
0.00.053.567 I ggml_metal_init: found device: Apple M4
0.00.053.569 I ggml_metal_init: picking default device: Apple M4
0.00.054.134 I ggml_metal_init: using embedded metal library
0.00.056.448 I ggml_metal_init: GPU name:   Apple M4
0.00.056.449 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.450 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.450 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.450 I ggml_metal_init: simdgroup reduction   = true
0.00.056.450 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.451 I ggml_metal_init: has bfloat            = true
0.00.056.451 I ggml_metal_init: use bfloat            = true
0.00.056.451 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.452 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.096 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.318 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.321 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.334 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.260 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.261 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.261 I llama_new_context_with_model: graph nodes  = 967
0.00.068.261 I llama_new_context_with_model: graph splits = 2
0.00.068.274 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.214 I 
0.00.616.258 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.270 I perplexity: tokenizing the input ..
0.00.624.103 I perplexity: tokenization took 7.831 ms
0.00.624.106 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.746.505 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.747.661 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.747.678 I llama_perf_context_print:        load time =     605.80 ms
0.00.747.679 I llama_perf_context_print: prompt eval time =     122.17 ms /   128 tokens (    0.95 ms per token,  1047.72 tokens per second)
0.00.747.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.747.682 I llama_perf_context_print:       total time =     131.47 ms /   129 tokens
0.00.748.185 I ggml_metal_free: deallocating

real	0m0.763s
user	0m0.078s
sys	0m0.102s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.252 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.644 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.649 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.656 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.659 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.659 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.660 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.660 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.665 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.666 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.666 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.614 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.450 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.452 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.452 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.452 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.453 I llama_model_loader: - type  f32:  194 tensors
0.00.024.454 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.454 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.686 I llm_load_vocab: special tokens cache size = 25
0.00.050.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.603 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.604 I llm_load_print_meta: arch             = gptneox
0.00.050.604 I llm_load_print_meta: vocab type       = BPE
0.00.050.604 I llm_load_print_meta: n_vocab          = 50304
0.00.050.604 I llm_load_print_meta: n_merges         = 50009
0.00.050.605 I llm_load_print_meta: vocab_only       = 0
0.00.050.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.605 I llm_load_print_meta: n_embd           = 2048
0.00.050.605 I llm_load_print_meta: n_layer          = 24
0.00.050.608 I llm_load_print_meta: n_head           = 16
0.00.050.609 I llm_load_print_meta: n_head_kv        = 16
0.00.050.609 I llm_load_print_meta: n_rot            = 32
0.00.050.609 I llm_load_print_meta: n_swa            = 0
0.00.050.609 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.609 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.610 I llm_load_print_meta: n_gqa            = 1
0.00.050.611 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.612 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.612 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.613 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.613 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.613 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.613 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.614 I llm_load_print_meta: n_ff             = 8192
0.00.050.614 I llm_load_print_meta: n_expert         = 0
0.00.050.614 I llm_load_print_meta: n_expert_used    = 0
0.00.050.614 I llm_load_print_meta: causal attn      = 1
0.00.050.615 I llm_load_print_meta: pooling type     = 0
0.00.050.615 I llm_load_print_meta: rope type        = 2
0.00.050.615 I llm_load_print_meta: rope scaling     = linear
0.00.050.616 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.616 I llm_load_print_meta: freq_scale_train = 1
0.00.050.618 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.619 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.619 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.619 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.619 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.619 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.619 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.619 I llm_load_print_meta: model type       = 1.4B
0.00.050.620 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.620 I llm_load_print_meta: model params     = 1.41 B
0.00.050.621 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.621 I llm_load_print_meta: general.name     = 1.4B
0.00.050.621 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.621 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.621 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.622 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.622 I llm_load_print_meta: LF token         = 128 ''
0.00.050.622 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.626 I llm_load_print_meta: max token length = 1024
0.00.052.663 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.664 I llm_load_tensors: offloading output layer to GPU
0.00.052.664 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.674 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.675 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.652 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.653 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.653 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.653 I llama_new_context_with_model: n_batch       = 2048
0.00.053.653 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.653 I llama_new_context_with_model: flash_attn    = 0
0.00.053.654 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.654 I llama_new_context_with_model: freq_scale    = 1
0.00.053.655 I ggml_metal_init: allocating
0.00.053.661 I ggml_metal_init: found device: Apple M4
0.00.053.663 I ggml_metal_init: picking default device: Apple M4
0.00.054.242 I ggml_metal_init: using embedded metal library
0.00.056.559 I ggml_metal_init: GPU name:   Apple M4
0.00.056.560 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.560 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.561 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.561 I ggml_metal_init: simdgroup reduction   = true
0.00.056.561 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.561 I ggml_metal_init: has bfloat            = true
0.00.056.561 I ggml_metal_init: use bfloat            = true
0.00.056.562 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.187 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.354 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.359 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.379 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.353 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.355 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.355 I llama_new_context_with_model: graph nodes  = 967
0.00.087.355 I llama_new_context_with_model: graph splits = 2
0.00.087.370 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.497 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.498 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.737.627 I main: llama threadpool init, n_threads = 4
0.00.737.667 I 
0.00.737.710 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.737.710 I 
0.00.737.936 I sampler seed: 1234
0.00.737.944 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.737.955 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.737.956 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.737.957 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.467.914 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65018.32 tokens per second)
0.01.467.915 I llama_perf_context_print:        load time =     728.37 ms
0.01.467.916 I llama_perf_context_print: prompt eval time =      43.49 ms /     7 tokens (    6.21 ms per token,   160.96 tokens per second)
0.01.467.916 I llama_perf_context_print:        eval time =     683.63 ms /    63 runs   (   10.85 ms per token,    92.15 tokens per second)
0.01.467.917 I llama_perf_context_print:       total time =     730.29 ms /    70 tokens
0.01.468.081 I ggml_metal_free: deallocating

real	0m1.485s
user	0m0.109s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.695 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.255 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.259 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.261 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.262 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.262 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.264 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.265 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.265 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.266 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.268 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.268 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.269 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.197 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.121 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.122 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.122 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.123 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.123 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.123 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.124 I llama_model_loader: - type  f32:  194 tensors
0.00.023.124 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.124 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.306 I llm_load_vocab: special tokens cache size = 25
0.00.049.199 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.202 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.202 I llm_load_print_meta: arch             = gptneox
0.00.049.203 I llm_load_print_meta: vocab type       = BPE
0.00.049.203 I llm_load_print_meta: n_vocab          = 50304
0.00.049.203 I llm_load_print_meta: n_merges         = 50009
0.00.049.203 I llm_load_print_meta: vocab_only       = 0
0.00.049.203 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.204 I llm_load_print_meta: n_embd           = 2048
0.00.049.204 I llm_load_print_meta: n_layer          = 24
0.00.049.206 I llm_load_print_meta: n_head           = 16
0.00.049.209 I llm_load_print_meta: n_head_kv        = 16
0.00.049.209 I llm_load_print_meta: n_rot            = 32
0.00.049.209 I llm_load_print_meta: n_swa            = 0
0.00.049.210 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.210 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.210 I llm_load_print_meta: n_gqa            = 1
0.00.049.211 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.212 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.213 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.213 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.213 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.213 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.214 I llm_load_print_meta: n_ff             = 8192
0.00.049.214 I llm_load_print_meta: n_expert         = 0
0.00.049.214 I llm_load_print_meta: n_expert_used    = 0
0.00.049.215 I llm_load_print_meta: causal attn      = 1
0.00.049.215 I llm_load_print_meta: pooling type     = 0
0.00.049.215 I llm_load_print_meta: rope type        = 2
0.00.049.215 I llm_load_print_meta: rope scaling     = linear
0.00.049.215 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.216 I llm_load_print_meta: freq_scale_train = 1
0.00.049.216 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.216 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.216 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.216 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.217 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.217 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.217 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.217 I llm_load_print_meta: model type       = 1.4B
0.00.049.218 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.219 I llm_load_print_meta: model params     = 1.41 B
0.00.049.220 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.221 I llm_load_print_meta: general.name     = 1.4B
0.00.049.221 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.221 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.222 I llm_load_print_meta: LF token         = 128 ''
0.00.049.222 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.222 I llm_load_print_meta: max token length = 1024
0.00.051.176 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.176 I llm_load_tensors: offloading output layer to GPU
0.00.051.177 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.187 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.188 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.095 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.096 I llama_new_context_with_model: n_ctx         = 128
0.00.052.096 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.096 I llama_new_context_with_model: n_batch       = 128
0.00.052.096 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.097 I llama_new_context_with_model: flash_attn    = 0
0.00.052.097 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.097 I llama_new_context_with_model: freq_scale    = 1
0.00.052.098 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.098 I ggml_metal_init: allocating
0.00.052.104 I ggml_metal_init: found device: Apple M4
0.00.052.106 I ggml_metal_init: picking default device: Apple M4
0.00.052.685 I ggml_metal_init: using embedded metal library
0.00.055.037 I ggml_metal_init: GPU name:   Apple M4
0.00.055.039 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.039 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.039 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.040 I ggml_metal_init: simdgroup reduction   = true
0.00.055.040 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.040 I ggml_metal_init: has bfloat            = true
0.00.055.040 I ggml_metal_init: use bfloat            = true
0.00.055.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.041 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.534 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.780 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.782 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.801 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.634 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.635 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.636 I llama_new_context_with_model: graph nodes  = 967
0.00.066.636 I llama_new_context_with_model: graph splits = 2
0.00.066.648 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.747 I 
0.00.667.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.810 I perplexity: tokenizing the input ..
0.00.675.531 I perplexity: tokenization took 7.72 ms
0.00.675.535 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.775 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.798.984 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.798.999 I llama_perf_context_print:        load time =     659.05 ms
0.00.799.000 I llama_perf_context_print: prompt eval time =     122.02 ms /   128 tokens (    0.95 ms per token,  1049.04 tokens per second)
0.00.799.001 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.001 I llama_perf_context_print:       total time =     131.25 ms /   129 tokens
0.00.799.369 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.077s
sys	0m0.101s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.444 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.937 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.937 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.940 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.940 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.941 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.941 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.942 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.942 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.942 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.942 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.943 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.944 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.944 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.945 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.936 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.859 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.860 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.860 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.861 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.861 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.861 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.862 I llama_model_loader: - type  f32:  194 tensors
0.00.026.862 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.862 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.821 I llm_load_vocab: special tokens cache size = 25
0.00.053.723 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.726 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.727 I llm_load_print_meta: arch             = gptneox
0.00.053.727 I llm_load_print_meta: vocab type       = BPE
0.00.053.727 I llm_load_print_meta: n_vocab          = 50304
0.00.053.728 I llm_load_print_meta: n_merges         = 50009
0.00.053.728 I llm_load_print_meta: vocab_only       = 0
0.00.053.728 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.728 I llm_load_print_meta: n_embd           = 2048
0.00.053.728 I llm_load_print_meta: n_layer          = 24
0.00.053.731 I llm_load_print_meta: n_head           = 16
0.00.053.732 I llm_load_print_meta: n_head_kv        = 16
0.00.053.732 I llm_load_print_meta: n_rot            = 32
0.00.053.732 I llm_load_print_meta: n_swa            = 0
0.00.053.734 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.735 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.735 I llm_load_print_meta: n_gqa            = 1
0.00.053.736 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.737 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.738 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.738 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.738 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.738 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.738 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.739 I llm_load_print_meta: n_ff             = 8192
0.00.053.739 I llm_load_print_meta: n_expert         = 0
0.00.053.739 I llm_load_print_meta: n_expert_used    = 0
0.00.053.740 I llm_load_print_meta: causal attn      = 1
0.00.053.740 I llm_load_print_meta: pooling type     = 0
0.00.053.740 I llm_load_print_meta: rope type        = 2
0.00.053.740 I llm_load_print_meta: rope scaling     = linear
0.00.053.740 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.741 I llm_load_print_meta: freq_scale_train = 1
0.00.053.741 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.741 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.741 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.743 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.743 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.743 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.743 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.744 I llm_load_print_meta: model type       = 1.4B
0.00.053.744 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.744 I llm_load_print_meta: model params     = 1.41 B
0.00.053.745 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.745 I llm_load_print_meta: general.name     = 1.4B
0.00.053.745 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.746 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.746 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.746 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.747 I llm_load_print_meta: LF token         = 128 ''
0.00.053.750 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.751 I llm_load_print_meta: max token length = 1024
0.00.055.687 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.687 I llm_load_tensors: offloading output layer to GPU
0.00.055.687 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.697 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.699 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.612 I llama_new_context_with_model: n_batch       = 2048
0.00.056.612 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.612 I llama_new_context_with_model: flash_attn    = 0
0.00.056.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.613 I llama_new_context_with_model: freq_scale    = 1
0.00.056.614 I ggml_metal_init: allocating
0.00.056.621 I ggml_metal_init: found device: Apple M4
0.00.056.623 I ggml_metal_init: picking default device: Apple M4
0.00.057.246 I ggml_metal_init: using embedded metal library
0.00.059.608 I ggml_metal_init: GPU name:   Apple M4
0.00.059.609 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.610 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.610 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.610 I ggml_metal_init: simdgroup reduction   = true
0.00.059.610 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.611 I ggml_metal_init: has bfloat            = true
0.00.059.611 I ggml_metal_init: use bfloat            = true
0.00.059.611 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.612 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.381 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.805 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.810 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.828 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.845 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.847 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.847 I llama_new_context_with_model: graph nodes  = 967
0.00.089.847 I llama_new_context_with_model: graph splits = 2
0.00.089.863 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.005 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.006 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.286 I main: llama threadpool init, n_threads = 4
0.00.750.329 I 
0.00.750.362 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.363 I 
0.00.750.594 I sampler seed: 1234
0.00.750.599 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.638 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.639 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.639 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.541.465 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.541.465 I llama_perf_context_print:        load time =     738.84 ms
0.01.541.466 I llama_perf_context_print: prompt eval time =      47.14 ms /     7 tokens (    6.73 ms per token,   148.49 tokens per second)
0.01.541.470 I llama_perf_context_print:        eval time =     740.79 ms /    63 runs   (   11.76 ms per token,    85.04 tokens per second)
0.01.541.471 I llama_perf_context_print:       total time =     791.18 ms /    70 tokens
0.01.541.691 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.110s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.559 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.348 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.361 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.363 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.363 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.367 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.367 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.369 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.226 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.257 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.080 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.081 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.081 I llama_model_loader: - type  f32:  194 tensors
0.00.024.082 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.082 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.834 I llm_load_vocab: special tokens cache size = 25
0.00.050.731 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.734 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.734 I llm_load_print_meta: arch             = gptneox
0.00.050.734 I llm_load_print_meta: vocab type       = BPE
0.00.050.735 I llm_load_print_meta: n_vocab          = 50304
0.00.050.735 I llm_load_print_meta: n_merges         = 50009
0.00.050.735 I llm_load_print_meta: vocab_only       = 0
0.00.050.735 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.735 I llm_load_print_meta: n_embd           = 2048
0.00.050.736 I llm_load_print_meta: n_layer          = 24
0.00.050.738 I llm_load_print_meta: n_head           = 16
0.00.050.741 I llm_load_print_meta: n_head_kv        = 16
0.00.050.742 I llm_load_print_meta: n_rot            = 32
0.00.050.742 I llm_load_print_meta: n_swa            = 0
0.00.050.742 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.742 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.743 I llm_load_print_meta: n_gqa            = 1
0.00.050.744 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.744 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.745 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.745 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.746 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.746 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.746 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.746 I llm_load_print_meta: n_ff             = 8192
0.00.050.747 I llm_load_print_meta: n_expert         = 0
0.00.050.747 I llm_load_print_meta: n_expert_used    = 0
0.00.050.748 I llm_load_print_meta: causal attn      = 1
0.00.050.751 I llm_load_print_meta: pooling type     = 0
0.00.050.752 I llm_load_print_meta: rope type        = 2
0.00.050.752 I llm_load_print_meta: rope scaling     = linear
0.00.050.752 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.753 I llm_load_print_meta: freq_scale_train = 1
0.00.050.753 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.754 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.754 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.754 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.754 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.754 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.755 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.755 I llm_load_print_meta: model type       = 1.4B
0.00.050.755 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.756 I llm_load_print_meta: model params     = 1.41 B
0.00.050.756 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.757 I llm_load_print_meta: general.name     = 1.4B
0.00.050.757 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.757 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.765 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.767 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.767 I llm_load_print_meta: LF token         = 128 ''
0.00.050.768 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.768 I llm_load_print_meta: max token length = 1024
0.00.052.550 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.550 I llm_load_tensors: offloading output layer to GPU
0.00.052.551 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.556 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.556 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.460 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.461 I llama_new_context_with_model: n_ctx         = 128
0.00.053.461 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.461 I llama_new_context_with_model: n_batch       = 128
0.00.053.462 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.462 I llama_new_context_with_model: flash_attn    = 0
0.00.053.462 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.463 I llama_new_context_with_model: freq_scale    = 1
0.00.053.463 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.463 I ggml_metal_init: allocating
0.00.053.467 I ggml_metal_init: found device: Apple M4
0.00.053.469 I ggml_metal_init: picking default device: Apple M4
0.00.054.009 I ggml_metal_init: using embedded metal library
0.00.056.302 I ggml_metal_init: GPU name:   Apple M4
0.00.056.303 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.304 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.304 I ggml_metal_init: simdgroup reduction   = true
0.00.056.304 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.304 I ggml_metal_init: has bfloat            = true
0.00.056.304 I ggml_metal_init: use bfloat            = true
0.00.056.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.305 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.935 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.532 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.535 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.554 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.433 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.434 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.434 I llama_new_context_with_model: graph nodes  = 967
0.00.068.436 I llama_new_context_with_model: graph splits = 2
0.00.068.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.447 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.419 I 
0.00.735.498 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.522 I perplexity: tokenizing the input ..
0.00.743.350 I perplexity: tokenization took 7.826 ms
0.00.743.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.878.053 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.879.240 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.879.260 I llama_perf_context_print:        load time =     725.85 ms
0.00.879.261 I llama_perf_context_print: prompt eval time =     134.47 ms /   128 tokens (    1.05 ms per token,   951.86 tokens per second)
0.00.879.262 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.262 I llama_perf_context_print:       total time =     143.85 ms /   129 tokens
0.00.879.766 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.079s
sys	0m0.109s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.989 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.189 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.193 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.194 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.194 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.195 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.195 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.195 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.196 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.197 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.197 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.197 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.198 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.198 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.929 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.974 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.009 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.010 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.011 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.011 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.011 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.011 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.036.012 I llama_model_loader: - type  f32:  194 tensors
0.00.036.012 I llama_model_loader: - type q5_1:   97 tensors
0.00.036.012 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.813 I llm_load_vocab: special tokens cache size = 25
0.00.064.884 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.887 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.887 I llm_load_print_meta: arch             = gptneox
0.00.064.888 I llm_load_print_meta: vocab type       = BPE
0.00.064.888 I llm_load_print_meta: n_vocab          = 50304
0.00.064.888 I llm_load_print_meta: n_merges         = 50009
0.00.064.888 I llm_load_print_meta: vocab_only       = 0
0.00.064.888 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.889 I llm_load_print_meta: n_embd           = 2048
0.00.064.889 I llm_load_print_meta: n_layer          = 24
0.00.064.892 I llm_load_print_meta: n_head           = 16
0.00.064.893 I llm_load_print_meta: n_head_kv        = 16
0.00.064.893 I llm_load_print_meta: n_rot            = 32
0.00.064.893 I llm_load_print_meta: n_swa            = 0
0.00.064.893 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.895 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.895 I llm_load_print_meta: n_gqa            = 1
0.00.064.896 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.897 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.897 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.899 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.899 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.899 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.899 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.900 I llm_load_print_meta: n_ff             = 8192
0.00.064.901 I llm_load_print_meta: n_expert         = 0
0.00.064.901 I llm_load_print_meta: n_expert_used    = 0
0.00.064.902 I llm_load_print_meta: causal attn      = 1
0.00.064.902 I llm_load_print_meta: pooling type     = 0
0.00.064.902 I llm_load_print_meta: rope type        = 2
0.00.064.903 I llm_load_print_meta: rope scaling     = linear
0.00.064.903 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.904 I llm_load_print_meta: freq_scale_train = 1
0.00.064.907 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.908 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.908 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.908 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.909 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.910 I llm_load_print_meta: model type       = 1.4B
0.00.064.910 I llm_load_print_meta: model ftype      = Q5_1
0.00.064.910 I llm_load_print_meta: model params     = 1.41 B
0.00.064.911 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.064.911 I llm_load_print_meta: general.name     = 1.4B
0.00.064.911 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.911 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.911 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.912 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.912 I llm_load_print_meta: LF token         = 128 ''
0.00.064.913 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.913 I llm_load_print_meta: max token length = 1024
0.00.066.762 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.763 I llm_load_tensors: offloading output layer to GPU
0.00.066.763 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.768 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.066.769 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.067.828 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.829 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.829 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.829 I llama_new_context_with_model: n_batch       = 2048
0.00.067.830 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.830 I llama_new_context_with_model: flash_attn    = 0
0.00.067.830 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.830 I llama_new_context_with_model: freq_scale    = 1
0.00.067.831 I ggml_metal_init: allocating
0.00.067.834 I ggml_metal_init: found device: Apple M4
0.00.067.836 I ggml_metal_init: picking default device: Apple M4
0.00.068.446 I ggml_metal_init: using embedded metal library
0.00.070.992 I ggml_metal_init: GPU name:   Apple M4
0.00.070.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.995 I ggml_metal_init: simdgroup reduction   = true
0.00.070.997 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.997 I ggml_metal_init: has bfloat            = true
0.00.070.997 I ggml_metal_init: use bfloat            = true
0.00.070.997 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.998 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.500 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.103.116 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.124 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.104.207 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.104.208 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.104.208 I llama_new_context_with_model: graph nodes  = 967
0.00.104.209 I llama_new_context_with_model: graph splits = 2
0.00.104.223 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.029 I main: llama threadpool init, n_threads = 4
0.00.811.072 I 
0.00.811.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.107 I 
0.00.811.324 I sampler seed: 1234
0.00.811.328 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.375 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.376 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.377 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.661.887 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.661.888 I llama_perf_context_print:        load time =     802.03 ms
0.01.661.888 I llama_perf_context_print: prompt eval time =      48.64 ms /     7 tokens (    6.95 ms per token,   143.92 tokens per second)
0.01.661.890 I llama_perf_context_print:        eval time =     799.05 ms /    63 runs   (   12.68 ms per token,    78.84 tokens per second)
0.01.661.891 I llama_perf_context_print:       total time =     850.86 ms /    70 tokens
0.01.662.088 I ggml_metal_free: deallocating

real	0m1.680s
user	0m0.113s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.849 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.528 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.536 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.401 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.310 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.311 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.311 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.312 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.312 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.312 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.313 I llama_model_loader: - type  f32:  194 tensors
0.00.023.313 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.313 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.343 I llm_load_vocab: special tokens cache size = 25
0.00.050.433 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.436 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.436 I llm_load_print_meta: arch             = gptneox
0.00.050.436 I llm_load_print_meta: vocab type       = BPE
0.00.050.436 I llm_load_print_meta: n_vocab          = 50304
0.00.050.437 I llm_load_print_meta: n_merges         = 50009
0.00.050.437 I llm_load_print_meta: vocab_only       = 0
0.00.050.437 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.437 I llm_load_print_meta: n_embd           = 2048
0.00.050.437 I llm_load_print_meta: n_layer          = 24
0.00.050.440 I llm_load_print_meta: n_head           = 16
0.00.050.441 I llm_load_print_meta: n_head_kv        = 16
0.00.050.441 I llm_load_print_meta: n_rot            = 32
0.00.050.441 I llm_load_print_meta: n_swa            = 0
0.00.050.442 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.442 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.442 I llm_load_print_meta: n_gqa            = 1
0.00.050.444 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.445 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.446 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.446 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.446 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.449 I llm_load_print_meta: n_ff             = 8192
0.00.050.449 I llm_load_print_meta: n_expert         = 0
0.00.050.449 I llm_load_print_meta: n_expert_used    = 0
0.00.050.449 I llm_load_print_meta: causal attn      = 1
0.00.050.450 I llm_load_print_meta: pooling type     = 0
0.00.050.450 I llm_load_print_meta: rope type        = 2
0.00.050.450 I llm_load_print_meta: rope scaling     = linear
0.00.050.450 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.451 I llm_load_print_meta: freq_scale_train = 1
0.00.050.451 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.451 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.451 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.452 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.452 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.452 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.452 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.452 I llm_load_print_meta: model type       = 1.4B
0.00.050.453 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.458 I llm_load_print_meta: model params     = 1.41 B
0.00.050.458 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.458 I llm_load_print_meta: general.name     = 1.4B
0.00.050.459 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.459 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.459 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.459 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.459 I llm_load_print_meta: LF token         = 128 ''
0.00.050.460 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.460 I llm_load_print_meta: max token length = 1024
0.00.052.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.467 I llm_load_tensors: offloading output layer to GPU
0.00.052.467 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.478 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.479 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.371 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.371 I llama_new_context_with_model: n_ctx         = 128
0.00.053.372 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.372 I llama_new_context_with_model: n_batch       = 128
0.00.053.372 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.372 I llama_new_context_with_model: flash_attn    = 0
0.00.053.373 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.373 I llama_new_context_with_model: freq_scale    = 1
0.00.053.373 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.374 I ggml_metal_init: allocating
0.00.053.380 I ggml_metal_init: found device: Apple M4
0.00.053.382 I ggml_metal_init: picking default device: Apple M4
0.00.053.980 I ggml_metal_init: using embedded metal library
0.00.056.325 I ggml_metal_init: GPU name:   Apple M4
0.00.056.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.327 I ggml_metal_init: simdgroup reduction   = true
0.00.056.327 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.327 I ggml_metal_init: has bfloat            = true
0.00.056.328 I ggml_metal_init: use bfloat            = true
0.00.056.328 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.329 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.000 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.277 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.279 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.293 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.195 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.196 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.196 I llama_new_context_with_model: graph nodes  = 967
0.00.068.196 I llama_new_context_with_model: graph splits = 2
0.00.068.209 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.209 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.634.705 I 
0.00.634.770 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.634.791 I perplexity: tokenizing the input ..
0.00.642.823 I perplexity: tokenization took 8.031 ms
0.00.642.826 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.898 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.779.054 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.779.067 I llama_perf_context_print:        load time =     625.85 ms
0.00.779.068 I llama_perf_context_print: prompt eval time =     134.82 ms /   128 tokens (    1.05 ms per token,   949.44 tokens per second)
0.00.779.068 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.069 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.779.418 I ggml_metal_free: deallocating

real	0m0.793s
user	0m0.079s
sys	0m0.123s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.017.010 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.722 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.024.728 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.730 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.731 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.731 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.732 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.732 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.734 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.734 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.735 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.735 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.735 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.737 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.738 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.738 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.621 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.608 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.033.609 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.609 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.610 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.610 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.610 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.033.611 I llama_model_loader: - type  f32:  194 tensors
0.00.033.611 I llama_model_loader: - type q2_K:   49 tensors
0.00.033.612 I llama_model_loader: - type q3_K:   48 tensors
0.00.033.612 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.734 I llm_load_vocab: special tokens cache size = 25
0.00.061.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.715 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.715 I llm_load_print_meta: arch             = gptneox
0.00.061.715 I llm_load_print_meta: vocab type       = BPE
0.00.061.716 I llm_load_print_meta: n_vocab          = 50304
0.00.061.716 I llm_load_print_meta: n_merges         = 50009
0.00.061.716 I llm_load_print_meta: vocab_only       = 0
0.00.061.716 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.718 I llm_load_print_meta: n_embd           = 2048
0.00.061.718 I llm_load_print_meta: n_layer          = 24
0.00.061.722 I llm_load_print_meta: n_head           = 16
0.00.061.724 I llm_load_print_meta: n_head_kv        = 16
0.00.061.724 I llm_load_print_meta: n_rot            = 32
0.00.061.726 I llm_load_print_meta: n_swa            = 0
0.00.061.726 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.727 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.727 I llm_load_print_meta: n_gqa            = 1
0.00.061.728 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.729 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.729 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.730 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.730 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.735 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.735 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.736 I llm_load_print_meta: n_ff             = 8192
0.00.061.736 I llm_load_print_meta: n_expert         = 0
0.00.061.736 I llm_load_print_meta: n_expert_used    = 0
0.00.061.737 I llm_load_print_meta: causal attn      = 1
0.00.061.737 I llm_load_print_meta: pooling type     = 0
0.00.061.737 I llm_load_print_meta: rope type        = 2
0.00.061.737 I llm_load_print_meta: rope scaling     = linear
0.00.061.738 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.738 I llm_load_print_meta: freq_scale_train = 1
0.00.061.738 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.738 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.739 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.739 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.739 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.739 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.739 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.739 I llm_load_print_meta: model type       = 1.4B
0.00.061.740 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.061.740 I llm_load_print_meta: model params     = 1.41 B
0.00.061.741 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.061.741 I llm_load_print_meta: general.name     = 1.4B
0.00.061.741 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.741 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.741 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.741 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.742 I llm_load_print_meta: LF token         = 128 ''
0.00.061.742 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.742 I llm_load_print_meta: max token length = 1024
0.00.063.614 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.614 I llm_load_tensors: offloading output layer to GPU
0.00.063.615 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.625 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.063.627 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.064.517 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.517 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.518 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.518 I llama_new_context_with_model: n_batch       = 2048
0.00.064.518 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.518 I llama_new_context_with_model: flash_attn    = 0
0.00.064.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.519 I llama_new_context_with_model: freq_scale    = 1
0.00.064.519 I ggml_metal_init: allocating
0.00.064.523 I ggml_metal_init: found device: Apple M4
0.00.064.526 I ggml_metal_init: picking default device: Apple M4
0.00.065.134 I ggml_metal_init: using embedded metal library
0.00.067.454 I ggml_metal_init: GPU name:   Apple M4
0.00.067.456 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.457 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.458 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.458 I ggml_metal_init: simdgroup reduction   = true
0.00.067.458 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.459 I ggml_metal_init: has bfloat            = true
0.00.067.459 I ggml_metal_init: use bfloat            = true
0.00.067.463 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.464 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.634 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.098.056 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.062 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.083 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.099.024 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.099.027 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.099.027 I llama_new_context_with_model: graph nodes  = 967
0.00.099.027 I llama_new_context_with_model: graph splits = 2
0.00.099.044 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.166 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.701 I main: llama threadpool init, n_threads = 4
0.00.564.788 I 
0.00.564.874 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.877 I 
0.00.565.388 I sampler seed: 1234
0.00.565.394 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.565.464 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.565.466 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.565.466 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.244.274 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 62008.73 tokens per second)
0.01.244.275 I llama_perf_context_print:        load time =     547.68 ms
0.01.244.276 I llama_perf_context_print: prompt eval time =      36.63 ms /     7 tokens (    5.23 ms per token,   191.10 tokens per second)
0.01.244.276 I llama_perf_context_print:        eval time =     639.26 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.244.277 I llama_perf_context_print:       total time =     679.58 ms /    70 tokens
0.01.244.466 I ggml_metal_free: deallocating

real	0m1.278s
user	0m0.121s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.117 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.664 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.668 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.670 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.670 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.671 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.673 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.673 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.673 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.674 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.676 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.676 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.678 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.447 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.507 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.262 I llama_model_loader: - type  f32:  194 tensors
0.00.024.262 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.263 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.425 I llm_load_vocab: special tokens cache size = 25
0.00.050.239 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.242 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.242 I llm_load_print_meta: arch             = gptneox
0.00.050.242 I llm_load_print_meta: vocab type       = BPE
0.00.050.243 I llm_load_print_meta: n_vocab          = 50304
0.00.050.243 I llm_load_print_meta: n_merges         = 50009
0.00.050.243 I llm_load_print_meta: vocab_only       = 0
0.00.050.243 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.243 I llm_load_print_meta: n_embd           = 2048
0.00.050.243 I llm_load_print_meta: n_layer          = 24
0.00.050.246 I llm_load_print_meta: n_head           = 16
0.00.050.247 I llm_load_print_meta: n_head_kv        = 16
0.00.050.247 I llm_load_print_meta: n_rot            = 32
0.00.050.247 I llm_load_print_meta: n_swa            = 0
0.00.050.250 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.250 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.251 I llm_load_print_meta: n_gqa            = 1
0.00.050.252 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.252 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.253 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.253 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.253 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.254 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.254 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.254 I llm_load_print_meta: n_ff             = 8192
0.00.050.256 I llm_load_print_meta: n_expert         = 0
0.00.050.256 I llm_load_print_meta: n_expert_used    = 0
0.00.050.256 I llm_load_print_meta: causal attn      = 1
0.00.050.256 I llm_load_print_meta: pooling type     = 0
0.00.050.256 I llm_load_print_meta: rope type        = 2
0.00.050.256 I llm_load_print_meta: rope scaling     = linear
0.00.050.257 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.257 I llm_load_print_meta: freq_scale_train = 1
0.00.050.257 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.258 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.258 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.259 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.259 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.260 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.260 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.260 I llm_load_print_meta: model type       = 1.4B
0.00.050.260 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.261 I llm_load_print_meta: model params     = 1.41 B
0.00.050.263 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.263 I llm_load_print_meta: general.name     = 1.4B
0.00.050.263 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.264 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.264 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.264 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.264 I llm_load_print_meta: LF token         = 128 ''
0.00.050.264 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.265 I llm_load_print_meta: max token length = 1024
0.00.052.080 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.080 I llm_load_tensors: offloading output layer to GPU
0.00.052.080 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.091 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.092 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.988 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.989 I llama_new_context_with_model: n_ctx         = 128
0.00.052.989 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.989 I llama_new_context_with_model: n_batch       = 128
0.00.052.989 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.989 I llama_new_context_with_model: flash_attn    = 0
0.00.052.990 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.990 I llama_new_context_with_model: freq_scale    = 1
0.00.052.990 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.991 I ggml_metal_init: allocating
0.00.052.997 I ggml_metal_init: found device: Apple M4
0.00.052.999 I ggml_metal_init: picking default device: Apple M4
0.00.053.560 I ggml_metal_init: using embedded metal library
0.00.055.882 I ggml_metal_init: GPU name:   Apple M4
0.00.055.884 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.884 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.885 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.885 I ggml_metal_init: simdgroup reduction   = true
0.00.055.885 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.885 I ggml_metal_init: has bfloat            = true
0.00.055.885 I ggml_metal_init: use bfloat            = true
0.00.055.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.886 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.267 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.560 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.562 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.479 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.480 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.480 I llama_new_context_with_model: graph nodes  = 967
0.00.067.480 I llama_new_context_with_model: graph splits = 2
0.00.067.492 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.380.496 I 
0.00.380.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.380.569 I perplexity: tokenizing the input ..
0.00.388.689 I perplexity: tokenization took 8.119 ms
0.00.388.697 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.521.182 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.522.378 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.522.402 I llama_perf_context_print:        load time =     370.37 ms
0.00.522.403 I llama_perf_context_print: prompt eval time =     132.26 ms /   128 tokens (    1.03 ms per token,   967.80 tokens per second)
0.00.522.404 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.522.404 I llama_perf_context_print:       total time =     141.91 ms /   129 tokens
0.00.522.876 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.078s
sys	0m0.068s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.551 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.934 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.013.939 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.940 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.942 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.942 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.943 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.943 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.944 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.944 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.945 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.945 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.946 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.946 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.725 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.461 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.462 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.463 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.463 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.463 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.464 I llama_model_loader: - type  f32:  194 tensors
0.00.022.464 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.464 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.465 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.465 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.621 I llm_load_vocab: special tokens cache size = 25
0.00.048.804 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.806 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.807 I llm_load_print_meta: arch             = gptneox
0.00.048.807 I llm_load_print_meta: vocab type       = BPE
0.00.048.808 I llm_load_print_meta: n_vocab          = 50304
0.00.048.808 I llm_load_print_meta: n_merges         = 50009
0.00.048.808 I llm_load_print_meta: vocab_only       = 0
0.00.048.808 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.808 I llm_load_print_meta: n_embd           = 2048
0.00.048.809 I llm_load_print_meta: n_layer          = 24
0.00.048.811 I llm_load_print_meta: n_head           = 16
0.00.048.812 I llm_load_print_meta: n_head_kv        = 16
0.00.048.812 I llm_load_print_meta: n_rot            = 32
0.00.048.812 I llm_load_print_meta: n_swa            = 0
0.00.048.812 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.813 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.813 I llm_load_print_meta: n_gqa            = 1
0.00.048.814 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.815 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.816 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.816 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.816 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.816 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.816 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.817 I llm_load_print_meta: n_ff             = 8192
0.00.048.817 I llm_load_print_meta: n_expert         = 0
0.00.048.817 I llm_load_print_meta: n_expert_used    = 0
0.00.048.818 I llm_load_print_meta: causal attn      = 1
0.00.048.818 I llm_load_print_meta: pooling type     = 0
0.00.048.818 I llm_load_print_meta: rope type        = 2
0.00.048.818 I llm_load_print_meta: rope scaling     = linear
0.00.048.819 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.821 I llm_load_print_meta: freq_scale_train = 1
0.00.048.821 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.821 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.822 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.822 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.822 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.822 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.822 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.822 I llm_load_print_meta: model type       = 1.4B
0.00.048.823 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.823 I llm_load_print_meta: model params     = 1.41 B
0.00.048.824 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.824 I llm_load_print_meta: general.name     = 1.4B
0.00.048.824 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.824 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.824 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.825 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.825 I llm_load_print_meta: LF token         = 128 ''
0.00.048.826 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.826 I llm_load_print_meta: max token length = 1024
0.00.050.368 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.369 I llm_load_tensors: offloading output layer to GPU
0.00.050.369 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.379 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.380 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.227 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.228 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.228 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.229 I llama_new_context_with_model: n_batch       = 2048
0.00.051.229 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.229 I llama_new_context_with_model: flash_attn    = 0
0.00.051.229 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.230 I llama_new_context_with_model: freq_scale    = 1
0.00.051.230 I ggml_metal_init: allocating
0.00.051.233 I ggml_metal_init: found device: Apple M4
0.00.051.235 I ggml_metal_init: picking default device: Apple M4
0.00.051.830 I ggml_metal_init: using embedded metal library
0.00.054.150 I ggml_metal_init: GPU name:   Apple M4
0.00.054.151 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.152 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.152 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.152 I ggml_metal_init: simdgroup reduction   = true
0.00.054.152 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.153 I ggml_metal_init: has bfloat            = true
0.00.054.153 I ggml_metal_init: use bfloat            = true
0.00.054.153 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.154 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.811 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.188 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.197 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.232 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.235 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.236 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.236 I llama_new_context_with_model: graph nodes  = 967
0.00.084.237 I llama_new_context_with_model: graph splits = 2
0.00.084.252 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.475 I main: llama threadpool init, n_threads = 4
0.00.535.519 I 
0.00.535.563 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.564 I 
0.00.535.809 I sampler seed: 1234
0.00.535.813 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.535.834 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.535.834 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.535.835 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.278.770 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.278.771 I llama_perf_context_print:        load time =     526.92 ms
0.01.278.772 I llama_perf_context_print: prompt eval time =      40.49 ms /     7 tokens (    5.78 ms per token,   172.90 tokens per second)
0.01.278.772 I llama_perf_context_print:        eval time =     699.46 ms /    63 runs   (   11.10 ms per token,    90.07 tokens per second)
0.01.278.777 I llama_perf_context_print:       total time =     743.30 ms /    70 tokens
0.01.278.948 I ggml_metal_free: deallocating

real	0m1.297s
user	0m0.109s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.479 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.876 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.013.881 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.882 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.013.883 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.883 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.013.884 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.013.884 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.013.885 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.013.886 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.013.888 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.013.888 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.013.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.013.889 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.013.889 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.013.892 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.013.892 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.013.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.690 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.446 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.447 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.447 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.448 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.448 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.448 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.449 I llama_model_loader: - type  f32:  194 tensors
0.00.022.449 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.449 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.449 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.449 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.662 I llm_load_vocab: special tokens cache size = 25
0.00.048.511 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.513 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.514 I llm_load_print_meta: arch             = gptneox
0.00.048.514 I llm_load_print_meta: vocab type       = BPE
0.00.048.514 I llm_load_print_meta: n_vocab          = 50304
0.00.048.514 I llm_load_print_meta: n_merges         = 50009
0.00.048.515 I llm_load_print_meta: vocab_only       = 0
0.00.048.515 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.515 I llm_load_print_meta: n_embd           = 2048
0.00.048.515 I llm_load_print_meta: n_layer          = 24
0.00.048.517 I llm_load_print_meta: n_head           = 16
0.00.048.518 I llm_load_print_meta: n_head_kv        = 16
0.00.048.519 I llm_load_print_meta: n_rot            = 32
0.00.048.519 I llm_load_print_meta: n_swa            = 0
0.00.048.519 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.521 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.522 I llm_load_print_meta: n_gqa            = 1
0.00.048.523 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.524 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.524 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.525 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.525 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.525 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.535 I llm_load_print_meta: n_ff             = 8192
0.00.048.536 I llm_load_print_meta: n_expert         = 0
0.00.048.536 I llm_load_print_meta: n_expert_used    = 0
0.00.048.536 I llm_load_print_meta: causal attn      = 1
0.00.048.536 I llm_load_print_meta: pooling type     = 0
0.00.048.538 I llm_load_print_meta: rope type        = 2
0.00.048.539 I llm_load_print_meta: rope scaling     = linear
0.00.048.539 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.540 I llm_load_print_meta: freq_scale_train = 1
0.00.048.540 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.540 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.540 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.540 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.540 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.541 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.541 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.541 I llm_load_print_meta: model type       = 1.4B
0.00.048.541 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.542 I llm_load_print_meta: model params     = 1.41 B
0.00.048.542 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.542 I llm_load_print_meta: general.name     = 1.4B
0.00.048.542 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.543 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.543 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.543 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.543 I llm_load_print_meta: LF token         = 128 ''
0.00.048.543 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.544 I llm_load_print_meta: max token length = 1024
0.00.050.467 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.468 I llm_load_tensors: offloading output layer to GPU
0.00.050.468 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.478 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.480 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.405 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.406 I llama_new_context_with_model: n_ctx         = 128
0.00.051.406 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.406 I llama_new_context_with_model: n_batch       = 128
0.00.051.406 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.406 I llama_new_context_with_model: flash_attn    = 0
0.00.051.407 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.407 I llama_new_context_with_model: freq_scale    = 1
0.00.051.407 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.408 I ggml_metal_init: allocating
0.00.051.413 I ggml_metal_init: found device: Apple M4
0.00.051.415 I ggml_metal_init: picking default device: Apple M4
0.00.051.975 I ggml_metal_init: using embedded metal library
0.00.054.285 I ggml_metal_init: GPU name:   Apple M4
0.00.054.287 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.287 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.287 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.289 I ggml_metal_init: simdgroup reduction   = true
0.00.054.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.289 I ggml_metal_init: has bfloat            = true
0.00.054.290 I ggml_metal_init: use bfloat            = true
0.00.054.290 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.798 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.087 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.089 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.102 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.039 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.040 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.040 I llama_new_context_with_model: graph nodes  = 967
0.00.066.040 I llama_new_context_with_model: graph splits = 2
0.00.066.053 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.054 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.689 I 
0.00.494.721 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.494.732 I perplexity: tokenizing the input ..
0.00.503.746 I perplexity: tokenization took 9.012 ms
0.00.503.754 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.848 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.636.297 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.636.310 I llama_perf_context_print:        load time =     486.21 ms
0.00.636.311 I llama_perf_context_print: prompt eval time =     130.86 ms /   128 tokens (    1.02 ms per token,   978.14 tokens per second)
0.00.636.312 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.636.312 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.636.633 I ggml_metal_free: deallocating

real	0m0.651s
user	0m0.078s
sys	0m0.093s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.012.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.128 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.133 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.135 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.135 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.135 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.136 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.136 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.137 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.137 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.138 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.138 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.139 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.903 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.912 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.708 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.708 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.709 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.710 I llama_model_loader: - type  f32:  194 tensors
0.00.026.710 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.710 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.711 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.077 I llm_load_vocab: special tokens cache size = 25
0.00.053.042 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.045 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.046 I llm_load_print_meta: arch             = gptneox
0.00.053.046 I llm_load_print_meta: vocab type       = BPE
0.00.053.046 I llm_load_print_meta: n_vocab          = 50304
0.00.053.046 I llm_load_print_meta: n_merges         = 50009
0.00.053.046 I llm_load_print_meta: vocab_only       = 0
0.00.053.047 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.047 I llm_load_print_meta: n_embd           = 2048
0.00.053.047 I llm_load_print_meta: n_layer          = 24
0.00.053.049 I llm_load_print_meta: n_head           = 16
0.00.053.050 I llm_load_print_meta: n_head_kv        = 16
0.00.053.050 I llm_load_print_meta: n_rot            = 32
0.00.053.051 I llm_load_print_meta: n_swa            = 0
0.00.053.052 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.054 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.054 I llm_load_print_meta: n_gqa            = 1
0.00.053.055 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.056 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.056 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.057 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.057 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.057 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.058 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.058 I llm_load_print_meta: n_ff             = 8192
0.00.053.060 I llm_load_print_meta: n_expert         = 0
0.00.053.060 I llm_load_print_meta: n_expert_used    = 0
0.00.053.060 I llm_load_print_meta: causal attn      = 1
0.00.053.060 I llm_load_print_meta: pooling type     = 0
0.00.053.060 I llm_load_print_meta: rope type        = 2
0.00.053.060 I llm_load_print_meta: rope scaling     = linear
0.00.053.061 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.061 I llm_load_print_meta: freq_scale_train = 1
0.00.053.061 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.062 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.062 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.062 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.062 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.062 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.062 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.062 I llm_load_print_meta: model type       = 1.4B
0.00.053.063 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.063 I llm_load_print_meta: model params     = 1.41 B
0.00.053.064 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.065 I llm_load_print_meta: general.name     = 1.4B
0.00.053.065 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.066 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.067 I llm_load_print_meta: LF token         = 128 ''
0.00.053.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.067 I llm_load_print_meta: max token length = 1024
0.00.054.802 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.802 I llm_load_tensors: offloading output layer to GPU
0.00.054.803 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.808 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.808 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.718 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.718 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.718 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.719 I llama_new_context_with_model: n_batch       = 2048
0.00.055.719 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.719 I llama_new_context_with_model: flash_attn    = 0
0.00.055.720 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.720 I llama_new_context_with_model: freq_scale    = 1
0.00.055.720 I ggml_metal_init: allocating
0.00.055.724 I ggml_metal_init: found device: Apple M4
0.00.055.726 I ggml_metal_init: picking default device: Apple M4
0.00.056.317 I ggml_metal_init: using embedded metal library
0.00.058.618 I ggml_metal_init: GPU name:   Apple M4
0.00.058.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.620 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.621 I ggml_metal_init: simdgroup reduction   = true
0.00.058.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.621 I ggml_metal_init: has bfloat            = true
0.00.058.621 I ggml_metal_init: use bfloat            = true
0.00.058.621 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.310 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.777 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.782 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.801 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.774 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.776 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.776 I llama_new_context_with_model: graph nodes  = 967
0.00.089.777 I llama_new_context_with_model: graph splits = 2
0.00.089.787 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.935 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.620.208 I main: llama threadpool init, n_threads = 4
0.00.620.253 I 
0.00.620.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.620.288 I 
0.00.620.519 I sampler seed: 1234
0.00.620.525 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.620.536 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.620.536 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.620.536 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.380.587 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56438.79 tokens per second)
0.01.380.588 I llama_perf_context_print:        load time =     607.45 ms
0.01.380.588 I llama_perf_context_print: prompt eval time =      47.09 ms /     7 tokens (    6.73 ms per token,   148.65 tokens per second)
0.01.380.589 I llama_perf_context_print:        eval time =     709.97 ms /    63 runs   (   11.27 ms per token,    88.74 tokens per second)
0.01.380.590 I llama_perf_context_print:       total time =     760.38 ms /    70 tokens
0.01.380.780 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.109s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.028 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.757 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.762 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.764 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.765 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.765 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.768 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.770 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.770 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.770 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.772 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.773 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.942 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.944 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.944 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.945 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.945 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.945 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.946 I llama_model_loader: - type  f32:  194 tensors
0.00.024.946 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.946 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.947 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.868 I llm_load_vocab: special tokens cache size = 25
0.00.051.991 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.996 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.997 I llm_load_print_meta: arch             = gptneox
0.00.051.999 I llm_load_print_meta: vocab type       = BPE
0.00.051.999 I llm_load_print_meta: n_vocab          = 50304
0.00.051.999 I llm_load_print_meta: n_merges         = 50009
0.00.052.000 I llm_load_print_meta: vocab_only       = 0
0.00.052.000 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.000 I llm_load_print_meta: n_embd           = 2048
0.00.052.000 I llm_load_print_meta: n_layer          = 24
0.00.052.004 I llm_load_print_meta: n_head           = 16
0.00.052.005 I llm_load_print_meta: n_head_kv        = 16
0.00.052.007 I llm_load_print_meta: n_rot            = 32
0.00.052.007 I llm_load_print_meta: n_swa            = 0
0.00.052.008 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.008 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.009 I llm_load_print_meta: n_gqa            = 1
0.00.052.010 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.010 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.011 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.012 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.012 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.012 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.012 I llm_load_print_meta: n_ff             = 8192
0.00.052.013 I llm_load_print_meta: n_expert         = 0
0.00.052.013 I llm_load_print_meta: n_expert_used    = 0
0.00.052.013 I llm_load_print_meta: causal attn      = 1
0.00.052.013 I llm_load_print_meta: pooling type     = 0
0.00.052.014 I llm_load_print_meta: rope type        = 2
0.00.052.014 I llm_load_print_meta: rope scaling     = linear
0.00.052.015 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.015 I llm_load_print_meta: freq_scale_train = 1
0.00.052.015 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.015 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.016 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.016 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.016 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.016 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.016 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.016 I llm_load_print_meta: model type       = 1.4B
0.00.052.016 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.017 I llm_load_print_meta: model params     = 1.41 B
0.00.052.017 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.017 I llm_load_print_meta: general.name     = 1.4B
0.00.052.018 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.021 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.022 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.022 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.022 I llm_load_print_meta: LF token         = 128 ''
0.00.052.022 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.023 I llm_load_print_meta: max token length = 1024
0.00.053.941 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.941 I llm_load_tensors: offloading output layer to GPU
0.00.053.941 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.952 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.954 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.901 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.902 I llama_new_context_with_model: n_ctx         = 128
0.00.054.902 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.902 I llama_new_context_with_model: n_batch       = 128
0.00.054.902 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.903 I llama_new_context_with_model: flash_attn    = 0
0.00.054.903 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.903 I llama_new_context_with_model: freq_scale    = 1
0.00.054.904 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.904 I ggml_metal_init: allocating
0.00.054.909 I ggml_metal_init: found device: Apple M4
0.00.054.911 I ggml_metal_init: picking default device: Apple M4
0.00.055.537 I ggml_metal_init: using embedded metal library
0.00.057.876 I ggml_metal_init: GPU name:   Apple M4
0.00.057.878 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.879 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.879 I ggml_metal_init: simdgroup reduction   = true
0.00.057.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.879 I ggml_metal_init: has bfloat            = true
0.00.057.879 I ggml_metal_init: use bfloat            = true
0.00.057.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.881 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.950 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.242 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.245 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.263 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.181 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.182 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.182 I llama_new_context_with_model: graph nodes  = 967
0.00.070.182 I llama_new_context_with_model: graph splits = 2
0.00.070.195 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.553.216 I 
0.00.553.268 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.553.288 I perplexity: tokenizing the input ..
0.00.561.431 I perplexity: tokenization took 8.141 ms
0.00.561.434 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.696.034 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.697.280 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.697.304 I llama_perf_context_print:        load time =     543.18 ms
0.00.697.305 I llama_perf_context_print: prompt eval time =     134.36 ms /   128 tokens (    1.05 ms per token,   952.64 tokens per second)
0.00.697.305 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.697.306 I llama_perf_context_print:       total time =     144.09 ms /   129 tokens
0.00.697.869 I ggml_metal_free: deallocating

real	0m0.714s
user	0m0.080s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.657 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.449 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.454 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.456 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.466 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.467 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.468 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.468 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.434 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.476 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.377 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.378 I llama_model_loader: - type  f32:  194 tensors
0.00.024.378 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.379 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.710 I llm_load_vocab: special tokens cache size = 25
0.00.050.698 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.701 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.701 I llm_load_print_meta: arch             = gptneox
0.00.050.701 I llm_load_print_meta: vocab type       = BPE
0.00.050.701 I llm_load_print_meta: n_vocab          = 50304
0.00.050.702 I llm_load_print_meta: n_merges         = 50009
0.00.050.702 I llm_load_print_meta: vocab_only       = 0
0.00.050.702 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.702 I llm_load_print_meta: n_embd           = 2048
0.00.050.702 I llm_load_print_meta: n_layer          = 24
0.00.050.705 I llm_load_print_meta: n_head           = 16
0.00.050.706 I llm_load_print_meta: n_head_kv        = 16
0.00.050.706 I llm_load_print_meta: n_rot            = 32
0.00.050.706 I llm_load_print_meta: n_swa            = 0
0.00.050.706 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.706 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.707 I llm_load_print_meta: n_gqa            = 1
0.00.050.708 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.709 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.709 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.710 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.710 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.710 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.710 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.713 I llm_load_print_meta: n_ff             = 8192
0.00.050.713 I llm_load_print_meta: n_expert         = 0
0.00.050.714 I llm_load_print_meta: n_expert_used    = 0
0.00.050.714 I llm_load_print_meta: causal attn      = 1
0.00.050.714 I llm_load_print_meta: pooling type     = 0
0.00.050.714 I llm_load_print_meta: rope type        = 2
0.00.050.714 I llm_load_print_meta: rope scaling     = linear
0.00.050.715 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.715 I llm_load_print_meta: freq_scale_train = 1
0.00.050.717 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.717 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.717 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.717 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.717 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.717 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.717 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.718 I llm_load_print_meta: model type       = 1.4B
0.00.050.718 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.719 I llm_load_print_meta: model params     = 1.41 B
0.00.050.719 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.720 I llm_load_print_meta: general.name     = 1.4B
0.00.050.720 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.720 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.720 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: LF token         = 128 ''
0.00.050.721 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.721 I llm_load_print_meta: max token length = 1024
0.00.052.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.712 I llm_load_tensors: offloading output layer to GPU
0.00.052.712 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.722 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.724 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.644 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.645 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.645 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.645 I llama_new_context_with_model: n_batch       = 2048
0.00.053.645 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.646 I llama_new_context_with_model: flash_attn    = 0
0.00.053.646 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.646 I llama_new_context_with_model: freq_scale    = 1
0.00.053.647 I ggml_metal_init: allocating
0.00.053.650 I ggml_metal_init: found device: Apple M4
0.00.053.652 I ggml_metal_init: picking default device: Apple M4
0.00.054.262 I ggml_metal_init: using embedded metal library
0.00.056.573 I ggml_metal_init: GPU name:   Apple M4
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.575 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.576 I ggml_metal_init: simdgroup reduction   = true
0.00.056.576 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.576 I ggml_metal_init: has bfloat            = true
0.00.056.576 I ggml_metal_init: use bfloat            = true
0.00.056.577 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.178 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.084.741 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.749 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.771 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.791 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.792 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.793 I llama_new_context_with_model: graph nodes  = 967
0.00.085.793 I llama_new_context_with_model: graph splits = 2
0.00.085.808 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.937 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.937 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.137 I main: llama threadpool init, n_threads = 4
0.00.721.184 I 
0.00.721.228 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.229 I 
0.00.721.457 I sampler seed: 1234
0.00.721.462 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.472 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.473 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.473 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.571.287 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57165.86 tokens per second)
0.01.571.287 I llama_perf_context_print:        load time =     712.47 ms
0.01.571.288 I llama_perf_context_print: prompt eval time =      55.49 ms /     7 tokens (    7.93 ms per token,   126.15 tokens per second)
0.01.571.289 I llama_perf_context_print:        eval time =     791.32 ms /    63 runs   (   12.56 ms per token,    79.61 tokens per second)
0.01.571.289 I llama_perf_context_print:       total time =     850.15 ms /    70 tokens
0.01.571.469 I ggml_metal_free: deallocating

real	0m1.588s
user	0m0.110s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.765 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.770 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.776 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.777 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.777 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.779 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.779 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.780 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.780 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.780 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.781 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.783 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.784 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.784 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.593 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.682 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.486 I llama_model_loader: - type  f32:  194 tensors
0.00.023.486 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.486 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.739 I llm_load_vocab: special tokens cache size = 25
0.00.050.814 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.816 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.817 I llm_load_print_meta: arch             = gptneox
0.00.050.817 I llm_load_print_meta: vocab type       = BPE
0.00.050.817 I llm_load_print_meta: n_vocab          = 50304
0.00.050.817 I llm_load_print_meta: n_merges         = 50009
0.00.050.817 I llm_load_print_meta: vocab_only       = 0
0.00.050.818 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.818 I llm_load_print_meta: n_embd           = 2048
0.00.050.818 I llm_load_print_meta: n_layer          = 24
0.00.050.821 I llm_load_print_meta: n_head           = 16
0.00.050.823 I llm_load_print_meta: n_head_kv        = 16
0.00.050.824 I llm_load_print_meta: n_rot            = 32
0.00.050.824 I llm_load_print_meta: n_swa            = 0
0.00.050.824 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.824 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.825 I llm_load_print_meta: n_gqa            = 1
0.00.050.826 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.826 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.827 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.827 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.827 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.827 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.828 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.828 I llm_load_print_meta: n_ff             = 8192
0.00.050.829 I llm_load_print_meta: n_expert         = 0
0.00.050.829 I llm_load_print_meta: n_expert_used    = 0
0.00.050.829 I llm_load_print_meta: causal attn      = 1
0.00.050.829 I llm_load_print_meta: pooling type     = 0
0.00.050.829 I llm_load_print_meta: rope type        = 2
0.00.050.830 I llm_load_print_meta: rope scaling     = linear
0.00.050.831 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.831 I llm_load_print_meta: freq_scale_train = 1
0.00.050.831 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.832 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.833 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.833 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.833 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.834 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.834 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.834 I llm_load_print_meta: model type       = 1.4B
0.00.050.834 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.835 I llm_load_print_meta: model params     = 1.41 B
0.00.050.835 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.836 I llm_load_print_meta: general.name     = 1.4B
0.00.050.836 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.836 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.837 I llm_load_print_meta: LF token         = 128 ''
0.00.050.837 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.837 I llm_load_print_meta: max token length = 1024
0.00.052.892 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.893 I llm_load_tensors: offloading output layer to GPU
0.00.052.893 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.903 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.904 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.833 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.834 I llama_new_context_with_model: n_ctx         = 128
0.00.053.834 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.834 I llama_new_context_with_model: n_batch       = 128
0.00.053.834 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.834 I llama_new_context_with_model: flash_attn    = 0
0.00.053.835 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.835 I llama_new_context_with_model: freq_scale    = 1
0.00.053.835 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.836 I ggml_metal_init: allocating
0.00.053.839 I ggml_metal_init: found device: Apple M4
0.00.053.841 I ggml_metal_init: picking default device: Apple M4
0.00.054.408 I ggml_metal_init: using embedded metal library
0.00.056.719 I ggml_metal_init: GPU name:   Apple M4
0.00.056.720 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.721 I ggml_metal_init: simdgroup reduction   = true
0.00.056.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.722 I ggml_metal_init: has bfloat            = true
0.00.056.722 I ggml_metal_init: use bfloat            = true
0.00.056.722 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.560 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.886 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.890 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.906 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.798 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.799 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.799 I llama_new_context_with_model: graph nodes  = 967
0.00.068.799 I llama_new_context_with_model: graph splits = 2
0.00.068.812 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.812 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.812 I 
0.00.630.843 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.854 I perplexity: tokenizing the input ..
0.00.638.871 I perplexity: tokenization took 8.015 ms
0.00.638.875 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.692 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.780.862 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.780.885 I llama_perf_context_print:        load time =     621.97 ms
0.00.780.887 I llama_perf_context_print: prompt eval time =     140.59 ms /   128 tokens (    1.10 ms per token,   910.46 tokens per second)
0.00.780.888 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.888 I llama_perf_context_print:       total time =     150.07 ms /   129 tokens
0.00.781.310 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.079s
sys	0m0.107s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.265 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.042 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.055 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.055 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.056 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.056 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.056 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.057 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.057 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.060 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.061 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.061 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.012 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.975 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.977 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.977 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.977 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.978 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.978 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.978 I llama_model_loader: - type  f32:  194 tensors
0.00.025.979 I llama_model_loader: - type q6_K:   98 tensors
0.00.047.020 I llm_load_vocab: special tokens cache size = 25
0.00.052.821 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.824 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.824 I llm_load_print_meta: arch             = gptneox
0.00.052.825 I llm_load_print_meta: vocab type       = BPE
0.00.052.825 I llm_load_print_meta: n_vocab          = 50304
0.00.052.825 I llm_load_print_meta: n_merges         = 50009
0.00.052.825 I llm_load_print_meta: vocab_only       = 0
0.00.052.825 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.826 I llm_load_print_meta: n_embd           = 2048
0.00.052.826 I llm_load_print_meta: n_layer          = 24
0.00.052.829 I llm_load_print_meta: n_head           = 16
0.00.052.831 I llm_load_print_meta: n_head_kv        = 16
0.00.052.831 I llm_load_print_meta: n_rot            = 32
0.00.052.831 I llm_load_print_meta: n_swa            = 0
0.00.052.832 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.832 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.833 I llm_load_print_meta: n_gqa            = 1
0.00.052.834 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.834 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.835 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.836 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.836 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.836 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.837 I llm_load_print_meta: n_ff             = 8192
0.00.052.837 I llm_load_print_meta: n_expert         = 0
0.00.052.837 I llm_load_print_meta: n_expert_used    = 0
0.00.052.837 I llm_load_print_meta: causal attn      = 1
0.00.052.839 I llm_load_print_meta: pooling type     = 0
0.00.052.840 I llm_load_print_meta: rope type        = 2
0.00.052.840 I llm_load_print_meta: rope scaling     = linear
0.00.052.841 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.841 I llm_load_print_meta: freq_scale_train = 1
0.00.052.841 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.842 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.842 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.842 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.842 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.842 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.842 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.843 I llm_load_print_meta: model type       = 1.4B
0.00.052.843 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.843 I llm_load_print_meta: model params     = 1.41 B
0.00.052.844 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.844 I llm_load_print_meta: general.name     = 1.4B
0.00.052.844 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.844 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.845 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.846 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.846 I llm_load_print_meta: LF token         = 128 ''
0.00.052.847 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.847 I llm_load_print_meta: max token length = 1024
0.00.054.700 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.701 I llm_load_tensors: offloading output layer to GPU
0.00.054.701 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.706 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.707 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.630 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.631 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.631 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.631 I llama_new_context_with_model: n_batch       = 2048
0.00.055.631 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.632 I llama_new_context_with_model: flash_attn    = 0
0.00.055.632 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.632 I llama_new_context_with_model: freq_scale    = 1
0.00.055.633 I ggml_metal_init: allocating
0.00.055.636 I ggml_metal_init: found device: Apple M4
0.00.055.638 I ggml_metal_init: picking default device: Apple M4
0.00.056.251 I ggml_metal_init: using embedded metal library
0.00.058.547 I ggml_metal_init: GPU name:   Apple M4
0.00.058.548 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.548 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.549 I ggml_metal_init: simdgroup reduction   = true
0.00.058.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.551 I ggml_metal_init: has bfloat            = true
0.00.058.551 I ggml_metal_init: use bfloat            = true
0.00.058.551 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.479 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.849 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.856 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.879 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.914 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.915 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.916 I llama_new_context_with_model: graph nodes  = 967
0.00.088.916 I llama_new_context_with_model: graph splits = 2
0.00.088.931 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.074 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.847 I main: llama threadpool init, n_threads = 4
0.00.767.886 I 
0.00.767.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.919 I 
0.00.768.154 I sampler seed: 1234
0.00.768.160 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.768.211 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.768.215 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.768.215 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.658.007 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.658.007 I llama_perf_context_print:        load time =     757.58 ms
0.01.658.008 I llama_perf_context_print: prompt eval time =      54.36 ms /     7 tokens (    7.77 ms per token,   128.76 tokens per second)
0.01.658.009 I llama_perf_context_print:        eval time =     832.82 ms /    63 runs   (   13.22 ms per token,    75.65 tokens per second)
0.01.658.009 I llama_perf_context_print:       total time =     890.16 ms /    70 tokens
0.01.658.208 I ggml_metal_free: deallocating

real	0m1.677s
user	0m0.111s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4382 (86bf31cf) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.403 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.094 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.098 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.101 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.103 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.104 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.104 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.105 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.105 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.972 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.736 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.737 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.737 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.737 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.738 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.738 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.738 I llama_model_loader: - type  f32:  194 tensors
0.00.023.739 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.666 I llm_load_vocab: special tokens cache size = 25
0.00.049.642 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.645 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.645 I llm_load_print_meta: arch             = gptneox
0.00.049.645 I llm_load_print_meta: vocab type       = BPE
0.00.049.646 I llm_load_print_meta: n_vocab          = 50304
0.00.049.646 I llm_load_print_meta: n_merges         = 50009
0.00.049.646 I llm_load_print_meta: vocab_only       = 0
0.00.049.646 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.646 I llm_load_print_meta: n_embd           = 2048
0.00.049.646 I llm_load_print_meta: n_layer          = 24
0.00.049.649 I llm_load_print_meta: n_head           = 16
0.00.049.650 I llm_load_print_meta: n_head_kv        = 16
0.00.049.650 I llm_load_print_meta: n_rot            = 32
0.00.049.650 I llm_load_print_meta: n_swa            = 0
0.00.049.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.651 I llm_load_print_meta: n_gqa            = 1
0.00.049.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.653 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.653 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.654 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.654 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.654 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.654 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.655 I llm_load_print_meta: n_ff             = 8192
0.00.049.655 I llm_load_print_meta: n_expert         = 0
0.00.049.655 I llm_load_print_meta: n_expert_used    = 0
0.00.049.655 I llm_load_print_meta: causal attn      = 1
0.00.049.655 I llm_load_print_meta: pooling type     = 0
0.00.049.655 I llm_load_print_meta: rope type        = 2
0.00.049.656 I llm_load_print_meta: rope scaling     = linear
0.00.049.658 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.658 I llm_load_print_meta: freq_scale_train = 1
0.00.049.659 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.659 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.659 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.659 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.659 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.659 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.660 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.660 I llm_load_print_meta: model type       = 1.4B
0.00.049.660 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.661 I llm_load_print_meta: model params     = 1.41 B
0.00.049.661 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.661 I llm_load_print_meta: general.name     = 1.4B
0.00.049.663 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.663 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.663 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.663 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.663 I llm_load_print_meta: LF token         = 128 ''
0.00.049.664 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.664 I llm_load_print_meta: max token length = 1024
0.00.051.249 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.249 I llm_load_tensors: offloading output layer to GPU
0.00.051.249 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.259 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.260 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.104 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.105 I llama_new_context_with_model: n_ctx         = 128
0.00.052.106 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.106 I llama_new_context_with_model: n_batch       = 128
0.00.052.106 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.106 I llama_new_context_with_model: flash_attn    = 0
0.00.052.107 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.107 I llama_new_context_with_model: freq_scale    = 1
0.00.052.107 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.108 I ggml_metal_init: allocating
0.00.052.114 I ggml_metal_init: found device: Apple M4
0.00.052.116 I ggml_metal_init: picking default device: Apple M4
0.00.052.685 I ggml_metal_init: using embedded metal library
0.00.055.041 I ggml_metal_init: GPU name:   Apple M4
0.00.055.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.044 I ggml_metal_init: simdgroup reduction   = true
0.00.055.044 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.044 I ggml_metal_init: has bfloat            = true
0.00.055.044 I ggml_metal_init: use bfloat            = true
0.00.055.045 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.045 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.587 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.868 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.871 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.884 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.728 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.728 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.729 I llama_new_context_with_model: graph nodes  = 967
0.00.066.729 I llama_new_context_with_model: graph splits = 2
0.00.066.741 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.742 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.209.960 I 
0.00.210.006 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.210.020 I perplexity: tokenizing the input ..
0.00.217.141 I perplexity: tokenization took 7.12 ms
0.00.217.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.357.570 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.358.830 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.358.858 I llama_perf_context_print:        load time =     200.55 ms
0.00.358.859 I llama_perf_context_print: prompt eval time =     140.19 ms /   128 tokens (    1.10 ms per token,   913.04 tokens per second)
0.00.358.860 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.358.860 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.359.370 I ggml_metal_free: deallocating

real	0m0.374s
user	0m0.076s
sys	0m0.046s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4382 (86bf31cf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14190a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14190a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14190af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14190b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14190baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14190c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14190c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14190cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14190d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14190d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14190db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14190e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14190eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14190f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14190fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141910260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141910980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1419110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1419117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141911f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1419126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141912dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1419134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141913d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1419144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141914770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141914d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1419159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141915f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1419161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141916690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141916950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1419171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141917720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1419179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141917e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141918320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1419187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141918c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141919100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1419195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141919a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141919ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14191a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14191a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14191ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14191b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14191bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14191c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14191c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14191cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14191d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14191d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14191dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14191e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14191ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14191f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14191f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14191f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1419201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141920490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141920930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141920dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141921270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141921710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141921bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141922050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1419224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141922990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141922e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1419232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141923770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141923c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141924160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1419246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141924c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141925150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1419256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141925bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141926140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x141926690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141926be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141927130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141927680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141927bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141928120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141928670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141928bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141929110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141929660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141929bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14192a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14192a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14192aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14192b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14192b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14192bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14191b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14192c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14192c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14192cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14192d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14192d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14192dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14192e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14192e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14192ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14192f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14192f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14192fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141930220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141930770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141930cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141931160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141931600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141931aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141931f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1419323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141932880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141932d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1419331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141933660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141933b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141933fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141934440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1419348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141934d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141935220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1419356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141935b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141936000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1419364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141936940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141936de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141937280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141937720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141937bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141938060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141938500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1419389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141938e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1419392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141939780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141939c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14193a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14193a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14193aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14193aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14193b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14193b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14193bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14193c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14193c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14193ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14193cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14193d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14193d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14193dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14193e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14193e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14193eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14193ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14193f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14193f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14193fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1419401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141940680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141940b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141940fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141941460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141941900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141941da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141942240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1419426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141942b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141943020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1419434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141943960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141943e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1419442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141944740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141944be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141945080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141945520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1419459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141945e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141946300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1419467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141946c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1419470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141947580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141947a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141947ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141948410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141948960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141948eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141949400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1419496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141949cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14194a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14194a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14194b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14194b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14194b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14194be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14194c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14194cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14194d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14194d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14194da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14194e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14194e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14194ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14194f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14194f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14194fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1419501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141950710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141950c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1419511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141951700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141951c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1419521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1419526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141952c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141953190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1419536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141953c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141954180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1419546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141954c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141955170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1419556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141955c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141956160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1419566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141956c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141957150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1419576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141957bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141958140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141958690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141958be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141959130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141959680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141959bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14195a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14195a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14195abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14195b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14195b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14195bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14195c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14195c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14195cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14195d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14195d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14195db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14195e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14195e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14195eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14195f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14195f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14195fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1419600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141960610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141960b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141961000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1419614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141961940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141961de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141962280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141962720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141962bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141963060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141963500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1419639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141963e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1419642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141964780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141964c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1419650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141965610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141965d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141966450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141966b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141967290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141967550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141967d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141968000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141968610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.492 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14190de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14190e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14190e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14190ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14190f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14190f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14190f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14190fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141910200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141910670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141910ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1419110c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1419119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141912130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141912910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141913000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1419136f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141913de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1419144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141914e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141915540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141915c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141916320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141916a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141917100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141917570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1419179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141917e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1419182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141918730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141918ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141919010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141919480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141919740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141919bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14191a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14191a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14191a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14191ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14191b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14191b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14191bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14191bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14191c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14191c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14191cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14191d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14191d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14191d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14191de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14191e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14191e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14191eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14191f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14191f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14191f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14191fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1419201c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141920630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141920aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141920f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141921380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1419217f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141921c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1419220d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141922540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1419229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141922e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141923290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141923700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141923b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141923fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141924450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1419248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141924d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1419251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141925610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x141925a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141925ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141926360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1419267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141926c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1419270b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x141927520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141927990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141927e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x141928270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1419286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141928b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x141928fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141929430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1419298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x141929d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14192a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14192a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14192aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14192aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14192b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14192b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14192bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14192c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14192c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14192c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14192cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14192d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14192d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14192db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14192dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14192e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14192e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14192ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14192f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14192f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14192fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14192feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141930320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141930790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141930c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141931070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1419314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141931950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141931dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141932230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1419326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141932b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141932f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1419333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141933860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141933cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141934140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1419345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141934a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141934e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141935300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141935770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141935be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141936050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1419364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141936930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141936da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141937210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141937680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141937af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141937f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1419383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141938840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141938cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141939120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141939590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141939a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141939e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14193a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14193a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14193abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14193b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14193b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14193b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14193bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14193c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14193c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14193cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14193cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14193d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14193d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14193dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14193e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14193e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14193e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14193ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14193f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14193f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14193fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141940010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141940480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1419408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141940d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1419411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141941640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141941ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141941f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141942390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141942800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141942c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1419430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141943550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1419439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141943e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1419442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141944710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141944b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141944ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141945460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1419458d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141945d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1419461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141946620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141946a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141946f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141947370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1419477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141947c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1419480c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x141948530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1419489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141948e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x141949280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1419496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141949b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141949fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14194a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14194abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14194b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14194b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14194b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14194bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14194c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14194c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14194cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14194cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14194d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14194d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14194dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14194e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14194e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14194e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14194ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14194f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14194f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14194fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141950010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141950480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1419508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141950d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1419511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141951640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141951ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141951f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141952390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141952800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141952c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1419530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141953550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1419539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141953e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1419542a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141954710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141954b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141954ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141955460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1419558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141955d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1419561b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141956620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141956a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141956f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141957370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1419577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141957c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1419580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141958530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1419589a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141958e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141959280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1419596f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141959b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141959fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14195a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14195a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14195ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14195b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14195b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14195ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14195bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14195c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14195c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14195cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14195d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14195d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14195d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14195ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14195e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14195e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14195eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14195f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14195f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141960010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141960700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141960b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141960fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141961450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1419618c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c9046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c904b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c904fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c905430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c9058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c905d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c906180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c9065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c906a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c906ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c907340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c907a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c908580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c908d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c909540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c909c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c90a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c90aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c90b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c90b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c90c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c90c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c90ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c90d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c90dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c90df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c90e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c90e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c90eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c90ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c90f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c90f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c90fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c910030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c9104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c910910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c910d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c9111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c911660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c911ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c911f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c9123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c912820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c912c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c913100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c913570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c9139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c913e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c9142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c914730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c914ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c915010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c915480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c9158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c915d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c9161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c916740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c916c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c9170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c917520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c917990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c917e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c918270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c9186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c918b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c918fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c919430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c9198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c919d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c91a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c91a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c91aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c91aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c91b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c91b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c91bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c91c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c91c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c91c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c91cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c91d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c91d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c91db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c91dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c91e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c91e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c91ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c91f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c91f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c91fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c91feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c920320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c920790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c920c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c921070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c9214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c921950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c921dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c922230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c9226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c922b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c922f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c9233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c923860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c923cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c924140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c9245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c924a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c924e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c925300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c925770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c925be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c926050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c9264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c926930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c926da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c927210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c927680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c927af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c927f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c9283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c928840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c928cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c929120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c929590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c929a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c929e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c92a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c92a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c92abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c92b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c92b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c92b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c92bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c92c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c92c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c92cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c92cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c92d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c92d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c92dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c92e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c92e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c92e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c92ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c92f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c92f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c92fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c930010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c930480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c9308f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c930d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c9311d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c931640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c931ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c931f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c932390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c932800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c932c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c9330e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c933550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c9339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c933e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c9342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c934710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c934b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c934ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c935460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c9358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c935d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c9361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c936620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c936a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c936f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c937370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c9377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c937c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c9380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c938530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c9389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c938e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c939280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c9396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c939b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c939fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c93a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c93a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c93ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c93b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c93b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c93ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c93bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c93c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c93c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c93cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c93d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c93d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c93d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c93ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c93e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c93e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c93eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c93efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c93f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c93f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c93fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c940170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c940700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c940b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c940fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c941b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c941df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c9420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c942520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c942990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c942e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c943270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c9436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c943b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c943fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c944430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c9448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c944d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c945180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c9455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c945a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c945ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c946340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c9467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c946c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c947090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c947500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c947970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c947de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c948250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c9486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c948b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c948fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c949410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c949880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c949cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c94a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c94a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c94aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c94b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c94b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c94bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c94bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c94c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c94c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c94cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c94d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c94d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c94d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c94de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c94e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c94e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c94ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c94f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c94f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c94f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c94fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c9501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c950650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c950ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c950f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c9513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c951810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c951c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c9520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c952560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c9529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c952e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c9532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c953720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c953b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c954000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c954470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c9548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c954d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c9551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c955630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c955aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c956510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c956c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c957350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c957a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c957d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c9581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c9587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c958db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.828s
user	0m0.290s
sys	0m0.306s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4382 (86bf31cf)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff0d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff0f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff0fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff10150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff10650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff12320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff13250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff13970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff14f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff156a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff18f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff1b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff1bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff1c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff1c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff1ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff1ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff1d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff1f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff23920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff23dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff24260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff24ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff25040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff262c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff26760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff28690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff28be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff29130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff29680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff2a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff2a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff2abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff2b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff2b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff2bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff2c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff2cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff2d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff2d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff2db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff2e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff2e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff2eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff2eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff30790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff30ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff31230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff31cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff32770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff33760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff43b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff456d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff45b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff46010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff46950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff46df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff47290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff48070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff48510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff4a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff4aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff4b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff4b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff4bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff4ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff4e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff4f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff50a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff51c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff521c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff52710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff52c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff53700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff53c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff541a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff54c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff55190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff556e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff55c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff56c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff58160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff58c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff596a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff59bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff5a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff5abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff5b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff5b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff5bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff5c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff5cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff5d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff5d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff5dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff5e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff5e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff5eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff5f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff5f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff5fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff60630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff61620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff62610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff62b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff63600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff63b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff63ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff64490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff64dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff65270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff65710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff65bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff66050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff66990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff66e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff67770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff67c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff68600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff68d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff69b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff6a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff6ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff6aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff6b600 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.092.988 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.992 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fe08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fe091e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fe09650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fe09ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fe09f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fe0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fe0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fe0ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fe0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fe0b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fe0bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fe0c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fe0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fe0d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fe0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fe0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fe0ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fe0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fe0f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fe10090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fe107b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fe10ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fe115f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fe11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fe12430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fe126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fe129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fe12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fe13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fe13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fe13c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fe14110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fe14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fe14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fe14cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fe15120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fe15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fe15b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fe16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fe16580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fe16a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fe16f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fe17480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fe17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fe17e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fe182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fe18760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fe18bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fe19040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fe194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fe19920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fe19d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fe1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fe1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fe1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fe1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fe1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fe1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fe1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fe1c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fe1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fe1d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fe1d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fe1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fe1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fe1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fe1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fe1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fe1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fe1f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fe1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fe1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fe20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fe20980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fe20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fe21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fe21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fe21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fe22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fe22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fe22eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fe23400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fe23950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fe23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fe243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fe24940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fe24e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fe253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fe25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fe25e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fe263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fe26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fe26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fe273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fe27910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fe27e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fe283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fe28900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fe28e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fe293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fe298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fe29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fe2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fe2a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fe2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fe2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fe2b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fe2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fe2c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fe2c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fe2ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fe2d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fe2d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fe2dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fe2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fe2e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fe2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fe2efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fe2f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fe2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fe2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fe30250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fe306f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fe30b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fe31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fe314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fe31970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fe31e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fe322b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fe32750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fe32bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fe33090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fe33530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fe339d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fe33e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fe34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fe347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fe34c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fe350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fe35590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fe35a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fe35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fe36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fe36810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fe36cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fe37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fe375f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fe37a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fe37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fe383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fe38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fe38d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fe391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fe39650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fe39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fe39f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fe3a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fe3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fe3ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fe3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fe3b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fe3bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fe3bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fe3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fe3c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fe3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fe3d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fe3d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fe3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fe3e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fe3e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fe3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fe3ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fe3f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fe3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fe3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fe400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fe40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fe409f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fe40e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fe41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fe417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fe41c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fe42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fe425b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fe42a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fe42ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fe43390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fe43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fe43cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fe44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fe44610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fe44ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fe45000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fe45550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fe45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fe45ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fe462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fe468c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fe46ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fe474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fe47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fe48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fe48430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fe48a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fe49050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fe49840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fe49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fe4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fe4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fe4add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fe4b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fe4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fe4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fe4c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fe4c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fe4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fe4d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fe4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fe4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fe4e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fe4e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fe4ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fe4f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fe4f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fe4fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fe502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe50820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe50d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe51d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe54d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe55d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe56270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe56d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe57d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe58250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe58cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe59790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe59ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe5a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe5acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fe5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fe5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fe5bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fe5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fe5c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fe5ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fe5d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fe5d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fe5dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fe5e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fe5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fe5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fe5ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fe5f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fe5f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fe5fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fe600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fe60590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fe60a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fe60ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fe61370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fe61810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fe61cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fe62200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fe62920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fe63040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fe63760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fe63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fe64140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fe64930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fe64bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fe65200 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ff0e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ff0df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ff0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ff0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ff0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ff0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ff0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ff100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ff0d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ff27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ff27e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ff28290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ff28b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ff29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ff29ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ff2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ff2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ff2afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ff2b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ff2c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ff2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ff2ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ff2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ff2dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ff2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ff2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ff2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ff2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ff2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ff2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ff2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ff301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ff30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ff30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ff30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ff311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ff31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ff31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ff31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ff323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ff32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ff32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ff33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ff33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ff339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ff33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ff342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ff34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ff34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ff35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ff35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ff358f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ff35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ff361d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ff36640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ff36ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ff36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ff37390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ff37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ff37c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ff380e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ff38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ff389c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ff38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ff392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ff39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ff39b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ff39ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ff3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ff3a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ff3ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ff3b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ff3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ff3ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ff3bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ff3c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ff3c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ff3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ff3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ff3d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ff3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ff3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ff3e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ff3e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ff3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ff3efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ff3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ff3f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ff3fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ff40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ff40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ff40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ff40ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ff41350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ff417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ff41c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ff420a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ff42510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ff42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ff42df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ff43260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ff436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ff43b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ff43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ff44420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ff44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ff44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ff45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ff455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ff45a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ff45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ff46330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ff467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ff46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ff47080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ff474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ff47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ff47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ff48240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ff486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ff48b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ff48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ff49400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ff49870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ff49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ff4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ff4a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ff4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ff4aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ff4b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ff4b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ff4bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ff4c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ff4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ff4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ff4cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ff4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ff4d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ff4db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ff4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ff4e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ff4e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ff4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ff4f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ff4f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ff4fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ff4fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ff502f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ff50760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ff50bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ff51040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ff514b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ff51920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ff51d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ff52200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ff52670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ff52ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ff52f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ff533c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ff53830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ff53ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ff54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ff54580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ff549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ff54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ff552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ff55740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ff55bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ff56020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ff56490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ff56900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ff56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ff571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ff57650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ff57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ff57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ff583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ff58810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ff58c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ff590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ff59560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ff599d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ff59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ff5a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ff5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ff5ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ff5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ff5b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ff5b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ff5bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ff5c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ff5c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ff5caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ff5cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ff5d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ff5d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ff5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ff5e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ff5e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ff5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ff5ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ff5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ff5f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ff5fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ff5ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ff60450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ff608c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ff60d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ff611a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ff61610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ff61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ff62200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ff62670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ff62ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ff62f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ff633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ff63830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ff63ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ff64110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ff64580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ff649f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ff64e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ff652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ff65740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ff65bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ff66020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ff66490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ff66900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ff66d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ff671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ff67650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ff67ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ff67f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ff683a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ff68810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ff68c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ff690f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ff69560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ff699d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ff69e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ff6a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ff6a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ff6ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ff6b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ff6b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ff1a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ff1ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ff1afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ff1b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ff1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ff1bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ff1c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ff1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ff1ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ff1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ff1d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ff1d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ff1dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ff1e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ff1e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ff1e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ff1ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ff1f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ff1f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ff1fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ff1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ff20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ff208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ff20d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ff21180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ff215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ff21a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ff21ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ff22340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ff227b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ff22c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ff23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ff23500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ff23970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ff23de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ff24250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ff246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ff24b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ff25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ff25910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ff26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ff266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ff26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ff26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ff27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ff18f10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.943s
user	0m0.243s
sys	0m0.148s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.62 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.65 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.27 sec*proc (2 tests)

Total Test time (real) =   1.28 sec
        1.30 real         0.76 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.15 user         0.04 sys
```
