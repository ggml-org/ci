### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.33 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.21 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.15 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.19 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.28 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  180.35 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.94 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.70 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.34 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.22 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.59 sec*proc (27 tests)

Total Test time (real) = 222.60 sec

real	3m42.642s
user	7m42.807s
sys	0m6.324s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.22 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.17 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.30 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.19 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.42 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.38 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.01 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.13 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.14 sec*proc (27 tests)

Total Test time (real) =  51.15 sec

real	0m51.164s
user	1m11.675s
sys	0m5.722s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.125 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.027.757 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.993 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.008 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.012 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.032.013 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.013 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.032.014 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.032.014 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.032.022 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.032.023 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.032.024 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.032.024 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.032.025 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.032.031 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.032.031 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.032.032 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.032.032 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.032.033 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.032.033 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.032.034 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.036.579 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.038.327 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.330 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.038.332 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.038.332 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.038.333 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.038.333 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.038.334 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.038.335 I llama_model_loader: - type  f32:  124 tensors
0.00.038.344 I llama_model_loader: - type  f16:   73 tensors
0.00.044.036 I llm_load_vocab: special tokens cache size = 5
0.00.046.610 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.046.614 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.046.615 I llm_load_print_meta: arch             = bert
0.00.046.615 I llm_load_print_meta: vocab type       = WPM
0.00.046.616 I llm_load_print_meta: n_vocab          = 30522
0.00.046.616 I llm_load_print_meta: n_merges         = 0
0.00.046.616 I llm_load_print_meta: vocab_only       = 0
0.00.046.617 I llm_load_print_meta: n_ctx_train      = 512
0.00.046.617 I llm_load_print_meta: n_embd           = 384
0.00.046.617 I llm_load_print_meta: n_layer          = 12
0.00.046.621 I llm_load_print_meta: n_head           = 12
0.00.046.622 I llm_load_print_meta: n_head_kv        = 12
0.00.046.648 I llm_load_print_meta: n_rot            = 32
0.00.046.649 I llm_load_print_meta: n_swa            = 0
0.00.046.649 I llm_load_print_meta: n_embd_head_k    = 32
0.00.046.649 I llm_load_print_meta: n_embd_head_v    = 32
0.00.046.650 I llm_load_print_meta: n_gqa            = 1
0.00.046.651 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.046.652 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.046.653 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.046.654 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.046.654 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.046.654 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.046.656 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.046.657 I llm_load_print_meta: n_ff             = 1536
0.00.046.657 I llm_load_print_meta: n_expert         = 0
0.00.046.657 I llm_load_print_meta: n_expert_used    = 0
0.00.046.657 I llm_load_print_meta: causal attn      = 0
0.00.046.658 I llm_load_print_meta: pooling type     = 2
0.00.046.658 I llm_load_print_meta: rope type        = 2
0.00.046.658 I llm_load_print_meta: rope scaling     = linear
0.00.046.659 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.046.660 I llm_load_print_meta: freq_scale_train = 1
0.00.046.660 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.046.663 I llm_load_print_meta: rope_finetuned   = unknown
0.00.046.663 I llm_load_print_meta: ssm_d_conv       = 0
0.00.046.663 I llm_load_print_meta: ssm_d_inner      = 0
0.00.046.663 I llm_load_print_meta: ssm_d_state      = 0
0.00.046.664 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.046.664 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.046.676 I llm_load_print_meta: model type       = 33M
0.00.046.676 I llm_load_print_meta: model ftype      = F16
0.00.046.677 I llm_load_print_meta: model params     = 33.21 M
0.00.046.678 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.046.678 I llm_load_print_meta: general.name     = Bge Small
0.00.046.679 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.046.679 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.046.679 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.046.680 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.046.680 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.046.682 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.046.683 I llm_load_print_meta: max token length = 21
0.00.048.899 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.048.900 I llm_load_tensors: offloading output layer to GPU
0.00.048.905 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.048.935 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.048.937 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.049.601 I llama_new_context_with_model: n_seq_max     = 1
0.00.049.603 I llama_new_context_with_model: n_ctx         = 512
0.00.049.603 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.049.603 I llama_new_context_with_model: n_batch       = 2048
0.00.049.604 I llama_new_context_with_model: n_ubatch      = 2048
0.00.049.604 I llama_new_context_with_model: flash_attn    = 0
0.00.049.605 I llama_new_context_with_model: freq_base     = 10000.0
0.00.049.605 I llama_new_context_with_model: freq_scale    = 1
0.00.049.606 I ggml_metal_init: allocating
0.00.049.618 I ggml_metal_init: found device: Apple M4
0.00.049.625 I ggml_metal_init: picking default device: Apple M4
0.00.050.640 I ggml_metal_init: using embedded metal library
0.00.055.598 I ggml_metal_init: GPU name:   Apple M4
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.602 I ggml_metal_init: simdgroup reduction   = true
0.00.055.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.603 I ggml_metal_init: has bfloat            = true
0.00.055.603 I ggml_metal_init: use bfloat            = true
0.00.055.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.182 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.070.185 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.070.187 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.071.125 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.071.126 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.071.127 I llama_new_context_with_model: graph nodes  = 429
0.00.071.127 I llama_new_context_with_model: graph splits = 2
0.00.071.151 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.079.620 I 
0.00.079.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.080.488 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.085.621 I llama_perf_context_print:        load time =      51.85 ms
0.00.085.623 I llama_perf_context_print: prompt eval time =       4.98 ms /     9 tokens (    0.55 ms per token,  1806.87 tokens per second)
0.00.085.623 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.085.624 I llama_perf_context_print:       total time =       6.00 ms /    10 tokens
0.00.085.814 I ggml_metal_free: deallocating

real	0m0.268s
user	0m0.056s
sys	0m0.038s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.034 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.128 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.273 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.277 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.278 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.282 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.283 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.283 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.284 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.284 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.285 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.285 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.285 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.287 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.288 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.288 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.288 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.288 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.289 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.289 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.866 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.575 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.577 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.577 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.577 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.578 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.578 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.578 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.578 I llama_model_loader: - type  f32:  124 tensors
0.00.014.579 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.154 I llm_load_vocab: special tokens cache size = 5
0.00.018.491 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.494 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.494 I llm_load_print_meta: arch             = bert
0.00.018.494 I llm_load_print_meta: vocab type       = WPM
0.00.018.495 I llm_load_print_meta: n_vocab          = 30522
0.00.018.495 I llm_load_print_meta: n_merges         = 0
0.00.018.495 I llm_load_print_meta: vocab_only       = 0
0.00.018.495 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.495 I llm_load_print_meta: n_embd           = 384
0.00.018.495 I llm_load_print_meta: n_layer          = 12
0.00.018.498 I llm_load_print_meta: n_head           = 12
0.00.018.498 I llm_load_print_meta: n_head_kv        = 12
0.00.018.505 I llm_load_print_meta: n_rot            = 32
0.00.018.505 I llm_load_print_meta: n_swa            = 0
0.00.018.505 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.506 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.506 I llm_load_print_meta: n_gqa            = 1
0.00.018.507 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.507 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.508 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.508 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.508 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.508 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.509 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.509 I llm_load_print_meta: n_ff             = 1536
0.00.018.509 I llm_load_print_meta: n_expert         = 0
0.00.018.510 I llm_load_print_meta: n_expert_used    = 0
0.00.018.510 I llm_load_print_meta: causal attn      = 0
0.00.018.510 I llm_load_print_meta: pooling type     = 2
0.00.018.510 I llm_load_print_meta: rope type        = 2
0.00.018.510 I llm_load_print_meta: rope scaling     = linear
0.00.018.510 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.511 I llm_load_print_meta: freq_scale_train = 1
0.00.018.511 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.511 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.511 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.511 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.512 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.512 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.512 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.516 I llm_load_print_meta: model type       = 33M
0.00.018.517 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.517 I llm_load_print_meta: model params     = 33.21 M
0.00.018.517 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.518 I llm_load_print_meta: general.name     = Bge Small
0.00.018.518 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.518 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.518 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.518 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.519 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.519 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.519 I llm_load_print_meta: max token length = 21
0.00.019.824 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.824 I llm_load_tensors: offloading output layer to GPU
0.00.019.824 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.832 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.833 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.202 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.203 I llama_new_context_with_model: n_ctx         = 512
0.00.020.203 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.204 I llama_new_context_with_model: n_batch       = 2048
0.00.020.204 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.204 I llama_new_context_with_model: flash_attn    = 0
0.00.020.204 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.205 I llama_new_context_with_model: freq_scale    = 1
0.00.020.205 I ggml_metal_init: allocating
0.00.020.209 I ggml_metal_init: found device: Apple M4
0.00.020.211 I ggml_metal_init: picking default device: Apple M4
0.00.020.840 I ggml_metal_init: using embedded metal library
0.00.023.321 I ggml_metal_init: GPU name:   Apple M4
0.00.023.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.323 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.324 I ggml_metal_init: simdgroup reduction   = true
0.00.023.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.324 I ggml_metal_init: has bfloat            = true
0.00.023.324 I ggml_metal_init: use bfloat            = true
0.00.023.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.874 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.877 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.878 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.437 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.438 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.438 I llama_new_context_with_model: graph nodes  = 429
0.00.034.438 I llama_new_context_with_model: graph splits = 2
0.00.034.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.434 I 
0.00.039.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.979 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.395 I llama_perf_context_print:        load time =      30.30 ms
0.00.044.396 I llama_perf_context_print: prompt eval time =       4.29 ms /     9 tokens (    0.48 ms per token,  2099.86 tokens per second)
0.00.044.397 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.397 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.044.582 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.030s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.125 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.615 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.039.606 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.617 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.039.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.619 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.039.628 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.039.629 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.039.630 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.039.631 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.039.631 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.039.632 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.039.633 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.039.637 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.039.638 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.039.638 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.039.639 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.640 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.047.890 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.050.495 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.055.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.055.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.055.756 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.055.757 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.055.757 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.055.757 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.055.758 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.055.758 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.055.759 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.055.759 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.055.760 I llama_model_loader: - type  f32:   41 tensors
0.00.055.760 I llama_model_loader: - type  f16:   29 tensors
0.00.074.805 W llm_load_vocab: empty token at index 5
0.00.079.560 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.080.913 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.080.940 I llm_load_vocab: special tokens cache size = 5
0.00.339.122 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.339.139 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.339.139 I llm_load_print_meta: arch             = jina-bert-v2
0.00.339.140 I llm_load_print_meta: vocab type       = BPE
0.00.339.141 I llm_load_print_meta: n_vocab          = 61056
0.00.339.141 I llm_load_print_meta: n_merges         = 39382
0.00.339.141 I llm_load_print_meta: vocab_only       = 0
0.00.339.141 I llm_load_print_meta: n_ctx_train      = 8192
0.00.339.141 I llm_load_print_meta: n_embd           = 384
0.00.339.141 I llm_load_print_meta: n_layer          = 4
0.00.339.148 I llm_load_print_meta: n_head           = 12
0.00.339.149 I llm_load_print_meta: n_head_kv        = 12
0.00.339.169 I llm_load_print_meta: n_rot            = 32
0.00.339.170 I llm_load_print_meta: n_swa            = 0
0.00.339.170 I llm_load_print_meta: n_embd_head_k    = 32
0.00.339.170 I llm_load_print_meta: n_embd_head_v    = 32
0.00.339.170 I llm_load_print_meta: n_gqa            = 1
0.00.339.171 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.339.171 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.339.172 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.339.176 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.339.177 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.339.177 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.339.178 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.339.178 I llm_load_print_meta: n_ff             = 1536
0.00.339.179 I llm_load_print_meta: n_expert         = 0
0.00.339.179 I llm_load_print_meta: n_expert_used    = 0
0.00.339.179 I llm_load_print_meta: causal attn      = 0
0.00.339.179 I llm_load_print_meta: pooling type     = -1
0.00.339.179 I llm_load_print_meta: rope type        = -1
0.00.339.179 I llm_load_print_meta: rope scaling     = linear
0.00.339.179 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.339.180 I llm_load_print_meta: freq_scale_train = 1
0.00.339.180 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.339.180 I llm_load_print_meta: rope_finetuned   = unknown
0.00.339.180 I llm_load_print_meta: ssm_d_conv       = 0
0.00.339.180 I llm_load_print_meta: ssm_d_inner      = 0
0.00.339.180 I llm_load_print_meta: ssm_d_state      = 0
0.00.339.181 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.339.181 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.339.196 I llm_load_print_meta: model type       = 33M
0.00.339.198 I llm_load_print_meta: model ftype      = F16
0.00.339.198 I llm_load_print_meta: model params     = 32.90 M
0.00.339.198 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.339.199 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.339.200 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.339.200 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.339.200 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.339.200 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.339.201 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.339.201 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.339.201 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.339.201 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.339.201 I llm_load_print_meta: max token length = 45
0.00.340.410 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.340.410 I llm_load_tensors: offloading output layer to GPU
0.00.340.410 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.340.430 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.340.431 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.341.476 I llama_new_context_with_model: n_seq_max     = 1
0.00.341.477 I llama_new_context_with_model: n_ctx         = 8192
0.00.341.477 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.341.477 I llama_new_context_with_model: n_batch       = 2048
0.00.341.478 I llama_new_context_with_model: n_ubatch      = 2048
0.00.341.478 I llama_new_context_with_model: flash_attn    = 0
0.00.341.478 I llama_new_context_with_model: freq_base     = 10000.0
0.00.341.478 I llama_new_context_with_model: freq_scale    = 1
0.00.341.479 I ggml_metal_init: allocating
0.00.341.482 I ggml_metal_init: found device: Apple M4
0.00.341.483 I ggml_metal_init: picking default device: Apple M4
0.00.342.323 I ggml_metal_init: using embedded metal library
0.00.345.368 I ggml_metal_init: GPU name:   Apple M4
0.00.345.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.370 I ggml_metal_init: simdgroup reduction   = true
0.00.345.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.371 I ggml_metal_init: has bfloat            = true
0.00.345.371 I ggml_metal_init: use bfloat            = true
0.00.345.371 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.372 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.357.346 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.349 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.350 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.357.913 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.357.914 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.357.914 I llama_new_context_with_model: graph nodes  = 154
0.00.357.914 I llama_new_context_with_model: graph splits = 2
0.00.357.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.369.590 I 
0.00.369.626 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.369.894 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.369.895 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.369.910 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.369.910 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.369.915 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.369.918 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.370.478 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.374.209 I llama_perf_context_print:        load time =     340.96 ms
0.00.374.210 I llama_perf_context_print: prompt eval time =       3.72 ms /    62 tokens (    0.06 ms per token, 16657.71 tokens per second)
0.00.374.211 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.374.214 I llama_perf_context_print:       total time =       4.62 ms /    63 tokens
0.00.374.382 I ggml_metal_free: deallocating

real	0m1.046s
user	0m0.341s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.138 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.299 I main: llama backend init
0.00.000.308 I main: load the model and apply lora adapter, if any
0.00.034.115 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.131 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.151 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.152 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.153 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.153 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.155 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.156 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.157 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.157 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.158 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.158 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.160 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.164 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.059.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.067.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.687 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.689 I llama_model_loader: - type  f32:  194 tensors
0.00.067.689 I llama_model_loader: - type  f16:   98 tensors
0.00.098.002 I llm_load_vocab: special tokens cache size = 25
0.00.104.805 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.104.808 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.104.808 I llm_load_print_meta: arch             = gptneox
0.00.104.808 I llm_load_print_meta: vocab type       = BPE
0.00.104.808 I llm_load_print_meta: n_vocab          = 50304
0.00.104.809 I llm_load_print_meta: n_merges         = 50009
0.00.104.809 I llm_load_print_meta: vocab_only       = 0
0.00.104.809 I llm_load_print_meta: n_ctx_train      = 2048
0.00.104.809 I llm_load_print_meta: n_embd           = 2048
0.00.104.809 I llm_load_print_meta: n_layer          = 24
0.00.104.812 I llm_load_print_meta: n_head           = 16
0.00.104.813 I llm_load_print_meta: n_head_kv        = 16
0.00.104.832 I llm_load_print_meta: n_rot            = 32
0.00.104.833 I llm_load_print_meta: n_swa            = 0
0.00.104.833 I llm_load_print_meta: n_embd_head_k    = 128
0.00.104.833 I llm_load_print_meta: n_embd_head_v    = 128
0.00.104.834 I llm_load_print_meta: n_gqa            = 1
0.00.104.834 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.104.835 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.104.836 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.104.836 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.104.836 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.104.836 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.104.836 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.104.837 I llm_load_print_meta: n_ff             = 8192
0.00.104.837 I llm_load_print_meta: n_expert         = 0
0.00.104.837 I llm_load_print_meta: n_expert_used    = 0
0.00.104.837 I llm_load_print_meta: causal attn      = 1
0.00.104.837 I llm_load_print_meta: pooling type     = 0
0.00.104.838 I llm_load_print_meta: rope type        = 2
0.00.104.838 I llm_load_print_meta: rope scaling     = linear
0.00.104.838 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.104.840 I llm_load_print_meta: freq_scale_train = 1
0.00.104.840 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.104.841 I llm_load_print_meta: rope_finetuned   = unknown
0.00.104.841 I llm_load_print_meta: ssm_d_conv       = 0
0.00.104.841 I llm_load_print_meta: ssm_d_inner      = 0
0.00.104.841 I llm_load_print_meta: ssm_d_state      = 0
0.00.104.841 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.104.842 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.104.852 I llm_load_print_meta: model type       = 1.4B
0.00.104.853 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.104.853 I llm_load_print_meta: model params     = 1.41 B
0.00.104.854 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.104.854 I llm_load_print_meta: general.name     = 1.4B
0.00.104.854 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.104.854 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.104.854 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.104.855 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.104.855 I llm_load_print_meta: LF token         = 128 ''
0.00.104.855 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.104.855 I llm_load_print_meta: max token length = 1024
0.00.107.405 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.406 I llm_load_tensors: offloading output layer to GPU
0.00.107.406 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.424 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.425 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.108.370 I llama_new_context_with_model: n_seq_max     = 1
0.00.108.371 I llama_new_context_with_model: n_ctx         = 2048
0.00.108.371 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.108.371 I llama_new_context_with_model: n_batch       = 2048
0.00.108.371 I llama_new_context_with_model: n_ubatch      = 512
0.00.108.372 I llama_new_context_with_model: flash_attn    = 0
0.00.108.372 I llama_new_context_with_model: freq_base     = 10000.0
0.00.108.372 I llama_new_context_with_model: freq_scale    = 1
0.00.108.373 I ggml_metal_init: allocating
0.00.108.376 I ggml_metal_init: found device: Apple M4
0.00.108.378 I ggml_metal_init: picking default device: Apple M4
0.00.109.075 I ggml_metal_init: using embedded metal library
0.00.161.427 I ggml_metal_init: GPU name:   Apple M4
0.00.161.433 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.161.434 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.161.434 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.161.434 I ggml_metal_init: simdgroup reduction   = true
0.00.161.434 I ggml_metal_init: simdgroup matrix mul. = true
0.00.161.435 I ggml_metal_init: has bfloat            = true
0.00.161.435 I ggml_metal_init: use bfloat            = true
0.00.161.435 I ggml_metal_init: hasUnifiedMemory      = true
0.00.161.437 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.269.589 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.269.595 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.269.626 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.270.890 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.270.891 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.270.892 I llama_new_context_with_model: graph nodes  = 967
0.00.270.892 I llama_new_context_with_model: graph splits = 2
0.00.270.917 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.363.115 I main: llama threadpool init, n_threads = 4
0.00.363.154 I 
0.00.363.190 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.363.191 I 
0.00.363.273 I sampler seed: 1234
0.00.363.277 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.363.304 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.363.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.363.305 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.204.518 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57211.93 tokens per second)
0.02.204.519 I llama_perf_context_print:        load time =     328.99 ms
0.02.204.519 I llama_perf_context_print: prompt eval time =      43.91 ms /     7 tokens (    6.27 ms per token,   159.42 tokens per second)
0.02.204.520 I llama_perf_context_print:        eval time =    1794.42 ms /    63 runs   (   28.48 ms per token,    35.11 tokens per second)
0.02.204.521 I llama_perf_context_print:       total time =    1841.40 ms /    70 tokens
0.02.204.756 I ggml_metal_free: deallocating

real	0m2.504s
user	0m0.157s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.554 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.470 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.490 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.498 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.499 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.500 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.501 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.502 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.503 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.442 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.910 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.315 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.316 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.316 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.317 I llama_model_loader: - type  f32:  194 tensors
0.00.056.318 I llama_model_loader: - type  f16:   98 tensors
0.00.087.714 I llm_load_vocab: special tokens cache size = 25
0.00.094.609 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.612 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.613 I llm_load_print_meta: arch             = gptneox
0.00.094.613 I llm_load_print_meta: vocab type       = BPE
0.00.094.613 I llm_load_print_meta: n_vocab          = 50304
0.00.094.613 I llm_load_print_meta: n_merges         = 50009
0.00.094.613 I llm_load_print_meta: vocab_only       = 0
0.00.094.614 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.614 I llm_load_print_meta: n_embd           = 2048
0.00.094.614 I llm_load_print_meta: n_layer          = 24
0.00.094.617 I llm_load_print_meta: n_head           = 16
0.00.094.617 I llm_load_print_meta: n_head_kv        = 16
0.00.094.629 I llm_load_print_meta: n_rot            = 32
0.00.094.630 I llm_load_print_meta: n_swa            = 0
0.00.094.630 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.630 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.630 I llm_load_print_meta: n_gqa            = 1
0.00.094.631 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.632 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.632 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.632 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.633 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.633 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.633 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.634 I llm_load_print_meta: n_ff             = 8192
0.00.094.634 I llm_load_print_meta: n_expert         = 0
0.00.094.634 I llm_load_print_meta: n_expert_used    = 0
0.00.094.634 I llm_load_print_meta: causal attn      = 1
0.00.094.634 I llm_load_print_meta: pooling type     = 0
0.00.094.634 I llm_load_print_meta: rope type        = 2
0.00.094.634 I llm_load_print_meta: rope scaling     = linear
0.00.094.635 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.635 I llm_load_print_meta: freq_scale_train = 1
0.00.094.635 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.635 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.636 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.636 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.636 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.636 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.638 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.648 I llm_load_print_meta: model type       = 1.4B
0.00.094.648 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.649 I llm_load_print_meta: model params     = 1.41 B
0.00.094.649 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.649 I llm_load_print_meta: general.name     = 1.4B
0.00.094.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.650 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.650 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.650 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.651 I llm_load_print_meta: LF token         = 128 ''
0.00.094.651 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.651 I llm_load_print_meta: max token length = 1024
0.00.097.194 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.194 I llm_load_tensors: offloading output layer to GPU
0.00.097.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.205 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.206 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.151 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.152 I llama_new_context_with_model: n_ctx         = 128
0.00.098.152 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.152 I llama_new_context_with_model: n_batch       = 128
0.00.098.152 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.152 I llama_new_context_with_model: flash_attn    = 0
0.00.098.153 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.153 I llama_new_context_with_model: freq_scale    = 1
0.00.098.153 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.154 I ggml_metal_init: allocating
0.00.098.163 I ggml_metal_init: found device: Apple M4
0.00.098.165 I ggml_metal_init: picking default device: Apple M4
0.00.098.795 I ggml_metal_init: using embedded metal library
0.00.101.397 I ggml_metal_init: GPU name:   Apple M4
0.00.101.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.400 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.400 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.400 I ggml_metal_init: simdgroup reduction   = true
0.00.101.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.401 I ggml_metal_init: has bfloat            = true
0.00.101.401 I ggml_metal_init: use bfloat            = true
0.00.101.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.486 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.112.488 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.112.501 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.328 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.113.329 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.113.329 I llama_new_context_with_model: graph nodes  = 967
0.00.113.329 I llama_new_context_with_model: graph splits = 2
0.00.113.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.998.405 I 
0.00.998.443 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.998.463 I perplexity: tokenizing the input ..
0.01.010.320 I perplexity: tokenization took 11.855 ms
0.01.010.345 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.143.545 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.145.117 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.145.141 I llama_perf_context_print:        load time =     972.67 ms
0.01.145.142 I llama_perf_context_print: prompt eval time =     132.78 ms /   128 tokens (    1.04 ms per token,   964.02 tokens per second)
0.01.145.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.145.144 I llama_perf_context_print:       total time =     146.73 ms /   129 tokens
0.01.145.797 I ggml_metal_free: deallocating

real	0m1.337s
user	0m0.122s
sys	0m0.201s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.841 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.459 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.463 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.465 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.466 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.467 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.467 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.468 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.471 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.471 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.500 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.444 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.444 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.445 I llama_model_loader: - type  f32:  194 tensors
0.00.025.445 I llama_model_loader: - type q8_0:   98 tensors
0.00.047.186 I llm_load_vocab: special tokens cache size = 25
0.00.053.180 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.184 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.185 I llm_load_print_meta: arch             = gptneox
0.00.053.185 I llm_load_print_meta: vocab type       = BPE
0.00.053.185 I llm_load_print_meta: n_vocab          = 50304
0.00.053.185 I llm_load_print_meta: n_merges         = 50009
0.00.053.186 I llm_load_print_meta: vocab_only       = 0
0.00.053.186 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.186 I llm_load_print_meta: n_embd           = 2048
0.00.053.188 I llm_load_print_meta: n_layer          = 24
0.00.053.194 I llm_load_print_meta: n_head           = 16
0.00.053.195 I llm_load_print_meta: n_head_kv        = 16
0.00.053.209 I llm_load_print_meta: n_rot            = 32
0.00.053.210 I llm_load_print_meta: n_swa            = 0
0.00.053.210 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.210 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.210 I llm_load_print_meta: n_gqa            = 1
0.00.053.211 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.212 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.212 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.213 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.214 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.216 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.216 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.217 I llm_load_print_meta: n_ff             = 8192
0.00.053.218 I llm_load_print_meta: n_expert         = 0
0.00.053.218 I llm_load_print_meta: n_expert_used    = 0
0.00.053.218 I llm_load_print_meta: causal attn      = 1
0.00.053.218 I llm_load_print_meta: pooling type     = 0
0.00.053.220 I llm_load_print_meta: rope type        = 2
0.00.053.220 I llm_load_print_meta: rope scaling     = linear
0.00.053.220 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.221 I llm_load_print_meta: freq_scale_train = 1
0.00.053.221 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.221 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.221 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.222 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.222 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.222 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.222 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.231 I llm_load_print_meta: model type       = 1.4B
0.00.053.232 I llm_load_print_meta: model ftype      = Q8_0
0.00.053.232 I llm_load_print_meta: model params     = 1.41 B
0.00.053.233 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.053.233 I llm_load_print_meta: general.name     = 1.4B
0.00.053.233 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.233 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.233 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.233 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.234 I llm_load_print_meta: LF token         = 128 ''
0.00.053.234 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.234 I llm_load_print_meta: max token length = 1024
0.00.055.567 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.567 I llm_load_tensors: offloading output layer to GPU
0.00.055.568 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.579 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.055.580 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.056.581 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.582 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.582 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.582 I llama_new_context_with_model: n_batch       = 2048
0.00.056.582 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.582 I llama_new_context_with_model: flash_attn    = 0
0.00.056.583 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.583 I llama_new_context_with_model: freq_scale    = 1
0.00.056.584 I ggml_metal_init: allocating
0.00.056.587 I ggml_metal_init: found device: Apple M4
0.00.056.589 I ggml_metal_init: picking default device: Apple M4
0.00.057.302 I ggml_metal_init: using embedded metal library
0.00.059.812 I ggml_metal_init: GPU name:   Apple M4
0.00.059.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.814 I ggml_metal_init: simdgroup reduction   = true
0.00.059.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.815 I ggml_metal_init: has bfloat            = true
0.00.059.815 I ggml_metal_init: use bfloat            = true
0.00.059.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.816 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.328 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.337 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.365 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.578 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.580 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.581 I llama_new_context_with_model: graph nodes  = 967
0.00.095.581 I llama_new_context_with_model: graph splits = 2
0.00.095.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.155.984 I main: llama threadpool init, n_threads = 4
0.01.156.018 I 
0.01.156.049 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.156.050 I 
0.01.156.272 I sampler seed: 1234
0.01.156.277 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.156.288 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.156.289 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.156.289 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.248.684 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.02.248.684 I llama_perf_context_print:        load time =    1146.14 ms
0.02.248.685 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   159.99 tokens per second)
0.02.248.686 I llama_perf_context_print:        eval time =    1045.56 ms /    63 runs   (   16.60 ms per token,    60.25 tokens per second)
0.02.248.686 I llama_perf_context_print:       total time =    1092.70 ms /    70 tokens
0.02.248.880 I ggml_metal_free: deallocating

real	0m2.268s
user	0m0.113s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.109 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.439 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.441 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.442 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.443 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.444 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.445 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.445 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.446 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.448 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.448 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.448 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.197 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.341 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.342 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.342 I llama_model_loader: - type  f32:  194 tensors
0.00.029.343 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.357 I llm_load_vocab: special tokens cache size = 25
0.00.058.404 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.407 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.408 I llm_load_print_meta: arch             = gptneox
0.00.058.408 I llm_load_print_meta: vocab type       = BPE
0.00.058.408 I llm_load_print_meta: n_vocab          = 50304
0.00.058.408 I llm_load_print_meta: n_merges         = 50009
0.00.058.409 I llm_load_print_meta: vocab_only       = 0
0.00.058.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.409 I llm_load_print_meta: n_embd           = 2048
0.00.058.411 I llm_load_print_meta: n_layer          = 24
0.00.058.414 I llm_load_print_meta: n_head           = 16
0.00.058.415 I llm_load_print_meta: n_head_kv        = 16
0.00.058.428 I llm_load_print_meta: n_rot            = 32
0.00.058.429 I llm_load_print_meta: n_swa            = 0
0.00.058.429 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.429 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.430 I llm_load_print_meta: n_gqa            = 1
0.00.058.430 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.431 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.432 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.432 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.432 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.432 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.432 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.433 I llm_load_print_meta: n_ff             = 8192
0.00.058.433 I llm_load_print_meta: n_expert         = 0
0.00.058.434 I llm_load_print_meta: n_expert_used    = 0
0.00.058.434 I llm_load_print_meta: causal attn      = 1
0.00.058.434 I llm_load_print_meta: pooling type     = 0
0.00.058.434 I llm_load_print_meta: rope type        = 2
0.00.058.434 I llm_load_print_meta: rope scaling     = linear
0.00.058.434 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.435 I llm_load_print_meta: freq_scale_train = 1
0.00.058.435 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.437 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.437 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.437 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.437 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.437 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.437 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.448 I llm_load_print_meta: model type       = 1.4B
0.00.058.448 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.448 I llm_load_print_meta: model params     = 1.41 B
0.00.058.449 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.449 I llm_load_print_meta: general.name     = 1.4B
0.00.058.449 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.449 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.450 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.450 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.450 I llm_load_print_meta: LF token         = 128 ''
0.00.058.450 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.450 I llm_load_print_meta: max token length = 1024
0.00.060.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.729 I llm_load_tensors: offloading output layer to GPU
0.00.060.729 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.741 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.742 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.061.715 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.716 I llama_new_context_with_model: n_ctx         = 128
0.00.061.716 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.061.716 I llama_new_context_with_model: n_batch       = 128
0.00.061.717 I llama_new_context_with_model: n_ubatch      = 128
0.00.061.717 I llama_new_context_with_model: flash_attn    = 0
0.00.061.717 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.718 I llama_new_context_with_model: freq_scale    = 1
0.00.061.718 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.718 I ggml_metal_init: allocating
0.00.061.722 I ggml_metal_init: found device: Apple M4
0.00.061.724 I ggml_metal_init: picking default device: Apple M4
0.00.062.373 I ggml_metal_init: using embedded metal library
0.00.064.801 I ggml_metal_init: GPU name:   Apple M4
0.00.064.803 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.803 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.804 I ggml_metal_init: simdgroup reduction   = true
0.00.064.804 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.804 I ggml_metal_init: has bfloat            = true
0.00.064.804 I ggml_metal_init: use bfloat            = true
0.00.064.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.386 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.388 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.407 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.398 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.399 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.399 I llama_new_context_with_model: graph nodes  = 967
0.00.077.400 I llama_new_context_with_model: graph splits = 2
0.00.077.412 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.030 I 
0.00.933.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.063 I perplexity: tokenizing the input ..
0.00.941.163 I perplexity: tokenization took 8.099 ms
0.00.941.174 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.065.649 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.066.871 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.066.883 I llama_perf_context_print:        load time =     921.92 ms
0.01.066.884 I llama_perf_context_print: prompt eval time =     124.25 ms /   128 tokens (    0.97 ms per token,  1030.21 tokens per second)
0.01.066.885 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.066.885 I llama_perf_context_print:       total time =     133.85 ms /   129 tokens
0.01.067.179 I ggml_metal_free: deallocating

real	0m1.083s
user	0m0.086s
sys	0m0.155s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.010.846 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.705 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.713 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.713 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.714 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.718 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.718 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.719 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.720 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.720 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.721 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.721 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.725 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.725 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.142 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.303 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.304 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.305 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.305 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.307 I llama_model_loader: - type  f32:  194 tensors
0.00.027.307 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.307 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.364 I llm_load_vocab: special tokens cache size = 25
0.00.054.419 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.422 I llm_load_print_meta: arch             = gptneox
0.00.054.423 I llm_load_print_meta: vocab type       = BPE
0.00.054.423 I llm_load_print_meta: n_vocab          = 50304
0.00.054.423 I llm_load_print_meta: n_merges         = 50009
0.00.054.423 I llm_load_print_meta: vocab_only       = 0
0.00.054.424 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.424 I llm_load_print_meta: n_embd           = 2048
0.00.054.424 I llm_load_print_meta: n_layer          = 24
0.00.054.429 I llm_load_print_meta: n_head           = 16
0.00.054.429 I llm_load_print_meta: n_head_kv        = 16
0.00.054.443 I llm_load_print_meta: n_rot            = 32
0.00.054.444 I llm_load_print_meta: n_swa            = 0
0.00.054.444 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.444 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.445 I llm_load_print_meta: n_gqa            = 1
0.00.054.446 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.446 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.447 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.448 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.448 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.448 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.449 I llm_load_print_meta: n_ff             = 8192
0.00.054.449 I llm_load_print_meta: n_expert         = 0
0.00.054.449 I llm_load_print_meta: n_expert_used    = 0
0.00.054.451 I llm_load_print_meta: causal attn      = 1
0.00.054.453 I llm_load_print_meta: pooling type     = 0
0.00.054.453 I llm_load_print_meta: rope type        = 2
0.00.054.453 I llm_load_print_meta: rope scaling     = linear
0.00.054.454 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.455 I llm_load_print_meta: freq_scale_train = 1
0.00.054.455 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.455 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.455 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.455 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.455 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.456 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.456 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.466 I llm_load_print_meta: model type       = 1.4B
0.00.054.466 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.467 I llm_load_print_meta: model params     = 1.41 B
0.00.054.467 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.467 I llm_load_print_meta: general.name     = 1.4B
0.00.054.468 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.468 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.468 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.468 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.468 I llm_load_print_meta: LF token         = 128 ''
0.00.054.469 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.469 I llm_load_print_meta: max token length = 1024
0.00.056.797 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.797 I llm_load_tensors: offloading output layer to GPU
0.00.056.798 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.810 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.811 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.890 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.891 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.891 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.891 I llama_new_context_with_model: n_batch       = 2048
0.00.057.891 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.891 I llama_new_context_with_model: flash_attn    = 0
0.00.057.892 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.892 I llama_new_context_with_model: freq_scale    = 1
0.00.057.893 I ggml_metal_init: allocating
0.00.057.900 I ggml_metal_init: found device: Apple M4
0.00.057.902 I ggml_metal_init: picking default device: Apple M4
0.00.058.675 I ggml_metal_init: using embedded metal library
0.00.061.246 I ggml_metal_init: GPU name:   Apple M4
0.00.061.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.248 I ggml_metal_init: simdgroup reduction   = true
0.00.061.248 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.248 I ggml_metal_init: has bfloat            = true
0.00.061.250 I ggml_metal_init: use bfloat            = true
0.00.061.250 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.096.661 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.672 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.706 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.886 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.889 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.889 I llama_new_context_with_model: graph nodes  = 967
0.00.097.889 I llama_new_context_with_model: graph splits = 2
0.00.097.906 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.077 I main: llama threadpool init, n_threads = 4
0.00.699.119 I 
0.00.699.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.699.151 I 
0.00.699.381 I sampler seed: 1234
0.00.699.385 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.426 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.431 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.432 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.378.879 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.378.879 I llama_perf_context_print:        load time =     688.23 ms
0.01.378.880 I llama_perf_context_print: prompt eval time =      43.13 ms /     7 tokens (    6.16 ms per token,   162.30 tokens per second)
0.01.378.881 I llama_perf_context_print:        eval time =     633.08 ms /    63 runs   (   10.05 ms per token,    99.51 tokens per second)
0.01.378.881 I llama_perf_context_print:       total time =     679.80 ms /    70 tokens
0.01.379.082 I ggml_metal_free: deallocating

real	0m1.398s
user	0m0.111s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.712 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.407 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.410 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.411 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.411 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.412 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.414 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.414 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.402 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.333 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.334 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.335 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.335 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.335 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.336 I llama_model_loader: - type  f32:  194 tensors
0.00.024.337 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.337 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.146 I llm_load_vocab: special tokens cache size = 25
0.00.050.144 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.147 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.147 I llm_load_print_meta: arch             = gptneox
0.00.050.148 I llm_load_print_meta: vocab type       = BPE
0.00.050.148 I llm_load_print_meta: n_vocab          = 50304
0.00.050.148 I llm_load_print_meta: n_merges         = 50009
0.00.050.148 I llm_load_print_meta: vocab_only       = 0
0.00.050.149 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.149 I llm_load_print_meta: n_embd           = 2048
0.00.050.149 I llm_load_print_meta: n_layer          = 24
0.00.050.151 I llm_load_print_meta: n_head           = 16
0.00.050.152 I llm_load_print_meta: n_head_kv        = 16
0.00.050.159 I llm_load_print_meta: n_rot            = 32
0.00.050.159 I llm_load_print_meta: n_swa            = 0
0.00.050.159 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.160 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.160 I llm_load_print_meta: n_gqa            = 1
0.00.050.161 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.162 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.162 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.163 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.163 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.163 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.163 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.164 I llm_load_print_meta: n_ff             = 8192
0.00.050.164 I llm_load_print_meta: n_expert         = 0
0.00.050.164 I llm_load_print_meta: n_expert_used    = 0
0.00.050.165 I llm_load_print_meta: causal attn      = 1
0.00.050.165 I llm_load_print_meta: pooling type     = 0
0.00.050.165 I llm_load_print_meta: rope type        = 2
0.00.050.165 I llm_load_print_meta: rope scaling     = linear
0.00.050.165 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.166 I llm_load_print_meta: freq_scale_train = 1
0.00.050.166 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.166 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.166 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.166 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.167 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.167 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.168 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.172 I llm_load_print_meta: model type       = 1.4B
0.00.050.173 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.173 I llm_load_print_meta: model params     = 1.41 B
0.00.050.174 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.175 I llm_load_print_meta: general.name     = 1.4B
0.00.050.176 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.176 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.176 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.176 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.177 I llm_load_print_meta: LF token         = 128 ''
0.00.050.177 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.177 I llm_load_print_meta: max token length = 1024
0.00.051.905 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.906 I llm_load_tensors: offloading output layer to GPU
0.00.051.906 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.912 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.913 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.829 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.830 I llama_new_context_with_model: n_ctx         = 128
0.00.052.830 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.830 I llama_new_context_with_model: n_batch       = 128
0.00.052.830 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.830 I llama_new_context_with_model: flash_attn    = 0
0.00.052.831 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.831 I llama_new_context_with_model: freq_scale    = 1
0.00.052.832 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.832 I ggml_metal_init: allocating
0.00.052.838 I ggml_metal_init: found device: Apple M4
0.00.052.840 I ggml_metal_init: picking default device: Apple M4
0.00.053.420 I ggml_metal_init: using embedded metal library
0.00.055.746 I ggml_metal_init: GPU name:   Apple M4
0.00.055.748 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.749 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.749 I ggml_metal_init: simdgroup reduction   = true
0.00.055.749 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.749 I ggml_metal_init: has bfloat            = true
0.00.055.749 I ggml_metal_init: use bfloat            = true
0.00.055.750 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.751 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.591 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.594 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.607 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.513 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.515 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.515 I llama_new_context_with_model: graph nodes  = 967
0.00.067.515 I llama_new_context_with_model: graph splits = 2
0.00.067.522 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.626.071 I 
0.00.626.108 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.626.116 I perplexity: tokenizing the input ..
0.00.633.937 I perplexity: tokenization took 7.819 ms
0.00.633.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.756.874 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.758.334 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.758.360 I llama_perf_context_print:        load time =     616.36 ms
0.00.758.361 I llama_perf_context_print: prompt eval time =     122.70 ms /   128 tokens (    0.96 ms per token,  1043.15 tokens per second)
0.00.758.362 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.363 I llama_perf_context_print:       total time =     132.29 ms /   129 tokens
0.00.758.901 I ggml_metal_free: deallocating

real	0m0.774s
user	0m0.077s
sys	0m0.098s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.667 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.778 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.783 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.790 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.797 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.798 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.798 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.800 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.800 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.685 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.649 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.650 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.651 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.652 I llama_model_loader: - type  f32:  194 tensors
0.00.024.652 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.653 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.414 I llm_load_vocab: special tokens cache size = 25
0.00.051.294 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.296 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.297 I llm_load_print_meta: arch             = gptneox
0.00.051.297 I llm_load_print_meta: vocab type       = BPE
0.00.051.297 I llm_load_print_meta: n_vocab          = 50304
0.00.051.297 I llm_load_print_meta: n_merges         = 50009
0.00.051.298 I llm_load_print_meta: vocab_only       = 0
0.00.051.298 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.298 I llm_load_print_meta: n_embd           = 2048
0.00.051.298 I llm_load_print_meta: n_layer          = 24
0.00.051.300 I llm_load_print_meta: n_head           = 16
0.00.051.301 I llm_load_print_meta: n_head_kv        = 16
0.00.051.313 I llm_load_print_meta: n_rot            = 32
0.00.051.313 I llm_load_print_meta: n_swa            = 0
0.00.051.313 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.317 I llm_load_print_meta: n_gqa            = 1
0.00.051.318 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.318 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.319 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.319 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.319 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.320 I llm_load_print_meta: n_ff             = 8192
0.00.051.320 I llm_load_print_meta: n_expert         = 0
0.00.051.320 I llm_load_print_meta: n_expert_used    = 0
0.00.051.320 I llm_load_print_meta: causal attn      = 1
0.00.051.321 I llm_load_print_meta: pooling type     = 0
0.00.051.321 I llm_load_print_meta: rope type        = 2
0.00.051.321 I llm_load_print_meta: rope scaling     = linear
0.00.051.321 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.321 I llm_load_print_meta: freq_scale_train = 1
0.00.051.322 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.322 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.322 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.322 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.322 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.322 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.322 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.332 I llm_load_print_meta: model type       = 1.4B
0.00.051.333 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.333 I llm_load_print_meta: model params     = 1.41 B
0.00.051.334 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.334 I llm_load_print_meta: general.name     = 1.4B
0.00.051.334 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.335 I llm_load_print_meta: LF token         = 128 ''
0.00.051.335 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.335 I llm_load_print_meta: max token length = 1024
0.00.053.375 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.375 I llm_load_tensors: offloading output layer to GPU
0.00.053.375 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.386 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.387 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.315 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.315 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.315 I llama_new_context_with_model: n_batch       = 2048
0.00.054.316 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.316 I llama_new_context_with_model: flash_attn    = 0
0.00.054.316 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.317 I llama_new_context_with_model: freq_scale    = 1
0.00.054.317 I ggml_metal_init: allocating
0.00.054.323 I ggml_metal_init: found device: Apple M4
0.00.054.325 I ggml_metal_init: picking default device: Apple M4
0.00.054.935 I ggml_metal_init: using embedded metal library
0.00.057.227 I ggml_metal_init: GPU name:   Apple M4
0.00.057.229 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.229 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.229 I ggml_metal_init: simdgroup reduction   = true
0.00.057.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.230 I ggml_metal_init: has bfloat            = true
0.00.057.230 I ggml_metal_init: use bfloat            = true
0.00.057.230 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.280 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.289 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.307 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.338 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.340 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.340 I llama_new_context_with_model: graph nodes  = 967
0.00.087.341 I llama_new_context_with_model: graph splits = 2
0.00.087.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.787 I main: llama threadpool init, n_threads = 4
0.00.731.823 I 
0.00.731.853 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.853 I 
0.00.732.004 I sampler seed: 1234
0.00.732.008 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.018 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.018 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.018 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.456.821 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63848.92 tokens per second)
0.01.456.821 I llama_perf_context_print:        load time =     723.12 ms
0.01.456.822 I llama_perf_context_print: prompt eval time =      39.58 ms /     7 tokens (    5.65 ms per token,   176.84 tokens per second)
0.01.456.823 I llama_perf_context_print:        eval time =     682.27 ms /    63 runs   (   10.83 ms per token,    92.34 tokens per second)
0.01.456.826 I llama_perf_context_print:       total time =     725.04 ms /    70 tokens
0.01.457.017 I ggml_metal_free: deallocating

real	0m1.473s
user	0m0.109s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.167 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.970 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.974 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.976 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.976 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.982 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.982 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.982 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.985 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.985 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.990 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.991 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.991 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.832 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.624 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.625 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.626 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.626 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.627 I llama_model_loader: - type  f32:  194 tensors
0.00.023.627 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.627 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.527 I llm_load_vocab: special tokens cache size = 25
0.00.049.513 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.516 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.516 I llm_load_print_meta: arch             = gptneox
0.00.049.516 I llm_load_print_meta: vocab type       = BPE
0.00.049.517 I llm_load_print_meta: n_vocab          = 50304
0.00.049.517 I llm_load_print_meta: n_merges         = 50009
0.00.049.517 I llm_load_print_meta: vocab_only       = 0
0.00.049.517 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.517 I llm_load_print_meta: n_embd           = 2048
0.00.049.517 I llm_load_print_meta: n_layer          = 24
0.00.049.520 I llm_load_print_meta: n_head           = 16
0.00.049.521 I llm_load_print_meta: n_head_kv        = 16
0.00.049.533 I llm_load_print_meta: n_rot            = 32
0.00.049.533 I llm_load_print_meta: n_swa            = 0
0.00.049.533 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.534 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.534 I llm_load_print_meta: n_gqa            = 1
0.00.049.535 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.536 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.536 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.537 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.537 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.537 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.537 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.538 I llm_load_print_meta: n_ff             = 8192
0.00.049.538 I llm_load_print_meta: n_expert         = 0
0.00.049.538 I llm_load_print_meta: n_expert_used    = 0
0.00.049.538 I llm_load_print_meta: causal attn      = 1
0.00.049.538 I llm_load_print_meta: pooling type     = 0
0.00.049.538 I llm_load_print_meta: rope type        = 2
0.00.049.539 I llm_load_print_meta: rope scaling     = linear
0.00.049.540 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.542 I llm_load_print_meta: freq_scale_train = 1
0.00.049.542 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.542 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.542 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.542 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.543 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.543 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.543 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.552 I llm_load_print_meta: model type       = 1.4B
0.00.049.553 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.553 I llm_load_print_meta: model params     = 1.41 B
0.00.049.553 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.554 I llm_load_print_meta: general.name     = 1.4B
0.00.049.554 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.554 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.554 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.554 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.555 I llm_load_print_meta: LF token         = 128 ''
0.00.049.556 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.556 I llm_load_print_meta: max token length = 1024
0.00.051.510 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.510 I llm_load_tensors: offloading output layer to GPU
0.00.051.510 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.521 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.522 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.462 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.462 I llama_new_context_with_model: n_ctx         = 128
0.00.052.463 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.463 I llama_new_context_with_model: n_batch       = 128
0.00.052.463 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.463 I llama_new_context_with_model: flash_attn    = 0
0.00.052.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.464 I llama_new_context_with_model: freq_scale    = 1
0.00.052.464 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.465 I ggml_metal_init: allocating
0.00.052.468 I ggml_metal_init: found device: Apple M4
0.00.052.470 I ggml_metal_init: picking default device: Apple M4
0.00.053.031 I ggml_metal_init: using embedded metal library
0.00.055.332 I ggml_metal_init: GPU name:   Apple M4
0.00.055.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.334 I ggml_metal_init: simdgroup reduction   = true
0.00.055.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.335 I ggml_metal_init: has bfloat            = true
0.00.055.335 I ggml_metal_init: use bfloat            = true
0.00.055.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.057 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.059 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.074 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.994 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.995 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.995 I llama_new_context_with_model: graph nodes  = 967
0.00.066.995 I llama_new_context_with_model: graph splits = 2
0.00.067.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.933 I 
0.00.684.965 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.990 I perplexity: tokenizing the input ..
0.00.692.633 I perplexity: tokenization took 7.641 ms
0.00.692.644 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.977 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.816.228 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.816.240 I llama_perf_context_print:        load time =     675.76 ms
0.00.816.241 I llama_perf_context_print: prompt eval time =     122.11 ms /   128 tokens (    0.95 ms per token,  1048.24 tokens per second)
0.00.816.243 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.243 I llama_perf_context_print:       total time =     131.31 ms /   129 tokens
0.00.816.553 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.076s
sys	0m0.103s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.011.904 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.852 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.862 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.862 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.864 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.864 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.864 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.864 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.865 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.865 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.867 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.867 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.609 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.618 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.426 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.427 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.427 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.428 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.428 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.428 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.429 I llama_model_loader: - type  f32:  194 tensors
0.00.026.429 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.430 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.413 I llm_load_vocab: special tokens cache size = 25
0.00.052.375 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.378 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.378 I llm_load_print_meta: arch             = gptneox
0.00.052.379 I llm_load_print_meta: vocab type       = BPE
0.00.052.379 I llm_load_print_meta: n_vocab          = 50304
0.00.052.379 I llm_load_print_meta: n_merges         = 50009
0.00.052.379 I llm_load_print_meta: vocab_only       = 0
0.00.052.379 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.379 I llm_load_print_meta: n_embd           = 2048
0.00.052.380 I llm_load_print_meta: n_layer          = 24
0.00.052.383 I llm_load_print_meta: n_head           = 16
0.00.052.383 I llm_load_print_meta: n_head_kv        = 16
0.00.052.395 I llm_load_print_meta: n_rot            = 32
0.00.052.395 I llm_load_print_meta: n_swa            = 0
0.00.052.396 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.396 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.396 I llm_load_print_meta: n_gqa            = 1
0.00.052.397 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.398 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.398 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.399 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.399 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.399 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.399 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.400 I llm_load_print_meta: n_ff             = 8192
0.00.052.400 I llm_load_print_meta: n_expert         = 0
0.00.052.400 I llm_load_print_meta: n_expert_used    = 0
0.00.052.400 I llm_load_print_meta: causal attn      = 1
0.00.052.401 I llm_load_print_meta: pooling type     = 0
0.00.052.401 I llm_load_print_meta: rope type        = 2
0.00.052.401 I llm_load_print_meta: rope scaling     = linear
0.00.052.401 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.402 I llm_load_print_meta: freq_scale_train = 1
0.00.052.402 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.402 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.403 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.403 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.404 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.404 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.405 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.414 I llm_load_print_meta: model type       = 1.4B
0.00.052.414 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.415 I llm_load_print_meta: model params     = 1.41 B
0.00.052.415 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.416 I llm_load_print_meta: general.name     = 1.4B
0.00.052.416 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.417 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.417 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.417 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.417 I llm_load_print_meta: LF token         = 128 ''
0.00.052.418 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.418 I llm_load_print_meta: max token length = 1024
0.00.054.367 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.367 I llm_load_tensors: offloading output layer to GPU
0.00.054.367 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.378 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.379 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.257 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.258 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.258 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.258 I llama_new_context_with_model: n_batch       = 2048
0.00.055.259 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.259 I llama_new_context_with_model: flash_attn    = 0
0.00.055.259 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.260 I llama_new_context_with_model: freq_scale    = 1
0.00.055.260 I ggml_metal_init: allocating
0.00.055.266 I ggml_metal_init: found device: Apple M4
0.00.055.268 I ggml_metal_init: picking default device: Apple M4
0.00.055.828 I ggml_metal_init: using embedded metal library
0.00.058.173 I ggml_metal_init: GPU name:   Apple M4
0.00.058.174 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.174 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.175 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.175 I ggml_metal_init: simdgroup reduction   = true
0.00.058.175 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.175 I ggml_metal_init: has bfloat            = true
0.00.058.175 I ggml_metal_init: use bfloat            = true
0.00.058.176 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.684 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.702 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.731 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.733 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.733 I llama_new_context_with_model: graph nodes  = 967
0.00.088.733 I llama_new_context_with_model: graph splits = 2
0.00.088.746 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.219 I main: llama threadpool init, n_threads = 4
0.00.750.259 I 
0.00.750.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.750.290 I 
0.00.750.536 I sampler seed: 1234
0.00.750.540 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.750.551 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.750.552 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.750.552 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.540.025 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 59966.22 tokens per second)
0.01.540.026 I llama_perf_context_print:        load time =     738.31 ms
0.01.540.027 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.46 tokens per second)
0.01.540.031 I llama_perf_context_print:        eval time =     743.44 ms /    63 runs   (   11.80 ms per token,    84.74 tokens per second)
0.01.540.032 I llama_perf_context_print:       total time =     789.81 ms /    70 tokens
0.01.540.227 I ggml_metal_free: deallocating

real	0m1.558s
user	0m0.108s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.885 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.198 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.209 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.213 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.961 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.014 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.848 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.849 I llama_model_loader: - type  f32:  194 tensors
0.00.023.850 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.850 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.710 I llm_load_vocab: special tokens cache size = 25
0.00.049.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.616 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.617 I llm_load_print_meta: arch             = gptneox
0.00.049.617 I llm_load_print_meta: vocab type       = BPE
0.00.049.617 I llm_load_print_meta: n_vocab          = 50304
0.00.049.617 I llm_load_print_meta: n_merges         = 50009
0.00.049.618 I llm_load_print_meta: vocab_only       = 0
0.00.049.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.618 I llm_load_print_meta: n_embd           = 2048
0.00.049.618 I llm_load_print_meta: n_layer          = 24
0.00.049.620 I llm_load_print_meta: n_head           = 16
0.00.049.621 I llm_load_print_meta: n_head_kv        = 16
0.00.049.628 I llm_load_print_meta: n_rot            = 32
0.00.049.628 I llm_load_print_meta: n_swa            = 0
0.00.049.628 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.629 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.629 I llm_load_print_meta: n_gqa            = 1
0.00.049.630 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.631 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.631 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.632 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.632 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.632 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.632 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.633 I llm_load_print_meta: n_ff             = 8192
0.00.049.633 I llm_load_print_meta: n_expert         = 0
0.00.049.633 I llm_load_print_meta: n_expert_used    = 0
0.00.049.634 I llm_load_print_meta: causal attn      = 1
0.00.049.634 I llm_load_print_meta: pooling type     = 0
0.00.049.634 I llm_load_print_meta: rope type        = 2
0.00.049.634 I llm_load_print_meta: rope scaling     = linear
0.00.049.636 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.638 I llm_load_print_meta: freq_scale_train = 1
0.00.049.638 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.638 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.638 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.638 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.639 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.639 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.639 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.643 I llm_load_print_meta: model type       = 1.4B
0.00.049.643 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.644 I llm_load_print_meta: model params     = 1.41 B
0.00.049.644 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.644 I llm_load_print_meta: general.name     = 1.4B
0.00.049.645 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.645 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.645 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.645 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.645 I llm_load_print_meta: LF token         = 128 ''
0.00.049.646 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.646 I llm_load_print_meta: max token length = 1024
0.00.052.399 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.399 I llm_load_tensors: offloading output layer to GPU
0.00.052.399 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.405 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.405 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.488 I llama_new_context_with_model: n_ctx         = 128
0.00.053.488 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.488 I llama_new_context_with_model: n_batch       = 128
0.00.053.488 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.488 I llama_new_context_with_model: flash_attn    = 0
0.00.053.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.489 I llama_new_context_with_model: freq_scale    = 1
0.00.053.489 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.490 I ggml_metal_init: allocating
0.00.053.493 I ggml_metal_init: found device: Apple M4
0.00.053.495 I ggml_metal_init: picking default device: Apple M4
0.00.054.076 I ggml_metal_init: using embedded metal library
0.00.056.414 I ggml_metal_init: GPU name:   Apple M4
0.00.056.415 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.416 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.416 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.416 I ggml_metal_init: simdgroup reduction   = true
0.00.056.416 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.416 I ggml_metal_init: has bfloat            = true
0.00.056.417 I ggml_metal_init: use bfloat            = true
0.00.056.417 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.418 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.012 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.877 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.878 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.879 I llama_new_context_with_model: graph nodes  = 967
0.00.067.879 I llama_new_context_with_model: graph splits = 2
0.00.067.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.843 I 
0.00.707.885 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.900 I perplexity: tokenizing the input ..
0.00.715.685 I perplexity: tokenization took 7.784 ms
0.00.715.696 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.788 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.851.934 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.851.949 I llama_perf_context_print:        load time =     697.95 ms
0.00.851.950 I llama_perf_context_print: prompt eval time =     134.84 ms /   128 tokens (    1.05 ms per token,   949.25 tokens per second)
0.00.851.952 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.952 I llama_perf_context_print:       total time =     144.11 ms /   129 tokens
0.00.852.462 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.077s
sys	0m0.126s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.723 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.204 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.206 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.207 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.207 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.207 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.211 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.212 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.213 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.215 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.219 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.219 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.061 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.829 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.830 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.830 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.831 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.831 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.831 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.832 I llama_model_loader: - type  f32:  194 tensors
0.00.024.832 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.832 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.863 I llm_load_vocab: special tokens cache size = 25
0.00.050.755 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.758 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.758 I llm_load_print_meta: arch             = gptneox
0.00.050.759 I llm_load_print_meta: vocab type       = BPE
0.00.050.759 I llm_load_print_meta: n_vocab          = 50304
0.00.050.759 I llm_load_print_meta: n_merges         = 50009
0.00.050.759 I llm_load_print_meta: vocab_only       = 0
0.00.050.760 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.760 I llm_load_print_meta: n_embd           = 2048
0.00.050.760 I llm_load_print_meta: n_layer          = 24
0.00.050.763 I llm_load_print_meta: n_head           = 16
0.00.050.763 I llm_load_print_meta: n_head_kv        = 16
0.00.050.775 I llm_load_print_meta: n_rot            = 32
0.00.050.776 I llm_load_print_meta: n_swa            = 0
0.00.050.776 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.776 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.777 I llm_load_print_meta: n_gqa            = 1
0.00.050.778 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.780 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.781 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.783 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.784 I llm_load_print_meta: n_ff             = 8192
0.00.050.785 I llm_load_print_meta: n_expert         = 0
0.00.050.785 I llm_load_print_meta: n_expert_used    = 0
0.00.050.787 I llm_load_print_meta: causal attn      = 1
0.00.050.788 I llm_load_print_meta: pooling type     = 0
0.00.050.788 I llm_load_print_meta: rope type        = 2
0.00.050.788 I llm_load_print_meta: rope scaling     = linear
0.00.050.788 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.789 I llm_load_print_meta: freq_scale_train = 1
0.00.050.789 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.789 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.789 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.789 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.789 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.789 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.790 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.799 I llm_load_print_meta: model type       = 1.4B
0.00.050.799 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.800 I llm_load_print_meta: model params     = 1.41 B
0.00.050.800 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.800 I llm_load_print_meta: general.name     = 1.4B
0.00.050.801 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.801 I llm_load_print_meta: LF token         = 128 ''
0.00.050.802 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.802 I llm_load_print_meta: max token length = 1024
0.00.052.785 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.785 I llm_load_tensors: offloading output layer to GPU
0.00.052.786 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.797 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.798 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.731 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.732 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.733 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.733 I llama_new_context_with_model: n_batch       = 2048
0.00.053.733 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.733 I llama_new_context_with_model: flash_attn    = 0
0.00.053.733 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.734 I llama_new_context_with_model: freq_scale    = 1
0.00.053.734 I ggml_metal_init: allocating
0.00.053.742 I ggml_metal_init: found device: Apple M4
0.00.053.744 I ggml_metal_init: picking default device: Apple M4
0.00.054.316 I ggml_metal_init: using embedded metal library
0.00.056.685 I ggml_metal_init: GPU name:   Apple M4
0.00.056.686 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.687 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.687 I ggml_metal_init: simdgroup reduction   = true
0.00.056.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.689 I ggml_metal_init: has bfloat            = true
0.00.056.689 I ggml_metal_init: use bfloat            = true
0.00.056.689 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.690 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.324 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.334 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.422 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.423 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.424 I llama_new_context_with_model: graph nodes  = 967
0.00.087.424 I llama_new_context_with_model: graph splits = 2
0.00.087.438 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.708.248 I main: llama threadpool init, n_threads = 4
0.00.708.287 I 
0.00.708.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.708.341 I 
0.00.708.564 I sampler seed: 1234
0.00.708.569 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.708.609 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.708.614 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.708.614 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.549.142 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54364.47 tokens per second)
0.01.549.143 I llama_perf_context_print:        load time =     699.52 ms
0.01.549.144 I llama_perf_context_print: prompt eval time =      42.27 ms /     7 tokens (    6.04 ms per token,   165.61 tokens per second)
0.01.549.144 I llama_perf_context_print:        eval time =     795.10 ms /    63 runs   (   12.62 ms per token,    79.23 tokens per second)
0.01.549.145 I llama_perf_context_print:       total time =     840.90 ms /    70 tokens
0.01.549.334 I ggml_metal_free: deallocating

real	0m1.565s
user	0m0.107s
sys	0m0.168s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.473 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.250 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.251 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.251 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.252 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.252 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.253 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.253 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.254 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.256 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.256 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.079 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.746 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.746 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.747 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.747 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.022.747 I llama_model_loader: - type  f32:  194 tensors
0.00.022.748 I llama_model_loader: - type q5_1:   97 tensors
0.00.022.748 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.646 I llm_load_vocab: special tokens cache size = 25
0.00.048.544 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.547 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.547 I llm_load_print_meta: arch             = gptneox
0.00.048.548 I llm_load_print_meta: vocab type       = BPE
0.00.048.548 I llm_load_print_meta: n_vocab          = 50304
0.00.048.548 I llm_load_print_meta: n_merges         = 50009
0.00.048.548 I llm_load_print_meta: vocab_only       = 0
0.00.048.548 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.549 I llm_load_print_meta: n_embd           = 2048
0.00.048.549 I llm_load_print_meta: n_layer          = 24
0.00.048.551 I llm_load_print_meta: n_head           = 16
0.00.048.552 I llm_load_print_meta: n_head_kv        = 16
0.00.048.564 I llm_load_print_meta: n_rot            = 32
0.00.048.565 I llm_load_print_meta: n_swa            = 0
0.00.048.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.565 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.566 I llm_load_print_meta: n_gqa            = 1
0.00.048.567 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.567 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.568 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.569 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.569 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.569 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.569 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.570 I llm_load_print_meta: n_ff             = 8192
0.00.048.570 I llm_load_print_meta: n_expert         = 0
0.00.048.570 I llm_load_print_meta: n_expert_used    = 0
0.00.048.570 I llm_load_print_meta: causal attn      = 1
0.00.048.570 I llm_load_print_meta: pooling type     = 0
0.00.048.570 I llm_load_print_meta: rope type        = 2
0.00.048.572 I llm_load_print_meta: rope scaling     = linear
0.00.048.572 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.572 I llm_load_print_meta: freq_scale_train = 1
0.00.048.573 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.573 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.573 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.573 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.574 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.574 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.575 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.584 I llm_load_print_meta: model type       = 1.4B
0.00.048.584 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.584 I llm_load_print_meta: model params     = 1.41 B
0.00.048.585 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.585 I llm_load_print_meta: general.name     = 1.4B
0.00.048.585 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.586 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.586 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.587 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.587 I llm_load_print_meta: LF token         = 128 ''
0.00.048.587 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.587 I llm_load_print_meta: max token length = 1024
0.00.050.536 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.536 I llm_load_tensors: offloading output layer to GPU
0.00.050.536 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.547 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.548 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.435 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.436 I llama_new_context_with_model: n_ctx         = 128
0.00.051.436 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.436 I llama_new_context_with_model: n_batch       = 128
0.00.051.436 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.436 I llama_new_context_with_model: flash_attn    = 0
0.00.051.437 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.437 I llama_new_context_with_model: freq_scale    = 1
0.00.051.437 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.438 I ggml_metal_init: allocating
0.00.051.444 I ggml_metal_init: found device: Apple M4
0.00.051.447 I ggml_metal_init: picking default device: Apple M4
0.00.052.022 I ggml_metal_init: using embedded metal library
0.00.054.341 I ggml_metal_init: GPU name:   Apple M4
0.00.054.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.343 I ggml_metal_init: simdgroup reduction   = true
0.00.054.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.343 I ggml_metal_init: has bfloat            = true
0.00.054.343 I ggml_metal_init: use bfloat            = true
0.00.054.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.952 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.956 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.970 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.792 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.793 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.794 I llama_new_context_with_model: graph nodes  = 967
0.00.065.794 I llama_new_context_with_model: graph splits = 2
0.00.065.806 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.513 I 
0.00.663.623 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.640 I perplexity: tokenizing the input ..
0.00.671.910 I perplexity: tokenization took 8.268 ms
0.00.671.920 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.594 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.807.747 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.807.764 I llama_perf_context_print:        load time =     655.03 ms
0.00.807.764 I llama_perf_context_print: prompt eval time =     134.45 ms /   128 tokens (    1.05 ms per token,   952.05 tokens per second)
0.00.807.765 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.766 I llama_perf_context_print:       total time =     144.26 ms /   129 tokens
0.00.808.042 I ggml_metal_free: deallocating

real	0m0.821s
user	0m0.077s
sys	0m0.133s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.764 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.588 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.589 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.589 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.590 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.591 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.591 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.591 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.592 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.592 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.595 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.441 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.442 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.442 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.443 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.443 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.443 I llama_model_loader: - type  f32:  194 tensors
0.00.025.443 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.444 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.444 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.553 I llm_load_vocab: special tokens cache size = 25
0.00.051.542 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.544 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.545 I llm_load_print_meta: arch             = gptneox
0.00.051.545 I llm_load_print_meta: vocab type       = BPE
0.00.051.545 I llm_load_print_meta: n_vocab          = 50304
0.00.051.546 I llm_load_print_meta: n_merges         = 50009
0.00.051.546 I llm_load_print_meta: vocab_only       = 0
0.00.051.546 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.546 I llm_load_print_meta: n_embd           = 2048
0.00.051.546 I llm_load_print_meta: n_layer          = 24
0.00.051.549 I llm_load_print_meta: n_head           = 16
0.00.051.550 I llm_load_print_meta: n_head_kv        = 16
0.00.051.562 I llm_load_print_meta: n_rot            = 32
0.00.051.564 I llm_load_print_meta: n_swa            = 0
0.00.051.565 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.565 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.566 I llm_load_print_meta: n_gqa            = 1
0.00.051.568 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.568 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.569 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.569 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.569 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.569 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.570 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.571 I llm_load_print_meta: n_ff             = 8192
0.00.051.571 I llm_load_print_meta: n_expert         = 0
0.00.051.572 I llm_load_print_meta: n_expert_used    = 0
0.00.051.574 I llm_load_print_meta: causal attn      = 1
0.00.051.574 I llm_load_print_meta: pooling type     = 0
0.00.051.574 I llm_load_print_meta: rope type        = 2
0.00.051.574 I llm_load_print_meta: rope scaling     = linear
0.00.051.575 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.575 I llm_load_print_meta: freq_scale_train = 1
0.00.051.576 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.576 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.576 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.576 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.576 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.577 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.580 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.590 I llm_load_print_meta: model type       = 1.4B
0.00.051.590 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.590 I llm_load_print_meta: model params     = 1.41 B
0.00.051.592 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.592 I llm_load_print_meta: general.name     = 1.4B
0.00.051.592 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.592 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.592 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.593 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: LF token         = 128 ''
0.00.051.594 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: max token length = 1024
0.00.053.478 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.478 I llm_load_tensors: offloading output layer to GPU
0.00.053.478 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.489 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.490 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.410 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.411 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.411 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.411 I llama_new_context_with_model: n_batch       = 2048
0.00.054.411 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.411 I llama_new_context_with_model: flash_attn    = 0
0.00.054.412 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.412 I llama_new_context_with_model: freq_scale    = 1
0.00.054.413 I ggml_metal_init: allocating
0.00.054.419 I ggml_metal_init: found device: Apple M4
0.00.054.422 I ggml_metal_init: picking default device: Apple M4
0.00.055.007 I ggml_metal_init: using embedded metal library
0.00.057.322 I ggml_metal_init: GPU name:   Apple M4
0.00.057.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.324 I ggml_metal_init: simdgroup reduction   = true
0.00.057.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.326 I ggml_metal_init: has bfloat            = true
0.00.057.326 I ggml_metal_init: use bfloat            = true
0.00.057.327 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.866 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.874 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.890 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.993 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.994 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.994 I llama_new_context_with_model: graph nodes  = 967
0.00.086.994 I llama_new_context_with_model: graph splits = 2
0.00.087.008 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.297 I main: llama threadpool init, n_threads = 4
0.00.451.335 I 
0.00.451.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.451.367 I 
0.00.451.598 I sampler seed: 1234
0.00.451.604 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.451.615 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.451.615 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.451.615 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.131.199 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.01.131.200 I llama_perf_context_print:        load time =     441.53 ms
0.01.131.201 I llama_perf_context_print: prompt eval time =      35.78 ms /     7 tokens (    5.11 ms per token,   195.62 tokens per second)
0.01.131.203 I llama_perf_context_print:        eval time =     640.78 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.131.203 I llama_perf_context_print:       total time =     679.90 ms /    70 tokens
0.01.131.394 I ggml_metal_free: deallocating

real	0m1.148s
user	0m0.108s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.176 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.833 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.837 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.842 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.846 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.788 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.884 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.833 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.833 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.834 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.835 I llama_model_loader: - type  f32:  194 tensors
0.00.024.835 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.835 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.836 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.390 I llm_load_vocab: special tokens cache size = 25
0.00.051.426 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.429 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.429 I llm_load_print_meta: arch             = gptneox
0.00.051.430 I llm_load_print_meta: vocab type       = BPE
0.00.051.430 I llm_load_print_meta: n_vocab          = 50304
0.00.051.430 I llm_load_print_meta: n_merges         = 50009
0.00.051.430 I llm_load_print_meta: vocab_only       = 0
0.00.051.430 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.431 I llm_load_print_meta: n_embd           = 2048
0.00.051.431 I llm_load_print_meta: n_layer          = 24
0.00.051.433 I llm_load_print_meta: n_head           = 16
0.00.051.434 I llm_load_print_meta: n_head_kv        = 16
0.00.051.445 I llm_load_print_meta: n_rot            = 32
0.00.051.446 I llm_load_print_meta: n_swa            = 0
0.00.051.446 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.446 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.447 I llm_load_print_meta: n_gqa            = 1
0.00.051.448 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.448 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.449 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.449 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.450 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.450 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.450 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.450 I llm_load_print_meta: n_ff             = 8192
0.00.051.451 I llm_load_print_meta: n_expert         = 0
0.00.051.451 I llm_load_print_meta: n_expert_used    = 0
0.00.051.451 I llm_load_print_meta: causal attn      = 1
0.00.051.451 I llm_load_print_meta: pooling type     = 0
0.00.051.451 I llm_load_print_meta: rope type        = 2
0.00.051.452 I llm_load_print_meta: rope scaling     = linear
0.00.051.452 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.452 I llm_load_print_meta: freq_scale_train = 1
0.00.051.452 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.452 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.453 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.453 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.453 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.453 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.453 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.462 I llm_load_print_meta: model type       = 1.4B
0.00.051.462 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.463 I llm_load_print_meta: model params     = 1.41 B
0.00.051.464 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.464 I llm_load_print_meta: general.name     = 1.4B
0.00.051.464 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.464 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.465 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.465 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.465 I llm_load_print_meta: LF token         = 128 ''
0.00.051.465 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.465 I llm_load_print_meta: max token length = 1024
0.00.053.060 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.060 I llm_load_tensors: offloading output layer to GPU
0.00.053.060 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.070 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.072 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.933 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.933 I llama_new_context_with_model: n_ctx         = 128
0.00.053.934 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.934 I llama_new_context_with_model: n_batch       = 128
0.00.053.934 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.934 I llama_new_context_with_model: flash_attn    = 0
0.00.053.934 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.935 I llama_new_context_with_model: freq_scale    = 1
0.00.053.935 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.936 I ggml_metal_init: allocating
0.00.053.939 I ggml_metal_init: found device: Apple M4
0.00.053.941 I ggml_metal_init: picking default device: Apple M4
0.00.054.504 I ggml_metal_init: using embedded metal library
0.00.056.818 I ggml_metal_init: GPU name:   Apple M4
0.00.056.819 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.820 I ggml_metal_init: simdgroup reduction   = true
0.00.056.820 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.821 I ggml_metal_init: has bfloat            = true
0.00.056.821 I ggml_metal_init: use bfloat            = true
0.00.056.821 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.822 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.641 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.655 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.560 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.561 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.562 I llama_new_context_with_model: graph nodes  = 967
0.00.068.562 I llama_new_context_with_model: graph splits = 2
0.00.068.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.398.901 I 
0.00.398.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.398.941 I perplexity: tokenizing the input ..
0.00.406.833 I perplexity: tokenization took 7.889 ms
0.00.406.844 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.539.522 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.540.699 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.540.722 I llama_perf_context_print:        load time =     388.72 ms
0.00.540.723 I llama_perf_context_print: prompt eval time =     132.45 ms /   128 tokens (    1.03 ms per token,   966.40 tokens per second)
0.00.540.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.540.724 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.541.272 I ggml_metal_free: deallocating

real	0m0.557s
user	0m0.079s
sys	0m0.078s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.927 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.258 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.265 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.265 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.266 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.267 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.267 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.267 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.268 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.268 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.272 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.000 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.756 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.756 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.757 I llama_model_loader: - type  f32:  194 tensors
0.00.023.757 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.758 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.758 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.758 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.772 I llm_load_vocab: special tokens cache size = 25
0.00.049.666 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.669 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.669 I llm_load_print_meta: arch             = gptneox
0.00.049.669 I llm_load_print_meta: vocab type       = BPE
0.00.049.670 I llm_load_print_meta: n_vocab          = 50304
0.00.049.670 I llm_load_print_meta: n_merges         = 50009
0.00.049.670 I llm_load_print_meta: vocab_only       = 0
0.00.049.670 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.670 I llm_load_print_meta: n_embd           = 2048
0.00.049.670 I llm_load_print_meta: n_layer          = 24
0.00.049.673 I llm_load_print_meta: n_head           = 16
0.00.049.674 I llm_load_print_meta: n_head_kv        = 16
0.00.049.685 I llm_load_print_meta: n_rot            = 32
0.00.049.685 I llm_load_print_meta: n_swa            = 0
0.00.049.685 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.686 I llm_load_print_meta: n_gqa            = 1
0.00.049.688 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.689 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.689 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.689 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.690 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.690 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.690 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.691 I llm_load_print_meta: n_ff             = 8192
0.00.049.691 I llm_load_print_meta: n_expert         = 0
0.00.049.691 I llm_load_print_meta: n_expert_used    = 0
0.00.049.691 I llm_load_print_meta: causal attn      = 1
0.00.049.693 I llm_load_print_meta: pooling type     = 0
0.00.049.693 I llm_load_print_meta: rope type        = 2
0.00.049.693 I llm_load_print_meta: rope scaling     = linear
0.00.049.693 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.693 I llm_load_print_meta: freq_scale_train = 1
0.00.049.694 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.695 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.695 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.695 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.695 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.695 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.695 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.704 I llm_load_print_meta: model type       = 1.4B
0.00.049.705 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.705 I llm_load_print_meta: model params     = 1.41 B
0.00.049.706 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.706 I llm_load_print_meta: general.name     = 1.4B
0.00.049.707 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.708 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.708 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.708 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.708 I llm_load_print_meta: LF token         = 128 ''
0.00.049.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.708 I llm_load_print_meta: max token length = 1024
0.00.051.237 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.237 I llm_load_tensors: offloading output layer to GPU
0.00.051.237 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.247 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.248 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.152 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.153 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.153 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.153 I llama_new_context_with_model: n_batch       = 2048
0.00.052.153 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.153 I llama_new_context_with_model: flash_attn    = 0
0.00.052.154 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.154 I llama_new_context_with_model: freq_scale    = 1
0.00.052.154 I ggml_metal_init: allocating
0.00.052.158 I ggml_metal_init: found device: Apple M4
0.00.052.159 I ggml_metal_init: picking default device: Apple M4
0.00.052.774 I ggml_metal_init: using embedded metal library
0.00.055.078 I ggml_metal_init: GPU name:   Apple M4
0.00.055.080 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.081 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.081 I ggml_metal_init: simdgroup reduction   = true
0.00.055.081 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.081 I ggml_metal_init: has bfloat            = true
0.00.055.081 I ggml_metal_init: use bfloat            = true
0.00.055.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.035 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.061 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.041 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.042 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.043 I llama_new_context_with_model: graph nodes  = 967
0.00.084.043 I llama_new_context_with_model: graph splits = 2
0.00.084.057 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.551.723 I main: llama threadpool init, n_threads = 4
0.00.551.765 I 
0.00.551.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.551.794 I 
0.00.552.043 I sampler seed: 1234
0.00.552.049 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.552.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.552.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.552.102 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.301.383 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.01.301.384 I llama_perf_context_print:        load time =     541.79 ms
0.01.301.385 I llama_perf_context_print: prompt eval time =      44.86 ms /     7 tokens (    6.41 ms per token,   156.03 tokens per second)
0.01.301.385 I llama_perf_context_print:        eval time =     701.34 ms /    63 runs   (   11.13 ms per token,    89.83 tokens per second)
0.01.301.386 I llama_perf_context_print:       total time =     749.67 ms /    70 tokens
0.01.301.581 I ggml_metal_free: deallocating

real	0m1.318s
user	0m0.109s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.679 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.171 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.173 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.173 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.174 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.175 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.175 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.176 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.176 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.176 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.178 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.178 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.934 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.856 I llama_model_loader: - type  f32:  194 tensors
0.00.022.856 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.856 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.856 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.856 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.667 I llm_load_vocab: special tokens cache size = 25
0.00.048.506 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.509 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.509 I llm_load_print_meta: arch             = gptneox
0.00.048.510 I llm_load_print_meta: vocab type       = BPE
0.00.048.510 I llm_load_print_meta: n_vocab          = 50304
0.00.048.510 I llm_load_print_meta: n_merges         = 50009
0.00.048.511 I llm_load_print_meta: vocab_only       = 0
0.00.048.511 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.511 I llm_load_print_meta: n_embd           = 2048
0.00.048.511 I llm_load_print_meta: n_layer          = 24
0.00.048.514 I llm_load_print_meta: n_head           = 16
0.00.048.514 I llm_load_print_meta: n_head_kv        = 16
0.00.048.526 I llm_load_print_meta: n_rot            = 32
0.00.048.527 I llm_load_print_meta: n_swa            = 0
0.00.048.528 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.528 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.529 I llm_load_print_meta: n_gqa            = 1
0.00.048.529 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.530 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.531 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.531 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.531 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.539 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.543 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.548 I llm_load_print_meta: n_ff             = 8192
0.00.048.548 I llm_load_print_meta: n_expert         = 0
0.00.048.548 I llm_load_print_meta: n_expert_used    = 0
0.00.048.548 I llm_load_print_meta: causal attn      = 1
0.00.048.548 I llm_load_print_meta: pooling type     = 0
0.00.048.549 I llm_load_print_meta: rope type        = 2
0.00.048.549 I llm_load_print_meta: rope scaling     = linear
0.00.048.549 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.549 I llm_load_print_meta: freq_scale_train = 1
0.00.048.550 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.550 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.550 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.550 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.551 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.551 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.551 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.561 I llm_load_print_meta: model type       = 1.4B
0.00.048.562 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.562 I llm_load_print_meta: model params     = 1.41 B
0.00.048.563 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.563 I llm_load_print_meta: general.name     = 1.4B
0.00.048.563 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.563 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.563 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.565 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.565 I llm_load_print_meta: LF token         = 128 ''
0.00.048.566 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.566 I llm_load_print_meta: max token length = 1024
0.00.050.440 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.441 I llm_load_tensors: offloading output layer to GPU
0.00.050.441 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.451 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.452 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.313 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.314 I llama_new_context_with_model: n_ctx         = 128
0.00.051.314 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.314 I llama_new_context_with_model: n_batch       = 128
0.00.051.314 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.314 I llama_new_context_with_model: flash_attn    = 0
0.00.051.315 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.315 I llama_new_context_with_model: freq_scale    = 1
0.00.051.315 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.316 I ggml_metal_init: allocating
0.00.051.322 I ggml_metal_init: found device: Apple M4
0.00.051.324 I ggml_metal_init: picking default device: Apple M4
0.00.051.873 I ggml_metal_init: using embedded metal library
0.00.054.242 I ggml_metal_init: GPU name:   Apple M4
0.00.054.243 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.244 I ggml_metal_init: simdgroup reduction   = true
0.00.054.244 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.245 I ggml_metal_init: has bfloat            = true
0.00.054.245 I ggml_metal_init: use bfloat            = true
0.00.054.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.814 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.816 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.830 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.738 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.739 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.739 I llama_new_context_with_model: graph nodes  = 967
0.00.065.739 I llama_new_context_with_model: graph splits = 2
0.00.065.751 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.487.003 I 
0.00.487.089 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.487.112 I perplexity: tokenizing the input ..
0.00.495.047 I perplexity: tokenization took 7.934 ms
0.00.495.057 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.627.333 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.628.617 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.628.633 I llama_perf_context_print:        load time =     478.32 ms
0.00.628.634 I llama_perf_context_print: prompt eval time =     132.01 ms /   128 tokens (    1.03 ms per token,   969.59 tokens per second)
0.00.628.635 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.628.635 I llama_perf_context_print:       total time =     141.63 ms /   129 tokens
0.00.629.150 I ggml_metal_free: deallocating

real	0m0.643s
user	0m0.076s
sys	0m0.090s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.010.800 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.144 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.145 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.145 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.146 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.147 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.148 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.150 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.150 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.996 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.090 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.006 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.006 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.007 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.008 I llama_model_loader: - type  f32:  194 tensors
0.00.025.008 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.008 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.008 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.119 I llm_load_vocab: special tokens cache size = 25
0.00.051.146 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.148 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.149 I llm_load_print_meta: arch             = gptneox
0.00.051.149 I llm_load_print_meta: vocab type       = BPE
0.00.051.149 I llm_load_print_meta: n_vocab          = 50304
0.00.051.150 I llm_load_print_meta: n_merges         = 50009
0.00.051.150 I llm_load_print_meta: vocab_only       = 0
0.00.051.150 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.150 I llm_load_print_meta: n_embd           = 2048
0.00.051.150 I llm_load_print_meta: n_layer          = 24
0.00.051.153 I llm_load_print_meta: n_head           = 16
0.00.051.154 I llm_load_print_meta: n_head_kv        = 16
0.00.051.166 I llm_load_print_meta: n_rot            = 32
0.00.051.166 I llm_load_print_meta: n_swa            = 0
0.00.051.166 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.166 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.167 I llm_load_print_meta: n_gqa            = 1
0.00.051.168 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.168 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.169 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.169 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.169 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.170 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.170 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.170 I llm_load_print_meta: n_ff             = 8192
0.00.051.171 I llm_load_print_meta: n_expert         = 0
0.00.051.171 I llm_load_print_meta: n_expert_used    = 0
0.00.051.171 I llm_load_print_meta: causal attn      = 1
0.00.051.171 I llm_load_print_meta: pooling type     = 0
0.00.051.171 I llm_load_print_meta: rope type        = 2
0.00.051.171 I llm_load_print_meta: rope scaling     = linear
0.00.051.172 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.172 I llm_load_print_meta: freq_scale_train = 1
0.00.051.172 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.172 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.172 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.173 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.173 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.173 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.173 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.182 I llm_load_print_meta: model type       = 1.4B
0.00.051.183 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.183 I llm_load_print_meta: model params     = 1.41 B
0.00.051.184 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.184 I llm_load_print_meta: general.name     = 1.4B
0.00.051.186 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.186 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.187 I llm_load_print_meta: LF token         = 128 ''
0.00.051.187 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.187 I llm_load_print_meta: max token length = 1024
0.00.053.063 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.064 I llm_load_tensors: offloading output layer to GPU
0.00.053.064 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.074 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.076 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.961 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.961 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.962 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.962 I llama_new_context_with_model: n_batch       = 2048
0.00.053.962 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.962 I llama_new_context_with_model: flash_attn    = 0
0.00.053.963 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.963 I llama_new_context_with_model: freq_scale    = 1
0.00.053.963 I ggml_metal_init: allocating
0.00.053.968 I ggml_metal_init: found device: Apple M4
0.00.053.972 I ggml_metal_init: picking default device: Apple M4
0.00.054.531 I ggml_metal_init: using embedded metal library
0.00.056.830 I ggml_metal_init: GPU name:   Apple M4
0.00.056.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.832 I ggml_metal_init: simdgroup reduction   = true
0.00.056.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.833 I ggml_metal_init: has bfloat            = true
0.00.056.833 I ggml_metal_init: use bfloat            = true
0.00.056.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.835 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.943 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.948 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.964 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.031 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.033 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.033 I llama_new_context_with_model: graph nodes  = 967
0.00.087.033 I llama_new_context_with_model: graph splits = 2
0.00.087.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.593 I main: llama threadpool init, n_threads = 4
0.00.635.636 I 
0.00.635.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.674 I 
0.00.635.922 I sampler seed: 1234
0.00.635.928 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.635.939 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.635.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.635.940 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.401.823 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.401.825 I llama_perf_context_print:        load time =     624.78 ms
0.01.401.825 I llama_perf_context_print: prompt eval time =      50.99 ms /     7 tokens (    7.28 ms per token,   137.29 tokens per second)
0.01.401.826 I llama_perf_context_print:        eval time =     711.70 ms /    63 runs   (   11.30 ms per token,    88.52 tokens per second)
0.01.401.827 I llama_perf_context_print:       total time =     766.24 ms /    70 tokens
0.01.402.026 I ggml_metal_free: deallocating

real	0m1.420s
user	0m0.110s
sys	0m0.151s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.351 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.778 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.780 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.781 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.781 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.786 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.786 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.787 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.788 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.788 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.789 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.622 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.615 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.615 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.617 I llama_model_loader: - type  f32:  194 tensors
0.00.024.618 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.618 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.618 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.247 I llm_load_vocab: special tokens cache size = 25
0.00.051.321 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.323 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.324 I llm_load_print_meta: arch             = gptneox
0.00.051.324 I llm_load_print_meta: vocab type       = BPE
0.00.051.324 I llm_load_print_meta: n_vocab          = 50304
0.00.051.325 I llm_load_print_meta: n_merges         = 50009
0.00.051.325 I llm_load_print_meta: vocab_only       = 0
0.00.051.325 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.325 I llm_load_print_meta: n_embd           = 2048
0.00.051.325 I llm_load_print_meta: n_layer          = 24
0.00.051.328 I llm_load_print_meta: n_head           = 16
0.00.051.329 I llm_load_print_meta: n_head_kv        = 16
0.00.051.336 I llm_load_print_meta: n_rot            = 32
0.00.051.336 I llm_load_print_meta: n_swa            = 0
0.00.051.336 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.336 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.337 I llm_load_print_meta: n_gqa            = 1
0.00.051.338 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.338 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.339 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.341 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.341 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.341 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.342 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.342 I llm_load_print_meta: n_ff             = 8192
0.00.051.342 I llm_load_print_meta: n_expert         = 0
0.00.051.343 I llm_load_print_meta: n_expert_used    = 0
0.00.051.343 I llm_load_print_meta: causal attn      = 1
0.00.051.343 I llm_load_print_meta: pooling type     = 0
0.00.051.343 I llm_load_print_meta: rope type        = 2
0.00.051.343 I llm_load_print_meta: rope scaling     = linear
0.00.051.344 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.345 I llm_load_print_meta: freq_scale_train = 1
0.00.051.346 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.346 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.346 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.346 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.346 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.346 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.346 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.351 I llm_load_print_meta: model type       = 1.4B
0.00.051.351 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.351 I llm_load_print_meta: model params     = 1.41 B
0.00.051.352 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.352 I llm_load_print_meta: general.name     = 1.4B
0.00.051.352 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.353 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.353 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.353 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.354 I llm_load_print_meta: LF token         = 128 ''
0.00.051.354 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.354 I llm_load_print_meta: max token length = 1024
0.00.053.076 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.077 I llm_load_tensors: offloading output layer to GPU
0.00.053.077 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.082 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.082 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.959 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.960 I llama_new_context_with_model: n_ctx         = 128
0.00.053.960 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.960 I llama_new_context_with_model: n_batch       = 128
0.00.053.960 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.960 I llama_new_context_with_model: flash_attn    = 0
0.00.053.961 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.961 I llama_new_context_with_model: freq_scale    = 1
0.00.053.961 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.962 I ggml_metal_init: allocating
0.00.053.967 I ggml_metal_init: found device: Apple M4
0.00.053.969 I ggml_metal_init: picking default device: Apple M4
0.00.054.517 I ggml_metal_init: using embedded metal library
0.00.056.810 I ggml_metal_init: GPU name:   Apple M4
0.00.056.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.812 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.812 I ggml_metal_init: simdgroup reduction   = true
0.00.056.812 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.813 I ggml_metal_init: has bfloat            = true
0.00.056.813 I ggml_metal_init: use bfloat            = true
0.00.056.813 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.500 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.502 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.524 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.434 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.435 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.435 I llama_new_context_with_model: graph nodes  = 967
0.00.068.435 I llama_new_context_with_model: graph splits = 2
0.00.068.448 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.590 I 
0.00.568.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.640 I perplexity: tokenizing the input ..
0.00.576.837 I perplexity: tokenization took 8.197 ms
0.00.576.848 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.502 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.712.770 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.712.784 I llama_perf_context_print:        load time =     558.24 ms
0.00.712.785 I llama_perf_context_print: prompt eval time =     134.42 ms /   128 tokens (    1.05 ms per token,   952.25 tokens per second)
0.00.712.785 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.712.786 I llama_perf_context_print:       total time =     144.19 ms /   129 tokens
0.00.713.303 I ggml_metal_free: deallocating

real	0m0.729s
user	0m0.078s
sys	0m0.107s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.187 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.583 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.588 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.590 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.591 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.594 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.595 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.597 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.368 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.181 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.182 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.183 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.184 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.184 I llama_model_loader: - type  f32:  194 tensors
0.00.023.184 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.185 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.230 I llm_load_vocab: special tokens cache size = 25
0.00.049.199 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.202 I llm_load_print_meta: arch             = gptneox
0.00.049.202 I llm_load_print_meta: vocab type       = BPE
0.00.049.202 I llm_load_print_meta: n_vocab          = 50304
0.00.049.203 I llm_load_print_meta: n_merges         = 50009
0.00.049.203 I llm_load_print_meta: vocab_only       = 0
0.00.049.203 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.203 I llm_load_print_meta: n_embd           = 2048
0.00.049.203 I llm_load_print_meta: n_layer          = 24
0.00.049.206 I llm_load_print_meta: n_head           = 16
0.00.049.206 I llm_load_print_meta: n_head_kv        = 16
0.00.049.219 I llm_load_print_meta: n_rot            = 32
0.00.049.219 I llm_load_print_meta: n_swa            = 0
0.00.049.219 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.219 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.220 I llm_load_print_meta: n_gqa            = 1
0.00.049.221 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.222 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.222 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.222 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.223 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.223 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.223 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.224 I llm_load_print_meta: n_ff             = 8192
0.00.049.225 I llm_load_print_meta: n_expert         = 0
0.00.049.225 I llm_load_print_meta: n_expert_used    = 0
0.00.049.225 I llm_load_print_meta: causal attn      = 1
0.00.049.225 I llm_load_print_meta: pooling type     = 0
0.00.049.226 I llm_load_print_meta: rope type        = 2
0.00.049.226 I llm_load_print_meta: rope scaling     = linear
0.00.049.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.227 I llm_load_print_meta: freq_scale_train = 1
0.00.049.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.229 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.229 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.229 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.229 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.239 I llm_load_print_meta: model type       = 1.4B
0.00.049.239 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.240 I llm_load_print_meta: model params     = 1.41 B
0.00.049.240 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.240 I llm_load_print_meta: general.name     = 1.4B
0.00.049.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.242 I llm_load_print_meta: LF token         = 128 ''
0.00.049.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.242 I llm_load_print_meta: max token length = 1024
0.00.051.170 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.170 I llm_load_tensors: offloading output layer to GPU
0.00.051.170 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.181 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.182 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.076 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.077 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.077 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.077 I llama_new_context_with_model: n_batch       = 2048
0.00.052.077 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.077 I llama_new_context_with_model: flash_attn    = 0
0.00.052.078 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.078 I llama_new_context_with_model: freq_scale    = 1
0.00.052.079 I ggml_metal_init: allocating
0.00.052.084 I ggml_metal_init: found device: Apple M4
0.00.052.087 I ggml_metal_init: picking default device: Apple M4
0.00.052.649 I ggml_metal_init: using embedded metal library
0.00.054.976 I ggml_metal_init: GPU name:   Apple M4
0.00.054.978 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.979 I ggml_metal_init: simdgroup reduction   = true
0.00.054.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.979 I ggml_metal_init: has bfloat            = true
0.00.054.979 I ggml_metal_init: use bfloat            = true
0.00.054.980 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.980 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.063 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.070 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.986 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.987 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.987 I llama_new_context_with_model: graph nodes  = 967
0.00.083.988 I llama_new_context_with_model: graph splits = 2
0.00.084.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.564 I main: llama threadpool init, n_threads = 4
0.00.723.606 I 
0.00.723.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.663 I 
0.00.723.908 I sampler seed: 1234
0.00.723.913 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.723.947 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.723.958 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.723.958 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.572.449 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59613.77 tokens per second)
0.01.572.450 I llama_perf_context_print:        load time =     714.37 ms
0.01.572.450 I llama_perf_context_print: prompt eval time =      51.65 ms /     7 tokens (    7.38 ms per token,   135.54 tokens per second)
0.01.572.451 I llama_perf_context_print:        eval time =     793.82 ms /    63 runs   (   12.60 ms per token,    79.36 tokens per second)
0.01.572.452 I llama_perf_context_print:       total time =     848.89 ms /    70 tokens
0.01.572.663 I ggml_metal_free: deallocating

real	0m1.587s
user	0m0.108s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.665 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.162 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.173 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.018.970 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.715 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.716 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.716 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.717 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.717 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.717 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.022.718 I llama_model_loader: - type  f32:  194 tensors
0.00.022.718 I llama_model_loader: - type q5_K:   61 tensors
0.00.022.718 I llama_model_loader: - type q6_K:   37 tensors
0.00.042.718 I llm_load_vocab: special tokens cache size = 25
0.00.048.606 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.609 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.609 I llm_load_print_meta: arch             = gptneox
0.00.048.610 I llm_load_print_meta: vocab type       = BPE
0.00.048.610 I llm_load_print_meta: n_vocab          = 50304
0.00.048.610 I llm_load_print_meta: n_merges         = 50009
0.00.048.610 I llm_load_print_meta: vocab_only       = 0
0.00.048.610 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.610 I llm_load_print_meta: n_embd           = 2048
0.00.048.611 I llm_load_print_meta: n_layer          = 24
0.00.048.613 I llm_load_print_meta: n_head           = 16
0.00.048.614 I llm_load_print_meta: n_head_kv        = 16
0.00.048.626 I llm_load_print_meta: n_rot            = 32
0.00.048.627 I llm_load_print_meta: n_swa            = 0
0.00.048.627 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.629 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.629 I llm_load_print_meta: n_gqa            = 1
0.00.048.630 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.631 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.631 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.632 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.632 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.632 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.632 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.633 I llm_load_print_meta: n_ff             = 8192
0.00.048.633 I llm_load_print_meta: n_expert         = 0
0.00.048.633 I llm_load_print_meta: n_expert_used    = 0
0.00.048.633 I llm_load_print_meta: causal attn      = 1
0.00.048.633 I llm_load_print_meta: pooling type     = 0
0.00.048.633 I llm_load_print_meta: rope type        = 2
0.00.048.634 I llm_load_print_meta: rope scaling     = linear
0.00.048.635 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.636 I llm_load_print_meta: freq_scale_train = 1
0.00.048.637 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.637 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.637 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.637 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.637 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.638 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.638 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.648 I llm_load_print_meta: model type       = 1.4B
0.00.048.649 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.048.649 I llm_load_print_meta: model params     = 1.41 B
0.00.048.649 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.048.650 I llm_load_print_meta: general.name     = 1.4B
0.00.048.650 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.651 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.652 I llm_load_print_meta: LF token         = 128 ''
0.00.048.652 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.652 I llm_load_print_meta: max token length = 1024
0.00.050.604 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.604 I llm_load_tensors: offloading output layer to GPU
0.00.050.604 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.615 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.050.616 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.591 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.592 I llama_new_context_with_model: n_ctx         = 128
0.00.051.592 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.592 I llama_new_context_with_model: n_batch       = 128
0.00.051.593 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.593 I llama_new_context_with_model: flash_attn    = 0
0.00.051.593 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.593 I llama_new_context_with_model: freq_scale    = 1
0.00.051.594 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.594 I ggml_metal_init: allocating
0.00.051.599 I ggml_metal_init: found device: Apple M4
0.00.051.601 I ggml_metal_init: picking default device: Apple M4
0.00.052.182 I ggml_metal_init: using embedded metal library
0.00.054.505 I ggml_metal_init: GPU name:   Apple M4
0.00.054.507 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.508 I ggml_metal_init: simdgroup reduction   = true
0.00.054.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.509 I ggml_metal_init: has bfloat            = true
0.00.054.510 I ggml_metal_init: use bfloat            = true
0.00.054.510 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.511 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.198 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.221 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.057 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.058 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.058 I llama_new_context_with_model: graph nodes  = 967
0.00.066.058 I llama_new_context_with_model: graph splits = 2
0.00.066.071 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.739 I 
0.00.644.767 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.776 I perplexity: tokenizing the input ..
0.00.652.830 I perplexity: tokenization took 8.053 ms
0.00.652.844 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.413 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.794.656 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.794.669 I llama_perf_context_print:        load time =     636.07 ms
0.00.794.671 I llama_perf_context_print: prompt eval time =     140.34 ms /   128 tokens (    1.10 ms per token,   912.04 tokens per second)
0.00.794.672 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.794.672 I llama_perf_context_print:       total time =     149.93 ms /   129 tokens
0.00.795.088 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.076s
sys	0m0.118s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.685 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.174 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.175 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.175 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.175 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.176 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.177 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.178 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.178 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.181 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.008 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.074 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.854 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.854 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.854 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.855 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.856 I llama_model_loader: - type  f32:  194 tensors
0.00.024.856 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.889 I llm_load_vocab: special tokens cache size = 25
0.00.050.672 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.674 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.675 I llm_load_print_meta: arch             = gptneox
0.00.050.675 I llm_load_print_meta: vocab type       = BPE
0.00.050.675 I llm_load_print_meta: n_vocab          = 50304
0.00.050.676 I llm_load_print_meta: n_merges         = 50009
0.00.050.676 I llm_load_print_meta: vocab_only       = 0
0.00.050.676 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.676 I llm_load_print_meta: n_embd           = 2048
0.00.050.676 I llm_load_print_meta: n_layer          = 24
0.00.050.679 I llm_load_print_meta: n_head           = 16
0.00.050.680 I llm_load_print_meta: n_head_kv        = 16
0.00.050.691 I llm_load_print_meta: n_rot            = 32
0.00.050.692 I llm_load_print_meta: n_swa            = 0
0.00.050.692 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.692 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.693 I llm_load_print_meta: n_gqa            = 1
0.00.050.694 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.696 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.697 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.697 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.697 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.698 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.698 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.698 I llm_load_print_meta: n_ff             = 8192
0.00.050.699 I llm_load_print_meta: n_expert         = 0
0.00.050.699 I llm_load_print_meta: n_expert_used    = 0
0.00.050.699 I llm_load_print_meta: causal attn      = 1
0.00.050.700 I llm_load_print_meta: pooling type     = 0
0.00.050.701 I llm_load_print_meta: rope type        = 2
0.00.050.701 I llm_load_print_meta: rope scaling     = linear
0.00.050.702 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.702 I llm_load_print_meta: freq_scale_train = 1
0.00.050.702 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.703 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.706 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.706 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.707 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.707 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.707 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.716 I llm_load_print_meta: model type       = 1.4B
0.00.050.716 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.717 I llm_load_print_meta: model params     = 1.41 B
0.00.050.717 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.717 I llm_load_print_meta: general.name     = 1.4B
0.00.050.717 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.717 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.718 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.719 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.719 I llm_load_print_meta: LF token         = 128 ''
0.00.050.719 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.719 I llm_load_print_meta: max token length = 1024
0.00.052.261 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.261 I llm_load_tensors: offloading output layer to GPU
0.00.052.262 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.271 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.273 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.114 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.115 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.115 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.115 I llama_new_context_with_model: n_batch       = 2048
0.00.053.115 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.116 I llama_new_context_with_model: flash_attn    = 0
0.00.053.116 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.116 I llama_new_context_with_model: freq_scale    = 1
0.00.053.117 I ggml_metal_init: allocating
0.00.053.120 I ggml_metal_init: found device: Apple M4
0.00.053.122 I ggml_metal_init: picking default device: Apple M4
0.00.053.688 I ggml_metal_init: using embedded metal library
0.00.055.985 I ggml_metal_init: GPU name:   Apple M4
0.00.055.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.987 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.988 I ggml_metal_init: simdgroup reduction   = true
0.00.055.989 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.989 I ggml_metal_init: has bfloat            = true
0.00.055.989 I ggml_metal_init: use bfloat            = true
0.00.055.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.990 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.939 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.944 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.961 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.011 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.013 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.013 I llama_new_context_with_model: graph nodes  = 967
0.00.085.013 I llama_new_context_with_model: graph splits = 2
0.00.085.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.925 I main: llama threadpool init, n_threads = 4
0.00.770.959 I 
0.00.770.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.985 I 
0.00.771.224 I sampler seed: 1234
0.00.771.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.239 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.241 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.241 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.651.477 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.651.478 I llama_perf_context_print:        load time =     761.24 ms
0.01.651.479 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.41 tokens per second)
0.01.651.479 I llama_perf_context_print:        eval time =     822.74 ms /    63 runs   (   13.06 ms per token,    76.57 tokens per second)
0.01.651.480 I llama_perf_context_print:       total time =     880.55 ms /    70 tokens
0.01.651.660 I ggml_metal_free: deallocating

real	0m1.667s
user	0m0.108s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4279 (86a19349) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.503 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.027 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.029 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.029 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.030 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.030 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.031 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.032 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.032 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.038 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.043 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.655 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.412 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.412 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.412 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.413 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.413 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.414 I llama_model_loader: - type  f32:  194 tensors
0.00.023.414 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.939 I llm_load_vocab: special tokens cache size = 25
0.00.049.945 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.949 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.950 I llm_load_print_meta: arch             = gptneox
0.00.049.950 I llm_load_print_meta: vocab type       = BPE
0.00.049.951 I llm_load_print_meta: n_vocab          = 50304
0.00.049.952 I llm_load_print_meta: n_merges         = 50009
0.00.049.952 I llm_load_print_meta: vocab_only       = 0
0.00.049.952 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.952 I llm_load_print_meta: n_embd           = 2048
0.00.049.952 I llm_load_print_meta: n_layer          = 24
0.00.049.955 I llm_load_print_meta: n_head           = 16
0.00.049.958 I llm_load_print_meta: n_head_kv        = 16
0.00.049.969 I llm_load_print_meta: n_rot            = 32
0.00.049.970 I llm_load_print_meta: n_swa            = 0
0.00.049.970 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.970 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.971 I llm_load_print_meta: n_gqa            = 1
0.00.049.971 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.972 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.972 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.972 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.973 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.973 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.973 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.973 I llm_load_print_meta: n_ff             = 8192
0.00.049.974 I llm_load_print_meta: n_expert         = 0
0.00.049.974 I llm_load_print_meta: n_expert_used    = 0
0.00.049.974 I llm_load_print_meta: causal attn      = 1
0.00.049.974 I llm_load_print_meta: pooling type     = 0
0.00.049.974 I llm_load_print_meta: rope type        = 2
0.00.049.974 I llm_load_print_meta: rope scaling     = linear
0.00.049.975 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.976 I llm_load_print_meta: freq_scale_train = 1
0.00.049.978 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.978 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.978 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.978 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.978 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.978 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.988 I llm_load_print_meta: model type       = 1.4B
0.00.049.989 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.989 I llm_load_print_meta: model params     = 1.41 B
0.00.049.989 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.989 I llm_load_print_meta: general.name     = 1.4B
0.00.049.990 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.990 I llm_load_print_meta: LF token         = 128 ''
0.00.049.991 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.991 I llm_load_print_meta: max token length = 1024
0.00.052.098 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.098 I llm_load_tensors: offloading output layer to GPU
0.00.052.098 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.109 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.110 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.033 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.034 I llama_new_context_with_model: n_ctx         = 128
0.00.053.034 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.034 I llama_new_context_with_model: n_batch       = 128
0.00.053.034 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.034 I llama_new_context_with_model: flash_attn    = 0
0.00.053.035 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.035 I llama_new_context_with_model: freq_scale    = 1
0.00.053.035 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.036 I ggml_metal_init: allocating
0.00.053.041 I ggml_metal_init: found device: Apple M4
0.00.053.044 I ggml_metal_init: picking default device: Apple M4
0.00.053.612 I ggml_metal_init: using embedded metal library
0.00.055.950 I ggml_metal_init: GPU name:   Apple M4
0.00.055.951 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.952 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.952 I ggml_metal_init: simdgroup reduction   = true
0.00.055.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.952 I ggml_metal_init: has bfloat            = true
0.00.055.952 I ggml_metal_init: use bfloat            = true
0.00.055.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.686 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.690 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.714 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.577 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.578 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.578 I llama_new_context_with_model: graph nodes  = 967
0.00.067.578 I llama_new_context_with_model: graph splits = 2
0.00.067.590 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.256.117 I 
0.00.256.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.256.153 I perplexity: tokenizing the input ..
0.00.263.999 I perplexity: tokenization took 7.844 ms
0.00.264.012 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.403.876 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.405.029 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.405.044 I llama_perf_context_print:        load time =     246.61 ms
0.00.405.045 I llama_perf_context_print: prompt eval time =     139.63 ms /   128 tokens (    1.09 ms per token,   916.72 tokens per second)
0.00.405.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.405.050 I llama_perf_context_print:       total time =     148.93 ms /   129 tokens
0.00.405.551 I ggml_metal_free: deallocating

real	0m0.420s
user	0m0.077s
sys	0m0.056s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4279 (86a19349)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13630ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13630b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13630b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13630be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13630c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13630c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13630cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13630d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13630daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13630dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13630e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13630e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13630f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13630fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1363104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136310bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136311310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136311a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136312150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x136312920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136313040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136313760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136313e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136314720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136314e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136315100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136315710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x136316380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1363168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136316b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136317020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1363172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136317b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1363180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136318370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136318810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136318cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136319150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1363195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136319a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136319f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13631a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13631a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13631ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13631afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13631b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13631bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13631c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13631cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13631d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13631d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13631dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13631e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13631e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13631f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13631f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13631faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13631fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136320370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136320b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136320e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1363212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136321760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136321c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1363220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136322540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1363229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136322e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136323320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1363237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136323c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136324100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1363245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136324af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136325040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136325590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136325ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x136326030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136326580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136326ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136327020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x136327570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136327ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136328010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x136328560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136328ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136329000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136329550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x136329aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136329ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13632a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13632aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13632afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13632b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13632ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13632bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13632c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13631c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13632c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13632d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13632d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13632dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13632e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13632e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13632ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13632f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13632f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13632fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136330110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136330660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136330bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136331100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136331650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136331af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136331f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136332430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1363328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136332d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136333210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1363336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136333b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136333ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136334490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136334930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136334dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136335270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136335710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136335bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136336050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1363364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136336990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136336e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1363372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136337770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136337c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1363380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136338550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1363389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136338e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136339330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1363397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x136339c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13633a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13633a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13633aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13633aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13633b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13633b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13633bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13633c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13633c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13633cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13633cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13633d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13633d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13633dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13633e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13633e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13633eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13633efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13633f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13633f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13633fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136340230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1363406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136340b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136341010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1363414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136341950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136341df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136342290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136342730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136342bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136343070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136343510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1363439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136343e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1363442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136344790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136344c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1363450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136345570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136345a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136345eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136346350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1363467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136346c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136347130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1363475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x136347a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136347f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1363483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136348850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136348da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1363492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136349840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x136349d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13634a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13634a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13634ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13634b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13634ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13634bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13634c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13634c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13634cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13634d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13634da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13634df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13634e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13634eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13634f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13634f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13634fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1363500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136350600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136350b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1363510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1363515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136351b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136352090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1363525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136352b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136353080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1363535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136353b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136354070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1363545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136354b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136355060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1363555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136355b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136356050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1363565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136356af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136357040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136357590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136357ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136358030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136358580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136358ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136359020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136359570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136359ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13635a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13635a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13635aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13635b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13635b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13635baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13635bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13635c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13635ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13635cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13635d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13635da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13635dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13635e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13635ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13635efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13635f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13635fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13635ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136360500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136360a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136360fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1363614f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136361990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136361e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1363622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136362770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136362c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1363630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136363550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1363639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136363e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136364330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1363647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136364c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136365110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1363655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136365a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136365fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1363666c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136366de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136367500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136367c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136367ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1363686d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136368990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136368fa0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.377 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13630c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13630cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13630d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13630d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13630daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13630df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13630e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13630e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13630ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13630f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13630f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13630fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136310410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136310b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136311370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136311a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136312150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136312840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136312f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1363138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136313fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136314690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136314d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136315470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136316440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1363168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136316d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136317190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136317600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136317a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136317ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1363181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136318610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136318a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136318ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136319360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1363197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136319c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13631a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13631a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13631a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13631ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13631b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13631b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13631bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13631bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13631c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13631c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13631cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13631d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13631d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13631da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13631ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13631e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13631e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13631ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13631f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13631f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13631f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13631fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136320250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1363206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136320b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136320fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136321410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136321880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136321cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136322160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1363225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136322a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136322eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136323320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136323790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136323c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136324070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1363244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136324950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136324dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136325230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1363256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x136325b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x136325f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1363263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x136326860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x136326cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136327140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1363275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136327a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136327e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136328300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136328770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136328be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136329050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1363294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136329930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136329da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13632a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13632a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13632aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13632af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13632b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13632b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13632bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13632c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13632c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13632ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13632ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13632d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13632d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13632dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13632e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13632e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13632e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13632ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13632f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13632f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13632fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13632ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1363303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136330820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136330c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136331100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136331570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1363319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136331e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1363322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136332730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136332ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136333010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136333480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1363338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136333d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1363341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136334640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136334ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136334f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x136335390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x136335800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x136335c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1363360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x136336550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1363369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136336e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1363372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136337710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136337b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136337ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136338460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1363388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136338d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1363391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136339620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136339a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136339f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13633a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13633a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13633ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13633b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13633b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13633b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13633be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13633c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13633c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13633cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13633cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13633d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13633d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13633dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13633e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13633e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13633ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13633eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13633f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13633f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13633fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1363400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136340510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136340980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136340df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136341260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1363416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136341b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136341fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136342420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136342890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x136342d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136343170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1363435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136343a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136343ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136344330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1363447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x136344c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136345080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1363454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x136345960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x136345dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136346240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1363466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136346b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x136346f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136347400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136347870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136347ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136348150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1363485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136348a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136348ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136349620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136349a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136349f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13634a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13634a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13634ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13634b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13634b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13634b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13634be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13634c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13634c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13634cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13634cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13634d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13634d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13634dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13634e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13634e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13634ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13634eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13634f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13634f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13634fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1363500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136350510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136350980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136350df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x136351260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1363516d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136351b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x136351fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136352420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136352890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x136352d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x136353170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1363535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x136353a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136353ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136354330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1363547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136354c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136355080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1363554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136355960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136355dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136356240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1363566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136356b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136356f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136357400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136357870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136357ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136358150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1363585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136358a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136358ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136359310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136359780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136359bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13635a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13635a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13635a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13635adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13635b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13635b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13635bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13635bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13635c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13635c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13635ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13635d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13635d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13635dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13635e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13635ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13635f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13635f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13635fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13635feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136360320 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13630c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13630cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13630d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13630d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13630daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13630df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13630e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13630e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13630ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13630f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13630f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13630fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136310410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136310b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136311370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136311a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136312150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136312840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136312f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1363138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136313fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136314690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136314d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136315470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x136315b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x136315fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x136316440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1363168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x136316d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x136317190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x136317600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x136317a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136317ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1363181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136318610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136318a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136318ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136319360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1363197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136319c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13631a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13631a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13631a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13631ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13631b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13631b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13631bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13631bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13631c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13631c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13631cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13631d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13631d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13631da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13631ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13631e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13631e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13631ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13631f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13631f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13631f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13631fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136320250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136405e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136406110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136406580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136406d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1364071a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136407610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136407a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136407ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x136408360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1364087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x136404e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x136405300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x136408a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x136408d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1364091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x136409630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x136409aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x136409f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13640a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13640a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13640ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13640b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13640b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13640b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13640be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13640c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13640c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13640cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13640cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13640d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13640d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13640dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13640e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13640e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13640ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13640eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13640f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13640f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13640fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1364100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136410520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136410990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136410e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136411270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1364116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136411b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136411fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136412430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1364128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136412d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136413180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1364135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136413a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136413ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136414340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1364147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136414c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136415090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136415500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136415970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136415de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136416250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1364166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136416b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x136416fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x136417410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x136417880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x136417cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x136418160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1364185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x136418a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x136418eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x136419320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x136419790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x136419c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13641a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13641a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13641a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13641adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13641b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13641b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13641bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13641bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13641c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13641c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13641ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13641d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13641d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13641da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13641de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13641e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13641e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13641ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13641f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13641f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13641f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13641fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136420210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136420680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136420af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136420f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1364213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136421840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136421cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136422120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136422590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136422a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136422e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1364232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136423750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136423bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136424030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1364244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136424910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136424d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1364251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136425660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136425ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136425f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1364263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136426820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x136426c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x136427100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x136427570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1364279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x136427e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1364282c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x136428730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x136428ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x136429010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x136429480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1364298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x136429d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13642a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13642a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13642aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13642af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13642b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13642b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13642bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13642c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13642c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13642ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13642d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13642db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13642dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13642e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13642ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13642f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13642f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13642fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136430170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1364306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136430c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136431160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1364316b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136431c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136432150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1364326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136432bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136433140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136433690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136433be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136434130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136434680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136434bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136435120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136435670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136435bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136436110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136436660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136436bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136437100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136437650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136437ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1364380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136438640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136438b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1364390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x136439630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x136439b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13643a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13643a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13643ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13643b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13643b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13643bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13643c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13643c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13643cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13643d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13643d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13643db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13643e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13643e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13643eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13643f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13643f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13643fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136440070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1364405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136440b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136441060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1364415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136441a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136441ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136442390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136442830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136442cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136443170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136443610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136443ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136443f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1364443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136444890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136444d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1364451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136445670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x136445b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136446060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136446780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136446ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1364475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136447ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136447fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136448790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136448a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136449060 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.828s
user	0m0.305s
sys	0m0.268s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4279 (86a19349)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1367103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136710af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1367110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136711650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136711c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1367121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136712760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136712d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1367132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1367137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136713cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1367141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136715490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136715ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1367163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x136716ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x136717200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x136717920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1367180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x136718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x136718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x136719650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x136719ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13671a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13671a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13671aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13671bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13671c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13671c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13671c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13671cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13671d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13671d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13671db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13671dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13671e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13671e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13671edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13671f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13671f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13671fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136720040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1367204e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1367207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136720db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1367213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136721ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1367222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136722900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136722f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136723520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136723b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136724140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136724930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136724dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136725270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136725530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136725b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136726330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1367265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136726a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136726f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1367273d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136727870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136727d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1367281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136728650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136728af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136728f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x136729430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1367298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x136729d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13672a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13672a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13672ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13672b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13672b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13672bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13672c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13672c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13672cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13672d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13672d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13672dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13672e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13672e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13672ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13672f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13672f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13672fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136730260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1367307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136731250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1367317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136731cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1367219d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136732160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136732910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136732e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1367333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136733900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136733e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1367343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1367348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136734e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136735390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1367358e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136735e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136736380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1367368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136736e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1367372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136737760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136737c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1367380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136738540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1367389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136738e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136739320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1367397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136739c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13673a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13673a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13673aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13673aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13673b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13673b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13673bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13673c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13673c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13673caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13673cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13673d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13673d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13673dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13673e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13673e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13673eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13673efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13673f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13673f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13673fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x136740220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1367406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136740b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136741000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1367414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136741940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136741de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136742280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136742720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136742bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136743060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136743500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1367439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136743e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1367442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x136744780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136744c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1367450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136745560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136745a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136745ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136746340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1367467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136747120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1367475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136747a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136747f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1367483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136748840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136748ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136749180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136749620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136749ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136749f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13674a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13674a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13674ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13674b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13674b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13674bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13674bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13674c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13674c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13674cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13674d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13674d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13674db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13674e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13674e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13674eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13674f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13674f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13674f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13674fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x136750440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x136750a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x136751240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1367516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1367519a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136751fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1367525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136752db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136753250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1367536f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136753b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136754340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136754890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136754de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136755330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136755880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136755dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136756320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136756870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136756dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136757310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136757860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136757db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136758300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136758850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136758da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1367592f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136759840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136759d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13675a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13675a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13675ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13675b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13675b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13675bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13675c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13675c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13675cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13675d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13675d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13675dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13675e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13675e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13675ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13675f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13675f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13675fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x136760280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1367607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x136760d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x136761270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1367617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136761d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136762260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1367627b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136762d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136763250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1367637a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x136763cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136764240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136764790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136764ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136765230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136765780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136765cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136766220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136766770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136766cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136767160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136767600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136767aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136767f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1367683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136768880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136768d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1367691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136769660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136769b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136769fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13676a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13676a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13676ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13676b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13676b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13676be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13676c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13676ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13676d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13676d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13676dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13676e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13676e770 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.856 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x137a04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x137a04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x137a053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x137a05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x137a05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x137a06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x137a06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x137a069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x137a06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x137a073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x137a07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x137a07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x137a089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x137a091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x137a099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x137a0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x137a0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x137a0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x137a0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x137a0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x137a0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x137a0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x137a0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x137a0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x137a0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x137a0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x137a0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x137a0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x137a0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x137a0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x137a0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x137a0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x137a10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x137a10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x137a109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x137a10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x137a11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x137a11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x137a11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x137a11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x137a12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x137a128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x137a12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x137a131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x137a13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x137a13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x137a13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x137a14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x137a147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x137a14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x137804230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1378046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x137804b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x137804f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1378053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x137805860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x137805dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1378062c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x137806730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x137806ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x137807010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x137807480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1378078f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x137807d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1378081d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x137808640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x137808ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x137808f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x137809390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x137809800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x137809c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13780a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13780a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13780a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13780ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13780b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13780b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13780bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13780bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13780c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13780c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13780cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13780d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13780d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13780da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13780df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13780e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13780e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13780ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13780f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13780f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13780f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13780fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x137810280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1378106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x137810b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x137810fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x137811440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1378118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x137811d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x137812190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x137812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x137812a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x137812ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x137813350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1378137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x137813c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1378140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x137814510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x137814980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x137814df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x137815260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1378156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x137815b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x137815fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x137816420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x137816890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x137816d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x137817170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1378175e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x137817a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x137817ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x137818330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1378187a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x137818c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x137819080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1378194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x137819960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x137819dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13781a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13781a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13781ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13781af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13781b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13781b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13781bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13781c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13781c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13781ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13781cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13781d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13781d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13781dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13781e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13781e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13781e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13781edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13781f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13781f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13781fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13781ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1378203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x137820850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x137820cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x137821130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1378215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x137821a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x137821e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1378222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x137822760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x137822bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x137823040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1378234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x137823920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x137823d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x137824200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x137824670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x137824ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x137824f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1378253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x137825830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x137825ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x137826110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x137826580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1378269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x137826e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1378272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x137827740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x137827bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x137828020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x137828490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x137828900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x137828d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1378291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x137829650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x137829ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x137829f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13782a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13782a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13782ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13782b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13782b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13782b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13782be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13782c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13782c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13782cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13782d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13782d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13782d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13782dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13782e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13782e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13782eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13782ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13782f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13782f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13782fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1378301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x137830660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1378311b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x137831470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x137831730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x137831ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x137832010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x137832480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1378328f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x137832d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1378331d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x137833640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x137833ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x137833f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x137834390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x137834800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x137834c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1378350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x137835550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1378359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x137835e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1378362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x137836710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x137836b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x137836ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x137837460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1378378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x137837d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1378381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x137838620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x137838a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x137838f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x137839370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1378397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x137839c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13783a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13783a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13783a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13783ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13783b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13783b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13783bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13783bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13783c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13783c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13783cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13783d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13783d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13783da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13783dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13783e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13783e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13783ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13783f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13783f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13783f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13783fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x137840260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1378406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x137840b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x137840fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x137841420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x137841890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x137841d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x137842170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1378425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x137842a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x137842ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x137843330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1378437a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x137843c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x137844080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1378444f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x137844960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x137844dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x137845840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x137845f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x137846680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x137846da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x137847060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1378474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x137847ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1378480e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1366055b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136605a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136605e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136606300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136606770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136606be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136607050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1366074c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136607930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136607eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136608320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1366089a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1366094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136609c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13660a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13660aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13660b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13660b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13660c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13660c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13660cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13660d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13660de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13660e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13660ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13660ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13660f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13660f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13660fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13660ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1366103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1366108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136610d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136611010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x136611480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1366118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136611d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1366121d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136612640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136612ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136612f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136613390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136613800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136613c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1366140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136614550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1366149c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1366152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136615710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136615b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136615ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136616460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1366168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136616d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1366171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136617720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136617c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136618090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136618500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136618970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136618de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136619250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1366196c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136619b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136619fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13661a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13661a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13661acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13661b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13661b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13661ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13661beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13661c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13661c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13661cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13661d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13661d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13661d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13661ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13661e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13661e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13661eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13661ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13661f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13661f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13661fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x136620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1366205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x136620a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x136620e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136621300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136621770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136621be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1366224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136622930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136622da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136623210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136623680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136623af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136623f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1366243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136624840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136624cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136625120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136625590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136625a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136625e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1366262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136626750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136626bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136627030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1366274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136627910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136627d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1366281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136628660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136628ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136628f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1366293b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136629820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136629c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13662a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13662a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13662a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13662ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13662b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13662b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13662bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13662c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13662c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13662c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13662cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13662d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13662d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13662dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13662df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13662e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13662e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13662ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13662f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13662f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13662f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13662fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1366302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x136630710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x136630b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136630ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136631460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1366318d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136631d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1366321b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136632620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136632a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136632f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136633370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1366337e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136633c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1366340c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136634530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1366349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136634e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136635280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1366356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136635b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136635fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136636440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1366368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136636d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136637190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136637600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136637a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136637ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136638350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1366387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136638c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1366390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136639510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136639980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136639df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13663a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13663a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13663ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13663afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13663b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13663b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13663bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13663c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13663c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13663ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13663cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13663d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13663d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13663dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13663e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13663e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13663e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13663edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13663f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13663f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13663fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13663ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x136640400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136640870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136640ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136641150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1366416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136641b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136641fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136642b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136642dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136643090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x136643500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136643970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136643de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1366446c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136644b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136644fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136645410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136645880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136645cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136646160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1366465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136646a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136646eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136647320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136647790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136647c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136648070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1366484e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136648950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136648dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1366496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136649b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136649f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13664a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13664a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13664acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13664b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13664b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13664ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13664c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13664c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13664c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13664ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13664d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13664d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13664db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13664dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13664e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13664e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13664ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13664f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13664f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13664fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13664fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136650340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1366507b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x136650c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136651090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136651500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136651970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136652250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1366526c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136652c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1366530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136653510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136653980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136653df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136654260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1366546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136654b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136654fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136655420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136655890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x136655d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136656170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1366565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136656a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1366575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x136657cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1366583e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x136658b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x136658dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136659080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1366594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136659960 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.910s
user	0m0.242s
sys	0m0.126s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.27 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.55 real         0.15 user         0.04 sys
```
