### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.38 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.17 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.11 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.25 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.24 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  181.47 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.89 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   26.09 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.41 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 224.16 sec*proc (27 tests)

Total Test time (real) = 224.17 sec

real	3m44.197s
user	7m45.258s
sys	0m6.366s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.17 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.92 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.17 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.31 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.38 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.04 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.21 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.92 sec*proc (27 tests)

Total Test time (real) =  50.93 sec

real	0m50.937s
user	1m11.984s
sys	0m5.439s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.123 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.561 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.585 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.594 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.597 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.030.598 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.599 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.030.600 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.030.600 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.030.602 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.030.603 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.030.604 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.030.604 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.030.605 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.030.609 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.610 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.611 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.030.611 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.030.612 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.613 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.030.613 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.036.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.037.385 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.388 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.037.389 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.037.389 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.037.390 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.037.390 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.037.391 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.037.391 I llama_model_loader: - type  f32:  124 tensors
0.00.037.392 I llama_model_loader: - type  f16:   73 tensors
0.00.042.743 I llm_load_vocab: special tokens cache size = 5
0.00.045.099 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.045.103 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.045.103 I llm_load_print_meta: arch             = bert
0.00.045.104 I llm_load_print_meta: vocab type       = WPM
0.00.045.104 I llm_load_print_meta: n_vocab          = 30522
0.00.045.105 I llm_load_print_meta: n_merges         = 0
0.00.045.105 I llm_load_print_meta: vocab_only       = 0
0.00.045.105 I llm_load_print_meta: n_ctx_train      = 512
0.00.045.105 I llm_load_print_meta: n_embd           = 384
0.00.045.106 I llm_load_print_meta: n_layer          = 12
0.00.045.109 I llm_load_print_meta: n_head           = 12
0.00.045.136 I llm_load_print_meta: n_head_kv        = 12
0.00.045.136 I llm_load_print_meta: n_rot            = 32
0.00.045.137 I llm_load_print_meta: n_swa            = 0
0.00.045.137 I llm_load_print_meta: n_embd_head_k    = 32
0.00.045.137 I llm_load_print_meta: n_embd_head_v    = 32
0.00.045.139 I llm_load_print_meta: n_gqa            = 1
0.00.045.140 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.045.141 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.045.141 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.045.142 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.045.142 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.045.143 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.045.143 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.045.144 I llm_load_print_meta: n_ff             = 1536
0.00.045.144 I llm_load_print_meta: n_expert         = 0
0.00.045.144 I llm_load_print_meta: n_expert_used    = 0
0.00.045.145 I llm_load_print_meta: causal attn      = 0
0.00.045.145 I llm_load_print_meta: pooling type     = 2
0.00.045.145 I llm_load_print_meta: rope type        = 2
0.00.045.146 I llm_load_print_meta: rope scaling     = linear
0.00.045.146 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.045.148 I llm_load_print_meta: freq_scale_train = 1
0.00.045.148 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.045.151 I llm_load_print_meta: rope_finetuned   = unknown
0.00.045.151 I llm_load_print_meta: ssm_d_conv       = 0
0.00.045.151 I llm_load_print_meta: ssm_d_inner      = 0
0.00.045.151 I llm_load_print_meta: ssm_d_state      = 0
0.00.045.152 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.045.152 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.045.163 I llm_load_print_meta: model type       = 33M
0.00.045.164 I llm_load_print_meta: model ftype      = F16
0.00.045.165 I llm_load_print_meta: model params     = 33.21 M
0.00.045.165 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.045.166 I llm_load_print_meta: general.name     = Bge Small
0.00.045.166 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.045.167 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.045.167 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.045.167 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.045.168 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.045.168 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.045.168 I llm_load_print_meta: max token length = 21
0.00.047.352 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.047.358 I llm_load_tensors: offloading output layer to GPU
0.00.047.359 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.047.388 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.047.390 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.048.060 I llama_new_context_with_model: n_seq_max     = 1
0.00.048.062 I llama_new_context_with_model: n_ctx         = 512
0.00.048.062 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.048.062 I llama_new_context_with_model: n_batch       = 2048
0.00.048.063 I llama_new_context_with_model: n_ubatch      = 2048
0.00.048.063 I llama_new_context_with_model: flash_attn    = 0
0.00.048.064 I llama_new_context_with_model: freq_base     = 10000.0
0.00.048.064 I llama_new_context_with_model: freq_scale    = 1
0.00.048.065 I ggml_metal_init: allocating
0.00.048.079 I ggml_metal_init: found device: Apple M4
0.00.048.086 I ggml_metal_init: picking default device: Apple M4
0.00.049.116 I ggml_metal_init: using embedded metal library
0.00.053.913 I ggml_metal_init: GPU name:   Apple M4
0.00.053.916 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.917 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.917 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.917 I ggml_metal_init: simdgroup reduction   = true
0.00.053.918 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.918 I ggml_metal_init: has bfloat            = true
0.00.053.918 I ggml_metal_init: use bfloat            = true
0.00.053.919 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.920 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.927 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.067.929 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.067.931 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.068.830 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.068.831 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.068.831 I llama_new_context_with_model: graph nodes  = 429
0.00.068.832 I llama_new_context_with_model: graph splits = 2
0.00.068.855 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.075.544 I 
0.00.075.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.076.306 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.081.068 I llama_perf_context_print:        load time =      49.97 ms
0.00.081.069 I llama_perf_context_print: prompt eval time =       4.60 ms /     9 tokens (    0.51 ms per token,  1955.67 tokens per second)
0.00.081.070 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.081.071 I llama_perf_context_print:       total time =       5.52 ms /    10 tokens
0.00.081.212 I ggml_metal_free: deallocating

real	0m0.258s
user	0m0.054s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.376 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.551 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.556 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.557 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.557 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.557 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.558 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.558 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.559 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.559 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.561 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.562 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.564 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.564 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.564 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.565 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.565 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.565 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.565 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.793 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.794 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.794 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.795 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.795 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.795 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.796 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.796 I llama_model_loader: - type  f32:  124 tensors
0.00.014.796 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.378 I llm_load_vocab: special tokens cache size = 5
0.00.018.731 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.733 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.734 I llm_load_print_meta: arch             = bert
0.00.018.734 I llm_load_print_meta: vocab type       = WPM
0.00.018.734 I llm_load_print_meta: n_vocab          = 30522
0.00.018.734 I llm_load_print_meta: n_merges         = 0
0.00.018.735 I llm_load_print_meta: vocab_only       = 0
0.00.018.735 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.735 I llm_load_print_meta: n_embd           = 384
0.00.018.735 I llm_load_print_meta: n_layer          = 12
0.00.018.738 I llm_load_print_meta: n_head           = 12
0.00.018.745 I llm_load_print_meta: n_head_kv        = 12
0.00.018.745 I llm_load_print_meta: n_rot            = 32
0.00.018.745 I llm_load_print_meta: n_swa            = 0
0.00.018.746 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.746 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.746 I llm_load_print_meta: n_gqa            = 1
0.00.018.747 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.747 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.748 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.748 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.749 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.749 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.749 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.750 I llm_load_print_meta: n_ff             = 1536
0.00.018.750 I llm_load_print_meta: n_expert         = 0
0.00.018.750 I llm_load_print_meta: n_expert_used    = 0
0.00.018.750 I llm_load_print_meta: causal attn      = 0
0.00.018.753 I llm_load_print_meta: pooling type     = 2
0.00.018.753 I llm_load_print_meta: rope type        = 2
0.00.018.753 I llm_load_print_meta: rope scaling     = linear
0.00.018.754 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.754 I llm_load_print_meta: freq_scale_train = 1
0.00.018.754 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.754 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.755 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.755 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.755 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.755 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.755 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.760 I llm_load_print_meta: model type       = 33M
0.00.018.760 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.762 I llm_load_print_meta: model params     = 33.21 M
0.00.018.762 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.762 I llm_load_print_meta: general.name     = Bge Small
0.00.018.763 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.763 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.763 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.763 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.763 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.766 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.766 I llm_load_print_meta: max token length = 21
0.00.020.064 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.064 I llm_load_tensors: offloading output layer to GPU
0.00.020.067 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.075 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.076 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.452 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.453 I llama_new_context_with_model: n_ctx         = 512
0.00.020.453 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.454 I llama_new_context_with_model: n_batch       = 2048
0.00.020.454 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.454 I llama_new_context_with_model: flash_attn    = 0
0.00.020.454 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.455 I llama_new_context_with_model: freq_scale    = 1
0.00.020.455 I ggml_metal_init: allocating
0.00.020.460 I ggml_metal_init: found device: Apple M4
0.00.020.462 I ggml_metal_init: picking default device: Apple M4
0.00.021.065 I ggml_metal_init: using embedded metal library
0.00.023.641 I ggml_metal_init: GPU name:   Apple M4
0.00.023.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.643 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.643 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.644 I ggml_metal_init: simdgroup reduction   = true
0.00.023.644 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.644 I ggml_metal_init: has bfloat            = true
0.00.023.644 I ggml_metal_init: use bfloat            = true
0.00.023.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.553 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.556 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.560 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.035.223 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.035.224 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.035.224 I llama_new_context_with_model: graph nodes  = 429
0.00.035.225 I llama_new_context_with_model: graph splits = 2
0.00.035.237 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.450 I 
0.00.040.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.045 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.496 I llama_perf_context_print:        load time =      31.07 ms
0.00.045.498 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2077.08 tokens per second)
0.00.045.498 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.499 I llama_perf_context_print:       total time =       5.05 ms /    10 tokens
0.00.045.640 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.132 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.158 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.746 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.751 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.753 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.755 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.756 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.757 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.758 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.759 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.760 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.760 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.761 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.765 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.766 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.766 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.400 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.483 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.484 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.484 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.485 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.485 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.486 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.486 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.486 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.487 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.487 I llama_model_loader: - type  f32:   41 tensors
0.00.050.488 I llama_model_loader: - type  f16:   29 tensors
0.00.068.865 W llm_load_vocab: empty token at index 5
0.00.073.564 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.830 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.856 I llm_load_vocab: special tokens cache size = 5
0.00.336.021 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.336.026 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.336.027 I llm_load_print_meta: arch             = jina-bert-v2
0.00.336.027 I llm_load_print_meta: vocab type       = BPE
0.00.336.028 I llm_load_print_meta: n_vocab          = 61056
0.00.336.028 I llm_load_print_meta: n_merges         = 39382
0.00.336.028 I llm_load_print_meta: vocab_only       = 0
0.00.336.028 I llm_load_print_meta: n_ctx_train      = 8192
0.00.336.029 I llm_load_print_meta: n_embd           = 384
0.00.336.029 I llm_load_print_meta: n_layer          = 4
0.00.336.035 I llm_load_print_meta: n_head           = 12
0.00.336.061 I llm_load_print_meta: n_head_kv        = 12
0.00.336.062 I llm_load_print_meta: n_rot            = 32
0.00.336.062 I llm_load_print_meta: n_swa            = 0
0.00.336.062 I llm_load_print_meta: n_embd_head_k    = 32
0.00.336.062 I llm_load_print_meta: n_embd_head_v    = 32
0.00.336.062 I llm_load_print_meta: n_gqa            = 1
0.00.336.063 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.336.064 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.336.064 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.336.065 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.336.065 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.336.066 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.336.066 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.336.066 I llm_load_print_meta: n_ff             = 1536
0.00.336.066 I llm_load_print_meta: n_expert         = 0
0.00.336.067 I llm_load_print_meta: n_expert_used    = 0
0.00.336.067 I llm_load_print_meta: causal attn      = 0
0.00.336.067 I llm_load_print_meta: pooling type     = -1
0.00.336.067 I llm_load_print_meta: rope type        = -1
0.00.336.067 I llm_load_print_meta: rope scaling     = linear
0.00.336.068 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.336.068 I llm_load_print_meta: freq_scale_train = 1
0.00.336.068 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.336.069 I llm_load_print_meta: rope_finetuned   = unknown
0.00.336.069 I llm_load_print_meta: ssm_d_conv       = 0
0.00.336.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.336.069 I llm_load_print_meta: ssm_d_state      = 0
0.00.336.070 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.336.070 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.336.091 I llm_load_print_meta: model type       = 33M
0.00.336.092 I llm_load_print_meta: model ftype      = F16
0.00.336.092 I llm_load_print_meta: model params     = 32.90 M
0.00.336.092 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.336.093 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.336.093 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.336.093 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.336.093 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.336.093 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.336.093 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.336.094 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.336.094 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.336.094 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.336.094 I llm_load_print_meta: max token length = 45
0.00.337.147 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.337.147 I llm_load_tensors: offloading output layer to GPU
0.00.337.147 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.337.170 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.337.171 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.337.912 I llama_new_context_with_model: n_seq_max     = 1
0.00.337.913 I llama_new_context_with_model: n_ctx         = 8192
0.00.337.913 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.337.914 I llama_new_context_with_model: n_batch       = 2048
0.00.337.914 I llama_new_context_with_model: n_ubatch      = 2048
0.00.337.914 I llama_new_context_with_model: flash_attn    = 0
0.00.337.915 I llama_new_context_with_model: freq_base     = 10000.0
0.00.337.915 I llama_new_context_with_model: freq_scale    = 1
0.00.337.916 I ggml_metal_init: allocating
0.00.337.919 I ggml_metal_init: found device: Apple M4
0.00.337.920 I ggml_metal_init: picking default device: Apple M4
0.00.338.949 I ggml_metal_init: using embedded metal library
0.00.341.734 I ggml_metal_init: GPU name:   Apple M4
0.00.341.735 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.736 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.736 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.736 I ggml_metal_init: simdgroup reduction   = true
0.00.341.736 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.737 I ggml_metal_init: has bfloat            = true
0.00.341.737 I ggml_metal_init: use bfloat            = true
0.00.341.737 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.738 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.583 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.353.587 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.353.589 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.354.149 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.354.150 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.354.150 I llama_new_context_with_model: graph nodes  = 154
0.00.354.150 I llama_new_context_with_model: graph splits = 2
0.00.354.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.897 I 
0.00.366.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.094 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.094 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.102 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.104 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.367.107 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.367.107 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.682 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.371.287 I llama_perf_context_print:        load time =     342.73 ms
0.00.371.288 I llama_perf_context_print: prompt eval time =       3.59 ms /    62 tokens (    0.06 ms per token, 17250.97 tokens per second)
0.00.371.291 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.371.291 I llama_perf_context_print:       total time =       4.39 ms /    63 tokens
0.00.371.511 I ggml_metal_free: deallocating

real	0m1.062s
user	0m0.342s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.105 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.220 I main: llama backend init
0.00.000.228 I main: load the model and apply lora adapter, if any
0.00.028.326 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.242 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.258 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.267 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.268 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.271 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.275 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.954 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.060.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.540 I llama_model_loader: - type  f32:  194 tensors
0.00.060.540 I llama_model_loader: - type  f16:   98 tensors
0.00.093.159 I llm_load_vocab: special tokens cache size = 25
0.00.100.069 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.100.071 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.100.072 I llm_load_print_meta: arch             = gptneox
0.00.100.072 I llm_load_print_meta: vocab type       = BPE
0.00.100.072 I llm_load_print_meta: n_vocab          = 50304
0.00.100.072 I llm_load_print_meta: n_merges         = 50009
0.00.100.073 I llm_load_print_meta: vocab_only       = 0
0.00.100.073 I llm_load_print_meta: n_ctx_train      = 2048
0.00.100.073 I llm_load_print_meta: n_embd           = 2048
0.00.100.073 I llm_load_print_meta: n_layer          = 24
0.00.100.076 I llm_load_print_meta: n_head           = 16
0.00.100.096 I llm_load_print_meta: n_head_kv        = 16
0.00.100.097 I llm_load_print_meta: n_rot            = 32
0.00.100.097 I llm_load_print_meta: n_swa            = 0
0.00.100.097 I llm_load_print_meta: n_embd_head_k    = 128
0.00.100.097 I llm_load_print_meta: n_embd_head_v    = 128
0.00.100.098 I llm_load_print_meta: n_gqa            = 1
0.00.100.099 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.100.099 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.100.102 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.100.102 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.100.103 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.100.103 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.100.103 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.100.104 I llm_load_print_meta: n_ff             = 8192
0.00.100.104 I llm_load_print_meta: n_expert         = 0
0.00.100.104 I llm_load_print_meta: n_expert_used    = 0
0.00.100.104 I llm_load_print_meta: causal attn      = 1
0.00.100.104 I llm_load_print_meta: pooling type     = 0
0.00.100.104 I llm_load_print_meta: rope type        = 2
0.00.100.105 I llm_load_print_meta: rope scaling     = linear
0.00.100.105 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.100.105 I llm_load_print_meta: freq_scale_train = 1
0.00.100.106 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.100.106 I llm_load_print_meta: rope_finetuned   = unknown
0.00.100.106 I llm_load_print_meta: ssm_d_conv       = 0
0.00.100.106 I llm_load_print_meta: ssm_d_inner      = 0
0.00.100.106 I llm_load_print_meta: ssm_d_state      = 0
0.00.100.106 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.100.106 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.100.116 I llm_load_print_meta: model type       = 1.4B
0.00.100.116 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.100.117 I llm_load_print_meta: model params     = 1.41 B
0.00.100.117 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.100.117 I llm_load_print_meta: general.name     = 1.4B
0.00.100.118 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.100.118 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.100.118 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.100.118 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.100.118 I llm_load_print_meta: LF token         = 128 ''
0.00.100.119 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.100.119 I llm_load_print_meta: max token length = 1024
0.00.102.712 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.102.712 I llm_load_tensors: offloading output layer to GPU
0.00.102.714 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.102.728 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.102.729 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.103.678 I llama_new_context_with_model: n_seq_max     = 1
0.00.103.679 I llama_new_context_with_model: n_ctx         = 2048
0.00.103.680 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.103.680 I llama_new_context_with_model: n_batch       = 2048
0.00.103.680 I llama_new_context_with_model: n_ubatch      = 512
0.00.103.680 I llama_new_context_with_model: flash_attn    = 0
0.00.103.681 I llama_new_context_with_model: freq_base     = 10000.0
0.00.103.681 I llama_new_context_with_model: freq_scale    = 1
0.00.103.681 I ggml_metal_init: allocating
0.00.103.690 I ggml_metal_init: found device: Apple M4
0.00.103.693 I ggml_metal_init: picking default device: Apple M4
0.00.104.399 I ggml_metal_init: using embedded metal library
0.00.113.845 I ggml_metal_init: GPU name:   Apple M4
0.00.113.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.849 I ggml_metal_init: simdgroup reduction   = true
0.00.113.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.849 I ggml_metal_init: has bfloat            = true
0.00.113.849 I ggml_metal_init: use bfloat            = true
0.00.113.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.158.461 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.158.468 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.158.489 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.159.473 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.159.474 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.159.475 I llama_new_context_with_model: graph nodes  = 967
0.00.159.475 I llama_new_context_with_model: graph splits = 2
0.00.159.500 I common_init_from_params: added EOS logit bias = -inf
0.00.159.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.239.287 I main: llama threadpool init, n_threads = 4
0.00.239.320 I 
0.00.239.360 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.239.361 I 
0.00.239.441 I sampler seed: 1234
0.00.239.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.239.469 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.239.471 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.239.471 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.089.143 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.02.089.144 I llama_perf_context_print:        load time =     210.95 ms
0.02.089.145 I llama_perf_context_print: prompt eval time =      43.82 ms /     7 tokens (    6.26 ms per token,   159.76 tokens per second)
0.02.089.146 I llama_perf_context_print:        eval time =    1802.91 ms /    63 runs   (   28.62 ms per token,    34.94 tokens per second)
0.02.089.146 I llama_perf_context_print:       total time =    1849.86 ms /    70 tokens
0.02.089.330 I ggml_metal_free: deallocating

real	0m2.372s
user	0m0.145s
sys	0m0.101s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.521 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.131 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.214 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.228 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.230 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.234 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.239 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.247 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.248 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.165 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.111 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.113 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.114 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.115 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.116 I llama_model_loader: - type  f32:  194 tensors
0.00.053.116 I llama_model_loader: - type  f16:   98 tensors
0.00.083.898 I llm_load_vocab: special tokens cache size = 25
0.00.090.956 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.959 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.959 I llm_load_print_meta: arch             = gptneox
0.00.090.959 I llm_load_print_meta: vocab type       = BPE
0.00.090.959 I llm_load_print_meta: n_vocab          = 50304
0.00.090.959 I llm_load_print_meta: n_merges         = 50009
0.00.090.960 I llm_load_print_meta: vocab_only       = 0
0.00.090.960 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.960 I llm_load_print_meta: n_embd           = 2048
0.00.090.960 I llm_load_print_meta: n_layer          = 24
0.00.090.963 I llm_load_print_meta: n_head           = 16
0.00.090.975 I llm_load_print_meta: n_head_kv        = 16
0.00.090.976 I llm_load_print_meta: n_rot            = 32
0.00.090.976 I llm_load_print_meta: n_swa            = 0
0.00.090.976 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.977 I llm_load_print_meta: n_gqa            = 1
0.00.090.978 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.979 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.979 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.979 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.980 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.980 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.980 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.981 I llm_load_print_meta: n_ff             = 8192
0.00.090.981 I llm_load_print_meta: n_expert         = 0
0.00.090.981 I llm_load_print_meta: n_expert_used    = 0
0.00.090.981 I llm_load_print_meta: causal attn      = 1
0.00.090.981 I llm_load_print_meta: pooling type     = 0
0.00.090.981 I llm_load_print_meta: rope type        = 2
0.00.090.981 I llm_load_print_meta: rope scaling     = linear
0.00.090.982 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.982 I llm_load_print_meta: freq_scale_train = 1
0.00.090.982 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.982 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.983 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.983 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.983 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.983 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.985 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.994 I llm_load_print_meta: model type       = 1.4B
0.00.090.995 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.995 I llm_load_print_meta: model params     = 1.41 B
0.00.090.996 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.996 I llm_load_print_meta: general.name     = 1.4B
0.00.090.996 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.996 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.996 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.997 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.998 I llm_load_print_meta: LF token         = 128 ''
0.00.090.998 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.998 I llm_load_print_meta: max token length = 1024
0.00.093.538 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.539 I llm_load_tensors: offloading output layer to GPU
0.00.093.539 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.549 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.550 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.482 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.483 I llama_new_context_with_model: n_ctx         = 128
0.00.094.483 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.483 I llama_new_context_with_model: n_batch       = 128
0.00.094.483 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.484 I llama_new_context_with_model: flash_attn    = 0
0.00.094.484 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.484 I llama_new_context_with_model: freq_scale    = 1
0.00.094.485 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.485 I ggml_metal_init: allocating
0.00.094.493 I ggml_metal_init: found device: Apple M4
0.00.094.496 I ggml_metal_init: picking default device: Apple M4
0.00.095.097 I ggml_metal_init: using embedded metal library
0.00.097.657 I ggml_metal_init: GPU name:   Apple M4
0.00.097.658 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.659 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.659 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.659 I ggml_metal_init: simdgroup reduction   = true
0.00.097.660 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.660 I ggml_metal_init: has bfloat            = true
0.00.097.660 I ggml_metal_init: use bfloat            = true
0.00.097.660 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.661 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.028 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.035 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.049 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.881 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.882 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.882 I llama_new_context_with_model: graph nodes  = 967
0.00.109.883 I llama_new_context_with_model: graph splits = 2
0.00.109.894 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.958.995 I 
0.00.959.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.959.069 I perplexity: tokenizing the input ..
0.00.972.099 I perplexity: tokenization took 13.024 ms
0.00.972.137 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.093.702 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.095.391 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.095.415 I llama_perf_context_print:        load time =     935.85 ms
0.01.095.416 I llama_perf_context_print: prompt eval time =     120.91 ms /   128 tokens (    0.94 ms per token,  1058.66 tokens per second)
0.01.095.421 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.095.425 I llama_perf_context_print:       total time =     136.42 ms /   129 tokens
0.01.096.142 I ggml_metal_free: deallocating

real	0m1.287s
user	0m0.125s
sys	0m0.202s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.707 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.543 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.549 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.551 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.553 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.553 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.554 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.554 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.555 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.556 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.556 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.556 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.557 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.557 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.557 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.559 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.559 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.560 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.543 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.036.018 I llama_model_loader: - type  f32:  194 tensors
0.00.036.019 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.668 I llm_load_vocab: special tokens cache size = 25
0.00.067.169 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.173 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.173 I llm_load_print_meta: arch             = gptneox
0.00.067.173 I llm_load_print_meta: vocab type       = BPE
0.00.067.173 I llm_load_print_meta: n_vocab          = 50304
0.00.067.174 I llm_load_print_meta: n_merges         = 50009
0.00.067.174 I llm_load_print_meta: vocab_only       = 0
0.00.067.174 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.174 I llm_load_print_meta: n_embd           = 2048
0.00.067.174 I llm_load_print_meta: n_layer          = 24
0.00.067.179 I llm_load_print_meta: n_head           = 16
0.00.067.193 I llm_load_print_meta: n_head_kv        = 16
0.00.067.194 I llm_load_print_meta: n_rot            = 32
0.00.067.194 I llm_load_print_meta: n_swa            = 0
0.00.067.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.195 I llm_load_print_meta: n_gqa            = 1
0.00.067.195 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.196 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.196 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.197 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.197 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.198 I llm_load_print_meta: n_ff             = 8192
0.00.067.198 I llm_load_print_meta: n_expert         = 0
0.00.067.198 I llm_load_print_meta: n_expert_used    = 0
0.00.067.198 I llm_load_print_meta: causal attn      = 1
0.00.067.198 I llm_load_print_meta: pooling type     = 0
0.00.067.200 I llm_load_print_meta: rope type        = 2
0.00.067.201 I llm_load_print_meta: rope scaling     = linear
0.00.067.202 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.203 I llm_load_print_meta: freq_scale_train = 1
0.00.067.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.203 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.203 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.203 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.203 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.203 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.214 I llm_load_print_meta: model type       = 1.4B
0.00.067.214 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.215 I llm_load_print_meta: model params     = 1.41 B
0.00.067.215 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.215 I llm_load_print_meta: general.name     = 1.4B
0.00.067.215 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.216 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.216 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.216 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.218 I llm_load_print_meta: LF token         = 128 ''
0.00.067.218 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.218 I llm_load_print_meta: max token length = 1024
0.00.069.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.826 I llm_load_tensors: offloading output layer to GPU
0.00.069.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.838 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.839 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.939 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.940 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.940 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.940 I llama_new_context_with_model: n_batch       = 2048
0.00.070.940 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.940 I llama_new_context_with_model: flash_attn    = 0
0.00.070.941 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.941 I llama_new_context_with_model: freq_scale    = 1
0.00.070.942 I ggml_metal_init: allocating
0.00.070.947 I ggml_metal_init: found device: Apple M4
0.00.070.949 I ggml_metal_init: picking default device: Apple M4
0.00.071.753 I ggml_metal_init: using embedded metal library
0.00.074.615 I ggml_metal_init: GPU name:   Apple M4
0.00.074.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.618 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.619 I ggml_metal_init: simdgroup reduction   = true
0.00.074.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.619 I ggml_metal_init: has bfloat            = true
0.00.074.619 I ggml_metal_init: use bfloat            = true
0.00.074.620 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.621 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.112.339 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.112.355 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.112.377 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.113.510 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.113.512 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.113.512 I llama_new_context_with_model: graph nodes  = 967
0.00.113.513 I llama_new_context_with_model: graph splits = 2
0.00.113.528 I common_init_from_params: added EOS logit bias = -inf
0.00.113.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.608.781 I main: llama threadpool init, n_threads = 4
0.01.608.856 I 
0.01.608.936 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.608.938 I 
0.01.609.474 I sampler seed: 1234
0.01.609.481 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.609.578 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.609.582 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.609.583 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.708.170 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51938.55 tokens per second)
0.02.708.170 I llama_perf_context_print:        load time =    1599.06 ms
0.02.708.172 I llama_perf_context_print: prompt eval time =      50.68 ms /     7 tokens (    7.24 ms per token,   138.14 tokens per second)
0.02.708.173 I llama_perf_context_print:        eval time =    1045.18 ms /    63 runs   (   16.59 ms per token,    60.28 tokens per second)
0.02.708.173 I llama_perf_context_print:       total time =    1099.40 ms /    70 tokens
0.02.708.365 I ggml_metal_free: deallocating

real	0m2.725s
user	0m0.127s
sys	0m0.264s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.898 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.233 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.242 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.245 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.245 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.246 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.246 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.247 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.248 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.248 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.248 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.249 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.250 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.252 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.252 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.253 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.649 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.215 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.619 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.620 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.620 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.621 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.621 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.622 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.623 I llama_model_loader: - type  f32:  194 tensors
0.00.031.623 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.969 I llm_load_vocab: special tokens cache size = 25
0.00.063.097 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.063.100 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.063.100 I llm_load_print_meta: arch             = gptneox
0.00.063.100 I llm_load_print_meta: vocab type       = BPE
0.00.063.100 I llm_load_print_meta: n_vocab          = 50304
0.00.063.100 I llm_load_print_meta: n_merges         = 50009
0.00.063.101 I llm_load_print_meta: vocab_only       = 0
0.00.063.101 I llm_load_print_meta: n_ctx_train      = 2048
0.00.063.101 I llm_load_print_meta: n_embd           = 2048
0.00.063.101 I llm_load_print_meta: n_layer          = 24
0.00.063.104 I llm_load_print_meta: n_head           = 16
0.00.063.117 I llm_load_print_meta: n_head_kv        = 16
0.00.063.118 I llm_load_print_meta: n_rot            = 32
0.00.063.118 I llm_load_print_meta: n_swa            = 0
0.00.063.118 I llm_load_print_meta: n_embd_head_k    = 128
0.00.063.118 I llm_load_print_meta: n_embd_head_v    = 128
0.00.063.119 I llm_load_print_meta: n_gqa            = 1
0.00.063.122 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.063.122 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.063.123 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.063.123 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.063.123 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.063.124 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.063.124 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.063.124 I llm_load_print_meta: n_ff             = 8192
0.00.063.124 I llm_load_print_meta: n_expert         = 0
0.00.063.125 I llm_load_print_meta: n_expert_used    = 0
0.00.063.125 I llm_load_print_meta: causal attn      = 1
0.00.063.125 I llm_load_print_meta: pooling type     = 0
0.00.063.125 I llm_load_print_meta: rope type        = 2
0.00.063.125 I llm_load_print_meta: rope scaling     = linear
0.00.063.126 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.063.126 I llm_load_print_meta: freq_scale_train = 1
0.00.063.126 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.063.126 I llm_load_print_meta: rope_finetuned   = unknown
0.00.063.126 I llm_load_print_meta: ssm_d_conv       = 0
0.00.063.126 I llm_load_print_meta: ssm_d_inner      = 0
0.00.063.126 I llm_load_print_meta: ssm_d_state      = 0
0.00.063.128 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.063.128 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.063.137 I llm_load_print_meta: model type       = 1.4B
0.00.063.138 I llm_load_print_meta: model ftype      = Q8_0
0.00.063.138 I llm_load_print_meta: model params     = 1.41 B
0.00.063.138 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.063.139 I llm_load_print_meta: general.name     = 1.4B
0.00.063.139 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.063.139 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.063.139 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.063.139 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.063.140 I llm_load_print_meta: LF token         = 128 ''
0.00.063.140 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.063.140 I llm_load_print_meta: max token length = 1024
0.00.065.296 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.296 I llm_load_tensors: offloading output layer to GPU
0.00.065.297 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.307 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.308 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.244 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.245 I llama_new_context_with_model: n_ctx         = 128
0.00.066.245 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.245 I llama_new_context_with_model: n_batch       = 128
0.00.066.245 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.245 I llama_new_context_with_model: flash_attn    = 0
0.00.066.246 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.246 I llama_new_context_with_model: freq_scale    = 1
0.00.066.246 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.246 I ggml_metal_init: allocating
0.00.066.255 I ggml_metal_init: found device: Apple M4
0.00.066.258 I ggml_metal_init: picking default device: Apple M4
0.00.066.827 I ggml_metal_init: using embedded metal library
0.00.069.175 I ggml_metal_init: GPU name:   Apple M4
0.00.069.176 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.177 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.177 I ggml_metal_init: simdgroup reduction   = true
0.00.069.177 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.177 I ggml_metal_init: has bfloat            = true
0.00.069.177 I ggml_metal_init: use bfloat            = true
0.00.069.178 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.178 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.014 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.079.016 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.079.030 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.079.965 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.079.967 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.079.967 I llama_new_context_with_model: graph nodes  = 967
0.00.079.968 I llama_new_context_with_model: graph splits = 2
0.00.079.981 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.873.448 I 
0.00.873.477 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.873.486 I perplexity: tokenizing the input ..
0.00.881.105 I perplexity: tokenization took 7.617 ms
0.00.881.115 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.005.476 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.006.653 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.006.665 I llama_perf_context_print:        load time =     862.55 ms
0.01.006.666 I llama_perf_context_print: prompt eval time =     124.14 ms /   128 tokens (    0.97 ms per token,  1031.13 tokens per second)
0.01.006.667 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.006.667 I llama_perf_context_print:       total time =     133.22 ms /   129 tokens
0.01.007.015 I ggml_metal_free: deallocating

real	0m1.023s
user	0m0.091s
sys	0m0.155s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.014.958 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.247 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.035.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.272 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.276 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.277 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.874 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.112 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.045.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.628 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.045.628 I llama_model_loader: - type  f32:  194 tensors
0.00.045.629 I llama_model_loader: - type q4_0:   97 tensors
0.00.045.629 I llama_model_loader: - type q6_K:    1 tensors
0.00.075.104 I llm_load_vocab: special tokens cache size = 25
0.00.085.215 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.220 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.220 I llm_load_print_meta: arch             = gptneox
0.00.085.220 I llm_load_print_meta: vocab type       = BPE
0.00.085.221 I llm_load_print_meta: n_vocab          = 50304
0.00.085.221 I llm_load_print_meta: n_merges         = 50009
0.00.085.221 I llm_load_print_meta: vocab_only       = 0
0.00.085.222 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.222 I llm_load_print_meta: n_embd           = 2048
0.00.085.222 I llm_load_print_meta: n_layer          = 24
0.00.085.229 I llm_load_print_meta: n_head           = 16
0.00.085.243 I llm_load_print_meta: n_head_kv        = 16
0.00.085.243 I llm_load_print_meta: n_rot            = 32
0.00.085.244 I llm_load_print_meta: n_swa            = 0
0.00.085.244 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.244 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.246 I llm_load_print_meta: n_gqa            = 1
0.00.085.247 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.248 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.248 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.250 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.256 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.257 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.257 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.258 I llm_load_print_meta: n_ff             = 8192
0.00.085.258 I llm_load_print_meta: n_expert         = 0
0.00.085.258 I llm_load_print_meta: n_expert_used    = 0
0.00.085.259 I llm_load_print_meta: causal attn      = 1
0.00.085.259 I llm_load_print_meta: pooling type     = 0
0.00.085.259 I llm_load_print_meta: rope type        = 2
0.00.085.260 I llm_load_print_meta: rope scaling     = linear
0.00.085.260 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.261 I llm_load_print_meta: freq_scale_train = 1
0.00.085.261 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.261 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.261 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.262 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.262 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.262 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.262 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.273 I llm_load_print_meta: model type       = 1.4B
0.00.085.273 I llm_load_print_meta: model ftype      = Q4_0
0.00.085.274 I llm_load_print_meta: model params     = 1.41 B
0.00.085.275 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.085.275 I llm_load_print_meta: general.name     = 1.4B
0.00.085.275 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.276 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.276 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.276 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.276 I llm_load_print_meta: LF token         = 128 ''
0.00.085.277 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.277 I llm_load_print_meta: max token length = 1024
0.00.088.385 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.385 I llm_load_tensors: offloading output layer to GPU
0.00.088.386 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.399 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.088.401 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.090.075 I llama_new_context_with_model: n_seq_max     = 1
0.00.090.076 I llama_new_context_with_model: n_ctx         = 2048
0.00.090.076 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.090.077 I llama_new_context_with_model: n_batch       = 2048
0.00.090.077 I llama_new_context_with_model: n_ubatch      = 512
0.00.090.077 I llama_new_context_with_model: flash_attn    = 0
0.00.090.078 I llama_new_context_with_model: freq_base     = 10000.0
0.00.090.078 I llama_new_context_with_model: freq_scale    = 1
0.00.090.079 I ggml_metal_init: allocating
0.00.090.087 I ggml_metal_init: found device: Apple M4
0.00.090.090 I ggml_metal_init: picking default device: Apple M4
0.00.091.077 I ggml_metal_init: using embedded metal library
0.00.094.950 I ggml_metal_init: GPU name:   Apple M4
0.00.094.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.953 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.954 I ggml_metal_init: simdgroup reduction   = true
0.00.094.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.954 I ggml_metal_init: has bfloat            = true
0.00.094.954 I ggml_metal_init: use bfloat            = true
0.00.094.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.488 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.136.497 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.521 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.137.557 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.137.558 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.137.558 I llama_new_context_with_model: graph nodes  = 967
0.00.137.559 I llama_new_context_with_model: graph splits = 2
0.00.137.575 I common_init_from_params: added EOS logit bias = -inf
0.00.137.576 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.382 I main: llama threadpool init, n_threads = 4
0.00.783.422 I 
0.00.783.455 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.783.457 I 
0.00.783.706 I sampler seed: 1234
0.00.783.711 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.783.733 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.783.734 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.783.734 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.471.313 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61900.61 tokens per second)
0.01.471.314 I llama_perf_context_print:        load time =     768.42 ms
0.01.471.314 I llama_perf_context_print: prompt eval time =      45.80 ms /     7 tokens (    6.54 ms per token,   152.83 tokens per second)
0.01.471.315 I llama_perf_context_print:        eval time =     638.94 ms /    63 runs   (   10.14 ms per token,    98.60 tokens per second)
0.01.471.317 I llama_perf_context_print:       total time =     687.93 ms /    70 tokens
0.01.471.510 I ggml_metal_free: deallocating

real	0m1.492s
user	0m0.133s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.030 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.928 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.929 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.929 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.931 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.931 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.932 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.933 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.933 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.933 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.934 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.938 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.940 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.817 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.654 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.655 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.656 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.656 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.657 I llama_model_loader: - type  f32:  194 tensors
0.00.024.657 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.658 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.929 I llm_load_vocab: special tokens cache size = 25
0.00.050.856 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.859 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.860 I llm_load_print_meta: arch             = gptneox
0.00.050.860 I llm_load_print_meta: vocab type       = BPE
0.00.050.860 I llm_load_print_meta: n_vocab          = 50304
0.00.050.860 I llm_load_print_meta: n_merges         = 50009
0.00.050.861 I llm_load_print_meta: vocab_only       = 0
0.00.050.861 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.861 I llm_load_print_meta: n_embd           = 2048
0.00.050.861 I llm_load_print_meta: n_layer          = 24
0.00.050.863 I llm_load_print_meta: n_head           = 16
0.00.050.876 I llm_load_print_meta: n_head_kv        = 16
0.00.050.876 I llm_load_print_meta: n_rot            = 32
0.00.050.876 I llm_load_print_meta: n_swa            = 0
0.00.050.876 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.876 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.877 I llm_load_print_meta: n_gqa            = 1
0.00.050.878 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.879 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.879 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.880 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.880 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.880 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.880 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.881 I llm_load_print_meta: n_ff             = 8192
0.00.050.881 I llm_load_print_meta: n_expert         = 0
0.00.050.883 I llm_load_print_meta: n_expert_used    = 0
0.00.050.883 I llm_load_print_meta: causal attn      = 1
0.00.050.883 I llm_load_print_meta: pooling type     = 0
0.00.050.883 I llm_load_print_meta: rope type        = 2
0.00.050.883 I llm_load_print_meta: rope scaling     = linear
0.00.050.884 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.884 I llm_load_print_meta: freq_scale_train = 1
0.00.050.884 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.884 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.884 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.885 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.885 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.885 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.894 I llm_load_print_meta: model type       = 1.4B
0.00.050.895 I llm_load_print_meta: model ftype      = Q4_0
0.00.050.895 I llm_load_print_meta: model params     = 1.41 B
0.00.050.895 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.050.896 I llm_load_print_meta: general.name     = 1.4B
0.00.050.896 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.896 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.896 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.896 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.896 I llm_load_print_meta: LF token         = 128 ''
0.00.050.897 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.897 I llm_load_print_meta: max token length = 1024
0.00.052.826 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.827 I llm_load_tensors: offloading output layer to GPU
0.00.052.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.837 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.839 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.746 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.747 I llama_new_context_with_model: n_ctx         = 128
0.00.053.747 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.747 I llama_new_context_with_model: n_batch       = 128
0.00.053.747 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.747 I llama_new_context_with_model: flash_attn    = 0
0.00.053.748 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.748 I llama_new_context_with_model: freq_scale    = 1
0.00.053.748 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.749 I ggml_metal_init: allocating
0.00.053.752 I ggml_metal_init: found device: Apple M4
0.00.053.754 I ggml_metal_init: picking default device: Apple M4
0.00.054.306 I ggml_metal_init: using embedded metal library
0.00.056.601 I ggml_metal_init: GPU name:   Apple M4
0.00.056.602 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.603 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.603 I ggml_metal_init: simdgroup reduction   = true
0.00.056.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.603 I ggml_metal_init: has bfloat            = true
0.00.056.603 I ggml_metal_init: use bfloat            = true
0.00.056.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.430 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.434 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.447 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.382 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.383 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.384 I llama_new_context_with_model: graph nodes  = 967
0.00.068.384 I llama_new_context_with_model: graph splits = 2
0.00.068.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.852 I 
0.00.618.939 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.957 I perplexity: tokenizing the input ..
0.00.627.063 I perplexity: tokenization took 8.106 ms
0.00.627.079 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.749.206 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.750.751 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.750.767 I llama_perf_context_print:        load time =     608.82 ms
0.00.750.768 I llama_perf_context_print: prompt eval time =     121.88 ms /   128 tokens (    0.95 ms per token,  1050.20 tokens per second)
0.00.750.769 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.750.769 I llama_perf_context_print:       total time =     131.92 ms /   129 tokens
0.00.751.095 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.077s
sys	0m0.102s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.711 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.398 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.410 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.410 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.416 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.358 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.384 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.385 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.385 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.386 I llama_model_loader: - type  f32:  194 tensors
0.00.024.386 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.387 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.543 I llm_load_vocab: special tokens cache size = 25
0.00.051.551 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.554 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.554 I llm_load_print_meta: arch             = gptneox
0.00.051.555 I llm_load_print_meta: vocab type       = BPE
0.00.051.555 I llm_load_print_meta: n_vocab          = 50304
0.00.051.555 I llm_load_print_meta: n_merges         = 50009
0.00.051.555 I llm_load_print_meta: vocab_only       = 0
0.00.051.555 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.556 I llm_load_print_meta: n_embd           = 2048
0.00.051.556 I llm_load_print_meta: n_layer          = 24
0.00.051.558 I llm_load_print_meta: n_head           = 16
0.00.051.571 I llm_load_print_meta: n_head_kv        = 16
0.00.051.571 I llm_load_print_meta: n_rot            = 32
0.00.051.571 I llm_load_print_meta: n_swa            = 0
0.00.051.572 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.572 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.572 I llm_load_print_meta: n_gqa            = 1
0.00.051.574 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.574 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.575 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.575 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.575 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.576 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.576 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.576 I llm_load_print_meta: n_ff             = 8192
0.00.051.576 I llm_load_print_meta: n_expert         = 0
0.00.051.577 I llm_load_print_meta: n_expert_used    = 0
0.00.051.578 I llm_load_print_meta: causal attn      = 1
0.00.051.580 I llm_load_print_meta: pooling type     = 0
0.00.051.580 I llm_load_print_meta: rope type        = 2
0.00.051.580 I llm_load_print_meta: rope scaling     = linear
0.00.051.580 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.580 I llm_load_print_meta: freq_scale_train = 1
0.00.051.581 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.581 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.581 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.581 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.581 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.581 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.581 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.591 I llm_load_print_meta: model type       = 1.4B
0.00.051.591 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.592 I llm_load_print_meta: model params     = 1.41 B
0.00.051.592 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.592 I llm_load_print_meta: general.name     = 1.4B
0.00.051.593 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: LF token         = 128 ''
0.00.051.594 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: max token length = 1024
0.00.053.611 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.611 I llm_load_tensors: offloading output layer to GPU
0.00.053.611 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.622 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.623 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.558 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.558 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.559 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.559 I llama_new_context_with_model: n_batch       = 2048
0.00.054.559 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.559 I llama_new_context_with_model: flash_attn    = 0
0.00.054.560 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.560 I llama_new_context_with_model: freq_scale    = 1
0.00.054.560 I ggml_metal_init: allocating
0.00.054.566 I ggml_metal_init: found device: Apple M4
0.00.054.568 I ggml_metal_init: picking default device: Apple M4
0.00.055.176 I ggml_metal_init: using embedded metal library
0.00.057.507 I ggml_metal_init: GPU name:   Apple M4
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.509 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.510 I ggml_metal_init: simdgroup reduction   = true
0.00.057.510 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.510 I ggml_metal_init: has bfloat            = true
0.00.057.511 I ggml_metal_init: use bfloat            = true
0.00.057.512 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.512 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.303 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.312 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.332 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.278 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.279 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.279 I llama_new_context_with_model: graph nodes  = 967
0.00.086.280 I llama_new_context_with_model: graph splits = 2
0.00.086.294 I common_init_from_params: added EOS logit bias = -inf
0.00.086.295 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.487 I main: llama threadpool init, n_threads = 4
0.00.705.526 I 
0.00.705.559 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.560 I 
0.00.705.800 I sampler seed: 1234
0.00.705.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.816 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.818 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.818 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.428.668 I llama_perf_sampler_print:    sampling time =       1.05 ms /    71 runs   (    0.01 ms per token, 67490.49 tokens per second)
0.01.428.669 I llama_perf_context_print:        load time =     696.77 ms
0.01.428.669 I llama_perf_context_print: prompt eval time =      39.63 ms /     7 tokens (    5.66 ms per token,   176.65 tokens per second)
0.01.428.671 I llama_perf_context_print:        eval time =     680.46 ms /    63 runs   (   10.80 ms per token,    92.59 tokens per second)
0.01.428.672 I llama_perf_context_print:       total time =     723.18 ms /    70 tokens
0.01.428.867 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.109s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.813 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.787 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.789 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.790 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.791 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.791 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.792 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.793 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.793 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.793 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.793 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.794 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.796 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.796 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.796 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.624 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.517 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.173 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.174 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.175 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.176 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.176 I llama_model_loader: - type  f32:  194 tensors
0.00.024.177 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.177 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.362 I llm_load_vocab: special tokens cache size = 25
0.00.051.559 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.563 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.564 I llm_load_print_meta: arch             = gptneox
0.00.051.564 I llm_load_print_meta: vocab type       = BPE
0.00.051.564 I llm_load_print_meta: n_vocab          = 50304
0.00.051.564 I llm_load_print_meta: n_merges         = 50009
0.00.051.565 I llm_load_print_meta: vocab_only       = 0
0.00.051.565 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.566 I llm_load_print_meta: n_embd           = 2048
0.00.051.567 I llm_load_print_meta: n_layer          = 24
0.00.051.571 I llm_load_print_meta: n_head           = 16
0.00.051.580 I llm_load_print_meta: n_head_kv        = 16
0.00.051.580 I llm_load_print_meta: n_rot            = 32
0.00.051.580 I llm_load_print_meta: n_swa            = 0
0.00.051.580 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.580 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.581 I llm_load_print_meta: n_gqa            = 1
0.00.051.582 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.582 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.583 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.586 I llm_load_print_meta: n_ff             = 8192
0.00.051.587 I llm_load_print_meta: n_expert         = 0
0.00.051.587 I llm_load_print_meta: n_expert_used    = 0
0.00.051.587 I llm_load_print_meta: causal attn      = 1
0.00.051.587 I llm_load_print_meta: pooling type     = 0
0.00.051.587 I llm_load_print_meta: rope type        = 2
0.00.051.587 I llm_load_print_meta: rope scaling     = linear
0.00.051.588 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.588 I llm_load_print_meta: freq_scale_train = 1
0.00.051.588 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.588 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.588 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.588 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.588 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.589 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.589 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.594 I llm_load_print_meta: model type       = 1.4B
0.00.051.595 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.595 I llm_load_print_meta: model params     = 1.41 B
0.00.051.596 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.596 I llm_load_print_meta: general.name     = 1.4B
0.00.051.596 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.596 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.597 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.598 I llm_load_print_meta: LF token         = 128 ''
0.00.051.598 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.598 I llm_load_print_meta: max token length = 1024
0.00.053.319 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.320 I llm_load_tensors: offloading output layer to GPU
0.00.053.320 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.326 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.326 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.271 I llama_new_context_with_model: n_ctx         = 128
0.00.054.271 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.271 I llama_new_context_with_model: n_batch       = 128
0.00.054.271 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.271 I llama_new_context_with_model: flash_attn    = 0
0.00.054.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.272 I llama_new_context_with_model: freq_scale    = 1
0.00.054.272 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.273 I ggml_metal_init: allocating
0.00.054.277 I ggml_metal_init: found device: Apple M4
0.00.054.279 I ggml_metal_init: picking default device: Apple M4
0.00.054.896 I ggml_metal_init: using embedded metal library
0.00.057.301 I ggml_metal_init: GPU name:   Apple M4
0.00.057.302 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.303 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.304 I ggml_metal_init: simdgroup reduction   = true
0.00.057.304 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.304 I ggml_metal_init: has bfloat            = true
0.00.057.304 I ggml_metal_init: use bfloat            = true
0.00.057.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.306 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.807 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.810 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.826 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.792 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.794 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.794 I llama_new_context_with_model: graph nodes  = 967
0.00.069.794 I llama_new_context_with_model: graph splits = 2
0.00.069.803 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.636.276 I 
0.00.636.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.636.366 I perplexity: tokenizing the input ..
0.00.652.158 I perplexity: tokenization took 15.788 ms
0.00.652.181 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.916 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.792.150 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.792.165 I llama_perf_context_print:        load time =     627.46 ms
0.00.792.169 I llama_perf_context_print: prompt eval time =     135.45 ms /   128 tokens (    1.06 ms per token,   944.98 tokens per second)
0.00.792.170 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.792.171 I llama_perf_context_print:       total time =     155.89 ms /   129 tokens
0.00.792.909 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.103s
sys	0m0.099s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.569 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.666 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.671 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.675 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.676 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.676 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.676 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.677 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.677 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.678 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.680 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.680 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.681 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.714 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.614 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.616 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.617 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.617 I llama_model_loader: - type  f32:  194 tensors
0.00.025.618 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.618 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.715 I llm_load_vocab: special tokens cache size = 25
0.00.052.785 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.788 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.788 I llm_load_print_meta: arch             = gptneox
0.00.052.789 I llm_load_print_meta: vocab type       = BPE
0.00.052.789 I llm_load_print_meta: n_vocab          = 50304
0.00.052.789 I llm_load_print_meta: n_merges         = 50009
0.00.052.789 I llm_load_print_meta: vocab_only       = 0
0.00.052.789 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.789 I llm_load_print_meta: n_embd           = 2048
0.00.052.790 I llm_load_print_meta: n_layer          = 24
0.00.052.793 I llm_load_print_meta: n_head           = 16
0.00.052.805 I llm_load_print_meta: n_head_kv        = 16
0.00.052.807 I llm_load_print_meta: n_rot            = 32
0.00.052.807 I llm_load_print_meta: n_swa            = 0
0.00.052.807 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.807 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.808 I llm_load_print_meta: n_gqa            = 1
0.00.052.809 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.810 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.810 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.810 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.810 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.812 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.813 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.813 I llm_load_print_meta: n_ff             = 8192
0.00.052.813 I llm_load_print_meta: n_expert         = 0
0.00.052.813 I llm_load_print_meta: n_expert_used    = 0
0.00.052.815 I llm_load_print_meta: causal attn      = 1
0.00.052.816 I llm_load_print_meta: pooling type     = 0
0.00.052.816 I llm_load_print_meta: rope type        = 2
0.00.052.816 I llm_load_print_meta: rope scaling     = linear
0.00.052.817 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.817 I llm_load_print_meta: freq_scale_train = 1
0.00.052.817 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.817 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.817 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.817 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.818 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.818 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.818 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.827 I llm_load_print_meta: model type       = 1.4B
0.00.052.828 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.828 I llm_load_print_meta: model params     = 1.41 B
0.00.052.829 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.829 I llm_load_print_meta: general.name     = 1.4B
0.00.052.829 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.829 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.829 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.829 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.830 I llm_load_print_meta: LF token         = 128 ''
0.00.052.830 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.830 I llm_load_print_meta: max token length = 1024
0.00.054.902 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.902 I llm_load_tensors: offloading output layer to GPU
0.00.054.902 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.913 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.914 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.894 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.895 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.895 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.895 I llama_new_context_with_model: n_batch       = 2048
0.00.055.895 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.895 I llama_new_context_with_model: flash_attn    = 0
0.00.055.896 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.896 I llama_new_context_with_model: freq_scale    = 1
0.00.055.896 I ggml_metal_init: allocating
0.00.055.903 I ggml_metal_init: found device: Apple M4
0.00.055.905 I ggml_metal_init: picking default device: Apple M4
0.00.056.495 I ggml_metal_init: using embedded metal library
0.00.058.887 I ggml_metal_init: GPU name:   Apple M4
0.00.058.889 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.889 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.889 I ggml_metal_init: simdgroup reduction   = true
0.00.058.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.890 I ggml_metal_init: has bfloat            = true
0.00.058.890 I ggml_metal_init: use bfloat            = true
0.00.058.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.892 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.209 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.215 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.236 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.342 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.343 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.344 I llama_new_context_with_model: graph nodes  = 967
0.00.089.344 I llama_new_context_with_model: graph splits = 2
0.00.089.357 I common_init_from_params: added EOS logit bias = -inf
0.00.089.359 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.780.519 I main: llama threadpool init, n_threads = 4
0.00.780.564 I 
0.00.780.594 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.780.596 I 
0.00.780.820 I sampler seed: 1234
0.00.780.824 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.780.856 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.780.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.780.857 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.573.034 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.573.035 I llama_perf_context_print:        load time =     770.94 ms
0.01.573.035 I llama_perf_context_print: prompt eval time =      47.06 ms /     7 tokens (    6.72 ms per token,   148.73 tokens per second)
0.01.573.036 I llama_perf_context_print:        eval time =     742.12 ms /    63 runs   (   11.78 ms per token,    84.89 tokens per second)
0.01.573.037 I llama_perf_context_print:       total time =     792.52 ms /    70 tokens
0.01.573.235 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.110s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.210 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.228 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.230 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.021.235 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.243 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.243 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.245 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.246 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.247 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.249 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.249 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.170 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.171 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.172 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.172 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.172 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.173 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.030.173 I llama_model_loader: - type  f32:  194 tensors
0.00.030.174 I llama_model_loader: - type q5_0:   97 tensors
0.00.030.174 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.605 I llm_load_vocab: special tokens cache size = 25
0.00.067.234 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.237 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.237 I llm_load_print_meta: arch             = gptneox
0.00.067.238 I llm_load_print_meta: vocab type       = BPE
0.00.067.238 I llm_load_print_meta: n_vocab          = 50304
0.00.067.238 I llm_load_print_meta: n_merges         = 50009
0.00.067.238 I llm_load_print_meta: vocab_only       = 0
0.00.067.238 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.238 I llm_load_print_meta: n_embd           = 2048
0.00.067.239 I llm_load_print_meta: n_layer          = 24
0.00.067.243 I llm_load_print_meta: n_head           = 16
0.00.067.256 I llm_load_print_meta: n_head_kv        = 16
0.00.067.256 I llm_load_print_meta: n_rot            = 32
0.00.067.257 I llm_load_print_meta: n_swa            = 0
0.00.067.257 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.257 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.258 I llm_load_print_meta: n_gqa            = 1
0.00.067.258 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.259 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.260 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.260 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.260 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.260 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.260 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.261 I llm_load_print_meta: n_ff             = 8192
0.00.067.261 I llm_load_print_meta: n_expert         = 0
0.00.067.261 I llm_load_print_meta: n_expert_used    = 0
0.00.067.261 I llm_load_print_meta: causal attn      = 1
0.00.067.262 I llm_load_print_meta: pooling type     = 0
0.00.067.262 I llm_load_print_meta: rope type        = 2
0.00.067.262 I llm_load_print_meta: rope scaling     = linear
0.00.067.262 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.263 I llm_load_print_meta: freq_scale_train = 1
0.00.067.263 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.263 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.266 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.266 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.266 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.266 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.266 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.276 I llm_load_print_meta: model type       = 1.4B
0.00.067.277 I llm_load_print_meta: model ftype      = Q5_0
0.00.067.277 I llm_load_print_meta: model params     = 1.41 B
0.00.067.279 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.067.279 I llm_load_print_meta: general.name     = 1.4B
0.00.067.279 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.279 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.280 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.280 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.281 I llm_load_print_meta: LF token         = 128 ''
0.00.067.281 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.281 I llm_load_print_meta: max token length = 1024
0.00.069.505 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.505 I llm_load_tensors: offloading output layer to GPU
0.00.069.506 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.516 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.069.517 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.070.540 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.541 I llama_new_context_with_model: n_ctx         = 128
0.00.070.541 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.542 I llama_new_context_with_model: n_batch       = 128
0.00.070.542 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.542 I llama_new_context_with_model: flash_attn    = 0
0.00.070.542 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.543 I llama_new_context_with_model: freq_scale    = 1
0.00.070.543 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.544 I ggml_metal_init: allocating
0.00.070.547 I ggml_metal_init: found device: Apple M4
0.00.070.549 I ggml_metal_init: picking default device: Apple M4
0.00.071.240 I ggml_metal_init: using embedded metal library
0.00.074.045 I ggml_metal_init: GPU name:   Apple M4
0.00.074.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.048 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.048 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.048 I ggml_metal_init: simdgroup reduction   = true
0.00.074.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.048 I ggml_metal_init: has bfloat            = true
0.00.074.049 I ggml_metal_init: use bfloat            = true
0.00.074.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.050 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.983 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.988 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.085.002 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.929 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.930 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.930 I llama_new_context_with_model: graph nodes  = 967
0.00.085.930 I llama_new_context_with_model: graph splits = 2
0.00.085.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.366 I 
0.00.738.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.414 I perplexity: tokenizing the input ..
0.00.748.778 I perplexity: tokenization took 10.366 ms
0.00.748.789 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.104 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.885.371 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.885.388 I llama_perf_context_print:        load time =     723.13 ms
0.00.885.389 I llama_perf_context_print: prompt eval time =     135.09 ms /   128 tokens (    1.06 ms per token,   947.49 tokens per second)
0.00.885.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.885.390 I llama_perf_context_print:       total time =     147.02 ms /   129 tokens
0.00.885.890 I ggml_metal_free: deallocating

real	0m0.903s
user	0m0.094s
sys	0m0.111s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.705 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.709 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.711 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.711 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.711 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.713 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.714 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.595 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.468 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.469 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.469 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.470 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.470 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.470 I llama_model_loader: - type  f32:  194 tensors
0.00.024.471 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.471 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.791 I llm_load_vocab: special tokens cache size = 25
0.00.050.744 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.747 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.748 I llm_load_print_meta: arch             = gptneox
0.00.050.748 I llm_load_print_meta: vocab type       = BPE
0.00.050.748 I llm_load_print_meta: n_vocab          = 50304
0.00.050.748 I llm_load_print_meta: n_merges         = 50009
0.00.050.749 I llm_load_print_meta: vocab_only       = 0
0.00.050.749 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.749 I llm_load_print_meta: n_embd           = 2048
0.00.050.749 I llm_load_print_meta: n_layer          = 24
0.00.050.752 I llm_load_print_meta: n_head           = 16
0.00.050.759 I llm_load_print_meta: n_head_kv        = 16
0.00.050.759 I llm_load_print_meta: n_rot            = 32
0.00.050.759 I llm_load_print_meta: n_swa            = 0
0.00.050.761 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.761 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.762 I llm_load_print_meta: n_gqa            = 1
0.00.050.763 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.763 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.764 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.764 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.765 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.765 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.765 I llm_load_print_meta: n_ff             = 8192
0.00.050.766 I llm_load_print_meta: n_expert         = 0
0.00.050.766 I llm_load_print_meta: n_expert_used    = 0
0.00.050.768 I llm_load_print_meta: causal attn      = 1
0.00.050.769 I llm_load_print_meta: pooling type     = 0
0.00.050.769 I llm_load_print_meta: rope type        = 2
0.00.050.770 I llm_load_print_meta: rope scaling     = linear
0.00.050.770 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.770 I llm_load_print_meta: freq_scale_train = 1
0.00.050.773 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.774 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.774 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.776 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.777 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.777 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.777 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.782 I llm_load_print_meta: model type       = 1.4B
0.00.050.783 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.783 I llm_load_print_meta: model params     = 1.41 B
0.00.050.783 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.784 I llm_load_print_meta: general.name     = 1.4B
0.00.050.784 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.784 I llm_load_print_meta: LF token         = 128 ''
0.00.050.785 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.785 I llm_load_print_meta: max token length = 1024
0.00.052.758 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.758 I llm_load_tensors: offloading output layer to GPU
0.00.052.758 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.769 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.770 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.796 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.797 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.797 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.797 I llama_new_context_with_model: n_batch       = 2048
0.00.053.797 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.797 I llama_new_context_with_model: flash_attn    = 0
0.00.053.798 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.798 I llama_new_context_with_model: freq_scale    = 1
0.00.053.798 I ggml_metal_init: allocating
0.00.053.801 I ggml_metal_init: found device: Apple M4
0.00.053.803 I ggml_metal_init: picking default device: Apple M4
0.00.054.414 I ggml_metal_init: using embedded metal library
0.00.056.737 I ggml_metal_init: GPU name:   Apple M4
0.00.056.739 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.740 I ggml_metal_init: simdgroup reduction   = true
0.00.056.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.742 I ggml_metal_init: has bfloat            = true
0.00.056.742 I ggml_metal_init: use bfloat            = true
0.00.056.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.306 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.311 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.329 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.385 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.386 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.387 I llama_new_context_with_model: graph nodes  = 967
0.00.087.387 I llama_new_context_with_model: graph splits = 2
0.00.087.402 I common_init_from_params: added EOS logit bias = -inf
0.00.087.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.902 I main: llama threadpool init, n_threads = 4
0.00.784.945 I 
0.00.784.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.975 I 
0.00.785.185 I sampler seed: 1234
0.00.785.191 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.785.231 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.785.235 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.785.235 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.626.004 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56259.90 tokens per second)
0.01.626.005 I llama_perf_context_print:        load time =     776.23 ms
0.01.626.006 I llama_perf_context_print: prompt eval time =      42.30 ms /     7 tokens (    6.04 ms per token,   165.50 tokens per second)
0.01.626.007 I llama_perf_context_print:        eval time =     795.74 ms /    63 runs   (   12.63 ms per token,    79.17 tokens per second)
0.01.626.007 I llama_perf_context_print:       total time =     841.10 ms /    70 tokens
0.01.626.223 I ggml_metal_free: deallocating

real	0m1.642s
user	0m0.109s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.840 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.428 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.461 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.284 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.285 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.285 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.286 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.286 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.286 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.287 I llama_model_loader: - type  f32:  194 tensors
0.00.024.287 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.287 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.523 I llm_load_vocab: special tokens cache size = 25
0.00.050.451 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.454 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.454 I llm_load_print_meta: arch             = gptneox
0.00.050.454 I llm_load_print_meta: vocab type       = BPE
0.00.050.455 I llm_load_print_meta: n_vocab          = 50304
0.00.050.455 I llm_load_print_meta: n_merges         = 50009
0.00.050.455 I llm_load_print_meta: vocab_only       = 0
0.00.050.455 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.455 I llm_load_print_meta: n_embd           = 2048
0.00.050.456 I llm_load_print_meta: n_layer          = 24
0.00.050.459 I llm_load_print_meta: n_head           = 16
0.00.050.471 I llm_load_print_meta: n_head_kv        = 16
0.00.050.472 I llm_load_print_meta: n_rot            = 32
0.00.050.473 I llm_load_print_meta: n_swa            = 0
0.00.050.473 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.473 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.474 I llm_load_print_meta: n_gqa            = 1
0.00.050.477 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.477 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.478 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.478 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.478 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.479 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.479 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.479 I llm_load_print_meta: n_ff             = 8192
0.00.050.479 I llm_load_print_meta: n_expert         = 0
0.00.050.480 I llm_load_print_meta: n_expert_used    = 0
0.00.050.480 I llm_load_print_meta: causal attn      = 1
0.00.050.480 I llm_load_print_meta: pooling type     = 0
0.00.050.480 I llm_load_print_meta: rope type        = 2
0.00.050.480 I llm_load_print_meta: rope scaling     = linear
0.00.050.481 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.481 I llm_load_print_meta: freq_scale_train = 1
0.00.050.481 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.482 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.482 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.482 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.482 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.482 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.484 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.493 I llm_load_print_meta: model type       = 1.4B
0.00.050.493 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.494 I llm_load_print_meta: model params     = 1.41 B
0.00.050.494 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.494 I llm_load_print_meta: general.name     = 1.4B
0.00.050.495 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.495 I llm_load_print_meta: LF token         = 128 ''
0.00.050.496 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.496 I llm_load_print_meta: max token length = 1024
0.00.052.462 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.462 I llm_load_tensors: offloading output layer to GPU
0.00.052.462 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.472 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.473 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.387 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.388 I llama_new_context_with_model: n_ctx         = 128
0.00.053.389 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.389 I llama_new_context_with_model: n_batch       = 128
0.00.053.389 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.389 I llama_new_context_with_model: flash_attn    = 0
0.00.053.389 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.390 I llama_new_context_with_model: freq_scale    = 1
0.00.053.390 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.390 I ggml_metal_init: allocating
0.00.053.396 I ggml_metal_init: found device: Apple M4
0.00.053.399 I ggml_metal_init: picking default device: Apple M4
0.00.053.960 I ggml_metal_init: using embedded metal library
0.00.056.299 I ggml_metal_init: GPU name:   Apple M4
0.00.056.300 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.301 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.301 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.301 I ggml_metal_init: simdgroup reduction   = true
0.00.056.301 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.301 I ggml_metal_init: has bfloat            = true
0.00.056.302 I ggml_metal_init: use bfloat            = true
0.00.056.302 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.302 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.988 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.992 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.007 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.882 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.883 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.883 I llama_new_context_with_model: graph nodes  = 967
0.00.067.883 I llama_new_context_with_model: graph splits = 2
0.00.067.896 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.393 I 
0.00.735.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.443 I perplexity: tokenizing the input ..
0.00.743.609 I perplexity: tokenization took 8.164 ms
0.00.743.620 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.878.399 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.879.571 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.879.588 I llama_perf_context_print:        load time =     725.55 ms
0.00.879.589 I llama_perf_context_print: prompt eval time =     134.55 ms /   128 tokens (    1.05 ms per token,   951.29 tokens per second)
0.00.879.591 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.591 I llama_perf_context_print:       total time =     144.20 ms /   129 tokens
0.00.880.127 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.078s
sys	0m0.127s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.721 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.248 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.254 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.257 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.258 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.258 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.259 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.259 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.260 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.261 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.261 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.139 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.157 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.999 I llama_model_loader: - type  f32:  194 tensors
0.00.023.999 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.000 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.556 I llm_load_vocab: special tokens cache size = 25
0.00.050.730 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.733 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.734 I llm_load_print_meta: arch             = gptneox
0.00.050.734 I llm_load_print_meta: vocab type       = BPE
0.00.050.734 I llm_load_print_meta: n_vocab          = 50304
0.00.050.734 I llm_load_print_meta: n_merges         = 50009
0.00.050.734 I llm_load_print_meta: vocab_only       = 0
0.00.050.735 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.735 I llm_load_print_meta: n_embd           = 2048
0.00.050.735 I llm_load_print_meta: n_layer          = 24
0.00.050.737 I llm_load_print_meta: n_head           = 16
0.00.050.750 I llm_load_print_meta: n_head_kv        = 16
0.00.050.750 I llm_load_print_meta: n_rot            = 32
0.00.050.750 I llm_load_print_meta: n_swa            = 0
0.00.050.751 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.752 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.753 I llm_load_print_meta: n_gqa            = 1
0.00.050.753 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.754 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.755 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.755 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.755 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.755 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.755 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.756 I llm_load_print_meta: n_ff             = 8192
0.00.050.756 I llm_load_print_meta: n_expert         = 0
0.00.050.756 I llm_load_print_meta: n_expert_used    = 0
0.00.050.756 I llm_load_print_meta: causal attn      = 1
0.00.050.756 I llm_load_print_meta: pooling type     = 0
0.00.050.757 I llm_load_print_meta: rope type        = 2
0.00.050.757 I llm_load_print_meta: rope scaling     = linear
0.00.050.757 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.758 I llm_load_print_meta: freq_scale_train = 1
0.00.050.758 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.758 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.758 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.758 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.758 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.758 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.758 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.768 I llm_load_print_meta: model type       = 1.4B
0.00.050.768 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.769 I llm_load_print_meta: model params     = 1.41 B
0.00.050.769 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.771 I llm_load_print_meta: general.name     = 1.4B
0.00.050.771 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.771 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.771 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.775 I llm_load_print_meta: LF token         = 128 ''
0.00.050.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.776 I llm_load_print_meta: max token length = 1024
0.00.052.654 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.654 I llm_load_tensors: offloading output layer to GPU
0.00.052.654 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.665 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.666 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.565 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.566 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.566 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.566 I llama_new_context_with_model: n_batch       = 2048
0.00.053.567 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.567 I llama_new_context_with_model: flash_attn    = 0
0.00.053.567 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.568 I llama_new_context_with_model: freq_scale    = 1
0.00.053.568 I ggml_metal_init: allocating
0.00.053.574 I ggml_metal_init: found device: Apple M4
0.00.053.579 I ggml_metal_init: picking default device: Apple M4
0.00.054.157 I ggml_metal_init: using embedded metal library
0.00.056.536 I ggml_metal_init: GPU name:   Apple M4
0.00.056.538 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.538 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.539 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.539 I ggml_metal_init: simdgroup reduction   = true
0.00.056.539 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.539 I ggml_metal_init: has bfloat            = true
0.00.056.539 I ggml_metal_init: use bfloat            = true
0.00.056.540 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.541 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.206 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.232 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.126 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.128 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.129 I llama_new_context_with_model: graph nodes  = 967
0.00.087.129 I llama_new_context_with_model: graph splits = 2
0.00.087.139 I common_init_from_params: added EOS logit bias = -inf
0.00.087.141 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.458.803 I main: llama threadpool init, n_threads = 4
0.00.458.846 I 
0.00.458.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.458.889 I 
0.00.459.124 I sampler seed: 1234
0.00.459.130 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.459.180 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.459.197 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.459.197 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.142.416 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 64958.83 tokens per second)
0.01.142.417 I llama_perf_context_print:        load time =     449.07 ms
0.01.142.417 I llama_perf_context_print: prompt eval time =      39.68 ms /     7 tokens (    5.67 ms per token,   176.43 tokens per second)
0.01.142.421 I llama_perf_context_print:        eval time =     640.75 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.142.422 I llama_perf_context_print:       total time =     683.62 ms /    70 tokens
0.01.142.606 I ggml_metal_free: deallocating

real	0m1.160s
user	0m0.110s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.827 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.328 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.334 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.337 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.338 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.339 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.340 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.342 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.343 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.343 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.344 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.345 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.059 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.851 I llama_model_loader: - type  f32:  194 tensors
0.00.023.851 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.851 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.851 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.966 I llm_load_vocab: special tokens cache size = 25
0.00.049.913 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.916 I llm_load_print_meta: arch             = gptneox
0.00.049.917 I llm_load_print_meta: vocab type       = BPE
0.00.049.917 I llm_load_print_meta: n_vocab          = 50304
0.00.049.917 I llm_load_print_meta: n_merges         = 50009
0.00.049.917 I llm_load_print_meta: vocab_only       = 0
0.00.049.917 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.918 I llm_load_print_meta: n_embd           = 2048
0.00.049.918 I llm_load_print_meta: n_layer          = 24
0.00.049.920 I llm_load_print_meta: n_head           = 16
0.00.049.932 I llm_load_print_meta: n_head_kv        = 16
0.00.049.932 I llm_load_print_meta: n_rot            = 32
0.00.049.932 I llm_load_print_meta: n_swa            = 0
0.00.049.932 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.933 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.933 I llm_load_print_meta: n_gqa            = 1
0.00.049.934 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.935 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.935 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.936 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.936 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.936 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.936 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.937 I llm_load_print_meta: n_ff             = 8192
0.00.049.937 I llm_load_print_meta: n_expert         = 0
0.00.049.937 I llm_load_print_meta: n_expert_used    = 0
0.00.049.937 I llm_load_print_meta: causal attn      = 1
0.00.049.938 I llm_load_print_meta: pooling type     = 0
0.00.049.938 I llm_load_print_meta: rope type        = 2
0.00.049.938 I llm_load_print_meta: rope scaling     = linear
0.00.049.938 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.939 I llm_load_print_meta: freq_scale_train = 1
0.00.049.939 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.939 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.939 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.939 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.939 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.939 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.939 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.948 I llm_load_print_meta: model type       = 1.4B
0.00.049.949 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.949 I llm_load_print_meta: model params     = 1.41 B
0.00.049.949 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.950 I llm_load_print_meta: general.name     = 1.4B
0.00.049.950 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.952 I llm_load_print_meta: LF token         = 128 ''
0.00.049.952 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.952 I llm_load_print_meta: max token length = 1024
0.00.051.464 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.464 I llm_load_tensors: offloading output layer to GPU
0.00.051.465 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.475 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.476 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.331 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.332 I llama_new_context_with_model: n_ctx         = 128
0.00.052.332 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.332 I llama_new_context_with_model: n_batch       = 128
0.00.052.332 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.332 I llama_new_context_with_model: flash_attn    = 0
0.00.052.333 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.333 I llama_new_context_with_model: freq_scale    = 1
0.00.052.333 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.334 I ggml_metal_init: allocating
0.00.052.340 I ggml_metal_init: found device: Apple M4
0.00.052.342 I ggml_metal_init: picking default device: Apple M4
0.00.052.932 I ggml_metal_init: using embedded metal library
0.00.055.273 I ggml_metal_init: GPU name:   Apple M4
0.00.055.275 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.275 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.275 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.276 I ggml_metal_init: simdgroup reduction   = true
0.00.055.276 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.276 I ggml_metal_init: has bfloat            = true
0.00.055.276 I ggml_metal_init: use bfloat            = true
0.00.055.276 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.909 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.911 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.924 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.791 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.792 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.793 I llama_new_context_with_model: graph nodes  = 967
0.00.066.793 I llama_new_context_with_model: graph splits = 2
0.00.066.805 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.317 I 
0.00.434.361 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.373 I perplexity: tokenizing the input ..
0.00.442.113 I perplexity: tokenization took 7.739 ms
0.00.442.128 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.574.581 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.575.762 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.575.779 I llama_perf_context_print:        load time =     424.49 ms
0.00.575.780 I llama_perf_context_print: prompt eval time =     132.20 ms /   128 tokens (    1.03 ms per token,   968.25 tokens per second)
0.00.575.781 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.575.782 I llama_perf_context_print:       total time =     141.46 ms /   129 tokens
0.00.576.240 I ggml_metal_free: deallocating

real	0m0.592s
user	0m0.077s
sys	0m0.080s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.807 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.250 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.257 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.258 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.258 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.259 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.260 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.260 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.260 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.261 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.261 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.261 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.265 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.265 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.259 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.304 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.238 I llama_model_loader: - type  f32:  194 tensors
0.00.024.238 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.238 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.238 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.239 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.391 I llm_load_vocab: special tokens cache size = 25
0.00.051.392 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.395 I llm_load_print_meta: arch             = gptneox
0.00.051.395 I llm_load_print_meta: vocab type       = BPE
0.00.051.395 I llm_load_print_meta: n_vocab          = 50304
0.00.051.396 I llm_load_print_meta: n_merges         = 50009
0.00.051.396 I llm_load_print_meta: vocab_only       = 0
0.00.051.396 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.396 I llm_load_print_meta: n_embd           = 2048
0.00.051.396 I llm_load_print_meta: n_layer          = 24
0.00.051.399 I llm_load_print_meta: n_head           = 16
0.00.051.412 I llm_load_print_meta: n_head_kv        = 16
0.00.051.413 I llm_load_print_meta: n_rot            = 32
0.00.051.413 I llm_load_print_meta: n_swa            = 0
0.00.051.413 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.413 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.414 I llm_load_print_meta: n_gqa            = 1
0.00.051.415 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.415 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.416 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.416 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.417 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.417 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.417 I llm_load_print_meta: n_ff             = 8192
0.00.051.419 I llm_load_print_meta: n_expert         = 0
0.00.051.420 I llm_load_print_meta: n_expert_used    = 0
0.00.051.421 I llm_load_print_meta: causal attn      = 1
0.00.051.421 I llm_load_print_meta: pooling type     = 0
0.00.051.421 I llm_load_print_meta: rope type        = 2
0.00.051.421 I llm_load_print_meta: rope scaling     = linear
0.00.051.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.422 I llm_load_print_meta: freq_scale_train = 1
0.00.051.422 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.422 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.422 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.422 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.422 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.422 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.423 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.432 I llm_load_print_meta: model type       = 1.4B
0.00.051.432 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.433 I llm_load_print_meta: model params     = 1.41 B
0.00.051.433 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.433 I llm_load_print_meta: general.name     = 1.4B
0.00.051.433 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.433 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.434 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.434 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.434 I llm_load_print_meta: LF token         = 128 ''
0.00.051.436 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.436 I llm_load_print_meta: max token length = 1024
0.00.053.389 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.389 I llm_load_tensors: offloading output layer to GPU
0.00.053.389 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.400 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.401 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.290 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.291 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.291 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.291 I llama_new_context_with_model: n_batch       = 2048
0.00.054.292 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.292 I llama_new_context_with_model: flash_attn    = 0
0.00.054.292 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.292 I llama_new_context_with_model: freq_scale    = 1
0.00.054.293 I ggml_metal_init: allocating
0.00.054.297 I ggml_metal_init: found device: Apple M4
0.00.054.299 I ggml_metal_init: picking default device: Apple M4
0.00.054.907 I ggml_metal_init: using embedded metal library
0.00.057.238 I ggml_metal_init: GPU name:   Apple M4
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.241 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.242 I ggml_metal_init: simdgroup reduction   = true
0.00.057.242 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.242 I ggml_metal_init: has bfloat            = true
0.00.057.242 I ggml_metal_init: use bfloat            = true
0.00.057.242 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.243 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.178 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.184 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.201 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.234 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.236 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.236 I llama_new_context_with_model: graph nodes  = 967
0.00.087.236 I llama_new_context_with_model: graph splits = 2
0.00.087.251 I common_init_from_params: added EOS logit bias = -inf
0.00.087.252 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.527.259 I main: llama threadpool init, n_threads = 4
0.00.527.305 I 
0.00.527.332 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.333 I 
0.00.527.480 I sampler seed: 1234
0.00.527.484 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.527.493 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.527.493 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.527.494 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.273.118 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57583.13 tokens per second)
0.01.273.119 I llama_perf_context_print:        load time =     518.45 ms
0.01.273.120 I llama_perf_context_print: prompt eval time =      40.40 ms /     7 tokens (    5.77 ms per token,   173.27 tokens per second)
0.01.273.121 I llama_perf_context_print:        eval time =     702.19 ms /    63 runs   (   11.15 ms per token,    89.72 tokens per second)
0.01.273.121 I llama_perf_context_print:       total time =     745.86 ms /    70 tokens
0.01.273.302 I ggml_metal_free: deallocating

real	0m1.290s
user	0m0.111s
sys	0m0.117s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.778 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.643 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.648 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.650 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.650 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.651 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.651 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.652 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.653 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.653 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.653 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.654 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.654 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.655 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.658 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.659 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.659 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.530 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.555 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.556 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.556 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.557 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.557 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.558 I llama_model_loader: - type  f32:  194 tensors
0.00.023.558 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.558 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.559 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.559 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.403 I llm_load_vocab: special tokens cache size = 25
0.00.050.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.603 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.604 I llm_load_print_meta: arch             = gptneox
0.00.050.604 I llm_load_print_meta: vocab type       = BPE
0.00.050.604 I llm_load_print_meta: n_vocab          = 50304
0.00.050.604 I llm_load_print_meta: n_merges         = 50009
0.00.050.604 I llm_load_print_meta: vocab_only       = 0
0.00.050.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.605 I llm_load_print_meta: n_embd           = 2048
0.00.050.605 I llm_load_print_meta: n_layer          = 24
0.00.050.608 I llm_load_print_meta: n_head           = 16
0.00.050.620 I llm_load_print_meta: n_head_kv        = 16
0.00.050.620 I llm_load_print_meta: n_rot            = 32
0.00.050.620 I llm_load_print_meta: n_swa            = 0
0.00.050.621 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.621 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.622 I llm_load_print_meta: n_gqa            = 1
0.00.050.623 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.623 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.624 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.624 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.624 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.624 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.625 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.625 I llm_load_print_meta: n_ff             = 8192
0.00.050.625 I llm_load_print_meta: n_expert         = 0
0.00.050.626 I llm_load_print_meta: n_expert_used    = 0
0.00.050.626 I llm_load_print_meta: causal attn      = 1
0.00.050.626 I llm_load_print_meta: pooling type     = 0
0.00.050.626 I llm_load_print_meta: rope type        = 2
0.00.050.626 I llm_load_print_meta: rope scaling     = linear
0.00.050.626 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.627 I llm_load_print_meta: freq_scale_train = 1
0.00.050.627 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.627 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.627 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.627 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.627 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.627 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.627 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.637 I llm_load_print_meta: model type       = 1.4B
0.00.050.637 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.637 I llm_load_print_meta: model params     = 1.41 B
0.00.050.638 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.638 I llm_load_print_meta: general.name     = 1.4B
0.00.050.638 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.638 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: LF token         = 128 ''
0.00.050.639 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.639 I llm_load_print_meta: max token length = 1024
0.00.052.539 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.539 I llm_load_tensors: offloading output layer to GPU
0.00.052.539 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.550 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.551 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.460 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.461 I llama_new_context_with_model: n_ctx         = 128
0.00.053.461 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.461 I llama_new_context_with_model: n_batch       = 128
0.00.053.461 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.462 I llama_new_context_with_model: flash_attn    = 0
0.00.053.462 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.462 I llama_new_context_with_model: freq_scale    = 1
0.00.053.462 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.463 I ggml_metal_init: allocating
0.00.053.466 I ggml_metal_init: found device: Apple M4
0.00.053.468 I ggml_metal_init: picking default device: Apple M4
0.00.054.043 I ggml_metal_init: using embedded metal library
0.00.056.356 I ggml_metal_init: GPU name:   Apple M4
0.00.056.358 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.359 I ggml_metal_init: simdgroup reduction   = true
0.00.056.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.359 I ggml_metal_init: has bfloat            = true
0.00.056.359 I ggml_metal_init: use bfloat            = true
0.00.056.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.358 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.362 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.375 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.316 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.317 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.317 I llama_new_context_with_model: graph nodes  = 967
0.00.068.318 I llama_new_context_with_model: graph splits = 2
0.00.068.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.481.509 I 
0.00.481.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.481.548 I perplexity: tokenizing the input ..
0.00.489.850 I perplexity: tokenization took 8.3 ms
0.00.489.861 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.622.229 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.623.498 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.623.512 I llama_perf_context_print:        load time =     472.73 ms
0.00.623.512 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.65 tokens per second)
0.00.623.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.514 I llama_perf_context_print:       total time =     142.00 ms /   129 tokens
0.00.623.956 I ggml_metal_free: deallocating

real	0m0.637s
user	0m0.079s
sys	0m0.085s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.689 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.199 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.201 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.201 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.202 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.202 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.204 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.204 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.205 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.206 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.208 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.111 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.250 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.252 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.254 I llama_model_loader: - type  f32:  194 tensors
0.00.024.254 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.254 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.254 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.587 I llm_load_vocab: special tokens cache size = 25
0.00.051.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.649 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.649 I llm_load_print_meta: arch             = gptneox
0.00.051.650 I llm_load_print_meta: vocab type       = BPE
0.00.051.650 I llm_load_print_meta: n_vocab          = 50304
0.00.051.650 I llm_load_print_meta: n_merges         = 50009
0.00.051.652 I llm_load_print_meta: vocab_only       = 0
0.00.051.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.652 I llm_load_print_meta: n_embd           = 2048
0.00.051.652 I llm_load_print_meta: n_layer          = 24
0.00.051.656 I llm_load_print_meta: n_head           = 16
0.00.051.669 I llm_load_print_meta: n_head_kv        = 16
0.00.051.669 I llm_load_print_meta: n_rot            = 32
0.00.051.670 I llm_load_print_meta: n_swa            = 0
0.00.051.670 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.670 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.670 I llm_load_print_meta: n_gqa            = 1
0.00.051.671 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.672 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.672 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.673 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.673 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.674 I llm_load_print_meta: n_ff             = 8192
0.00.051.674 I llm_load_print_meta: n_expert         = 0
0.00.051.674 I llm_load_print_meta: n_expert_used    = 0
0.00.051.674 I llm_load_print_meta: causal attn      = 1
0.00.051.674 I llm_load_print_meta: pooling type     = 0
0.00.051.674 I llm_load_print_meta: rope type        = 2
0.00.051.675 I llm_load_print_meta: rope scaling     = linear
0.00.051.675 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.677 I llm_load_print_meta: freq_scale_train = 1
0.00.051.677 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.677 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.677 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.678 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.678 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.678 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.678 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.687 I llm_load_print_meta: model type       = 1.4B
0.00.051.688 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.688 I llm_load_print_meta: model params     = 1.41 B
0.00.051.689 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.690 I llm_load_print_meta: general.name     = 1.4B
0.00.051.690 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.690 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.690 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.690 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.691 I llm_load_print_meta: LF token         = 128 ''
0.00.051.691 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.691 I llm_load_print_meta: max token length = 1024
0.00.053.760 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.760 I llm_load_tensors: offloading output layer to GPU
0.00.053.760 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.771 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.772 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.725 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.726 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.726 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.726 I llama_new_context_with_model: n_batch       = 2048
0.00.054.726 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.726 I llama_new_context_with_model: flash_attn    = 0
0.00.054.727 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.727 I llama_new_context_with_model: freq_scale    = 1
0.00.054.727 I ggml_metal_init: allocating
0.00.054.733 I ggml_metal_init: found device: Apple M4
0.00.054.735 I ggml_metal_init: picking default device: Apple M4
0.00.055.339 I ggml_metal_init: using embedded metal library
0.00.057.667 I ggml_metal_init: GPU name:   Apple M4
0.00.057.668 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.669 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.669 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.669 I ggml_metal_init: simdgroup reduction   = true
0.00.057.669 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.670 I ggml_metal_init: has bfloat            = true
0.00.057.670 I ggml_metal_init: use bfloat            = true
0.00.057.670 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.671 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.144 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.154 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.171 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.161 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.162 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.162 I llama_new_context_with_model: graph nodes  = 967
0.00.087.163 I llama_new_context_with_model: graph splits = 2
0.00.087.176 I common_init_from_params: added EOS logit bias = -inf
0.00.087.177 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.627.556 I main: llama threadpool init, n_threads = 4
0.00.627.597 I 
0.00.627.650 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.627.651 I 
0.00.627.871 I sampler seed: 1234
0.00.627.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.627.948 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.627.950 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.627.950 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.391.976 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.391.977 I llama_perf_context_print:        load time =     617.86 ms
0.01.391.978 I llama_perf_context_print: prompt eval time =      51.00 ms /     7 tokens (    7.29 ms per token,   137.27 tokens per second)
0.01.391.979 I llama_perf_context_print:        eval time =     709.99 ms /    63 runs   (   11.27 ms per token,    88.73 tokens per second)
0.01.391.979 I llama_perf_context_print:       total time =     764.42 ms /    70 tokens
0.01.392.163 I ggml_metal_free: deallocating

real	0m1.409s
user	0m0.110s
sys	0m0.143s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.617 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.110 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.118 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.119 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.119 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.120 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.120 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.121 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.121 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.121 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.122 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.122 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.009 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.830 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.831 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.831 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.832 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.832 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.833 I llama_model_loader: - type  f32:  194 tensors
0.00.023.833 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.833 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.834 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.094 I llm_load_vocab: special tokens cache size = 25
0.00.050.015 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.018 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.019 I llm_load_print_meta: arch             = gptneox
0.00.050.019 I llm_load_print_meta: vocab type       = BPE
0.00.050.019 I llm_load_print_meta: n_vocab          = 50304
0.00.050.019 I llm_load_print_meta: n_merges         = 50009
0.00.050.020 I llm_load_print_meta: vocab_only       = 0
0.00.050.020 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.020 I llm_load_print_meta: n_embd           = 2048
0.00.050.020 I llm_load_print_meta: n_layer          = 24
0.00.050.024 I llm_load_print_meta: n_head           = 16
0.00.050.037 I llm_load_print_meta: n_head_kv        = 16
0.00.050.037 I llm_load_print_meta: n_rot            = 32
0.00.050.037 I llm_load_print_meta: n_swa            = 0
0.00.050.038 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.038 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.039 I llm_load_print_meta: n_gqa            = 1
0.00.050.039 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.041 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.042 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.042 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.042 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.042 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.042 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.043 I llm_load_print_meta: n_ff             = 8192
0.00.050.043 I llm_load_print_meta: n_expert         = 0
0.00.050.043 I llm_load_print_meta: n_expert_used    = 0
0.00.050.043 I llm_load_print_meta: causal attn      = 1
0.00.050.044 I llm_load_print_meta: pooling type     = 0
0.00.050.044 I llm_load_print_meta: rope type        = 2
0.00.050.044 I llm_load_print_meta: rope scaling     = linear
0.00.050.045 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.045 I llm_load_print_meta: freq_scale_train = 1
0.00.050.045 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.046 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.046 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.046 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.046 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.046 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.046 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.056 I llm_load_print_meta: model type       = 1.4B
0.00.050.056 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.056 I llm_load_print_meta: model params     = 1.41 B
0.00.050.057 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.057 I llm_load_print_meta: general.name     = 1.4B
0.00.050.057 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.058 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: LF token         = 128 ''
0.00.050.059 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.059 I llm_load_print_meta: max token length = 1024
0.00.052.005 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.005 I llm_load_tensors: offloading output layer to GPU
0.00.052.005 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.015 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.016 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.941 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.942 I llama_new_context_with_model: n_ctx         = 128
0.00.052.942 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.943 I llama_new_context_with_model: n_batch       = 128
0.00.052.943 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.943 I llama_new_context_with_model: flash_attn    = 0
0.00.052.943 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.944 I llama_new_context_with_model: freq_scale    = 1
0.00.052.944 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.944 I ggml_metal_init: allocating
0.00.052.947 I ggml_metal_init: found device: Apple M4
0.00.052.949 I ggml_metal_init: picking default device: Apple M4
0.00.053.491 I ggml_metal_init: using embedded metal library
0.00.055.800 I ggml_metal_init: GPU name:   Apple M4
0.00.055.802 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.803 I ggml_metal_init: simdgroup reduction   = true
0.00.055.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.803 I ggml_metal_init: has bfloat            = true
0.00.055.803 I ggml_metal_init: use bfloat            = true
0.00.055.803 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.569 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.571 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.584 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.554 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.555 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.555 I llama_new_context_with_model: graph nodes  = 967
0.00.067.555 I llama_new_context_with_model: graph splits = 2
0.00.067.568 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.567.702 I 
0.00.567.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.567.752 I perplexity: tokenizing the input ..
0.00.575.750 I perplexity: tokenization took 7.997 ms
0.00.575.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.470 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.710.640 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.710.653 I llama_perf_context_print:        load time =     558.08 ms
0.00.710.654 I llama_perf_context_print: prompt eval time =     133.48 ms /   128 tokens (    1.04 ms per token,   958.95 tokens per second)
0.00.710.656 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.710.656 I llama_perf_context_print:       total time =     142.95 ms /   129 tokens
0.00.711.068 I ggml_metal_free: deallocating

real	0m0.725s
user	0m0.078s
sys	0m0.098s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.008.639 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.235 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.241 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.242 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.242 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.242 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.243 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.244 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.244 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.244 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.245 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.245 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.246 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.246 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.248 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.248 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.249 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.018 I llama_model_loader: - type  f32:  194 tensors
0.00.023.019 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.019 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.195 I llm_load_vocab: special tokens cache size = 25
0.00.050.200 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.203 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.203 I llm_load_print_meta: arch             = gptneox
0.00.050.204 I llm_load_print_meta: vocab type       = BPE
0.00.050.204 I llm_load_print_meta: n_vocab          = 50304
0.00.050.204 I llm_load_print_meta: n_merges         = 50009
0.00.050.204 I llm_load_print_meta: vocab_only       = 0
0.00.050.204 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.205 I llm_load_print_meta: n_embd           = 2048
0.00.050.205 I llm_load_print_meta: n_layer          = 24
0.00.050.207 I llm_load_print_meta: n_head           = 16
0.00.050.220 I llm_load_print_meta: n_head_kv        = 16
0.00.050.220 I llm_load_print_meta: n_rot            = 32
0.00.050.220 I llm_load_print_meta: n_swa            = 0
0.00.050.220 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.221 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.221 I llm_load_print_meta: n_gqa            = 1
0.00.050.222 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.223 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.224 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.224 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.224 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.224 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.225 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.225 I llm_load_print_meta: n_ff             = 8192
0.00.050.225 I llm_load_print_meta: n_expert         = 0
0.00.050.226 I llm_load_print_meta: n_expert_used    = 0
0.00.050.226 I llm_load_print_meta: causal attn      = 1
0.00.050.226 I llm_load_print_meta: pooling type     = 0
0.00.050.226 I llm_load_print_meta: rope type        = 2
0.00.050.226 I llm_load_print_meta: rope scaling     = linear
0.00.050.227 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.227 I llm_load_print_meta: freq_scale_train = 1
0.00.050.227 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.227 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.228 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.228 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.228 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.237 I llm_load_print_meta: model type       = 1.4B
0.00.050.237 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.238 I llm_load_print_meta: model params     = 1.41 B
0.00.050.238 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.241 I llm_load_print_meta: general.name     = 1.4B
0.00.050.241 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.241 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: LF token         = 128 ''
0.00.050.242 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.242 I llm_load_print_meta: max token length = 1024
0.00.052.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.250 I llm_load_tensors: offloading output layer to GPU
0.00.052.251 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.261 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.262 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.166 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.167 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.167 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.167 I llama_new_context_with_model: n_batch       = 2048
0.00.053.167 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.168 I llama_new_context_with_model: flash_attn    = 0
0.00.053.168 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.168 I llama_new_context_with_model: freq_scale    = 1
0.00.053.168 I ggml_metal_init: allocating
0.00.053.174 I ggml_metal_init: found device: Apple M4
0.00.053.177 I ggml_metal_init: picking default device: Apple M4
0.00.053.735 I ggml_metal_init: using embedded metal library
0.00.056.045 I ggml_metal_init: GPU name:   Apple M4
0.00.056.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.047 I ggml_metal_init: simdgroup reduction   = true
0.00.056.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.047 I ggml_metal_init: has bfloat            = true
0.00.056.047 I ggml_metal_init: use bfloat            = true
0.00.056.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.199 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.206 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.225 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.226 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.226 I llama_new_context_with_model: graph nodes  = 967
0.00.086.227 I llama_new_context_with_model: graph splits = 2
0.00.086.241 I common_init_from_params: added EOS logit bias = -inf
0.00.086.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.847 I main: llama threadpool init, n_threads = 4
0.00.704.891 I 
0.00.704.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.936 I 
0.00.705.160 I sampler seed: 1234
0.00.705.164 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.705.175 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.705.175 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.705.176 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.553.222 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59265.44 tokens per second)
0.01.553.223 I llama_perf_context_print:        load time =     696.20 ms
0.01.553.223 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.553.224 I llama_perf_context_print:        eval time =     793.42 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.553.224 I llama_perf_context_print:       total time =     848.38 ms /    70 tokens
0.01.553.418 I ggml_metal_free: deallocating

real	0m1.570s
user	0m0.110s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.886 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.598 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.603 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.604 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.605 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.605 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.606 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.614 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.614 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.615 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.367 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.388 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.221 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.221 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.222 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.223 I llama_model_loader: - type  f32:  194 tensors
0.00.023.223 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.223 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.221 I llm_load_vocab: special tokens cache size = 25
0.00.050.184 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.187 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.187 I llm_load_print_meta: arch             = gptneox
0.00.050.188 I llm_load_print_meta: vocab type       = BPE
0.00.050.188 I llm_load_print_meta: n_vocab          = 50304
0.00.050.188 I llm_load_print_meta: n_merges         = 50009
0.00.050.188 I llm_load_print_meta: vocab_only       = 0
0.00.050.188 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.189 I llm_load_print_meta: n_embd           = 2048
0.00.050.189 I llm_load_print_meta: n_layer          = 24
0.00.050.192 I llm_load_print_meta: n_head           = 16
0.00.050.199 I llm_load_print_meta: n_head_kv        = 16
0.00.050.200 I llm_load_print_meta: n_rot            = 32
0.00.050.200 I llm_load_print_meta: n_swa            = 0
0.00.050.200 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.200 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.201 I llm_load_print_meta: n_gqa            = 1
0.00.050.202 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.202 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.203 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.204 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.204 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.204 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.204 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.205 I llm_load_print_meta: n_ff             = 8192
0.00.050.205 I llm_load_print_meta: n_expert         = 0
0.00.050.205 I llm_load_print_meta: n_expert_used    = 0
0.00.050.205 I llm_load_print_meta: causal attn      = 1
0.00.050.206 I llm_load_print_meta: pooling type     = 0
0.00.050.206 I llm_load_print_meta: rope type        = 2
0.00.050.206 I llm_load_print_meta: rope scaling     = linear
0.00.050.206 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.207 I llm_load_print_meta: freq_scale_train = 1
0.00.050.207 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.207 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.207 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.208 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.209 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.209 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.209 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.213 I llm_load_print_meta: model type       = 1.4B
0.00.050.214 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.214 I llm_load_print_meta: model params     = 1.41 B
0.00.050.215 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.215 I llm_load_print_meta: general.name     = 1.4B
0.00.050.215 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.215 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.215 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.217 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.217 I llm_load_print_meta: LF token         = 128 ''
0.00.050.217 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.217 I llm_load_print_meta: max token length = 1024
0.00.052.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.016 I llm_load_tensors: offloading output layer to GPU
0.00.052.016 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.022 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.022 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.924 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.925 I llama_new_context_with_model: n_ctx         = 128
0.00.052.925 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.925 I llama_new_context_with_model: n_batch       = 128
0.00.052.925 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.925 I llama_new_context_with_model: flash_attn    = 0
0.00.052.926 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.926 I llama_new_context_with_model: freq_scale    = 1
0.00.052.926 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.927 I ggml_metal_init: allocating
0.00.052.930 I ggml_metal_init: found device: Apple M4
0.00.052.932 I ggml_metal_init: picking default device: Apple M4
0.00.053.532 I ggml_metal_init: using embedded metal library
0.00.055.846 I ggml_metal_init: GPU name:   Apple M4
0.00.055.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.848 I ggml_metal_init: simdgroup reduction   = true
0.00.055.848 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.848 I ggml_metal_init: has bfloat            = true
0.00.055.848 I ggml_metal_init: use bfloat            = true
0.00.055.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.979 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.984 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.999 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.946 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.947 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.947 I llama_new_context_with_model: graph nodes  = 967
0.00.067.947 I llama_new_context_with_model: graph splits = 2
0.00.067.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.639.476 I 
0.00.639.505 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.639.513 I perplexity: tokenizing the input ..
0.00.647.870 I perplexity: tokenization took 8.356 ms
0.00.647.883 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.282 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.789.491 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.789.516 I llama_perf_context_print:        load time =     630.59 ms
0.00.789.517 I llama_perf_context_print: prompt eval time =     140.17 ms /   128 tokens (    1.10 ms per token,   913.15 tokens per second)
0.00.789.518 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.789.518 I llama_perf_context_print:       total time =     150.04 ms /   129 tokens
0.00.789.847 I ggml_metal_free: deallocating

real	0m0.802s
user	0m0.079s
sys	0m0.116s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.044 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.868 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.872 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.872 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.873 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.873 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.873 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.818 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.858 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.731 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.731 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.732 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.732 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.733 I llama_model_loader: - type  f32:  194 tensors
0.00.024.733 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.965 I llm_load_vocab: special tokens cache size = 25
0.00.051.999 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.002 I llm_load_print_meta: arch             = gptneox
0.00.052.002 I llm_load_print_meta: vocab type       = BPE
0.00.052.003 I llm_load_print_meta: n_vocab          = 50304
0.00.052.003 I llm_load_print_meta: n_merges         = 50009
0.00.052.003 I llm_load_print_meta: vocab_only       = 0
0.00.052.003 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.003 I llm_load_print_meta: n_embd           = 2048
0.00.052.003 I llm_load_print_meta: n_layer          = 24
0.00.052.006 I llm_load_print_meta: n_head           = 16
0.00.052.019 I llm_load_print_meta: n_head_kv        = 16
0.00.052.020 I llm_load_print_meta: n_rot            = 32
0.00.052.020 I llm_load_print_meta: n_swa            = 0
0.00.052.020 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.020 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.021 I llm_load_print_meta: n_gqa            = 1
0.00.052.022 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.023 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.023 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.024 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.024 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.024 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.024 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.025 I llm_load_print_meta: n_ff             = 8192
0.00.052.025 I llm_load_print_meta: n_expert         = 0
0.00.052.025 I llm_load_print_meta: n_expert_used    = 0
0.00.052.025 I llm_load_print_meta: causal attn      = 1
0.00.052.027 I llm_load_print_meta: pooling type     = 0
0.00.052.029 I llm_load_print_meta: rope type        = 2
0.00.052.029 I llm_load_print_meta: rope scaling     = linear
0.00.052.029 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.030 I llm_load_print_meta: freq_scale_train = 1
0.00.052.030 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.030 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.030 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.030 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.030 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.031 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.031 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.040 I llm_load_print_meta: model type       = 1.4B
0.00.052.040 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.041 I llm_load_print_meta: model params     = 1.41 B
0.00.052.041 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.042 I llm_load_print_meta: general.name     = 1.4B
0.00.052.043 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.043 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.043 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.043 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.043 I llm_load_print_meta: LF token         = 128 ''
0.00.052.043 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.044 I llm_load_print_meta: max token length = 1024
0.00.054.082 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.082 I llm_load_tensors: offloading output layer to GPU
0.00.054.082 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.093 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.094 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.016 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.016 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.017 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.017 I llama_new_context_with_model: n_batch       = 2048
0.00.055.017 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.017 I llama_new_context_with_model: flash_attn    = 0
0.00.055.018 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.018 I llama_new_context_with_model: freq_scale    = 1
0.00.055.018 I ggml_metal_init: allocating
0.00.055.025 I ggml_metal_init: found device: Apple M4
0.00.055.027 I ggml_metal_init: picking default device: Apple M4
0.00.055.610 I ggml_metal_init: using embedded metal library
0.00.057.921 I ggml_metal_init: GPU name:   Apple M4
0.00.057.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.922 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.923 I ggml_metal_init: simdgroup reduction   = true
0.00.057.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.925 I ggml_metal_init: has bfloat            = true
0.00.057.925 I ggml_metal_init: use bfloat            = true
0.00.057.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.018 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.023 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.041 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.067 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.068 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.069 I llama_new_context_with_model: graph nodes  = 967
0.00.088.069 I llama_new_context_with_model: graph splits = 2
0.00.088.083 I common_init_from_params: added EOS logit bias = -inf
0.00.088.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.798 I main: llama threadpool init, n_threads = 4
0.00.769.837 I 
0.00.769.881 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.882 I 
0.00.770.120 I sampler seed: 1234
0.00.770.125 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.146 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.146 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.146 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.650.664 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.650.664 I llama_perf_context_print:        load time =     760.75 ms
0.01.650.665 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.63 tokens per second)
0.01.650.666 I llama_perf_context_print:        eval time =     823.14 ms /    63 runs   (   13.07 ms per token,    76.54 tokens per second)
0.01.650.668 I llama_perf_context_print:       total time =     880.87 ms /    70 tokens
0.01.650.860 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.111s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4317 (869ec41e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.737 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.494 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.501 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.501 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.502 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.503 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.504 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.507 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.287 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.103 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.104 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.105 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.105 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.106 I llama_model_loader: - type  f32:  194 tensors
0.00.024.106 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.362 I llm_load_vocab: special tokens cache size = 25
0.00.050.164 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.167 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.167 I llm_load_print_meta: arch             = gptneox
0.00.050.168 I llm_load_print_meta: vocab type       = BPE
0.00.050.168 I llm_load_print_meta: n_vocab          = 50304
0.00.050.168 I llm_load_print_meta: n_merges         = 50009
0.00.050.168 I llm_load_print_meta: vocab_only       = 0
0.00.050.169 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.169 I llm_load_print_meta: n_embd           = 2048
0.00.050.169 I llm_load_print_meta: n_layer          = 24
0.00.050.172 I llm_load_print_meta: n_head           = 16
0.00.050.186 I llm_load_print_meta: n_head_kv        = 16
0.00.050.187 I llm_load_print_meta: n_rot            = 32
0.00.050.187 I llm_load_print_meta: n_swa            = 0
0.00.050.188 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.188 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.190 I llm_load_print_meta: n_gqa            = 1
0.00.050.190 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.191 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.191 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.192 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.192 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.192 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.192 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.193 I llm_load_print_meta: n_ff             = 8192
0.00.050.193 I llm_load_print_meta: n_expert         = 0
0.00.050.193 I llm_load_print_meta: n_expert_used    = 0
0.00.050.193 I llm_load_print_meta: causal attn      = 1
0.00.050.193 I llm_load_print_meta: pooling type     = 0
0.00.050.194 I llm_load_print_meta: rope type        = 2
0.00.050.194 I llm_load_print_meta: rope scaling     = linear
0.00.050.194 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.194 I llm_load_print_meta: freq_scale_train = 1
0.00.050.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.195 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.195 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.195 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.196 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.196 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.196 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.206 I llm_load_print_meta: model type       = 1.4B
0.00.050.206 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.206 I llm_load_print_meta: model params     = 1.41 B
0.00.050.207 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.207 I llm_load_print_meta: general.name     = 1.4B
0.00.050.207 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.207 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: LF token         = 128 ''
0.00.050.208 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.208 I llm_load_print_meta: max token length = 1024
0.00.052.151 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.151 I llm_load_tensors: offloading output layer to GPU
0.00.052.152 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.162 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.163 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.067 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.068 I llama_new_context_with_model: n_ctx         = 128
0.00.053.069 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.069 I llama_new_context_with_model: n_batch       = 128
0.00.053.069 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.069 I llama_new_context_with_model: flash_attn    = 0
0.00.053.069 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.070 I llama_new_context_with_model: freq_scale    = 1
0.00.053.070 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.070 I ggml_metal_init: allocating
0.00.053.074 I ggml_metal_init: found device: Apple M4
0.00.053.076 I ggml_metal_init: picking default device: Apple M4
0.00.053.620 I ggml_metal_init: using embedded metal library
0.00.055.895 I ggml_metal_init: GPU name:   Apple M4
0.00.055.897 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.897 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.897 I ggml_metal_init: simdgroup reduction   = true
0.00.055.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.898 I ggml_metal_init: has bfloat            = true
0.00.055.898 I ggml_metal_init: use bfloat            = true
0.00.055.898 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.573 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.575 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.588 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.504 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.505 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.505 I llama_new_context_with_model: graph nodes  = 967
0.00.067.506 I llama_new_context_with_model: graph splits = 2
0.00.067.518 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.233.196 I 
0.00.233.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.233.232 I perplexity: tokenizing the input ..
0.00.240.840 I perplexity: tokenization took 7.607 ms
0.00.240.851 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.381.384 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.382.656 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.382.675 I llama_perf_context_print:        load time =     223.46 ms
0.00.382.676 I llama_perf_context_print: prompt eval time =     140.18 ms /   128 tokens (    1.10 ms per token,   913.09 tokens per second)
0.00.382.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.382.678 I llama_perf_context_print:       total time =     149.48 ms /   129 tokens
0.00.383.159 I ggml_metal_free: deallocating

real	0m0.400s
user	0m0.077s
sys	0m0.051s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4317 (869ec41e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146b08170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146b08880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146b08e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146b093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146b09990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146b09f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146b0a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146b0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146b0b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146b0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146b0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146b0bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146b0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146b0d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146b0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146b0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146b0e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146b0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146b0f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146b0fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146b105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146b10cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146b113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146b11c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146b123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146b12660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146b12c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146b138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146b13e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146b140e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146b14580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146b14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146b150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146b15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146b158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146b15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146b16210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146b166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146b16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146b16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146b17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146b17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146b17dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146b18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146b18530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146b18b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146b19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146b19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146b1a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146b1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146b1aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146b1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146b1b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146b1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146b1c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146b1cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146b1d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146b1d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146b1d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146b1e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146b1e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146b1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146b1ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146b1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146b1f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146b1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146b1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146b203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146b20880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146b20d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146b211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146b21660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146b21b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146b22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146b225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146b22af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146b23040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146b23590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146b23ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146b24030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146b24580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146b24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146b25020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146b25570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146b25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146b26010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146b26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146b26ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146b27000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146b27550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146b27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146b27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146b28540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146b28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146b28fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146b29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146b29a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146b19760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146b29ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146b2a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146b2abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146b2b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146b2b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146b2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146b2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146b2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146b2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146b2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146b2d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146b2dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146b2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146b2e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146b2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146b2f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146b2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146b2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146b2fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146b302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146b30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146b30c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146b310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146b31550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146b319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146b31e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146b32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146b327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146b32c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146b33110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146b335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146b33a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146b33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146b34390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146b34830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146b34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146b35170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146b35610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146b35ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146b35f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146b363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146b36890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146b36d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146b371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146b37670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146b37b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146b37fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146b38450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146b388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146b38d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146b39230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146b396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146b39b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146b3a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146b3a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146b3a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146b3adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146b3b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146b3b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146b3bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146b3c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146b3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146b3c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146b3ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146b3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146b3d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146b3dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146b3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146b3e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146b3ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146b3eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146b3f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146b3f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146b3fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146b40130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146b405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146b40a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146b40f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146b413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146b41850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146b41cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146b42190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146b42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146b42ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146b42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146b43410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146b438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146b43d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146b441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146b44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146b44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146b44fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146b45470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146b45910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146b45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146b46300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146b46850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146b46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146b472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146b475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146b47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146b481d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146b487e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146b48fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146b49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146b49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146b49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146b4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146b4ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146b4afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146b4b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146b4b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146b4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146b4c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146b4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146b4d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146b4d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146b4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146b4e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146b4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146b4eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146b4f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146b4f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146b4fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146b50090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146b505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146b50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146b51080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146b515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146b51b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146b52070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146b525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146b52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146b53060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146b535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146b53b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146b54050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146b545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146b54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146b55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146b55590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146b55ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146b56030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146b56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146b56ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146b57020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146b57570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146b57ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146b58010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146b58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146b58ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146b59000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146b59550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146b59aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146b59ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146b5a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146b5aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146b5afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146b5b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146b5ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146b5bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146b5c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146b5ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146b5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146b5d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146b5da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146b5dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146b5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146b5ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146b5eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146b5f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146b5f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146b5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146b60170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146b60610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146b60ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146b60f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146b613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146b61890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146b61d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146b621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146b62670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146b62b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146b62fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146b63500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146b63c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146b64340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146b64a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146b65180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146b65440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146b65c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146b65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146b66500 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146d05030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146d054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146d05910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146d08ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146d08f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146d093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146d09830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146d09ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146d0a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146d0a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146d0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146d0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146d0bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146d0c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146d0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146d0d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146d0da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146d0e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146d0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146d0f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146d0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146d0fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146d105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146d10ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146d11400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146d116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146d11980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146d11df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146d12260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146d126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146d12b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146d13070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146d134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146d137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146d13c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146d14080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146d144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146d14960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146d14dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146d15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146d156b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146d15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146d15f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146d16400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146d16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146d16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146d17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146d175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146d17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146d17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146d18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146d18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146d18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146d19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146d194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146d19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146d19eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146d1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146d1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146d1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146d1b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146d1b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146d1b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146d1be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146d1c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146d1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146d1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146d1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146d1d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146d1d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146d1dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146d1e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146d1e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146d1eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146d1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146d1f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146d1f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146d1fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146d200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146d20550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146d209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146d20e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146d212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146d21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146d21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146d21ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146d22460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146d228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146d22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146d231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146d23620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146d23a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146d23f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146d24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146d247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146d24c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146d250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146d25530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146d259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146d25e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146d26280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146d266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146d26b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146d26fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146d27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146d278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146d27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146d28190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146d28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146d28a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146d28ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146d29350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146d297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146d29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146d2a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146d2a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146d2a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146d2adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146d2b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146d2b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146d2bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146d2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146d2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146d2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146d2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146d2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146d2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146d2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146d2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146d2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146d2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146d2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146d2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146d2f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146d2f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146d2fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146d30240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146d306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146d30b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146d30f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146d31400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146d31870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146d31ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146d32150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146d325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146d32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146d32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146d33310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146d33780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146d33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146d34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146d344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146d34940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146d34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146d35220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146d35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146d35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146d35f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146d363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146d36850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146d36cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146d37130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146d375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146d37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146d37e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146d382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146d38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146d38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146d39040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146d394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146d39920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146d39d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146d3a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146d3a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146d3aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146d3af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146d3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146d3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146d3bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146d3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146d3c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146d3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146d3ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146d3d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146d3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146d3dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146d3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146d3e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146d3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146d3ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146d3f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146d3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146d3fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146d3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146d403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146d40810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146d40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146d410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146d41560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146d419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146d41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146d422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146d42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146d42b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146d43000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146d43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146d438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146d43e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146d442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146d44750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146d452a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146d45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146d45820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146d45c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146d46100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146d46570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146d469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146d46e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146d472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146d47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146d47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146d48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146d48480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146d488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146d48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146d491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146d49640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146d49ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146d49f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146d4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146d4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146d4ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146d4b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146d4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146d4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146d4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146d4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146d4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146d4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146d4cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146d4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146d4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146d4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146d4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146d4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146d4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146d4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146d4f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146d4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146d4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146d500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146d50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146d509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146d50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146d51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146d516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146d51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146d51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146d52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146d528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146d52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146d53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146d53600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146d53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146d53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146d54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146d547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146d54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146d550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146d55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146d55980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146d55df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146d56260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146d566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146d56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146d56fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146d57420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146d57890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146d57d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146d58170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146d585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146d58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146d58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146d59930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146d5a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146d5a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146d5ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146d5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146d5b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146d5bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146d5c1d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x146d08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x146d093b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x146d09820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x146d09c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x146d0a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x146d0a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x146d0a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x146d0ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x146d0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x146d0b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x146d0bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x146d0c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x146d0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x146d0d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x146d0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x146d0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x146d0e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x146d0eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x146d0f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x146d0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x146d10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x146d10cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x146d113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x146d11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x146d121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x146d12630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x146d12aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x146d12f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x146d13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x146d137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x146d13c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x146d140d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x146d14540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x146d14800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x146d14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x146d150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x146d15550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x146d159c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x146d15e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x146d162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x146d16710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x146d16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x146d16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x146d17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x146d178d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x146d17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x146d181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x146d18620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x146d18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x146d18f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x146d19370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x146d197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x146d19c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x146d1a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x146d1a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x146d1a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x146d1ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x146d1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x146d1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x146d1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x146d1bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x146d1c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x146d1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x146d1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x146d1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x146d1d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x146d1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x146d1dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x146d1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x146d1e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x146d1ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x146d1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x146d1f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x146d1f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x146d1fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x146d20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x146d206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x146d20b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x146d20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x146d21420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x146d21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x146d21d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x146d22170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x146d225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x146d22a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x146d22ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x146d23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x146d237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x146d23c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x146d24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x146d244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x146d24960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x146d24dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x146d25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x146d256b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x146d25b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x146d25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x146d26400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x146d26870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x146d26ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x146d27150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x146d275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x146d27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x146d27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x146d28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x146d28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x146d28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x146d29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x146d294d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x146d29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x146d29db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x146d2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x146d2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x146d2ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x146d2af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x146d2b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x146d2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x146d2bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x146d2c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x146d2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x146d2ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x146d2ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x146d2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x146d2d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x146d2dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x146d2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x146d2e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x146d2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x146d2ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x146d2f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x146d2f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x146d2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x146d2ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x146d303c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x146d30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x146d30ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x146d31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x146d31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x146d319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x146d31e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x146d322d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x146d32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x146d32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x146d33020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x146d33490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x146d33900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x146d33d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x146d341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x146d34650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x146d34ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x146d34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x146d353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x146d35810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x146d35c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x146d360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x146d36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x146d369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x146d36e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x146d372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x146d37720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x146d37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x146d38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x146d38470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x146d388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x146d38d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x146d391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x146d39630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x146d39aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x146d39f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x146d3a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x146d3a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x146d3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x146d3b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x146d3b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x146d3b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x146d3be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x146d3c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x146d3c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x146d3cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x146d3cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x146d3d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x146d3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x146d3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x146d3e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x146d3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x146d3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x146d3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x146d3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x146d3f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x146d3fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x146d400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x146d40520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x146d40990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x146d40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x146d41270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x146d416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x146d41b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x146d41fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x146d42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x146d428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x146d42d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x146d43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x146d435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x146d43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x146d43ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x146d44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x146d447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x146d44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x146d45090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x146d45500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x146d45c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x146d460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x146d46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x146d469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x146d46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x146d472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x146d47720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x146d47b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x146d48000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x146d48470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x146d488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x146d48d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x146d491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x146d49630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x146d49aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x146d49f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x146d4a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x146d4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x146d4ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x146d4b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x146d4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x146d4b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x146d4be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x146d4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x146d4c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x146d4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x146d4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x146d4d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x146d4d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x146d4dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x146d4e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x146d4e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x146d4ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x146d4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x146d4f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x146d4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x146d4fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x146d500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x146d50520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x146d50990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x146d50e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x146d51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x146d516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x146d51b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x146d51fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x146d52430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x146d528a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x146d52d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x146d53180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x146d535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x146d53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x146d53ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x146d54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x146d547b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x146d54c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x146d55090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x146d55500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x146d55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x146d55de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x146d56250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x146d566c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x146d56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x146d56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x146d57410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x146d57880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x146d57cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x146d58160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x146d585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x146d58a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x146d58eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x146d59320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x146d59790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x146d59c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x146d5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x146d5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x146d5b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x146d5b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x146d5bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x146d5c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x146d04d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x146d051f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.829s
user	0m0.296s
sys	0m0.306s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4317 (869ec41e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147e0b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147e0bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147e0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147e0c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147e0cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147e0d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147e0d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147e0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147e0e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147e0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147e0eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147e0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147e0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147e10470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147e10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147e113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147e11ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147e121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147e12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147e130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147e137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147e13f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147e14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147e14ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147e155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147e158b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147e15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147e16b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147e17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147e17330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147e177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147e17a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147e18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147e18860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147e18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147e18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147e19460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147e19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147e19da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147e1a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147e1a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147e1ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147e1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147e1b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147e1b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147e1bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147e1c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147e1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147e1d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147e1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147e1def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147e1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147e1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147e1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147e1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147e1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147e20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147e20510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147e20b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147e21310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147e215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147e21a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147e21f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147e223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147e22850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147e22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147e23190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147e23630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147e23ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147e23f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147e24410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147e248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147e24d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147e252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147e257f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147e25d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147e26290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147e267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147e26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147e27280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147e277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147e27d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147e28270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147e287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147e28d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147e29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147e297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147e29d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147e2a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147e2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147e2acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147e2b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147e2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147e2bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147e2c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147e2c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147e2ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147e1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147e2d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147e2d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147e2de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147e2e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147e2e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147e2ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147e2f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147e2f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147e2fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147e30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147e308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147e30e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147e31360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147e318b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147e31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147e322a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147e32740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147e32be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147e33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147e33520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147e339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147e33e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147e34300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147e347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147e34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147e350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147e35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147e35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147e35ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147e36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147e36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147e36ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147e37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147e375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147e37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147e37f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147e383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147e38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147e38d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147e391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147e39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147e39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147e39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147e3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147e3a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147e3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147e3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147e3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147e3bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147e3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147e3c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147e3c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147e3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147e3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147e3d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147e3dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147e3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147e3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147e3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147e3f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147e3f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147e3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147e400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147e40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147e409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147e40e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147e41320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147e417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147e41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147e42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147e425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147e42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147e42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147e43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147e43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147e43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147e44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147e44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147e44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147e44f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147e45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147e45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147e461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147e46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147e46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147e46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147e47440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147e478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147e47d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147e48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147e486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147e48b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147e49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147e49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147e49aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147e49ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147e4a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147e4a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147e4ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147e4b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147e4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147e4c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147e4c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147e4c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147e4cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147e4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147e4dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147e4e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147e4e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147e4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147e4f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147e4f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147e4fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147e50310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147e50860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147e50db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147e51300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147e51850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147e51da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147e522f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147e52840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147e52d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147e532e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147e53830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147e53d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147e542d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147e54820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147e54d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147e552c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147e55810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147e55d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147e562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147e56800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147e56d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147e572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147e577f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147e57d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147e58290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147e587e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147e58d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147e59280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147e597d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147e59d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147e5a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147e5a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147e5ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147e5b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147e5b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147e5bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147e5c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147e5c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147e5ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147e5d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147e5d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147e5dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147e5e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147e5e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147e5ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147e5f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147e5f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147e5fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147e60210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147e60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147e60cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147e61200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147e61750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147e61ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147e62140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147e625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147e62a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147e62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147e633c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147e63860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147e63d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147e641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147e64640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147e64ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147e64f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147e65420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147e658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147e65d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147e66200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147e66750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147e66e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147e67590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147e67cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147e683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147e68690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147e68e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147e69140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147e69750 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x149806050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1498064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x149806930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x149806da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x149807210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x149807680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x149807af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x149807f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1498083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x149808840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x149808cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x149809390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x149809eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14980a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14980ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14980b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14980bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14980c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14980caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14980d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14980d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14980e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14980e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14980ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14980f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14980f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14980fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x149810050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1498104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x149810930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x149810da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1498112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x149811740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x149811a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x149811e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1498122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x149812750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x149812bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x149813030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1498134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x149813910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x149813d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1498141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x149814660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x149814ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x149814f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1498153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x149815820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x149815c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x149816100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x149816570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1498169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x149816e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1498172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x149817730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x149817ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x149818110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x149818610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x149818a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x149818ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x149819360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1498197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x149819c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14981a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14981a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14981a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14981ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14981b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14981b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14981bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14981bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14981c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14981c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14981cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14981d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14981d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14981da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14981ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14981e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14981e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14981ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14981f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14981f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14981f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14981fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x149820250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1498206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x149820b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x149820fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x149821410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x149821880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x149821cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x149822160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1498225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x149822a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x149822eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x149823320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x149823790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x149823c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x149824070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1498244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x149824950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x149824dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x149825230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1498256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x149825b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x149825f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1498263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x149826860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x149826cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x149827140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1498275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x149827a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x149827e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x149828300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x149828770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x149828be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x149829050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1498294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x149829930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x149829da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14982a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14982a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14982aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14982af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14982b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14982b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14982bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14982c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14982c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14982ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14982ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14982d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14982d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14982dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14982e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14982e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14982e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14982ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14982f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14982f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14982fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14982ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1498303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x149830820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x149830c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x149831100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x149831570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1498319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x149831e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1498322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x149832730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x149832ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x149833010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x149833480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1498338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x149833d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1498341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x149834640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x149834ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x149834f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x149835390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x149835800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x149835c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1498360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x149836550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1498369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x149836e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1498372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x149837710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x149837b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x149837ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x149838460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1498388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x149838d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1498391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x149839620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x149839a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x149839f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14983a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14983a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14983ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14983b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14983b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14983b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14983be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14983c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14983c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14983cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14983cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14983d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14983d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14983dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14983e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14983e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14983ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14983eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14983f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14983f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14983fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1498400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x149840510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x149840980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x149840df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x149841260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1498416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x149841b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1498420d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x149842540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1498429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x149843500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1498437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x149843a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x149843ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x149844360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1498447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x149844c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1498450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x149845520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x149845990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x149845e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x149846270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1498466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x149846b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x149846fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x149847430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1498478a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x149847d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x149848180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1498485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x149848a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x149848ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x149849340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1498497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x149849c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14984a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14984a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14984a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14984ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14984b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14984b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14984bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14984bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14984c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14984c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14984ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14984d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14984d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14984da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14984deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14984e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14984e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14984ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14984f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14984f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14984f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14984fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x149850230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1498506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x149850b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x149850f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1498513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x149851860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x149851cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x149852140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1498525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x149852a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x149852e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x149853300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x149853770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x149853be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x149854050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1498544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x149854930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x149854da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x149855210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x149855680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x149855af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x149855f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1498563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x149856840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x149856cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x149857120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x149857b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1498582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1498589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1498590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1498593b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x149859820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x149859e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14985a430 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147e0c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147e0c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x147e0cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147e0d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147e0d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x147e0d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147e0dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147e0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147e0b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x147e25cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147e25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147e263e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147e26cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x147e27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x147e27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x147e28320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x147e28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x147e29100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x147e297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x147e2a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x147e2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x147e2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x147e2b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x147e2bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x147e2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x147e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x147e2cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x147e2d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x147e2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x147e2da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x147e2dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x147e2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147e2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147e2ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x147e2eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147e2f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x147e2f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147e2fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147e30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147e30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147e30970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x147e30de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147e31250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x147e316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147e31b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147e31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147e32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147e32880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x147e32cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147e33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x147e335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147e33a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x147e33eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147e34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147e34790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x147e34c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147e35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147e354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x147e35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147e35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147e36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147e366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147e36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147e36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147e373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x147e37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147e37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147e38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x147e385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x147e38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x147e38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x147e39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x147e39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x147e39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x147e3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x147e3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x147e3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x147e3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x147e3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x147e3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x147e3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x147e3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x147e3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x147e3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x147e3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x147e3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x147e3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x147e3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x147e3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x147e3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x147e3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147e3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147e3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147e3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147e3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147e3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147e401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x147e40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147e40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147e40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147e413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x147e41820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147e41c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147e42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x147e42570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147e429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147e42e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147e432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x147e43730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147e43ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147e44010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147e44480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147e448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x147e44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147e451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x147e45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147e45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147e45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147e46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147e46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x147e46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147e470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x147e47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147e479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147e47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147e482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x147e48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x147e48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x147e48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x147e49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x147e498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x147e49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x147e4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x147e4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x147e4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x147e4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x147e4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x147e4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x147e4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x147e4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x147e4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x147e4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x147e4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x147e4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x147e4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x147e4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x147e4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x147e4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147e4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x147e4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147e4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147e4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x147e4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147e4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147e50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147e507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x147e50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147e510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147e51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147e51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147e51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147e52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147e526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x147e52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147e52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x147e53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147e53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147e53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147e54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x147e545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147e54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147e54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147e55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147e557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147e55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147e56080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x147e564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147e56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147e56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147e57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147e576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147e57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x147e57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x147e58400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x147e58870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x147e58ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x147e59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x147e595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x147e59a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x147e59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x147e5a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x147e5a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x147e5abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x147e5b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x147e5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x147e5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x147e5bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x147e5c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x147e5c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x147e5cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x147e5cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x147e5d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x147e5d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x147e5dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x147e5e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147e5e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147e5ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x147e5ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147e5f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147e5f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147e5fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147e60350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147e607c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x147e60c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147e610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147e61510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147e61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147e61df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147e62260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147e626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147e62b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x147e62fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147e63420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147e63890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147e63d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x147e64170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147e645e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147e64a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147e64ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x147e65330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147e657a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147e65c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x147e66080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x147e664f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x147e66960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x147e66dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x147e67240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x147e676b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x147e67b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x147e67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x147e68400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x147e68870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x147e68ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x147e69150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x147e695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x147e18850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x147e18cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x147e19130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x147e195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x147e19a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147e19e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147e1a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147e1a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147e1abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147e1b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147e1b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147e1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x147e1bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147e1c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147e1c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147e1cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x147e1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147e1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147e1d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147e1dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147e1e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147e1e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147e1e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147e1ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147e1f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147e1f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147e1fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147e20020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147e20490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147e20900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147e20d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147e211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147e21650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x147e21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x147e21f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x147e223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x147e22810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x147e22c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x147e23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x147e23a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x147e24150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x147e24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x147e24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x147e25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x147e25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x147e17060 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.948s
user	0m0.243s
sys	0m0.146s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.56 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.58 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.14 sec*proc (2 tests)

Total Test time (real) =   1.15 sec
        1.17 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.26 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
