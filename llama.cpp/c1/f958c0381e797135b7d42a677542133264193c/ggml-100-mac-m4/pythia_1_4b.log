Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.619s
user	0m0.824s
sys	0m1.284s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 28%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-chat
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Built target test-log
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Built target test-grammar-integration
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-arg-parser
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-arg-parser
[ 62%] Built target test-chat-template
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-backend-ops
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-autorelease
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Built target test-quantize-perf
[ 68%] Built target test-rope
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-embedding
[ 71%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-eval-callback
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-imatrix
[ 74%] Built target llama-gbnf-validator
[ 75%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Built target llama-infill
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookahead
[ 80%] Built target llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-lookup
[ 80%] Generating loading.html.hpp
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-merge
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Built target llama-cli
[ 82%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-passkey
[ 82%] Built target llama-parallel
[ 83%] Generating index.html.gz.hpp
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Built target llama-perplexity
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Built target llama-quantize
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 86%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-run
[ 93%] Built target llama-tts
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-gen-docs
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.243s
user	0m6.598s
sys	0m9.982s

main: quantize time =  5746.25 ms
main:    total time =  5746.25 ms

main: quantize time =  4392.73 ms
main:    total time =  4392.73 ms

main: quantize time =  3242.12 ms
main:    total time =  3242.12 ms

main: quantize time =  3562.41 ms
main:    total time =  3562.41 ms

main: quantize time =  2003.75 ms
main:    total time =  2003.75 ms

main: quantize time =  5405.54 ms
main:    total time =  5405.54 ms

main: quantize time =  5979.62 ms
main:    total time =  5979.62 ms

main: quantize time =  7303.46 ms
main:    total time =  7303.46 ms

main: quantize time =  6198.49 ms
main:    total time =  6198.49 ms

main: quantize time =  4592.73 ms
main:    total time =  4592.73 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.148 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.329 I main: llama backend init
0.00.000.335 I main: load the model and apply lora adapter, if any
0.00.103.014 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.115.344 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.115.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.115.364 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.115.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.115.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.115.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.115.367 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.115.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.115.370 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.115.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.115.371 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.115.372 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.115.372 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.115.389 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.115.394 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.115.395 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.115.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.122.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.124.378 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.131.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.131.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.131.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.131.243 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.131.244 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.131.245 I llama_model_loader: - type  f32:  194 tensors
0.00.131.245 I llama_model_loader: - type  f16:   98 tensors
0.00.131.247 I print_info: file format = GGUF V3 (latest)
0.00.131.248 I print_info: file type   = all F32 (guessed)
0.00.131.250 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.148.911 I load: special tokens cache size = 25
0.00.159.011 I load: token to piece cache size = 0.2984 MB
0.00.159.015 I print_info: arch             = gptneox
0.00.159.016 I print_info: vocab_only       = 0
0.00.159.016 I print_info: n_ctx_train      = 2048
0.00.159.016 I print_info: n_embd           = 2048
0.00.159.016 I print_info: n_layer          = 24
0.00.159.022 I print_info: n_head           = 16
0.00.159.023 I print_info: n_head_kv        = 16
0.00.159.023 I print_info: n_rot            = 32
0.00.159.024 I print_info: n_swa            = 0
0.00.159.024 I print_info: n_embd_head_k    = 128
0.00.159.024 I print_info: n_embd_head_v    = 128
0.00.159.028 I print_info: n_gqa            = 1
0.00.159.029 I print_info: n_embd_k_gqa     = 2048
0.00.159.031 I print_info: n_embd_v_gqa     = 2048
0.00.159.032 I print_info: f_norm_eps       = 1.0e-05
0.00.159.033 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.159.033 I print_info: f_clamp_kqv      = 0.0e+00
0.00.159.033 I print_info: f_max_alibi_bias = 0.0e+00
0.00.159.034 I print_info: f_logit_scale    = 0.0e+00
0.00.159.035 I print_info: n_ff             = 8192
0.00.159.035 I print_info: n_expert         = 0
0.00.159.035 I print_info: n_expert_used    = 0
0.00.159.035 I print_info: causal attn      = 1
0.00.159.036 I print_info: pooling type     = 0
0.00.159.036 I print_info: rope type        = 2
0.00.159.036 I print_info: rope scaling     = linear
0.00.159.036 I print_info: freq_base_train  = 10000.0
0.00.159.037 I print_info: freq_scale_train = 1
0.00.159.037 I print_info: n_ctx_orig_yarn  = 2048
0.00.159.038 I print_info: rope_finetuned   = unknown
0.00.159.038 I print_info: ssm_d_conv       = 0
0.00.159.038 I print_info: ssm_d_inner      = 0
0.00.159.038 I print_info: ssm_d_state      = 0
0.00.159.038 I print_info: ssm_dt_rank      = 0
0.00.159.038 I print_info: ssm_dt_b_c_rms   = 0
0.00.159.039 I print_info: model type       = 1.4B
0.00.159.039 I print_info: model params     = 1.41 B
0.00.159.039 I print_info: general.name     = 1.4B
0.00.159.040 I print_info: vocab type       = BPE
0.00.159.040 I print_info: n_vocab          = 50304
0.00.159.041 I print_info: n_merges         = 50009
0.00.159.041 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.159.041 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.159.041 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.159.044 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.159.044 I print_info: LF token         = 187 'Ċ'
0.00.159.044 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.159.044 I print_info: max token length = 1024
0.00.159.045 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.212.732 I load_tensors: offloading 24 repeating layers to GPU
0.00.212.736 I load_tensors: offloading output layer to GPU
0.00.212.736 I load_tensors: offloaded 25/25 layers to GPU
0.00.212.762 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.212.763 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.213.374 I llama_init_from_model: n_seq_max     = 1
0.00.213.374 I llama_init_from_model: n_ctx         = 2048
0.00.213.375 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.213.375 I llama_init_from_model: n_batch       = 2048
0.00.213.375 I llama_init_from_model: n_ubatch      = 512
0.00.213.375 I llama_init_from_model: flash_attn    = 0
0.00.213.376 I llama_init_from_model: freq_base     = 10000.0
0.00.213.376 I llama_init_from_model: freq_scale    = 1
0.00.213.377 I ggml_metal_init: allocating
0.00.213.418 I ggml_metal_init: found device: Apple M4
0.00.213.425 I ggml_metal_init: picking default device: Apple M4
0.00.214.093 I ggml_metal_init: using embedded metal library
0.00.229.368 I ggml_metal_init: GPU name:   Apple M4
0.00.229.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.229.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.229.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.229.371 I ggml_metal_init: simdgroup reduction   = true
0.00.229.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.229.371 I ggml_metal_init: has residency sets    = true
0.00.229.371 I ggml_metal_init: has bfloat            = true
0.00.229.371 I ggml_metal_init: use bfloat            = true
0.00.229.372 I ggml_metal_init: hasUnifiedMemory      = true
0.00.229.372 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.294.330 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.325.589 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.325.596 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.325.649 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.329.775 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.329.777 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.329.777 I llama_init_from_model: graph nodes  = 967
0.00.329.777 I llama_init_from_model: graph splits = 2
0.00.329.783 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.329.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.329.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.396.271 I main: llama threadpool init, n_threads = 4
0.00.396.313 I 
0.00.396.330 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.396.330 I 
0.00.396.507 I sampler seed: 1234
0.00.396.512 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.396.537 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.396.539 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.396.539 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.235.434 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.02.235.435 I llama_perf_context_print:        load time =     292.41 ms
0.02.235.435 I llama_perf_context_print: prompt eval time =      53.47 ms /     7 tokens (    7.64 ms per token,   130.92 tokens per second)
0.02.235.436 I llama_perf_context_print:        eval time =    1782.59 ms /    63 runs   (   28.30 ms per token,    35.34 tokens per second)
0.02.235.436 I llama_perf_context_print:       total time =    1839.98 ms /    70 tokens
0.02.235.639 I ggml_metal_free: deallocating

real	0m2.552s
user	0m0.134s
sys	0m0.153s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.079 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.060 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.066 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.069 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.070 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.070 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.070 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.070 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.071 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.072 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.073 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.073 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.074 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.077 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.077 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.078 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.085 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.050 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.051 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.052 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.052 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.052 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.053 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.053 I llama_model_loader: - type  f32:  194 tensors
0.00.034.054 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.055 I print_info: file format = GGUF V3 (latest)
0.00.034.055 I print_info: file type   = Q8_0
0.00.034.056 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.042.608 I load: special tokens cache size = 25
0.00.048.493 I load: token to piece cache size = 0.2984 MB
0.00.048.498 I print_info: arch             = gptneox
0.00.048.499 I print_info: vocab_only       = 0
0.00.048.499 I print_info: n_ctx_train      = 2048
0.00.048.501 I print_info: n_embd           = 2048
0.00.048.501 I print_info: n_layer          = 24
0.00.048.508 I print_info: n_head           = 16
0.00.048.509 I print_info: n_head_kv        = 16
0.00.048.509 I print_info: n_rot            = 32
0.00.048.509 I print_info: n_swa            = 0
0.00.048.510 I print_info: n_embd_head_k    = 128
0.00.048.510 I print_info: n_embd_head_v    = 128
0.00.048.511 I print_info: n_gqa            = 1
0.00.048.511 I print_info: n_embd_k_gqa     = 2048
0.00.048.512 I print_info: n_embd_v_gqa     = 2048
0.00.048.513 I print_info: f_norm_eps       = 1.0e-05
0.00.048.513 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.513 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.513 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.514 I print_info: f_logit_scale    = 0.0e+00
0.00.048.514 I print_info: n_ff             = 8192
0.00.048.515 I print_info: n_expert         = 0
0.00.048.515 I print_info: n_expert_used    = 0
0.00.048.515 I print_info: causal attn      = 1
0.00.048.515 I print_info: pooling type     = 0
0.00.048.518 I print_info: rope type        = 2
0.00.048.518 I print_info: rope scaling     = linear
0.00.048.519 I print_info: freq_base_train  = 10000.0
0.00.048.519 I print_info: freq_scale_train = 1
0.00.048.520 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.520 I print_info: rope_finetuned   = unknown
0.00.048.520 I print_info: ssm_d_conv       = 0
0.00.048.520 I print_info: ssm_d_inner      = 0
0.00.048.521 I print_info: ssm_d_state      = 0
0.00.048.521 I print_info: ssm_dt_rank      = 0
0.00.048.521 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.521 I print_info: model type       = 1.4B
0.00.048.522 I print_info: model params     = 1.41 B
0.00.048.522 I print_info: general.name     = 1.4B
0.00.048.523 I print_info: vocab type       = BPE
0.00.048.523 I print_info: n_vocab          = 50304
0.00.048.523 I print_info: n_merges         = 50009
0.00.048.523 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.523 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: LF token         = 187 'Ċ'
0.00.048.524 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.524 I print_info: max token length = 1024
0.00.048.525 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.188.587 I load_tensors: offloading 24 repeating layers to GPU
0.01.188.594 I load_tensors: offloading output layer to GPU
0.01.188.595 I load_tensors: offloaded 25/25 layers to GPU
0.01.188.617 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.188.620 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.189.566 I llama_init_from_model: n_seq_max     = 1
0.01.189.569 I llama_init_from_model: n_ctx         = 2048
0.01.189.569 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.189.569 I llama_init_from_model: n_batch       = 2048
0.01.189.570 I llama_init_from_model: n_ubatch      = 512
0.01.189.571 I llama_init_from_model: flash_attn    = 0
0.01.189.571 I llama_init_from_model: freq_base     = 10000.0
0.01.189.572 I llama_init_from_model: freq_scale    = 1
0.01.189.573 I ggml_metal_init: allocating
0.01.189.596 I ggml_metal_init: found device: Apple M4
0.01.189.608 I ggml_metal_init: picking default device: Apple M4
0.01.191.332 I ggml_metal_init: using embedded metal library
0.01.197.244 I ggml_metal_init: GPU name:   Apple M4
0.01.197.248 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.197.249 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.197.250 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.197.250 I ggml_metal_init: simdgroup reduction   = true
0.01.197.251 I ggml_metal_init: simdgroup matrix mul. = true
0.01.197.251 I ggml_metal_init: has residency sets    = true
0.01.197.251 I ggml_metal_init: has bfloat            = true
0.01.197.251 I ggml_metal_init: use bfloat            = true
0.01.197.252 I ggml_metal_init: hasUnifiedMemory      = true
0.01.197.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.216.464 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.279.857 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.279.864 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.279.906 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.284.451 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.284.453 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.284.454 I llama_init_from_model: graph nodes  = 967
0.01.284.454 I llama_init_from_model: graph splits = 2
0.01.284.459 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.284.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.284.588 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.340.227 I main: llama threadpool init, n_threads = 4
0.01.340.271 I 
0.01.340.290 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.340.290 I 
0.01.340.466 I sampler seed: 1234
0.01.340.471 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.340.482 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.340.482 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.340.484 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.421.517 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.02.421.517 I llama_perf_context_print:        load time =    1329.42 ms
0.02.421.518 I llama_perf_context_print: prompt eval time =      49.25 ms /     7 tokens (    7.04 ms per token,   142.13 tokens per second)
0.02.421.519 I llama_perf_context_print:        eval time =    1028.92 ms /    63 runs   (   16.33 ms per token,    61.23 tokens per second)
0.02.421.519 I llama_perf_context_print:       total time =    1082.01 ms /    70 tokens
0.02.421.742 I ggml_metal_free: deallocating

real	0m2.441s
user	0m0.112s
sys	0m0.300s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.010.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.399 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.409 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.411 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.411 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.411 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.412 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.412 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.412 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.413 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.414 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.415 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.415 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.089 I llama_model_loader: - type  f32:  194 tensors
0.00.027.089 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.089 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.090 I print_info: file format = GGUF V3 (latest)
0.00.027.091 I print_info: file type   = Q4_0
0.00.027.092 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.968 I load: special tokens cache size = 25
0.00.041.094 I load: token to piece cache size = 0.2984 MB
0.00.041.101 I print_info: arch             = gptneox
0.00.041.101 I print_info: vocab_only       = 0
0.00.041.101 I print_info: n_ctx_train      = 2048
0.00.041.102 I print_info: n_embd           = 2048
0.00.041.102 I print_info: n_layer          = 24
0.00.041.107 I print_info: n_head           = 16
0.00.041.108 I print_info: n_head_kv        = 16
0.00.041.108 I print_info: n_rot            = 32
0.00.041.108 I print_info: n_swa            = 0
0.00.041.108 I print_info: n_embd_head_k    = 128
0.00.041.108 I print_info: n_embd_head_v    = 128
0.00.041.109 I print_info: n_gqa            = 1
0.00.041.110 I print_info: n_embd_k_gqa     = 2048
0.00.041.112 I print_info: n_embd_v_gqa     = 2048
0.00.041.112 I print_info: f_norm_eps       = 1.0e-05
0.00.041.113 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.113 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.114 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.114 I print_info: f_logit_scale    = 0.0e+00
0.00.041.115 I print_info: n_ff             = 8192
0.00.041.115 I print_info: n_expert         = 0
0.00.041.115 I print_info: n_expert_used    = 0
0.00.041.115 I print_info: causal attn      = 1
0.00.041.115 I print_info: pooling type     = 0
0.00.041.115 I print_info: rope type        = 2
0.00.041.115 I print_info: rope scaling     = linear
0.00.041.116 I print_info: freq_base_train  = 10000.0
0.00.041.116 I print_info: freq_scale_train = 1
0.00.041.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.117 I print_info: rope_finetuned   = unknown
0.00.041.117 I print_info: ssm_d_conv       = 0
0.00.041.117 I print_info: ssm_d_inner      = 0
0.00.041.117 I print_info: ssm_d_state      = 0
0.00.041.118 I print_info: ssm_dt_rank      = 0
0.00.041.118 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.118 I print_info: model type       = 1.4B
0.00.041.118 I print_info: model params     = 1.41 B
0.00.041.118 I print_info: general.name     = 1.4B
0.00.041.119 I print_info: vocab type       = BPE
0.00.041.119 I print_info: n_vocab          = 50304
0.00.041.120 I print_info: n_merges         = 50009
0.00.041.122 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.122 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.122 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.123 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.123 I print_info: LF token         = 187 'Ċ'
0.00.041.123 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.123 I print_info: max token length = 1024
0.00.041.124 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.609.513 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.529 I load_tensors: offloading output layer to GPU
0.00.609.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.565 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.609.567 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.611.247 I llama_init_from_model: n_seq_max     = 1
0.00.611.250 I llama_init_from_model: n_ctx         = 2048
0.00.611.250 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.611.251 I llama_init_from_model: n_batch       = 2048
0.00.611.251 I llama_init_from_model: n_ubatch      = 512
0.00.611.252 I llama_init_from_model: flash_attn    = 0
0.00.611.254 I llama_init_from_model: freq_base     = 10000.0
0.00.611.254 I llama_init_from_model: freq_scale    = 1
0.00.611.257 I ggml_metal_init: allocating
0.00.611.337 I ggml_metal_init: found device: Apple M4
0.00.611.351 I ggml_metal_init: picking default device: Apple M4
0.00.613.209 I ggml_metal_init: using embedded metal library
0.00.618.832 I ggml_metal_init: GPU name:   Apple M4
0.00.618.837 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.838 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.839 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.840 I ggml_metal_init: simdgroup reduction   = true
0.00.618.841 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.841 I ggml_metal_init: has residency sets    = true
0.00.618.841 I ggml_metal_init: has bfloat            = true
0.00.618.842 I ggml_metal_init: use bfloat            = true
0.00.618.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.844 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.125 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.674 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.695.682 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.695.717 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.157 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.159 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.159 I llama_init_from_model: graph nodes  = 967
0.00.700.160 I llama_init_from_model: graph splits = 2
0.00.700.165 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.350 I main: llama threadpool init, n_threads = 4
0.00.758.396 I 
0.00.758.412 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.412 I 
0.00.758.578 I sampler seed: 1234
0.00.758.583 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.758.637 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.758.640 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.758.640 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.434.227 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.434.228 I llama_perf_context_print:        load time =     746.67 ms
0.01.434.229 I llama_perf_context_print: prompt eval time =      46.86 ms /     7 tokens (    6.69 ms per token,   149.39 tokens per second)
0.01.434.229 I llama_perf_context_print:        eval time =     625.90 ms /    63 runs   (    9.93 ms per token,   100.65 tokens per second)
0.01.434.230 I llama_perf_context_print:       total time =     676.56 ms /    70 tokens
0.01.434.461 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.010.202 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.119 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.124 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.126 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.128 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.129 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.129 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.130 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.133 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.134 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.137 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.912 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.912 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.913 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.914 I llama_model_loader: - type  f32:  194 tensors
0.00.026.914 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.914 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.915 I print_info: file format = GGUF V3 (latest)
0.00.026.915 I print_info: file type   = Q4_1
0.00.026.916 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.035.084 I load: special tokens cache size = 25
0.00.041.123 I load: token to piece cache size = 0.2984 MB
0.00.041.126 I print_info: arch             = gptneox
0.00.041.126 I print_info: vocab_only       = 0
0.00.041.127 I print_info: n_ctx_train      = 2048
0.00.041.127 I print_info: n_embd           = 2048
0.00.041.127 I print_info: n_layer          = 24
0.00.041.130 I print_info: n_head           = 16
0.00.041.131 I print_info: n_head_kv        = 16
0.00.041.131 I print_info: n_rot            = 32
0.00.041.131 I print_info: n_swa            = 0
0.00.041.131 I print_info: n_embd_head_k    = 128
0.00.041.132 I print_info: n_embd_head_v    = 128
0.00.041.132 I print_info: n_gqa            = 1
0.00.041.133 I print_info: n_embd_k_gqa     = 2048
0.00.041.136 I print_info: n_embd_v_gqa     = 2048
0.00.041.136 I print_info: f_norm_eps       = 1.0e-05
0.00.041.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.137 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.137 I print_info: f_logit_scale    = 0.0e+00
0.00.041.138 I print_info: n_ff             = 8192
0.00.041.138 I print_info: n_expert         = 0
0.00.041.138 I print_info: n_expert_used    = 0
0.00.041.138 I print_info: causal attn      = 1
0.00.041.144 I print_info: pooling type     = 0
0.00.041.148 I print_info: rope type        = 2
0.00.041.149 I print_info: rope scaling     = linear
0.00.041.149 I print_info: freq_base_train  = 10000.0
0.00.041.150 I print_info: freq_scale_train = 1
0.00.041.150 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.151 I print_info: rope_finetuned   = unknown
0.00.041.151 I print_info: ssm_d_conv       = 0
0.00.041.151 I print_info: ssm_d_inner      = 0
0.00.041.151 I print_info: ssm_d_state      = 0
0.00.041.151 I print_info: ssm_dt_rank      = 0
0.00.041.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.152 I print_info: model type       = 1.4B
0.00.041.153 I print_info: model params     = 1.41 B
0.00.041.153 I print_info: general.name     = 1.4B
0.00.041.154 I print_info: vocab type       = BPE
0.00.041.154 I print_info: n_vocab          = 50304
0.00.041.154 I print_info: n_merges         = 50009
0.00.041.155 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.155 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.158 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.158 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.158 I print_info: LF token         = 187 'Ċ'
0.00.041.159 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.159 I print_info: max token length = 1024
0.00.041.159 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.619.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.619.139 I load_tensors: offloading output layer to GPU
0.00.619.140 I load_tensors: offloaded 25/25 layers to GPU
0.00.619.167 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.619.168 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.620.667 I llama_init_from_model: n_seq_max     = 1
0.00.620.671 I llama_init_from_model: n_ctx         = 2048
0.00.620.671 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.620.672 I llama_init_from_model: n_batch       = 2048
0.00.620.672 I llama_init_from_model: n_ubatch      = 512
0.00.620.672 I llama_init_from_model: flash_attn    = 0
0.00.620.675 I llama_init_from_model: freq_base     = 10000.0
0.00.620.675 I llama_init_from_model: freq_scale    = 1
0.00.620.678 I ggml_metal_init: allocating
0.00.620.733 I ggml_metal_init: found device: Apple M4
0.00.620.745 I ggml_metal_init: picking default device: Apple M4
0.00.622.409 I ggml_metal_init: using embedded metal library
0.00.628.152 I ggml_metal_init: GPU name:   Apple M4
0.00.628.163 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.628.163 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.628.164 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.628.165 I ggml_metal_init: simdgroup reduction   = true
0.00.628.165 I ggml_metal_init: simdgroup matrix mul. = true
0.00.628.165 I ggml_metal_init: has residency sets    = true
0.00.628.165 I ggml_metal_init: has bfloat            = true
0.00.628.166 I ggml_metal_init: use bfloat            = true
0.00.628.170 I ggml_metal_init: hasUnifiedMemory      = true
0.00.628.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.649.533 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.047 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.713.052 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.020 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.718.022 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.718.023 I llama_init_from_model: graph nodes  = 967
0.00.718.023 I llama_init_from_model: graph splits = 2
0.00.718.030 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.718.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.718.164 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.174 I main: llama threadpool init, n_threads = 4
0.00.775.212 I 
0.00.775.227 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.227 I 
0.00.775.371 I sampler seed: 1234
0.00.775.375 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.393 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.393 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.393 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.502.917 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56393.96 tokens per second)
0.01.502.918 I llama_perf_context_print:        load time =     764.19 ms
0.01.502.920 I llama_perf_context_print: prompt eval time =      48.80 ms /     7 tokens (    6.97 ms per token,   143.43 tokens per second)
0.01.502.921 I llama_perf_context_print:        eval time =     676.03 ms /    63 runs   (   10.73 ms per token,    93.19 tokens per second)
0.01.502.921 I llama_perf_context_print:       total time =     728.52 ms /    70 tokens
0.01.503.161 I ggml_metal_free: deallocating

real	0m1.522s
user	0m0.112s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.011.228 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.949 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.956 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.956 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.957 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.957 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.958 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.958 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.959 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.959 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.959 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.960 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.962 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.962 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.714 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.742 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.468 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.469 I llama_model_loader: - type  f32:  194 tensors
0.00.027.469 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.470 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.470 I print_info: file format = GGUF V3 (latest)
0.00.027.471 I print_info: file type   = Q5_0
0.00.027.476 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.274 I load: special tokens cache size = 25
0.00.041.277 I load: token to piece cache size = 0.2984 MB
0.00.041.280 I print_info: arch             = gptneox
0.00.041.280 I print_info: vocab_only       = 0
0.00.041.280 I print_info: n_ctx_train      = 2048
0.00.041.280 I print_info: n_embd           = 2048
0.00.041.280 I print_info: n_layer          = 24
0.00.041.283 I print_info: n_head           = 16
0.00.041.284 I print_info: n_head_kv        = 16
0.00.041.286 I print_info: n_rot            = 32
0.00.041.287 I print_info: n_swa            = 0
0.00.041.287 I print_info: n_embd_head_k    = 128
0.00.041.287 I print_info: n_embd_head_v    = 128
0.00.041.288 I print_info: n_gqa            = 1
0.00.041.289 I print_info: n_embd_k_gqa     = 2048
0.00.041.289 I print_info: n_embd_v_gqa     = 2048
0.00.041.290 I print_info: f_norm_eps       = 1.0e-05
0.00.041.290 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.290 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.292 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.293 I print_info: f_logit_scale    = 0.0e+00
0.00.041.294 I print_info: n_ff             = 8192
0.00.041.294 I print_info: n_expert         = 0
0.00.041.294 I print_info: n_expert_used    = 0
0.00.041.294 I print_info: causal attn      = 1
0.00.041.295 I print_info: pooling type     = 0
0.00.041.295 I print_info: rope type        = 2
0.00.041.299 I print_info: rope scaling     = linear
0.00.041.299 I print_info: freq_base_train  = 10000.0
0.00.041.299 I print_info: freq_scale_train = 1
0.00.041.300 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.300 I print_info: rope_finetuned   = unknown
0.00.041.300 I print_info: ssm_d_conv       = 0
0.00.041.300 I print_info: ssm_d_inner      = 0
0.00.041.300 I print_info: ssm_d_state      = 0
0.00.041.300 I print_info: ssm_dt_rank      = 0
0.00.041.300 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.301 I print_info: model type       = 1.4B
0.00.041.302 I print_info: model params     = 1.41 B
0.00.041.302 I print_info: general.name     = 1.4B
0.00.041.303 I print_info: vocab type       = BPE
0.00.041.303 I print_info: n_vocab          = 50304
0.00.041.303 I print_info: n_merges         = 50009
0.00.041.303 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.303 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: LF token         = 187 'Ċ'
0.00.041.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.304 I print_info: max token length = 1024
0.00.041.305 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.650.802 I load_tensors: offloading 24 repeating layers to GPU
0.00.650.816 I load_tensors: offloading output layer to GPU
0.00.650.817 I load_tensors: offloaded 25/25 layers to GPU
0.00.650.850 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.650.851 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.652.582 I llama_init_from_model: n_seq_max     = 1
0.00.652.584 I llama_init_from_model: n_ctx         = 2048
0.00.652.585 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.652.586 I llama_init_from_model: n_batch       = 2048
0.00.652.586 I llama_init_from_model: n_ubatch      = 512
0.00.652.587 I llama_init_from_model: flash_attn    = 0
0.00.652.588 I llama_init_from_model: freq_base     = 10000.0
0.00.652.589 I llama_init_from_model: freq_scale    = 1
0.00.652.591 I ggml_metal_init: allocating
0.00.652.670 I ggml_metal_init: found device: Apple M4
0.00.652.684 I ggml_metal_init: picking default device: Apple M4
0.00.654.667 I ggml_metal_init: using embedded metal library
0.00.661.352 I ggml_metal_init: GPU name:   Apple M4
0.00.661.357 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.661.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.661.358 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.661.359 I ggml_metal_init: simdgroup reduction   = true
0.00.661.359 I ggml_metal_init: simdgroup matrix mul. = true
0.00.661.359 I ggml_metal_init: has residency sets    = true
0.00.661.359 I ggml_metal_init: has bfloat            = true
0.00.661.360 I ggml_metal_init: use bfloat            = true
0.00.661.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.661.362 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.678.722 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.734.130 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.734.138 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.734.174 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.738.572 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.738.573 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.738.573 I llama_init_from_model: graph nodes  = 967
0.00.738.574 I llama_init_from_model: graph splits = 2
0.00.738.579 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.738.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.738.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.348 I main: llama threadpool init, n_threads = 4
0.00.799.392 I 
0.00.799.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.408 I 
0.00.799.557 I sampler seed: 1234
0.00.799.561 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.572 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.572 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.573 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.594.559 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.594.561 I llama_perf_context_print:        load time =     787.38 ms
0.01.594.561 I llama_perf_context_print: prompt eval time =      48.19 ms /     7 tokens (    6.88 ms per token,   145.25 tokens per second)
0.01.594.562 I llama_perf_context_print:        eval time =     743.90 ms /    63 runs   (   11.81 ms per token,    84.69 tokens per second)
0.01.594.563 I llama_perf_context_print:       total time =     795.94 ms /    70 tokens
0.01.594.832 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.108s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.774 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.776 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.777 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.778 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.778 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.779 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.779 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.780 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.783 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.785 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.786 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.596 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.363 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.363 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.363 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.364 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.364 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.364 I llama_model_loader: - type  f32:  194 tensors
0.00.025.365 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.365 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.366 I print_info: file format = GGUF V3 (latest)
0.00.025.366 I print_info: file type   = Q5_1
0.00.025.367 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.196 I load: special tokens cache size = 25
0.00.039.184 I load: token to piece cache size = 0.2984 MB
0.00.039.187 I print_info: arch             = gptneox
0.00.039.187 I print_info: vocab_only       = 0
0.00.039.187 I print_info: n_ctx_train      = 2048
0.00.039.187 I print_info: n_embd           = 2048
0.00.039.188 I print_info: n_layer          = 24
0.00.039.190 I print_info: n_head           = 16
0.00.039.191 I print_info: n_head_kv        = 16
0.00.039.193 I print_info: n_rot            = 32
0.00.039.193 I print_info: n_swa            = 0
0.00.039.193 I print_info: n_embd_head_k    = 128
0.00.039.194 I print_info: n_embd_head_v    = 128
0.00.039.194 I print_info: n_gqa            = 1
0.00.039.195 I print_info: n_embd_k_gqa     = 2048
0.00.039.196 I print_info: n_embd_v_gqa     = 2048
0.00.039.196 I print_info: f_norm_eps       = 1.0e-05
0.00.039.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.197 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.197 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.197 I print_info: f_logit_scale    = 0.0e+00
0.00.039.198 I print_info: n_ff             = 8192
0.00.039.198 I print_info: n_expert         = 0
0.00.039.198 I print_info: n_expert_used    = 0
0.00.039.198 I print_info: causal attn      = 1
0.00.039.199 I print_info: pooling type     = 0
0.00.039.200 I print_info: rope type        = 2
0.00.039.201 I print_info: rope scaling     = linear
0.00.039.202 I print_info: freq_base_train  = 10000.0
0.00.039.202 I print_info: freq_scale_train = 1
0.00.039.202 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.202 I print_info: rope_finetuned   = unknown
0.00.039.203 I print_info: ssm_d_conv       = 0
0.00.039.203 I print_info: ssm_d_inner      = 0
0.00.039.203 I print_info: ssm_d_state      = 0
0.00.039.203 I print_info: ssm_dt_rank      = 0
0.00.039.204 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.208 I print_info: model type       = 1.4B
0.00.039.208 I print_info: model params     = 1.41 B
0.00.039.209 I print_info: general.name     = 1.4B
0.00.039.210 I print_info: vocab type       = BPE
0.00.039.210 I print_info: n_vocab          = 50304
0.00.039.210 I print_info: n_merges         = 50009
0.00.039.210 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.210 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.211 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.211 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.212 I print_info: LF token         = 187 'Ċ'
0.00.039.212 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.212 I print_info: max token length = 1024
0.00.039.213 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.318 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.333 I load_tensors: offloading output layer to GPU
0.00.655.334 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.368 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.655.369 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.656.998 I llama_init_from_model: n_seq_max     = 1
0.00.657.002 I llama_init_from_model: n_ctx         = 2048
0.00.657.002 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.657.003 I llama_init_from_model: n_batch       = 2048
0.00.657.003 I llama_init_from_model: n_ubatch      = 512
0.00.657.003 I llama_init_from_model: flash_attn    = 0
0.00.657.006 I llama_init_from_model: freq_base     = 10000.0
0.00.657.007 I llama_init_from_model: freq_scale    = 1
0.00.657.008 I ggml_metal_init: allocating
0.00.657.085 I ggml_metal_init: found device: Apple M4
0.00.657.098 I ggml_metal_init: picking default device: Apple M4
0.00.658.884 I ggml_metal_init: using embedded metal library
0.00.665.263 I ggml_metal_init: GPU name:   Apple M4
0.00.665.266 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.268 I ggml_metal_init: simdgroup reduction   = true
0.00.665.269 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.269 I ggml_metal_init: has residency sets    = true
0.00.665.269 I ggml_metal_init: has bfloat            = true
0.00.665.269 I ggml_metal_init: use bfloat            = true
0.00.665.270 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.272 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.181 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.738.602 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.738.609 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.738.651 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.872 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.873 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.874 I llama_init_from_model: graph nodes  = 967
0.00.742.874 I llama_init_from_model: graph splits = 2
0.00.742.881 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.013 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.014 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.548 I main: llama threadpool init, n_threads = 4
0.00.803.589 I 
0.00.803.603 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.603 I 
0.00.803.753 I sampler seed: 1234
0.00.803.757 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.770 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.770 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.652.919 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.652.920 I llama_perf_context_print:        load time =     794.07 ms
0.01.652.921 I llama_perf_context_print: prompt eval time =      50.47 ms /     7 tokens (    7.21 ms per token,   138.71 tokens per second)
0.01.652.922 I llama_perf_context_print:        eval time =     795.82 ms /    63 runs   (   12.63 ms per token,    79.16 tokens per second)
0.01.652.922 I llama_perf_context_print:       total time =     850.09 ms /    70 tokens
0.01.653.171 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.109s
sys	0m0.209s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.942 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.460 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.460 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.461 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.466 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.467 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.467 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.468 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.265 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.309 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.095 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.098 I llama_model_loader: - type  f32:  194 tensors
0.00.025.098 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.098 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.099 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.099 I print_info: file format = GGUF V3 (latest)
0.00.025.100 I print_info: file type   = Q2_K - Medium
0.00.025.101 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.918 I load: special tokens cache size = 25
0.00.038.621 I load: token to piece cache size = 0.2984 MB
0.00.038.624 I print_info: arch             = gptneox
0.00.038.624 I print_info: vocab_only       = 0
0.00.038.624 I print_info: n_ctx_train      = 2048
0.00.038.625 I print_info: n_embd           = 2048
0.00.038.625 I print_info: n_layer          = 24
0.00.038.628 I print_info: n_head           = 16
0.00.038.628 I print_info: n_head_kv        = 16
0.00.038.630 I print_info: n_rot            = 32
0.00.038.631 I print_info: n_swa            = 0
0.00.038.631 I print_info: n_embd_head_k    = 128
0.00.038.631 I print_info: n_embd_head_v    = 128
0.00.038.632 I print_info: n_gqa            = 1
0.00.038.632 I print_info: n_embd_k_gqa     = 2048
0.00.038.633 I print_info: n_embd_v_gqa     = 2048
0.00.038.634 I print_info: f_norm_eps       = 1.0e-05
0.00.038.634 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.634 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.634 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.634 I print_info: f_logit_scale    = 0.0e+00
0.00.038.635 I print_info: n_ff             = 8192
0.00.038.635 I print_info: n_expert         = 0
0.00.038.635 I print_info: n_expert_used    = 0
0.00.038.635 I print_info: causal attn      = 1
0.00.038.636 I print_info: pooling type     = 0
0.00.038.636 I print_info: rope type        = 2
0.00.038.640 I print_info: rope scaling     = linear
0.00.038.640 I print_info: freq_base_train  = 10000.0
0.00.038.640 I print_info: freq_scale_train = 1
0.00.038.641 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.641 I print_info: rope_finetuned   = unknown
0.00.038.641 I print_info: ssm_d_conv       = 0
0.00.038.641 I print_info: ssm_d_inner      = 0
0.00.038.641 I print_info: ssm_d_state      = 0
0.00.038.642 I print_info: ssm_dt_rank      = 0
0.00.038.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.642 I print_info: model type       = 1.4B
0.00.038.642 I print_info: model params     = 1.41 B
0.00.038.643 I print_info: general.name     = 1.4B
0.00.038.643 I print_info: vocab type       = BPE
0.00.038.643 I print_info: n_vocab          = 50304
0.00.038.643 I print_info: n_merges         = 50009
0.00.038.643 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.644 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.644 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.644 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.644 I print_info: LF token         = 187 'Ċ'
0.00.038.645 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.645 I print_info: max token length = 1024
0.00.038.646 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.366.056 I load_tensors: offloading 24 repeating layers to GPU
0.00.366.070 I load_tensors: offloading output layer to GPU
0.00.366.071 I load_tensors: offloaded 25/25 layers to GPU
0.00.366.105 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.366.106 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.784 I llama_init_from_model: n_seq_max     = 1
0.00.367.789 I llama_init_from_model: n_ctx         = 2048
0.00.367.790 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.367.790 I llama_init_from_model: n_batch       = 2048
0.00.367.791 I llama_init_from_model: n_ubatch      = 512
0.00.367.791 I llama_init_from_model: flash_attn    = 0
0.00.367.793 I llama_init_from_model: freq_base     = 10000.0
0.00.367.794 I llama_init_from_model: freq_scale    = 1
0.00.367.796 I ggml_metal_init: allocating
0.00.367.893 I ggml_metal_init: found device: Apple M4
0.00.367.908 I ggml_metal_init: picking default device: Apple M4
0.00.369.774 I ggml_metal_init: using embedded metal library
0.00.375.187 I ggml_metal_init: GPU name:   Apple M4
0.00.375.201 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.375.202 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.375.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.375.203 I ggml_metal_init: simdgroup reduction   = true
0.00.375.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.375.204 I ggml_metal_init: has residency sets    = true
0.00.375.204 I ggml_metal_init: has bfloat            = true
0.00.375.205 I ggml_metal_init: use bfloat            = true
0.00.375.206 I ggml_metal_init: hasUnifiedMemory      = true
0.00.375.230 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.395.706 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.450.708 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.450.716 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.450.751 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.454.979 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.454.981 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.454.981 I llama_init_from_model: graph nodes  = 967
0.00.454.982 I llama_init_from_model: graph splits = 2
0.00.454.988 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.455.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.455.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.399 I main: llama threadpool init, n_threads = 4
0.00.516.443 I 
0.00.516.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.458 I 
0.00.516.638 I sampler seed: 1234
0.00.516.643 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.516.677 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.516.677 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.516.677 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.198.830 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51976.57 tokens per second)
0.01.198.831 I llama_perf_context_print:        load time =     505.76 ms
0.01.198.831 I llama_perf_context_print: prompt eval time =      43.55 ms /     7 tokens (    6.22 ms per token,   160.74 tokens per second)
0.01.198.832 I llama_perf_context_print:        eval time =     635.69 ms /    63 runs   (   10.09 ms per token,    99.10 tokens per second)
0.01.198.832 I llama_perf_context_print:       total time =     683.12 ms /    70 tokens
0.01.199.032 I ggml_metal_free: deallocating

real	0m1.217s
user	0m0.110s
sys	0m0.167s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.653 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.057 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.062 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.064 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.065 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.065 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.065 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.066 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.067 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.067 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.067 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.068 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.068 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.069 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.072 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.073 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.073 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.876 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.599 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.600 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.602 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.602 I llama_model_loader: - type  f32:  194 tensors
0.00.025.603 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.603 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.603 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.603 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.604 I print_info: file format = GGUF V3 (latest)
0.00.025.605 I print_info: file type   = Q3_K - Medium
0.00.025.605 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.473 I load: special tokens cache size = 25
0.00.039.598 I load: token to piece cache size = 0.2984 MB
0.00.039.600 I print_info: arch             = gptneox
0.00.039.600 I print_info: vocab_only       = 0
0.00.039.601 I print_info: n_ctx_train      = 2048
0.00.039.601 I print_info: n_embd           = 2048
0.00.039.601 I print_info: n_layer          = 24
0.00.039.604 I print_info: n_head           = 16
0.00.039.605 I print_info: n_head_kv        = 16
0.00.039.605 I print_info: n_rot            = 32
0.00.039.605 I print_info: n_swa            = 0
0.00.039.607 I print_info: n_embd_head_k    = 128
0.00.039.607 I print_info: n_embd_head_v    = 128
0.00.039.608 I print_info: n_gqa            = 1
0.00.039.609 I print_info: n_embd_k_gqa     = 2048
0.00.039.610 I print_info: n_embd_v_gqa     = 2048
0.00.039.611 I print_info: f_norm_eps       = 1.0e-05
0.00.039.611 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.611 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.611 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.611 I print_info: f_logit_scale    = 0.0e+00
0.00.039.612 I print_info: n_ff             = 8192
0.00.039.612 I print_info: n_expert         = 0
0.00.039.612 I print_info: n_expert_used    = 0
0.00.039.614 I print_info: causal attn      = 1
0.00.039.615 I print_info: pooling type     = 0
0.00.039.615 I print_info: rope type        = 2
0.00.039.615 I print_info: rope scaling     = linear
0.00.039.615 I print_info: freq_base_train  = 10000.0
0.00.039.616 I print_info: freq_scale_train = 1
0.00.039.616 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.616 I print_info: rope_finetuned   = unknown
0.00.039.616 I print_info: ssm_d_conv       = 0
0.00.039.617 I print_info: ssm_d_inner      = 0
0.00.039.617 I print_info: ssm_d_state      = 0
0.00.039.617 I print_info: ssm_dt_rank      = 0
0.00.039.617 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.617 I print_info: model type       = 1.4B
0.00.039.622 I print_info: model params     = 1.41 B
0.00.039.622 I print_info: general.name     = 1.4B
0.00.039.622 I print_info: vocab type       = BPE
0.00.039.623 I print_info: n_vocab          = 50304
0.00.039.623 I print_info: n_merges         = 50009
0.00.039.623 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.623 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.623 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.624 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.624 I print_info: LF token         = 187 'Ċ'
0.00.039.624 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.624 I print_info: max token length = 1024
0.00.039.625 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.434.978 I load_tensors: offloading 24 repeating layers to GPU
0.00.434.994 I load_tensors: offloading output layer to GPU
0.00.434.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.435.028 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.435.030 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.436.692 I llama_init_from_model: n_seq_max     = 1
0.00.436.694 I llama_init_from_model: n_ctx         = 2048
0.00.436.695 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.436.696 I llama_init_from_model: n_batch       = 2048
0.00.436.696 I llama_init_from_model: n_ubatch      = 512
0.00.436.697 I llama_init_from_model: flash_attn    = 0
0.00.436.698 I llama_init_from_model: freq_base     = 10000.0
0.00.436.699 I llama_init_from_model: freq_scale    = 1
0.00.436.701 I ggml_metal_init: allocating
0.00.436.777 I ggml_metal_init: found device: Apple M4
0.00.436.791 I ggml_metal_init: picking default device: Apple M4
0.00.438.737 I ggml_metal_init: using embedded metal library
0.00.444.838 I ggml_metal_init: GPU name:   Apple M4
0.00.444.842 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.444.843 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.444.844 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.444.845 I ggml_metal_init: simdgroup reduction   = true
0.00.444.845 I ggml_metal_init: simdgroup matrix mul. = true
0.00.444.846 I ggml_metal_init: has residency sets    = true
0.00.444.846 I ggml_metal_init: has bfloat            = true
0.00.444.846 I ggml_metal_init: use bfloat            = true
0.00.444.847 I ggml_metal_init: hasUnifiedMemory      = true
0.00.444.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.464.082 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.523.614 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.523.620 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.523.664 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.527.868 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.527.870 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.527.870 I llama_init_from_model: graph nodes  = 967
0.00.527.870 I llama_init_from_model: graph splits = 2
0.00.527.876 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.528.010 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.528.011 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.684 I main: llama threadpool init, n_threads = 4
0.00.584.760 I 
0.00.584.779 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.779 I 
0.00.584.945 I sampler seed: 1234
0.00.584.950 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.584.994 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.584.995 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.584.998 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.329.259 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52866.72 tokens per second)
0.01.329.259 I llama_perf_context_print:        load time =     574.33 ms
0.01.329.260 I llama_perf_context_print: prompt eval time =      50.06 ms /     7 tokens (    7.15 ms per token,   139.84 tokens per second)
0.01.329.261 I llama_perf_context_print:        eval time =     691.30 ms /    63 runs   (   10.97 ms per token,    91.13 tokens per second)
0.01.329.261 I llama_perf_context_print:       total time =     745.27 ms /    70 tokens
0.01.329.493 I ggml_metal_free: deallocating

real	0m1.345s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.767 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.209 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.210 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.210 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.210 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.211 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.211 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.212 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.212 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.041 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.856 I llama_model_loader: - type  f32:  194 tensors
0.00.024.857 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.857 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.857 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.858 I print_info: file format = GGUF V3 (latest)
0.00.024.858 I print_info: file type   = Q4_K - Medium
0.00.024.859 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.595 I load: special tokens cache size = 25
0.00.038.681 I load: token to piece cache size = 0.2984 MB
0.00.038.685 I print_info: arch             = gptneox
0.00.038.685 I print_info: vocab_only       = 0
0.00.038.685 I print_info: n_ctx_train      = 2048
0.00.038.685 I print_info: n_embd           = 2048
0.00.038.685 I print_info: n_layer          = 24
0.00.038.688 I print_info: n_head           = 16
0.00.038.688 I print_info: n_head_kv        = 16
0.00.038.689 I print_info: n_rot            = 32
0.00.038.689 I print_info: n_swa            = 0
0.00.038.689 I print_info: n_embd_head_k    = 128
0.00.038.689 I print_info: n_embd_head_v    = 128
0.00.038.690 I print_info: n_gqa            = 1
0.00.038.690 I print_info: n_embd_k_gqa     = 2048
0.00.038.692 I print_info: n_embd_v_gqa     = 2048
0.00.038.693 I print_info: f_norm_eps       = 1.0e-05
0.00.038.693 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.693 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.693 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.695 I print_info: f_logit_scale    = 0.0e+00
0.00.038.696 I print_info: n_ff             = 8192
0.00.038.696 I print_info: n_expert         = 0
0.00.038.696 I print_info: n_expert_used    = 0
0.00.038.696 I print_info: causal attn      = 1
0.00.038.698 I print_info: pooling type     = 0
0.00.038.700 I print_info: rope type        = 2
0.00.038.700 I print_info: rope scaling     = linear
0.00.038.700 I print_info: freq_base_train  = 10000.0
0.00.038.701 I print_info: freq_scale_train = 1
0.00.038.701 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.701 I print_info: rope_finetuned   = unknown
0.00.038.701 I print_info: ssm_d_conv       = 0
0.00.038.701 I print_info: ssm_d_inner      = 0
0.00.038.701 I print_info: ssm_d_state      = 0
0.00.038.701 I print_info: ssm_dt_rank      = 0
0.00.038.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.702 I print_info: model type       = 1.4B
0.00.038.702 I print_info: model params     = 1.41 B
0.00.038.702 I print_info: general.name     = 1.4B
0.00.038.703 I print_info: vocab type       = BPE
0.00.038.703 I print_info: n_vocab          = 50304
0.00.038.707 I print_info: n_merges         = 50009
0.00.038.707 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.707 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.708 I print_info: LF token         = 187 'Ċ'
0.00.038.708 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.709 I print_info: max token length = 1024
0.00.038.709 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.539.217 I load_tensors: offloading 24 repeating layers to GPU
0.00.539.232 I load_tensors: offloading output layer to GPU
0.00.539.233 I load_tensors: offloaded 25/25 layers to GPU
0.00.539.268 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.539.269 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.541.052 I llama_init_from_model: n_seq_max     = 1
0.00.541.055 I llama_init_from_model: n_ctx         = 2048
0.00.541.055 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.541.056 I llama_init_from_model: n_batch       = 2048
0.00.541.056 I llama_init_from_model: n_ubatch      = 512
0.00.541.056 I llama_init_from_model: flash_attn    = 0
0.00.541.059 I llama_init_from_model: freq_base     = 10000.0
0.00.541.060 I llama_init_from_model: freq_scale    = 1
0.00.541.062 I ggml_metal_init: allocating
0.00.541.140 I ggml_metal_init: found device: Apple M4
0.00.541.160 I ggml_metal_init: picking default device: Apple M4
0.00.542.958 I ggml_metal_init: using embedded metal library
0.00.549.322 I ggml_metal_init: GPU name:   Apple M4
0.00.549.326 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.326 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.327 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.328 I ggml_metal_init: simdgroup reduction   = true
0.00.549.328 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.328 I ggml_metal_init: has residency sets    = true
0.00.549.329 I ggml_metal_init: has bfloat            = true
0.00.549.329 I ggml_metal_init: use bfloat            = true
0.00.549.330 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.566.927 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.342 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.621.348 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.621.382 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.707 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.625.709 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.625.709 I llama_init_from_model: graph nodes  = 967
0.00.625.710 I llama_init_from_model: graph splits = 2
0.00.625.714 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.625.848 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.625.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.952 I main: llama threadpool init, n_threads = 4
0.00.685.998 I 
0.00.686.017 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.019 I 
0.00.686.184 I sampler seed: 1234
0.00.686.189 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.199 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.205 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.445.667 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.01.445.668 I llama_perf_context_print:        load time =     676.48 ms
0.01.445.669 I llama_perf_context_print: prompt eval time =      55.67 ms /     7 tokens (    7.95 ms per token,   125.74 tokens per second)
0.01.445.670 I llama_perf_context_print:        eval time =     700.82 ms /    63 runs   (   11.12 ms per token,    89.89 tokens per second)
0.01.445.671 I llama_perf_context_print:       total time =     760.42 ms /    70 tokens
0.01.445.898 I ggml_metal_free: deallocating

real	0m1.462s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.360 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.365 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.366 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.367 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.367 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.368 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.368 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.369 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.369 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.370 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.371 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.374 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.374 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.904 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.905 I llama_model_loader: - type  f32:  194 tensors
0.00.026.905 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.906 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.906 I print_info: file format = GGUF V3 (latest)
0.00.026.907 I print_info: file type   = Q5_K - Medium
0.00.026.909 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.106 I load: special tokens cache size = 25
0.00.041.208 I load: token to piece cache size = 0.2984 MB
0.00.041.211 I print_info: arch             = gptneox
0.00.041.211 I print_info: vocab_only       = 0
0.00.041.211 I print_info: n_ctx_train      = 2048
0.00.041.211 I print_info: n_embd           = 2048
0.00.041.211 I print_info: n_layer          = 24
0.00.041.214 I print_info: n_head           = 16
0.00.041.215 I print_info: n_head_kv        = 16
0.00.041.215 I print_info: n_rot            = 32
0.00.041.215 I print_info: n_swa            = 0
0.00.041.216 I print_info: n_embd_head_k    = 128
0.00.041.216 I print_info: n_embd_head_v    = 128
0.00.041.217 I print_info: n_gqa            = 1
0.00.041.217 I print_info: n_embd_k_gqa     = 2048
0.00.041.218 I print_info: n_embd_v_gqa     = 2048
0.00.041.219 I print_info: f_norm_eps       = 1.0e-05
0.00.041.219 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.219 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.219 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.219 I print_info: f_logit_scale    = 0.0e+00
0.00.041.220 I print_info: n_ff             = 8192
0.00.041.220 I print_info: n_expert         = 0
0.00.041.220 I print_info: n_expert_used    = 0
0.00.041.220 I print_info: causal attn      = 1
0.00.041.221 I print_info: pooling type     = 0
0.00.041.222 I print_info: rope type        = 2
0.00.041.224 I print_info: rope scaling     = linear
0.00.041.225 I print_info: freq_base_train  = 10000.0
0.00.041.225 I print_info: freq_scale_train = 1
0.00.041.225 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.225 I print_info: rope_finetuned   = unknown
0.00.041.226 I print_info: ssm_d_conv       = 0
0.00.041.226 I print_info: ssm_d_inner      = 0
0.00.041.226 I print_info: ssm_d_state      = 0
0.00.041.226 I print_info: ssm_dt_rank      = 0
0.00.041.226 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.226 I print_info: model type       = 1.4B
0.00.041.227 I print_info: model params     = 1.41 B
0.00.041.227 I print_info: general.name     = 1.4B
0.00.041.227 I print_info: vocab type       = BPE
0.00.041.227 I print_info: n_vocab          = 50304
0.00.041.228 I print_info: n_merges         = 50009
0.00.041.228 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.228 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.228 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.235 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.235 I print_info: LF token         = 187 'Ċ'
0.00.041.235 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.236 I print_info: max token length = 1024
0.00.041.236 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.615.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.177 I load_tensors: offloading output layer to GPU
0.00.615.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.210 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.615.212 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.617.447 I llama_init_from_model: n_seq_max     = 1
0.00.617.459 I llama_init_from_model: n_ctx         = 2048
0.00.617.459 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.617.460 I llama_init_from_model: n_batch       = 2048
0.00.617.460 I llama_init_from_model: n_ubatch      = 512
0.00.617.460 I llama_init_from_model: flash_attn    = 0
0.00.617.463 I llama_init_from_model: freq_base     = 10000.0
0.00.617.463 I llama_init_from_model: freq_scale    = 1
0.00.617.467 I ggml_metal_init: allocating
0.00.617.546 I ggml_metal_init: found device: Apple M4
0.00.617.578 I ggml_metal_init: picking default device: Apple M4
0.00.619.968 I ggml_metal_init: using embedded metal library
0.00.626.754 I ggml_metal_init: GPU name:   Apple M4
0.00.626.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.626.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.626.760 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.626.761 I ggml_metal_init: simdgroup reduction   = true
0.00.626.761 I ggml_metal_init: simdgroup matrix mul. = true
0.00.626.761 I ggml_metal_init: has residency sets    = true
0.00.626.762 I ggml_metal_init: has bfloat            = true
0.00.626.762 I ggml_metal_init: use bfloat            = true
0.00.626.763 I ggml_metal_init: hasUnifiedMemory      = true
0.00.626.764 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.115 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.699.120 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.153 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.403 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.405 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.405 I llama_init_from_model: graph nodes  = 967
0.00.703.405 I llama_init_from_model: graph splits = 2
0.00.703.419 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.550 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.766.045 I main: llama threadpool init, n_threads = 4
0.00.766.085 I 
0.00.766.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.766.098 I 
0.00.766.259 I sampler seed: 1234
0.00.766.264 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.766.275 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.766.275 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.766.275 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.603.751 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.01.603.751 I llama_perf_context_print:        load time =     754.59 ms
0.01.603.754 I llama_perf_context_print: prompt eval time =      51.10 ms /     7 tokens (    7.30 ms per token,   136.98 tokens per second)
0.01.603.754 I llama_perf_context_print:        eval time =     783.96 ms /    63 runs   (   12.44 ms per token,    80.36 tokens per second)
0.01.603.755 I llama_perf_context_print:       total time =     838.42 ms /    70 tokens
0.01.604.012 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.110s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.008.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.630 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.634 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.636 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.637 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.638 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.638 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.639 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.639 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.639 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.640 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.640 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.640 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.641 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.643 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.643 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.646 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.651 I llama_model_loader: - type  f32:  194 tensors
0.00.025.651 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.652 I print_info: file format = GGUF V3 (latest)
0.00.025.653 I print_info: file type   = Q6_K
0.00.025.653 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.028 I load: special tokens cache size = 25
0.00.040.257 I load: token to piece cache size = 0.2984 MB
0.00.040.263 I print_info: arch             = gptneox
0.00.040.263 I print_info: vocab_only       = 0
0.00.040.263 I print_info: n_ctx_train      = 2048
0.00.040.263 I print_info: n_embd           = 2048
0.00.040.263 I print_info: n_layer          = 24
0.00.040.268 I print_info: n_head           = 16
0.00.040.268 I print_info: n_head_kv        = 16
0.00.040.268 I print_info: n_rot            = 32
0.00.040.269 I print_info: n_swa            = 0
0.00.040.269 I print_info: n_embd_head_k    = 128
0.00.040.269 I print_info: n_embd_head_v    = 128
0.00.040.300 I print_info: n_gqa            = 1
0.00.040.306 I print_info: n_embd_k_gqa     = 2048
0.00.040.306 I print_info: n_embd_v_gqa     = 2048
0.00.040.307 I print_info: f_norm_eps       = 1.0e-05
0.00.040.308 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.308 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.311 I print_info: f_logit_scale    = 0.0e+00
0.00.040.313 I print_info: n_ff             = 8192
0.00.040.313 I print_info: n_expert         = 0
0.00.040.313 I print_info: n_expert_used    = 0
0.00.040.313 I print_info: causal attn      = 1
0.00.040.313 I print_info: pooling type     = 0
0.00.040.313 I print_info: rope type        = 2
0.00.040.314 I print_info: rope scaling     = linear
0.00.040.314 I print_info: freq_base_train  = 10000.0
0.00.040.314 I print_info: freq_scale_train = 1
0.00.040.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.315 I print_info: rope_finetuned   = unknown
0.00.040.315 I print_info: ssm_d_conv       = 0
0.00.040.315 I print_info: ssm_d_inner      = 0
0.00.040.315 I print_info: ssm_d_state      = 0
0.00.040.315 I print_info: ssm_dt_rank      = 0
0.00.040.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.316 I print_info: model type       = 1.4B
0.00.040.316 I print_info: model params     = 1.41 B
0.00.040.316 I print_info: general.name     = 1.4B
0.00.040.323 I print_info: vocab type       = BPE
0.00.040.323 I print_info: n_vocab          = 50304
0.00.040.323 I print_info: n_merges         = 50009
0.00.040.324 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.324 I print_info: LF token         = 187 'Ċ'
0.00.040.326 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.327 I print_info: max token length = 1024
0.00.040.327 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.772.775 I load_tensors: offloading 24 repeating layers to GPU
0.00.772.783 I load_tensors: offloading output layer to GPU
0.00.772.784 I load_tensors: offloaded 25/25 layers to GPU
0.00.772.813 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.772.815 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.773.757 I llama_init_from_model: n_seq_max     = 1
0.00.773.759 I llama_init_from_model: n_ctx         = 2048
0.00.773.759 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.773.760 I llama_init_from_model: n_batch       = 2048
0.00.773.761 I llama_init_from_model: n_ubatch      = 512
0.00.773.761 I llama_init_from_model: flash_attn    = 0
0.00.773.763 I llama_init_from_model: freq_base     = 10000.0
0.00.773.763 I llama_init_from_model: freq_scale    = 1
0.00.773.764 I ggml_metal_init: allocating
0.00.773.810 I ggml_metal_init: found device: Apple M4
0.00.773.824 I ggml_metal_init: picking default device: Apple M4
0.00.775.428 I ggml_metal_init: using embedded metal library
0.00.781.391 I ggml_metal_init: GPU name:   Apple M4
0.00.781.395 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.781.397 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.781.397 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.781.398 I ggml_metal_init: simdgroup reduction   = true
0.00.781.398 I ggml_metal_init: simdgroup matrix mul. = true
0.00.781.398 I ggml_metal_init: has residency sets    = true
0.00.781.398 I ggml_metal_init: has bfloat            = true
0.00.781.399 I ggml_metal_init: use bfloat            = true
0.00.781.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.781.405 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.797.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.849.856 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.849.863 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.849.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.854.666 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.854.668 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.854.668 I llama_init_from_model: graph nodes  = 967
0.00.854.669 I llama_init_from_model: graph splits = 2
0.00.854.675 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.854.807 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.854.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.924.276 I main: llama threadpool init, n_threads = 4
0.00.924.322 I 
0.00.924.338 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.924.338 I 
0.00.924.510 I sampler seed: 1234
0.00.924.514 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.924.554 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.924.555 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.924.555 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.795.779 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.795.779 I llama_perf_context_print:        load time =     914.69 ms
0.01.795.781 I llama_perf_context_print: prompt eval time =      54.16 ms /     7 tokens (    7.74 ms per token,   129.26 tokens per second)
0.01.795.782 I llama_perf_context_print:        eval time =     814.14 ms /    63 runs   (   12.92 ms per token,    77.38 tokens per second)
0.01.795.782 I llama_perf_context_print:       total time =     872.19 ms /    70 tokens
0.01.795.987 I ggml_metal_free: deallocating

real	0m1.815s
user	0m0.109s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.633 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.026.790 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.480 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.492 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.495 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.496 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.496 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.497 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.501 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.502 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.503 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.509 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.510 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.511 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.163 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.164 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.164 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.164 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.165 I llama_model_loader: - type  f32:  194 tensors
0.00.060.166 I llama_model_loader: - type  f16:   98 tensors
0.00.060.166 I print_info: file format = GGUF V3 (latest)
0.00.060.167 I print_info: file type   = all F32 (guessed)
0.00.060.168 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.073.931 I load: special tokens cache size = 25
0.00.082.130 I load: token to piece cache size = 0.2984 MB
0.00.082.136 I print_info: arch             = gptneox
0.00.082.136 I print_info: vocab_only       = 0
0.00.082.137 I print_info: n_ctx_train      = 2048
0.00.082.137 I print_info: n_embd           = 2048
0.00.082.137 I print_info: n_layer          = 24
0.00.082.140 I print_info: n_head           = 16
0.00.082.141 I print_info: n_head_kv        = 16
0.00.082.141 I print_info: n_rot            = 32
0.00.082.141 I print_info: n_swa            = 0
0.00.082.141 I print_info: n_embd_head_k    = 128
0.00.082.142 I print_info: n_embd_head_v    = 128
0.00.082.142 I print_info: n_gqa            = 1
0.00.082.143 I print_info: n_embd_k_gqa     = 2048
0.00.082.144 I print_info: n_embd_v_gqa     = 2048
0.00.082.145 I print_info: f_norm_eps       = 1.0e-05
0.00.082.145 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.082.145 I print_info: f_clamp_kqv      = 0.0e+00
0.00.082.145 I print_info: f_max_alibi_bias = 0.0e+00
0.00.082.146 I print_info: f_logit_scale    = 0.0e+00
0.00.082.146 I print_info: n_ff             = 8192
0.00.082.146 I print_info: n_expert         = 0
0.00.082.147 I print_info: n_expert_used    = 0
0.00.082.147 I print_info: causal attn      = 1
0.00.082.147 I print_info: pooling type     = 0
0.00.082.147 I print_info: rope type        = 2
0.00.082.147 I print_info: rope scaling     = linear
0.00.082.148 I print_info: freq_base_train  = 10000.0
0.00.082.148 I print_info: freq_scale_train = 1
0.00.082.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.082.148 I print_info: rope_finetuned   = unknown
0.00.082.149 I print_info: ssm_d_conv       = 0
0.00.082.149 I print_info: ssm_d_inner      = 0
0.00.082.151 I print_info: ssm_d_state      = 0
0.00.082.151 I print_info: ssm_dt_rank      = 0
0.00.082.152 I print_info: ssm_dt_b_c_rms   = 0
0.00.082.152 I print_info: model type       = 1.4B
0.00.082.152 I print_info: model params     = 1.41 B
0.00.082.152 I print_info: general.name     = 1.4B
0.00.082.153 I print_info: vocab type       = BPE
0.00.082.153 I print_info: n_vocab          = 50304
0.00.082.153 I print_info: n_merges         = 50009
0.00.082.153 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.082.154 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.082.154 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.082.154 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.082.154 I print_info: LF token         = 187 'Ċ'
0.00.082.156 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.082.156 I print_info: max token length = 1024
0.00.082.158 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.440.726 I load_tensors: offloading 24 repeating layers to GPU
0.01.440.730 I load_tensors: offloading output layer to GPU
0.01.440.731 I load_tensors: offloaded 25/25 layers to GPU
0.01.440.755 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.440.756 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.441.510 I llama_init_from_model: n_seq_max     = 1
0.01.441.511 I llama_init_from_model: n_ctx         = 128
0.01.441.512 I llama_init_from_model: n_ctx_per_seq = 128
0.01.441.512 I llama_init_from_model: n_batch       = 128
0.01.441.512 I llama_init_from_model: n_ubatch      = 128
0.01.441.512 I llama_init_from_model: flash_attn    = 0
0.01.441.513 I llama_init_from_model: freq_base     = 10000.0
0.01.441.513 I llama_init_from_model: freq_scale    = 1
0.01.441.514 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.441.515 I ggml_metal_init: allocating
0.01.441.554 I ggml_metal_init: found device: Apple M4
0.01.441.562 I ggml_metal_init: picking default device: Apple M4
0.01.442.609 I ggml_metal_init: using embedded metal library
0.01.446.430 I ggml_metal_init: GPU name:   Apple M4
0.01.446.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.446.433 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.446.433 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.446.433 I ggml_metal_init: simdgroup reduction   = true
0.01.446.434 I ggml_metal_init: simdgroup matrix mul. = true
0.01.446.434 I ggml_metal_init: has residency sets    = true
0.01.446.434 I ggml_metal_init: has bfloat            = true
0.01.446.434 I ggml_metal_init: use bfloat            = true
0.01.446.434 I ggml_metal_init: hasUnifiedMemory      = true
0.01.446.435 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.456.932 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.458.636 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.458.638 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.458.664 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.460.316 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.460.318 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.460.318 I llama_init_from_model: graph nodes  = 967
0.01.460.318 I llama_init_from_model: graph splits = 2
0.01.460.320 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.460.320 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.495.050 I 
0.01.495.077 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.495.082 I perplexity: tokenizing the input ..
0.01.500.154 I perplexity: tokenization took 5.07 ms
0.01.500.159 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.618.513 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.619.867 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.619.897 I llama_perf_context_print:        load time =    1468.23 ms
0.01.619.898 I llama_perf_context_print: prompt eval time =     118.09 ms /   128 tokens (    0.92 ms per token,  1083.96 tokens per second)
0.01.619.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.619.899 I llama_perf_context_print:       total time =     124.85 ms /   129 tokens
0.01.620.254 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.100s
sys	0m0.263s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.237 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.932 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.076 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.090 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.916 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.765 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.765 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.766 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.767 I llama_model_loader: - type  f32:  194 tensors
0.00.025.767 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.768 I print_info: file format = GGUF V3 (latest)
0.00.025.768 I print_info: file type   = Q8_0
0.00.025.770 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.852 I load: special tokens cache size = 25
0.00.039.903 I load: token to piece cache size = 0.2984 MB
0.00.039.907 I print_info: arch             = gptneox
0.00.039.908 I print_info: vocab_only       = 0
0.00.039.908 I print_info: n_ctx_train      = 2048
0.00.039.908 I print_info: n_embd           = 2048
0.00.039.908 I print_info: n_layer          = 24
0.00.039.913 I print_info: n_head           = 16
0.00.039.914 I print_info: n_head_kv        = 16
0.00.039.914 I print_info: n_rot            = 32
0.00.039.914 I print_info: n_swa            = 0
0.00.039.914 I print_info: n_embd_head_k    = 128
0.00.039.916 I print_info: n_embd_head_v    = 128
0.00.039.917 I print_info: n_gqa            = 1
0.00.039.918 I print_info: n_embd_k_gqa     = 2048
0.00.039.918 I print_info: n_embd_v_gqa     = 2048
0.00.039.919 I print_info: f_norm_eps       = 1.0e-05
0.00.039.919 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.919 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.919 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.920 I print_info: f_logit_scale    = 0.0e+00
0.00.039.921 I print_info: n_ff             = 8192
0.00.039.921 I print_info: n_expert         = 0
0.00.039.921 I print_info: n_expert_used    = 0
0.00.039.921 I print_info: causal attn      = 1
0.00.039.921 I print_info: pooling type     = 0
0.00.039.923 I print_info: rope type        = 2
0.00.039.923 I print_info: rope scaling     = linear
0.00.039.924 I print_info: freq_base_train  = 10000.0
0.00.039.924 I print_info: freq_scale_train = 1
0.00.039.924 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.924 I print_info: rope_finetuned   = unknown
0.00.039.924 I print_info: ssm_d_conv       = 0
0.00.039.925 I print_info: ssm_d_inner      = 0
0.00.039.925 I print_info: ssm_d_state      = 0
0.00.039.925 I print_info: ssm_dt_rank      = 0
0.00.039.925 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.925 I print_info: model type       = 1.4B
0.00.039.925 I print_info: model params     = 1.41 B
0.00.039.925 I print_info: general.name     = 1.4B
0.00.039.926 I print_info: vocab type       = BPE
0.00.039.926 I print_info: n_vocab          = 50304
0.00.039.926 I print_info: n_merges         = 50009
0.00.039.928 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.928 I print_info: LF token         = 187 'Ċ'
0.00.039.928 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.929 I print_info: max token length = 1024
0.00.039.929 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.916.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.916.976 I load_tensors: offloading output layer to GPU
0.00.916.977 I load_tensors: offloaded 25/25 layers to GPU
0.00.917.005 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.917.007 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.918.417 I llama_init_from_model: n_seq_max     = 1
0.00.918.419 I llama_init_from_model: n_ctx         = 128
0.00.918.419 I llama_init_from_model: n_ctx_per_seq = 128
0.00.918.419 I llama_init_from_model: n_batch       = 128
0.00.918.420 I llama_init_from_model: n_ubatch      = 128
0.00.918.420 I llama_init_from_model: flash_attn    = 0
0.00.918.421 I llama_init_from_model: freq_base     = 10000.0
0.00.918.421 I llama_init_from_model: freq_scale    = 1
0.00.918.422 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.918.423 I ggml_metal_init: allocating
0.00.918.501 I ggml_metal_init: found device: Apple M4
0.00.918.511 I ggml_metal_init: picking default device: Apple M4
0.00.919.941 I ggml_metal_init: using embedded metal library
0.00.925.454 I ggml_metal_init: GPU name:   Apple M4
0.00.925.457 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.925.458 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.925.459 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.925.460 I ggml_metal_init: simdgroup reduction   = true
0.00.925.460 I ggml_metal_init: simdgroup matrix mul. = true
0.00.925.460 I ggml_metal_init: has residency sets    = true
0.00.925.461 I ggml_metal_init: has bfloat            = true
0.00.925.461 I ggml_metal_init: use bfloat            = true
0.00.925.461 I ggml_metal_init: hasUnifiedMemory      = true
0.00.925.463 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.940.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.944.317 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.944.327 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.944.384 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.947.554 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.947.555 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.947.556 I llama_init_from_model: graph nodes  = 967
0.00.947.556 I llama_init_from_model: graph splits = 2
0.00.947.559 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.947.559 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.975.298 I 
0.00.975.355 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.975.363 I perplexity: tokenizing the input ..
0.00.982.555 I perplexity: tokenization took 7.19 ms
0.00.982.562 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.121.498 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.122.976 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.123.002 I llama_perf_context_print:        load time =     965.35 ms
0.01.123.004 I llama_perf_context_print: prompt eval time =     137.94 ms /   128 tokens (    1.08 ms per token,   927.97 tokens per second)
0.01.123.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.123.006 I llama_perf_context_print:       total time =     147.71 ms /   129 tokens
0.01.123.412 I ggml_metal_free: deallocating

real	0m1.140s
user	0m0.076s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.250 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.770 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.170 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.175 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.184 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.185 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.185 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.186 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.188 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.189 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.189 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.057 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.078 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.966 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.967 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.968 I llama_model_loader: - type  f32:  194 tensors
0.00.025.968 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.969 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.970 I print_info: file format = GGUF V3 (latest)
0.00.025.970 I print_info: file type   = Q4_0
0.00.025.972 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.127 I load: special tokens cache size = 25
0.00.040.237 I load: token to piece cache size = 0.2984 MB
0.00.040.241 I print_info: arch             = gptneox
0.00.040.241 I print_info: vocab_only       = 0
0.00.040.241 I print_info: n_ctx_train      = 2048
0.00.040.242 I print_info: n_embd           = 2048
0.00.040.242 I print_info: n_layer          = 24
0.00.040.246 I print_info: n_head           = 16
0.00.040.246 I print_info: n_head_kv        = 16
0.00.040.247 I print_info: n_rot            = 32
0.00.040.247 I print_info: n_swa            = 0
0.00.040.247 I print_info: n_embd_head_k    = 128
0.00.040.247 I print_info: n_embd_head_v    = 128
0.00.040.248 I print_info: n_gqa            = 1
0.00.040.249 I print_info: n_embd_k_gqa     = 2048
0.00.040.249 I print_info: n_embd_v_gqa     = 2048
0.00.040.250 I print_info: f_norm_eps       = 1.0e-05
0.00.040.250 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.252 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.252 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.252 I print_info: f_logit_scale    = 0.0e+00
0.00.040.253 I print_info: n_ff             = 8192
0.00.040.253 I print_info: n_expert         = 0
0.00.040.253 I print_info: n_expert_used    = 0
0.00.040.253 I print_info: causal attn      = 1
0.00.040.253 I print_info: pooling type     = 0
0.00.040.253 I print_info: rope type        = 2
0.00.040.254 I print_info: rope scaling     = linear
0.00.040.254 I print_info: freq_base_train  = 10000.0
0.00.040.254 I print_info: freq_scale_train = 1
0.00.040.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.257 I print_info: rope_finetuned   = unknown
0.00.040.257 I print_info: ssm_d_conv       = 0
0.00.040.257 I print_info: ssm_d_inner      = 0
0.00.040.257 I print_info: ssm_d_state      = 0
0.00.040.257 I print_info: ssm_dt_rank      = 0
0.00.040.257 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.258 I print_info: model type       = 1.4B
0.00.040.258 I print_info: model params     = 1.41 B
0.00.040.258 I print_info: general.name     = 1.4B
0.00.040.259 I print_info: vocab type       = BPE
0.00.040.259 I print_info: n_vocab          = 50304
0.00.040.260 I print_info: n_merges         = 50009
0.00.040.261 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.261 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.261 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.261 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.261 I print_info: LF token         = 187 'Ċ'
0.00.040.261 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.262 I print_info: max token length = 1024
0.00.040.262 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.584.610 I load_tensors: offloading 24 repeating layers to GPU
0.00.584.627 I load_tensors: offloading output layer to GPU
0.00.584.628 I load_tensors: offloaded 25/25 layers to GPU
0.00.584.663 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.584.664 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.312 I llama_init_from_model: n_seq_max     = 1
0.00.586.315 I llama_init_from_model: n_ctx         = 128
0.00.586.316 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.316 I llama_init_from_model: n_batch       = 128
0.00.586.317 I llama_init_from_model: n_ubatch      = 128
0.00.586.317 I llama_init_from_model: flash_attn    = 0
0.00.586.319 I llama_init_from_model: freq_base     = 10000.0
0.00.586.320 I llama_init_from_model: freq_scale    = 1
0.00.586.320 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.323 I ggml_metal_init: allocating
0.00.586.408 I ggml_metal_init: found device: Apple M4
0.00.586.421 I ggml_metal_init: picking default device: Apple M4
0.00.588.278 I ggml_metal_init: using embedded metal library
0.00.593.970 I ggml_metal_init: GPU name:   Apple M4
0.00.593.977 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.978 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.979 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.980 I ggml_metal_init: simdgroup reduction   = true
0.00.593.980 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.980 I ggml_metal_init: has residency sets    = true
0.00.593.980 I ggml_metal_init: has bfloat            = true
0.00.593.981 I ggml_metal_init: use bfloat            = true
0.00.593.982 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.613.757 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.617.354 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.617.359 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.617.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.620.588 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.620.590 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.620.590 I llama_init_from_model: graph nodes  = 967
0.00.620.591 I llama_init_from_model: graph splits = 2
0.00.620.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.620.594 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.908 I 
0.00.647.968 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.979 I perplexity: tokenizing the input ..
0.00.653.806 I perplexity: tokenization took 5.827 ms
0.00.653.809 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.777.344 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.778.671 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.778.697 I llama_perf_context_print:        load time =     638.12 ms
0.00.778.698 I llama_perf_context_print: prompt eval time =     123.31 ms /   128 tokens (    0.96 ms per token,  1038.03 tokens per second)
0.00.778.699 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.778.699 I llama_perf_context_print:       total time =     130.79 ms /   129 tokens
0.00.779.084 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.143 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.144 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.145 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.147 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.148 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.149 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.151 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.151 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.151 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.950 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.017 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.875 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.877 I llama_model_loader: - type  f32:  194 tensors
0.00.024.877 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.878 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.878 I print_info: file format = GGUF V3 (latest)
0.00.024.879 I print_info: file type   = Q4_1
0.00.024.880 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.965 I load: special tokens cache size = 25
0.00.038.814 I load: token to piece cache size = 0.2984 MB
0.00.038.818 I print_info: arch             = gptneox
0.00.038.818 I print_info: vocab_only       = 0
0.00.038.819 I print_info: n_ctx_train      = 2048
0.00.038.819 I print_info: n_embd           = 2048
0.00.038.819 I print_info: n_layer          = 24
0.00.038.823 I print_info: n_head           = 16
0.00.038.824 I print_info: n_head_kv        = 16
0.00.038.824 I print_info: n_rot            = 32
0.00.038.824 I print_info: n_swa            = 0
0.00.038.824 I print_info: n_embd_head_k    = 128
0.00.038.825 I print_info: n_embd_head_v    = 128
0.00.038.825 I print_info: n_gqa            = 1
0.00.038.826 I print_info: n_embd_k_gqa     = 2048
0.00.038.827 I print_info: n_embd_v_gqa     = 2048
0.00.038.827 I print_info: f_norm_eps       = 1.0e-05
0.00.038.828 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.828 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.828 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.828 I print_info: f_logit_scale    = 0.0e+00
0.00.038.829 I print_info: n_ff             = 8192
0.00.038.830 I print_info: n_expert         = 0
0.00.038.830 I print_info: n_expert_used    = 0
0.00.038.830 I print_info: causal attn      = 1
0.00.038.833 I print_info: pooling type     = 0
0.00.038.833 I print_info: rope type        = 2
0.00.038.833 I print_info: rope scaling     = linear
0.00.038.833 I print_info: freq_base_train  = 10000.0
0.00.038.833 I print_info: freq_scale_train = 1
0.00.038.834 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.834 I print_info: rope_finetuned   = unknown
0.00.038.834 I print_info: ssm_d_conv       = 0
0.00.038.834 I print_info: ssm_d_inner      = 0
0.00.038.834 I print_info: ssm_d_state      = 0
0.00.038.834 I print_info: ssm_dt_rank      = 0
0.00.038.835 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.835 I print_info: model type       = 1.4B
0.00.038.835 I print_info: model params     = 1.41 B
0.00.038.835 I print_info: general.name     = 1.4B
0.00.038.836 I print_info: vocab type       = BPE
0.00.038.836 I print_info: n_vocab          = 50304
0.00.038.836 I print_info: n_merges         = 50009
0.00.038.836 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.837 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.841 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.842 I print_info: LF token         = 187 'Ċ'
0.00.038.842 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.842 I print_info: max token length = 1024
0.00.038.842 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.623.850 I load_tensors: offloading 24 repeating layers to GPU
0.00.623.866 I load_tensors: offloading output layer to GPU
0.00.623.866 I load_tensors: offloaded 25/25 layers to GPU
0.00.623.901 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.623.903 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.625.422 I llama_init_from_model: n_seq_max     = 1
0.00.625.426 I llama_init_from_model: n_ctx         = 128
0.00.625.427 I llama_init_from_model: n_ctx_per_seq = 128
0.00.625.427 I llama_init_from_model: n_batch       = 128
0.00.625.427 I llama_init_from_model: n_ubatch      = 128
0.00.625.428 I llama_init_from_model: flash_attn    = 0
0.00.625.431 I llama_init_from_model: freq_base     = 10000.0
0.00.625.431 I llama_init_from_model: freq_scale    = 1
0.00.625.432 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.625.434 I ggml_metal_init: allocating
0.00.625.516 I ggml_metal_init: found device: Apple M4
0.00.625.530 I ggml_metal_init: picking default device: Apple M4
0.00.627.318 I ggml_metal_init: using embedded metal library
0.00.632.780 I ggml_metal_init: GPU name:   Apple M4
0.00.632.801 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.801 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.803 I ggml_metal_init: simdgroup reduction   = true
0.00.632.803 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.803 I ggml_metal_init: has residency sets    = true
0.00.632.803 I ggml_metal_init: has bfloat            = true
0.00.632.804 I ggml_metal_init: use bfloat            = true
0.00.632.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.652.770 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.656.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.656.393 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.656.463 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.659.752 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.659.754 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.659.755 I llama_init_from_model: graph nodes  = 967
0.00.659.755 I llama_init_from_model: graph splits = 2
0.00.659.759 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.659.760 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.705 I 
0.00.687.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.687.775 I perplexity: tokenizing the input ..
0.00.694.646 I perplexity: tokenization took 6.87 ms
0.00.694.652 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.830.552 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.831.880 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.831.902 I llama_perf_context_print:        load time =     678.82 ms
0.00.831.903 I llama_perf_context_print: prompt eval time =     135.67 ms /   128 tokens (    1.06 ms per token,   943.47 tokens per second)
0.00.831.904 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.831.904 I llama_perf_context_print:       total time =     144.20 ms /   129 tokens
0.00.832.284 I ggml_metal_free: deallocating

real	0m0.847s
user	0m0.079s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.257 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.264 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.267 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.268 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.033 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.846 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.848 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.848 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.848 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.848 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.849 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.850 I llama_model_loader: - type  f32:  194 tensors
0.00.025.850 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.850 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.851 I print_info: file format = GGUF V3 (latest)
0.00.025.851 I print_info: file type   = Q5_0
0.00.025.852 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.419 I load: special tokens cache size = 25
0.00.040.510 I load: token to piece cache size = 0.2984 MB
0.00.040.514 I print_info: arch             = gptneox
0.00.040.514 I print_info: vocab_only       = 0
0.00.040.514 I print_info: n_ctx_train      = 2048
0.00.040.515 I print_info: n_embd           = 2048
0.00.040.515 I print_info: n_layer          = 24
0.00.040.518 I print_info: n_head           = 16
0.00.040.519 I print_info: n_head_kv        = 16
0.00.040.519 I print_info: n_rot            = 32
0.00.040.519 I print_info: n_swa            = 0
0.00.040.519 I print_info: n_embd_head_k    = 128
0.00.040.519 I print_info: n_embd_head_v    = 128
0.00.040.520 I print_info: n_gqa            = 1
0.00.040.521 I print_info: n_embd_k_gqa     = 2048
0.00.040.522 I print_info: n_embd_v_gqa     = 2048
0.00.040.522 I print_info: f_norm_eps       = 1.0e-05
0.00.040.523 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.523 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.523 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.523 I print_info: f_logit_scale    = 0.0e+00
0.00.040.524 I print_info: n_ff             = 8192
0.00.040.524 I print_info: n_expert         = 0
0.00.040.524 I print_info: n_expert_used    = 0
0.00.040.524 I print_info: causal attn      = 1
0.00.040.524 I print_info: pooling type     = 0
0.00.040.525 I print_info: rope type        = 2
0.00.040.525 I print_info: rope scaling     = linear
0.00.040.528 I print_info: freq_base_train  = 10000.0
0.00.040.528 I print_info: freq_scale_train = 1
0.00.040.529 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.529 I print_info: rope_finetuned   = unknown
0.00.040.529 I print_info: ssm_d_conv       = 0
0.00.040.529 I print_info: ssm_d_inner      = 0
0.00.040.529 I print_info: ssm_d_state      = 0
0.00.040.531 I print_info: ssm_dt_rank      = 0
0.00.040.531 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.531 I print_info: model type       = 1.4B
0.00.040.532 I print_info: model params     = 1.41 B
0.00.040.532 I print_info: general.name     = 1.4B
0.00.040.532 I print_info: vocab type       = BPE
0.00.040.532 I print_info: n_vocab          = 50304
0.00.040.533 I print_info: n_merges         = 50009
0.00.040.533 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.533 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.533 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.533 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.534 I print_info: LF token         = 187 'Ċ'
0.00.040.534 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.535 I print_info: max token length = 1024
0.00.040.535 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.675.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.675.791 I load_tensors: offloading output layer to GPU
0.00.675.792 I load_tensors: offloaded 25/25 layers to GPU
0.00.675.827 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.675.829 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.677.467 I llama_init_from_model: n_seq_max     = 1
0.00.677.469 I llama_init_from_model: n_ctx         = 128
0.00.677.470 I llama_init_from_model: n_ctx_per_seq = 128
0.00.677.471 I llama_init_from_model: n_batch       = 128
0.00.677.471 I llama_init_from_model: n_ubatch      = 128
0.00.677.471 I llama_init_from_model: flash_attn    = 0
0.00.677.473 I llama_init_from_model: freq_base     = 10000.0
0.00.677.474 I llama_init_from_model: freq_scale    = 1
0.00.677.474 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.677.476 I ggml_metal_init: allocating
0.00.677.539 I ggml_metal_init: found device: Apple M4
0.00.677.552 I ggml_metal_init: picking default device: Apple M4
0.00.678.986 I ggml_metal_init: using embedded metal library
0.00.685.382 I ggml_metal_init: GPU name:   Apple M4
0.00.685.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.685.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.685.388 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.685.388 I ggml_metal_init: simdgroup reduction   = true
0.00.685.389 I ggml_metal_init: simdgroup matrix mul. = true
0.00.685.389 I ggml_metal_init: has residency sets    = true
0.00.685.389 I ggml_metal_init: has bfloat            = true
0.00.685.390 I ggml_metal_init: use bfloat            = true
0.00.685.390 I ggml_metal_init: hasUnifiedMemory      = true
0.00.685.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.702.796 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.706.355 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.706.358 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.706.399 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.494 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.709.496 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.709.497 I llama_init_from_model: graph nodes  = 967
0.00.709.497 I llama_init_from_model: graph splits = 2
0.00.709.500 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.709.500 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.960 I 
0.00.741.029 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.039 I perplexity: tokenizing the input ..
0.00.746.283 I perplexity: tokenization took 5.242 ms
0.00.746.288 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.880.843 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.882.191 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.882.212 I llama_perf_context_print:        load time =     731.02 ms
0.00.882.213 I llama_perf_context_print: prompt eval time =     134.15 ms /   128 tokens (    1.05 ms per token,   954.13 tokens per second)
0.00.882.214 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.214 I llama_perf_context_print:       total time =     141.26 ms /   129 tokens
0.00.882.593 I ggml_metal_free: deallocating

real	0m0.898s
user	0m0.077s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.961 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.095 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.100 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.104 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.105 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.107 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.108 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.108 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.108 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.109 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.111 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.111 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.773 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.773 I llama_model_loader: - type  f32:  194 tensors
0.00.024.774 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.774 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.775 I print_info: file format = GGUF V3 (latest)
0.00.024.775 I print_info: file type   = Q5_1
0.00.024.776 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.750 I load: special tokens cache size = 25
0.00.038.961 I load: token to piece cache size = 0.2984 MB
0.00.038.964 I print_info: arch             = gptneox
0.00.038.964 I print_info: vocab_only       = 0
0.00.038.965 I print_info: n_ctx_train      = 2048
0.00.038.965 I print_info: n_embd           = 2048
0.00.038.965 I print_info: n_layer          = 24
0.00.038.968 I print_info: n_head           = 16
0.00.038.969 I print_info: n_head_kv        = 16
0.00.038.969 I print_info: n_rot            = 32
0.00.038.969 I print_info: n_swa            = 0
0.00.038.969 I print_info: n_embd_head_k    = 128
0.00.038.970 I print_info: n_embd_head_v    = 128
0.00.038.970 I print_info: n_gqa            = 1
0.00.038.971 I print_info: n_embd_k_gqa     = 2048
0.00.038.972 I print_info: n_embd_v_gqa     = 2048
0.00.038.972 I print_info: f_norm_eps       = 1.0e-05
0.00.038.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.973 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.974 I print_info: f_logit_scale    = 0.0e+00
0.00.038.974 I print_info: n_ff             = 8192
0.00.038.974 I print_info: n_expert         = 0
0.00.038.975 I print_info: n_expert_used    = 0
0.00.038.975 I print_info: causal attn      = 1
0.00.038.975 I print_info: pooling type     = 0
0.00.038.975 I print_info: rope type        = 2
0.00.038.975 I print_info: rope scaling     = linear
0.00.038.976 I print_info: freq_base_train  = 10000.0
0.00.038.976 I print_info: freq_scale_train = 1
0.00.038.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.976 I print_info: rope_finetuned   = unknown
0.00.038.976 I print_info: ssm_d_conv       = 0
0.00.038.977 I print_info: ssm_d_inner      = 0
0.00.038.977 I print_info: ssm_d_state      = 0
0.00.038.977 I print_info: ssm_dt_rank      = 0
0.00.038.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.977 I print_info: model type       = 1.4B
0.00.038.979 I print_info: model params     = 1.41 B
0.00.038.979 I print_info: general.name     = 1.4B
0.00.038.980 I print_info: vocab type       = BPE
0.00.038.980 I print_info: n_vocab          = 50304
0.00.038.980 I print_info: n_merges         = 50009
0.00.038.980 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.981 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.981 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.981 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.981 I print_info: LF token         = 187 'Ċ'
0.00.038.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.982 I print_info: max token length = 1024
0.00.038.982 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.609 I load_tensors: offloading output layer to GPU
0.00.655.610 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.649 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.655.652 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.657.419 I llama_init_from_model: n_seq_max     = 1
0.00.657.422 I llama_init_from_model: n_ctx         = 128
0.00.657.423 I llama_init_from_model: n_ctx_per_seq = 128
0.00.657.424 I llama_init_from_model: n_batch       = 128
0.00.657.424 I llama_init_from_model: n_ubatch      = 128
0.00.657.424 I llama_init_from_model: flash_attn    = 0
0.00.657.427 I llama_init_from_model: freq_base     = 10000.0
0.00.657.427 I llama_init_from_model: freq_scale    = 1
0.00.657.428 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.657.430 I ggml_metal_init: allocating
0.00.657.492 I ggml_metal_init: found device: Apple M4
0.00.657.505 I ggml_metal_init: picking default device: Apple M4
0.00.659.052 I ggml_metal_init: using embedded metal library
0.00.665.369 I ggml_metal_init: GPU name:   Apple M4
0.00.665.373 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.374 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.376 I ggml_metal_init: simdgroup reduction   = true
0.00.665.376 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.376 I ggml_metal_init: has residency sets    = true
0.00.665.376 I ggml_metal_init: has bfloat            = true
0.00.665.377 I ggml_metal_init: use bfloat            = true
0.00.665.377 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.379 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.684.167 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.751 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.687.755 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.687.797 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.862 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.690.864 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.690.865 I llama_init_from_model: graph nodes  = 967
0.00.690.865 I llama_init_from_model: graph splits = 2
0.00.690.868 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.690.869 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.085 I 
0.00.717.145 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.150 I perplexity: tokenizing the input ..
0.00.724.146 I perplexity: tokenization took 6.994 ms
0.00.724.152 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.634 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.859.933 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.859.957 I llama_perf_context_print:        load time =     708.11 ms
0.00.859.958 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.49 tokens per second)
0.00.859.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.959 I llama_perf_context_print:       total time =     142.88 ms /   129 tokens
0.00.860.340 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.080s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.914 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.918 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.926 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.926 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.927 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.927 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.927 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.929 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.929 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.929 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.930 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.932 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.934 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.936 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.937 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.937 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.861 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.873 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.874 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.875 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.876 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.876 I llama_model_loader: - type  f32:  194 tensors
0.00.025.877 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.877 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.877 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.878 I print_info: file format = GGUF V3 (latest)
0.00.025.878 I print_info: file type   = Q2_K - Medium
0.00.025.880 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.451 I load: special tokens cache size = 25
0.00.040.530 I load: token to piece cache size = 0.2984 MB
0.00.040.534 I print_info: arch             = gptneox
0.00.040.534 I print_info: vocab_only       = 0
0.00.040.534 I print_info: n_ctx_train      = 2048
0.00.040.534 I print_info: n_embd           = 2048
0.00.040.535 I print_info: n_layer          = 24
0.00.040.538 I print_info: n_head           = 16
0.00.040.539 I print_info: n_head_kv        = 16
0.00.040.539 I print_info: n_rot            = 32
0.00.040.539 I print_info: n_swa            = 0
0.00.040.539 I print_info: n_embd_head_k    = 128
0.00.040.539 I print_info: n_embd_head_v    = 128
0.00.040.540 I print_info: n_gqa            = 1
0.00.040.541 I print_info: n_embd_k_gqa     = 2048
0.00.040.547 I print_info: n_embd_v_gqa     = 2048
0.00.040.548 I print_info: f_norm_eps       = 1.0e-05
0.00.040.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.549 I print_info: f_logit_scale    = 0.0e+00
0.00.040.550 I print_info: n_ff             = 8192
0.00.040.550 I print_info: n_expert         = 0
0.00.040.550 I print_info: n_expert_used    = 0
0.00.040.550 I print_info: causal attn      = 1
0.00.040.551 I print_info: pooling type     = 0
0.00.040.552 I print_info: rope type        = 2
0.00.040.553 I print_info: rope scaling     = linear
0.00.040.553 I print_info: freq_base_train  = 10000.0
0.00.040.553 I print_info: freq_scale_train = 1
0.00.040.554 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.554 I print_info: rope_finetuned   = unknown
0.00.040.554 I print_info: ssm_d_conv       = 0
0.00.040.554 I print_info: ssm_d_inner      = 0
0.00.040.554 I print_info: ssm_d_state      = 0
0.00.040.554 I print_info: ssm_dt_rank      = 0
0.00.040.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.555 I print_info: model type       = 1.4B
0.00.040.555 I print_info: model params     = 1.41 B
0.00.040.556 I print_info: general.name     = 1.4B
0.00.040.556 I print_info: vocab type       = BPE
0.00.040.556 I print_info: n_vocab          = 50304
0.00.040.556 I print_info: n_merges         = 50009
0.00.040.557 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.557 I print_info: LF token         = 187 'Ċ'
0.00.040.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.558 I print_info: max token length = 1024
0.00.040.558 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.365.632 I load_tensors: offloading 24 repeating layers to GPU
0.00.365.648 I load_tensors: offloading output layer to GPU
0.00.365.648 I load_tensors: offloaded 25/25 layers to GPU
0.00.365.680 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.365.681 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.367.270 I llama_init_from_model: n_seq_max     = 1
0.00.367.274 I llama_init_from_model: n_ctx         = 128
0.00.367.275 I llama_init_from_model: n_ctx_per_seq = 128
0.00.367.275 I llama_init_from_model: n_batch       = 128
0.00.367.276 I llama_init_from_model: n_ubatch      = 128
0.00.367.276 I llama_init_from_model: flash_attn    = 0
0.00.367.278 I llama_init_from_model: freq_base     = 10000.0
0.00.367.278 I llama_init_from_model: freq_scale    = 1
0.00.367.279 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.367.281 I ggml_metal_init: allocating
0.00.367.353 I ggml_metal_init: found device: Apple M4
0.00.367.366 I ggml_metal_init: picking default device: Apple M4
0.00.369.138 I ggml_metal_init: using embedded metal library
0.00.374.487 I ggml_metal_init: GPU name:   Apple M4
0.00.374.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.374.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.374.508 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.374.508 I ggml_metal_init: simdgroup reduction   = true
0.00.374.508 I ggml_metal_init: simdgroup matrix mul. = true
0.00.374.509 I ggml_metal_init: has residency sets    = true
0.00.374.509 I ggml_metal_init: has bfloat            = true
0.00.374.509 I ggml_metal_init: use bfloat            = true
0.00.374.511 I ggml_metal_init: hasUnifiedMemory      = true
0.00.374.516 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.396.055 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.399.743 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.399.752 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.399.845 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.403.232 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.403.234 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.403.235 I llama_init_from_model: graph nodes  = 967
0.00.403.236 I llama_init_from_model: graph splits = 2
0.00.403.238 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.403.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.539 I 
0.00.434.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.619 I perplexity: tokenizing the input ..
0.00.441.341 I perplexity: tokenization took 6.72 ms
0.00.441.347 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.577.913 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.579.387 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.579.409 I llama_perf_context_print:        load time =     424.61 ms
0.00.579.410 I llama_perf_context_print: prompt eval time =     136.32 ms /   128 tokens (    1.07 ms per token,   938.94 tokens per second)
0.00.579.411 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.579.411 I llama_perf_context_print:       total time =     144.87 ms /   129 tokens
0.00.579.777 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.080s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.954 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.234 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.240 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.247 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.249 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.251 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.251 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.252 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.252 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.252 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.255 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.240 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.240 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.242 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.242 I llama_model_loader: - type  f32:  194 tensors
0.00.025.243 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.243 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.243 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.243 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.244 I print_info: file format = GGUF V3 (latest)
0.00.025.244 I print_info: file type   = Q3_K - Medium
0.00.025.246 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.544 I load: special tokens cache size = 25
0.00.039.673 I load: token to piece cache size = 0.2984 MB
0.00.039.676 I print_info: arch             = gptneox
0.00.039.676 I print_info: vocab_only       = 0
0.00.039.677 I print_info: n_ctx_train      = 2048
0.00.039.677 I print_info: n_embd           = 2048
0.00.039.677 I print_info: n_layer          = 24
0.00.039.680 I print_info: n_head           = 16
0.00.039.681 I print_info: n_head_kv        = 16
0.00.039.681 I print_info: n_rot            = 32
0.00.039.681 I print_info: n_swa            = 0
0.00.039.682 I print_info: n_embd_head_k    = 128
0.00.039.683 I print_info: n_embd_head_v    = 128
0.00.039.684 I print_info: n_gqa            = 1
0.00.039.685 I print_info: n_embd_k_gqa     = 2048
0.00.039.685 I print_info: n_embd_v_gqa     = 2048
0.00.039.686 I print_info: f_norm_eps       = 1.0e-05
0.00.039.686 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.686 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.687 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.687 I print_info: f_logit_scale    = 0.0e+00
0.00.039.687 I print_info: n_ff             = 8192
0.00.039.688 I print_info: n_expert         = 0
0.00.039.690 I print_info: n_expert_used    = 0
0.00.039.690 I print_info: causal attn      = 1
0.00.039.690 I print_info: pooling type     = 0
0.00.039.690 I print_info: rope type        = 2
0.00.039.690 I print_info: rope scaling     = linear
0.00.039.691 I print_info: freq_base_train  = 10000.0
0.00.039.691 I print_info: freq_scale_train = 1
0.00.039.691 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.691 I print_info: rope_finetuned   = unknown
0.00.039.692 I print_info: ssm_d_conv       = 0
0.00.039.692 I print_info: ssm_d_inner      = 0
0.00.039.692 I print_info: ssm_d_state      = 0
0.00.039.692 I print_info: ssm_dt_rank      = 0
0.00.039.693 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.694 I print_info: model type       = 1.4B
0.00.039.694 I print_info: model params     = 1.41 B
0.00.039.694 I print_info: general.name     = 1.4B
0.00.039.695 I print_info: vocab type       = BPE
0.00.039.695 I print_info: n_vocab          = 50304
0.00.039.695 I print_info: n_merges         = 50009
0.00.039.695 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.696 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.696 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.696 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: LF token         = 187 'Ċ'
0.00.039.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: max token length = 1024
0.00.039.700 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.456.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.456.833 I load_tensors: offloading output layer to GPU
0.00.456.834 I load_tensors: offloaded 25/25 layers to GPU
0.00.456.860 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.456.861 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.458.293 I llama_init_from_model: n_seq_max     = 1
0.00.458.297 I llama_init_from_model: n_ctx         = 128
0.00.458.298 I llama_init_from_model: n_ctx_per_seq = 128
0.00.458.298 I llama_init_from_model: n_batch       = 128
0.00.458.299 I llama_init_from_model: n_ubatch      = 128
0.00.458.299 I llama_init_from_model: flash_attn    = 0
0.00.458.302 I llama_init_from_model: freq_base     = 10000.0
0.00.458.302 I llama_init_from_model: freq_scale    = 1
0.00.458.303 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.458.305 I ggml_metal_init: allocating
0.00.458.365 I ggml_metal_init: found device: Apple M4
0.00.458.378 I ggml_metal_init: picking default device: Apple M4
0.00.460.092 I ggml_metal_init: using embedded metal library
0.00.465.514 I ggml_metal_init: GPU name:   Apple M4
0.00.465.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.465.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.465.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.465.523 I ggml_metal_init: simdgroup reduction   = true
0.00.465.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.465.524 I ggml_metal_init: has residency sets    = true
0.00.465.524 I ggml_metal_init: has bfloat            = true
0.00.465.524 I ggml_metal_init: use bfloat            = true
0.00.465.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.465.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.487.211 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.490.844 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.490.851 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.490.901 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.494.142 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.494.143 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.494.144 I llama_init_from_model: graph nodes  = 967
0.00.494.144 I llama_init_from_model: graph splits = 2
0.00.494.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.494.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.524.052 I 
0.00.524.132 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.524.184 I perplexity: tokenizing the input ..
0.00.531.127 I perplexity: tokenization took 6.942 ms
0.00.531.131 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.675.533 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.676.876 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.676.900 I llama_perf_context_print:        load time =     515.08 ms
0.00.676.901 I llama_perf_context_print: prompt eval time =     144.17 ms /   128 tokens (    1.13 ms per token,   887.82 tokens per second)
0.00.676.902 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.676.902 I llama_perf_context_print:       total time =     152.85 ms /   129 tokens
0.00.677.280 I ggml_metal_free: deallocating

real	0m0.691s
user	0m0.082s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.111 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.924 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.931 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.938 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.938 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.938 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.939 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.940 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.940 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.941 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.941 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.941 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.947 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.800 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.588 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.589 I llama_model_loader: - type  f32:  194 tensors
0.00.024.589 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.589 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.589 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.590 I print_info: file format = GGUF V3 (latest)
0.00.024.591 I print_info: file type   = Q4_K - Medium
0.00.024.592 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.303 I load: special tokens cache size = 25
0.00.038.416 I load: token to piece cache size = 0.2984 MB
0.00.038.419 I print_info: arch             = gptneox
0.00.038.419 I print_info: vocab_only       = 0
0.00.038.419 I print_info: n_ctx_train      = 2048
0.00.038.420 I print_info: n_embd           = 2048
0.00.038.420 I print_info: n_layer          = 24
0.00.038.423 I print_info: n_head           = 16
0.00.038.424 I print_info: n_head_kv        = 16
0.00.038.424 I print_info: n_rot            = 32
0.00.038.427 I print_info: n_swa            = 0
0.00.038.428 I print_info: n_embd_head_k    = 128
0.00.038.428 I print_info: n_embd_head_v    = 128
0.00.038.428 I print_info: n_gqa            = 1
0.00.038.430 I print_info: n_embd_k_gqa     = 2048
0.00.038.431 I print_info: n_embd_v_gqa     = 2048
0.00.038.432 I print_info: f_norm_eps       = 1.0e-05
0.00.038.432 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.432 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.432 I print_info: f_logit_scale    = 0.0e+00
0.00.038.434 I print_info: n_ff             = 8192
0.00.038.434 I print_info: n_expert         = 0
0.00.038.434 I print_info: n_expert_used    = 0
0.00.038.435 I print_info: causal attn      = 1
0.00.038.435 I print_info: pooling type     = 0
0.00.038.435 I print_info: rope type        = 2
0.00.038.435 I print_info: rope scaling     = linear
0.00.038.435 I print_info: freq_base_train  = 10000.0
0.00.038.436 I print_info: freq_scale_train = 1
0.00.038.436 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.436 I print_info: rope_finetuned   = unknown
0.00.038.436 I print_info: ssm_d_conv       = 0
0.00.038.437 I print_info: ssm_d_inner      = 0
0.00.038.437 I print_info: ssm_d_state      = 0
0.00.038.441 I print_info: ssm_dt_rank      = 0
0.00.038.441 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.441 I print_info: model type       = 1.4B
0.00.038.442 I print_info: model params     = 1.41 B
0.00.038.442 I print_info: general.name     = 1.4B
0.00.038.442 I print_info: vocab type       = BPE
0.00.038.442 I print_info: n_vocab          = 50304
0.00.038.442 I print_info: n_merges         = 50009
0.00.038.443 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.444 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.444 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.444 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.444 I print_info: LF token         = 187 'Ċ'
0.00.038.444 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.445 I print_info: max token length = 1024
0.00.038.447 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.522.997 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.010 I load_tensors: offloading output layer to GPU
0.00.523.011 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.046 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.047 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.524.602 I llama_init_from_model: n_seq_max     = 1
0.00.524.605 I llama_init_from_model: n_ctx         = 128
0.00.524.606 I llama_init_from_model: n_ctx_per_seq = 128
0.00.524.606 I llama_init_from_model: n_batch       = 128
0.00.524.607 I llama_init_from_model: n_ubatch      = 128
0.00.524.607 I llama_init_from_model: flash_attn    = 0
0.00.524.609 I llama_init_from_model: freq_base     = 10000.0
0.00.524.610 I llama_init_from_model: freq_scale    = 1
0.00.524.611 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.524.613 I ggml_metal_init: allocating
0.00.524.700 I ggml_metal_init: found device: Apple M4
0.00.524.713 I ggml_metal_init: picking default device: Apple M4
0.00.526.521 I ggml_metal_init: using embedded metal library
0.00.533.357 I ggml_metal_init: GPU name:   Apple M4
0.00.533.362 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.533.363 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.533.364 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.533.364 I ggml_metal_init: simdgroup reduction   = true
0.00.533.364 I ggml_metal_init: simdgroup matrix mul. = true
0.00.533.365 I ggml_metal_init: has residency sets    = true
0.00.533.365 I ggml_metal_init: has bfloat            = true
0.00.533.365 I ggml_metal_init: use bfloat            = true
0.00.533.366 I ggml_metal_init: hasUnifiedMemory      = true
0.00.533.368 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.550.941 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.554.538 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.554.542 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.554.586 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.557.683 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.557.685 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.557.685 I llama_init_from_model: graph nodes  = 967
0.00.557.686 I llama_init_from_model: graph splits = 2
0.00.557.689 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.557.689 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.584.616 I 
0.00.584.675 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.584.682 I perplexity: tokenizing the input ..
0.00.591.732 I perplexity: tokenization took 7.045 ms
0.00.591.746 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.626 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.726.950 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.726.973 I llama_perf_context_print:        load time =     575.49 ms
0.00.726.974 I llama_perf_context_print: prompt eval time =     133.28 ms /   128 tokens (    1.04 ms per token,   960.37 tokens per second)
0.00.726.975 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.975 I llama_perf_context_print:       total time =     142.36 ms /   129 tokens
0.00.727.359 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.120 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.098 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.103 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.110 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.110 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.110 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.111 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.111 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.112 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.112 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.113 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.116 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.116 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.927 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.575 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.577 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.577 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.577 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.578 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.578 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.579 I llama_model_loader: - type  f32:  194 tensors
0.00.025.579 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.579 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.580 I print_info: file format = GGUF V3 (latest)
0.00.025.580 I print_info: file type   = Q5_K - Medium
0.00.025.581 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.579 I load: special tokens cache size = 25
0.00.039.651 I load: token to piece cache size = 0.2984 MB
0.00.039.655 I print_info: arch             = gptneox
0.00.039.655 I print_info: vocab_only       = 0
0.00.039.655 I print_info: n_ctx_train      = 2048
0.00.039.655 I print_info: n_embd           = 2048
0.00.039.656 I print_info: n_layer          = 24
0.00.039.660 I print_info: n_head           = 16
0.00.039.661 I print_info: n_head_kv        = 16
0.00.039.661 I print_info: n_rot            = 32
0.00.039.661 I print_info: n_swa            = 0
0.00.039.661 I print_info: n_embd_head_k    = 128
0.00.039.661 I print_info: n_embd_head_v    = 128
0.00.039.662 I print_info: n_gqa            = 1
0.00.039.663 I print_info: n_embd_k_gqa     = 2048
0.00.039.664 I print_info: n_embd_v_gqa     = 2048
0.00.039.664 I print_info: f_norm_eps       = 1.0e-05
0.00.039.665 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.665 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.665 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.665 I print_info: f_logit_scale    = 0.0e+00
0.00.039.666 I print_info: n_ff             = 8192
0.00.039.666 I print_info: n_expert         = 0
0.00.039.666 I print_info: n_expert_used    = 0
0.00.039.666 I print_info: causal attn      = 1
0.00.039.666 I print_info: pooling type     = 0
0.00.039.666 I print_info: rope type        = 2
0.00.039.667 I print_info: rope scaling     = linear
0.00.039.667 I print_info: freq_base_train  = 10000.0
0.00.039.667 I print_info: freq_scale_train = 1
0.00.039.668 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.668 I print_info: rope_finetuned   = unknown
0.00.039.668 I print_info: ssm_d_conv       = 0
0.00.039.668 I print_info: ssm_d_inner      = 0
0.00.039.668 I print_info: ssm_d_state      = 0
0.00.039.669 I print_info: ssm_dt_rank      = 0
0.00.039.669 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.669 I print_info: model type       = 1.4B
0.00.039.669 I print_info: model params     = 1.41 B
0.00.039.669 I print_info: general.name     = 1.4B
0.00.039.670 I print_info: vocab type       = BPE
0.00.039.670 I print_info: n_vocab          = 50304
0.00.039.673 I print_info: n_merges         = 50009
0.00.039.673 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.673 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: LF token         = 187 'Ċ'
0.00.039.674 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.674 I print_info: max token length = 1024
0.00.039.675 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.197 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.213 I load_tensors: offloading output layer to GPU
0.00.603.214 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.248 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.603.256 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.604.999 I llama_init_from_model: n_seq_max     = 1
0.00.605.002 I llama_init_from_model: n_ctx         = 128
0.00.605.003 I llama_init_from_model: n_ctx_per_seq = 128
0.00.605.003 I llama_init_from_model: n_batch       = 128
0.00.605.003 I llama_init_from_model: n_ubatch      = 128
0.00.605.004 I llama_init_from_model: flash_attn    = 0
0.00.605.006 I llama_init_from_model: freq_base     = 10000.0
0.00.605.006 I llama_init_from_model: freq_scale    = 1
0.00.605.007 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.605.009 I ggml_metal_init: allocating
0.00.605.111 I ggml_metal_init: found device: Apple M4
0.00.605.142 I ggml_metal_init: picking default device: Apple M4
0.00.606.518 I ggml_metal_init: using embedded metal library
0.00.612.928 I ggml_metal_init: GPU name:   Apple M4
0.00.612.932 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.933 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.933 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.934 I ggml_metal_init: simdgroup reduction   = true
0.00.612.934 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.935 I ggml_metal_init: has residency sets    = true
0.00.612.935 I ggml_metal_init: has bfloat            = true
0.00.612.935 I ggml_metal_init: use bfloat            = true
0.00.612.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.938 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.629.764 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.200 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.258 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.560 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.562 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.562 I llama_init_from_model: graph nodes  = 967
0.00.636.563 I llama_init_from_model: graph splits = 2
0.00.636.566 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.566 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.889 I 
0.00.671.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.956 I perplexity: tokenizing the input ..
0.00.677.400 I perplexity: tokenization took 5.442 ms
0.00.677.406 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.875 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.819.292 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.819.317 I llama_perf_context_print:        load time =     661.76 ms
0.00.819.318 I llama_perf_context_print: prompt eval time =     140.24 ms /   128 tokens (    1.10 ms per token,   912.72 tokens per second)
0.00.819.319 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.819.319 I llama_perf_context_print:       total time =     147.43 ms /   129 tokens
0.00.819.669 I ggml_metal_free: deallocating

real	0m0.836s
user	0m0.075s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.153 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.107 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.112 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.119 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.123 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.123 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.124 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.124 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.126 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.126 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.029 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.916 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.917 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.917 I llama_model_loader: - type  f32:  194 tensors
0.00.024.918 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.918 I print_info: file format = GGUF V3 (latest)
0.00.024.919 I print_info: file type   = Q6_K
0.00.024.921 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.066 I load: special tokens cache size = 25
0.00.039.052 I load: token to piece cache size = 0.2984 MB
0.00.039.055 I print_info: arch             = gptneox
0.00.039.055 I print_info: vocab_only       = 0
0.00.039.055 I print_info: n_ctx_train      = 2048
0.00.039.055 I print_info: n_embd           = 2048
0.00.039.056 I print_info: n_layer          = 24
0.00.039.060 I print_info: n_head           = 16
0.00.039.061 I print_info: n_head_kv        = 16
0.00.039.061 I print_info: n_rot            = 32
0.00.039.061 I print_info: n_swa            = 0
0.00.039.061 I print_info: n_embd_head_k    = 128
0.00.039.062 I print_info: n_embd_head_v    = 128
0.00.039.063 I print_info: n_gqa            = 1
0.00.039.063 I print_info: n_embd_k_gqa     = 2048
0.00.039.064 I print_info: n_embd_v_gqa     = 2048
0.00.039.065 I print_info: f_norm_eps       = 1.0e-05
0.00.039.065 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.065 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.065 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.065 I print_info: f_logit_scale    = 0.0e+00
0.00.039.067 I print_info: n_ff             = 8192
0.00.039.067 I print_info: n_expert         = 0
0.00.039.067 I print_info: n_expert_used    = 0
0.00.039.067 I print_info: causal attn      = 1
0.00.039.067 I print_info: pooling type     = 0
0.00.039.067 I print_info: rope type        = 2
0.00.039.068 I print_info: rope scaling     = linear
0.00.039.068 I print_info: freq_base_train  = 10000.0
0.00.039.070 I print_info: freq_scale_train = 1
0.00.039.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.072 I print_info: rope_finetuned   = unknown
0.00.039.073 I print_info: ssm_d_conv       = 0
0.00.039.073 I print_info: ssm_d_inner      = 0
0.00.039.073 I print_info: ssm_d_state      = 0
0.00.039.073 I print_info: ssm_dt_rank      = 0
0.00.039.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.077 I print_info: model type       = 1.4B
0.00.039.077 I print_info: model params     = 1.41 B
0.00.039.078 I print_info: general.name     = 1.4B
0.00.039.078 I print_info: vocab type       = BPE
0.00.039.078 I print_info: n_vocab          = 50304
0.00.039.078 I print_info: n_merges         = 50009
0.00.039.079 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.079 I print_info: LF token         = 187 'Ċ'
0.00.039.079 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.080 I print_info: max token length = 1024
0.00.039.080 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.363 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.376 I load_tensors: offloading output layer to GPU
0.00.632.377 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.407 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.632.408 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.633.972 I llama_init_from_model: n_seq_max     = 1
0.00.633.975 I llama_init_from_model: n_ctx         = 128
0.00.633.976 I llama_init_from_model: n_ctx_per_seq = 128
0.00.633.976 I llama_init_from_model: n_batch       = 128
0.00.633.976 I llama_init_from_model: n_ubatch      = 128
0.00.633.977 I llama_init_from_model: flash_attn    = 0
0.00.633.978 I llama_init_from_model: freq_base     = 10000.0
0.00.633.979 I llama_init_from_model: freq_scale    = 1
0.00.633.980 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.633.982 I ggml_metal_init: allocating
0.00.634.000 I ggml_metal_init: found device: Apple M4
0.00.634.010 I ggml_metal_init: picking default device: Apple M4
0.00.635.347 I ggml_metal_init: using embedded metal library
0.00.642.586 I ggml_metal_init: GPU name:   Apple M4
0.00.642.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.592 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.593 I ggml_metal_init: simdgroup reduction   = true
0.00.642.593 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.594 I ggml_metal_init: has residency sets    = true
0.00.642.594 I ggml_metal_init: has bfloat            = true
0.00.642.594 I ggml_metal_init: use bfloat            = true
0.00.642.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.597 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.989 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.664.720 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.664.731 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.664.784 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.668.146 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.668.148 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.668.148 I llama_init_from_model: graph nodes  = 967
0.00.668.149 I llama_init_from_model: graph splits = 2
0.00.668.152 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.668.152 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.882 I 
0.00.705.941 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.948 I perplexity: tokenizing the input ..
0.00.712.071 I perplexity: tokenization took 6.121 ms
0.00.712.075 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.851.789 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.853.161 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.853.184 I llama_perf_context_print:        load time =     696.71 ms
0.00.853.185 I llama_perf_context_print: prompt eval time =     138.95 ms /   128 tokens (    1.09 ms per token,   921.22 tokens per second)
0.00.853.186 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.853.186 I llama_perf_context_print:       total time =     147.31 ms /   129 tokens
0.00.853.545 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.079s
sys	0m0.147s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.204 I build: 4709 (c1f958c0) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.303 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.396 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.404 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.410 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.411 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.411 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.412 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.415 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.417 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.418 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.161 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.046 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.048 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.048 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.048 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.048 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.049 I llama_model_loader: - type  f32:  194 tensors
0.00.037.049 I llama_model_loader: - type  f16:   98 tensors
0.00.037.050 I print_info: file format = GGUF V3 (latest)
0.00.037.051 I print_info: file type   = all F32 (guessed)
0.00.037.052 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.044.946 I load: special tokens cache size = 25
0.00.051.144 I load: token to piece cache size = 0.2984 MB
0.00.051.148 I print_info: arch             = gptneox
0.00.051.149 I print_info: vocab_only       = 0
0.00.051.149 I print_info: n_ctx_train      = 2048
0.00.051.149 I print_info: n_embd           = 2048
0.00.051.149 I print_info: n_layer          = 24
0.00.051.154 I print_info: n_head           = 16
0.00.051.155 I print_info: n_head_kv        = 16
0.00.051.155 I print_info: n_rot            = 32
0.00.051.156 I print_info: n_swa            = 0
0.00.051.156 I print_info: n_embd_head_k    = 128
0.00.051.156 I print_info: n_embd_head_v    = 128
0.00.051.157 I print_info: n_gqa            = 1
0.00.051.158 I print_info: n_embd_k_gqa     = 2048
0.00.051.159 I print_info: n_embd_v_gqa     = 2048
0.00.051.159 I print_info: f_norm_eps       = 1.0e-05
0.00.051.160 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.160 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.160 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.164 I print_info: f_logit_scale    = 0.0e+00
0.00.051.165 I print_info: n_ff             = 8192
0.00.051.165 I print_info: n_expert         = 0
0.00.051.165 I print_info: n_expert_used    = 0
0.00.051.165 I print_info: causal attn      = 1
0.00.051.166 I print_info: pooling type     = 0
0.00.051.166 I print_info: rope type        = 2
0.00.051.166 I print_info: rope scaling     = linear
0.00.051.167 I print_info: freq_base_train  = 10000.0
0.00.051.167 I print_info: freq_scale_train = 1
0.00.051.167 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.169 I print_info: rope_finetuned   = unknown
0.00.051.170 I print_info: ssm_d_conv       = 0
0.00.051.170 I print_info: ssm_d_inner      = 0
0.00.051.170 I print_info: ssm_d_state      = 0
0.00.051.170 I print_info: ssm_dt_rank      = 0
0.00.051.170 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.171 I print_info: model type       = 1.4B
0.00.051.171 I print_info: model params     = 1.41 B
0.00.051.171 I print_info: general.name     = 1.4B
0.00.051.172 I print_info: vocab type       = BPE
0.00.051.172 I print_info: n_vocab          = 50304
0.00.051.172 I print_info: n_merges         = 50009
0.00.051.173 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.173 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.173 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.173 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.174 I print_info: LF token         = 187 'Ċ'
0.00.051.176 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.176 I print_info: max token length = 1024
0.00.051.176 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.311.346 I load_tensors: offloading 24 repeating layers to GPU
0.01.311.350 I load_tensors: offloading output layer to GPU
0.01.311.351 I load_tensors: offloaded 25/25 layers to GPU
0.01.311.373 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.311.374 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.312.295 I llama_init_from_model: n_seq_max     = 1
0.01.312.297 I llama_init_from_model: n_ctx         = 128
0.01.312.297 I llama_init_from_model: n_ctx_per_seq = 128
0.01.312.297 I llama_init_from_model: n_batch       = 128
0.01.312.297 I llama_init_from_model: n_ubatch      = 128
0.01.312.298 I llama_init_from_model: flash_attn    = 0
0.01.312.298 I llama_init_from_model: freq_base     = 10000.0
0.01.312.299 I llama_init_from_model: freq_scale    = 1
0.01.312.299 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.312.303 I ggml_metal_init: allocating
0.01.312.378 I ggml_metal_init: found device: Apple M4
0.01.312.385 I ggml_metal_init: picking default device: Apple M4
0.01.313.583 I ggml_metal_init: using embedded metal library
0.01.317.668 I ggml_metal_init: GPU name:   Apple M4
0.01.317.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.317.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.317.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.317.672 I ggml_metal_init: simdgroup reduction   = true
0.01.317.672 I ggml_metal_init: simdgroup matrix mul. = true
0.01.317.672 I ggml_metal_init: has residency sets    = true
0.01.317.672 I ggml_metal_init: has bfloat            = true
0.01.317.672 I ggml_metal_init: use bfloat            = true
0.01.317.673 I ggml_metal_init: hasUnifiedMemory      = true
0.01.317.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.329.376 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.331.121 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.331.123 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.331.149 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.332.966 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.332.968 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.332.968 I llama_init_from_model: graph nodes  = 967
0.01.332.968 I llama_init_from_model: graph splits = 2
0.01.332.970 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.332.970 I 
0.01.332.997 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.332.999 I compute_imatrix: tokenizing the input ..
0.01.337.360 I compute_imatrix: tokenization took 4.36 ms
0.01.337.362 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.610.768 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.613.117 I llama_perf_context_print:        load time =    1593.45 ms
0.01.613.117 I llama_perf_context_print: prompt eval time =     271.55 ms /   128 tokens (    2.12 ms per token,   471.37 tokens per second)
0.01.613.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.613.118 I llama_perf_context_print:       total time =    1595.80 ms /   129 tokens
0.01.613.641 I ggml_metal_free: deallocating

real	0m1.794s
user	0m0.109s
sys	0m0.248s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4709 (c1f958c0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138604ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138608560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1386089d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138608e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1386092b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138609720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138609b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13860a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13860a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13860a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13860ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13860b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13860bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13860c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13860ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13860d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13860dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13860e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13860eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13860f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13860fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138610160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138610880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138611840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138611b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138612230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138612950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138612dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138613380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138613890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138613fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138614430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1386148a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138614d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138615180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1386155f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138615a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138615ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138616340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1386167b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138617090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138617970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138617de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1386189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1386192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13861a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13861a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13861abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13861ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13861b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13861b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13861bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13861c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13861c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13861ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13861cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13861d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13861d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13861de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13861e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13861e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13861ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13861f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13861f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13861fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138620240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1386207f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138620da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138621350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138621eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138622a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138623570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138623b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1386240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138624680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138624c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1386251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138625790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138625d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1386262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1386268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138626e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138627400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1386279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138627f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1386180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1386286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138628b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138628fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138629550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13862a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13862a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13862ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13862b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13862b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13862bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13862c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13862c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13862ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13862d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13862d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13862de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13862e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13862e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13862ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13862f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13862f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13862fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138630190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138630690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138630b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138631090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138631590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138631a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138631f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138632490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138632e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138633390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138633890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138633d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138634290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138634790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138634c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138635190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138635690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138636590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138636a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138636f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138637490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138637990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138637e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138638390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138638890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138639790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138639c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13863a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13863a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13863ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13863b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13863b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13863ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13863bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13863c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13863c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13863ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13863d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13863d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13863dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13863e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13863e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13863ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13863f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13863f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13863fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138640090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138640590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138640a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138640f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138641990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138641e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138642d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138643790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138643c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138644190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138644690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138645090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138645590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138645a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138645f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138646490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138646990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138646f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1386474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138647aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138648050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138648660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138648c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138649280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138649a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138649f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13864a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13864a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13864adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13864b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13864ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13864bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13864c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13864cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13864d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13864d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13864db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13864e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13864e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13864eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13864f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13864f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13864fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138650090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1386505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138650b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138651080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1386515d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138651b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138652070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1386525c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138652b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138653060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1386535b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138653b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138654050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1386545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138654af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138655040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138655590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138655ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138656030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138656580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138656ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138657020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138657570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138657ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138658010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138658560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138658ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138659000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138659550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138659aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138659ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13865a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13865aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13865afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13865b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13865ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13865bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13865c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13865ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13865cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13865d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13865da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13865dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13865e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13865ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13865efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13865f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108604230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1086046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108604b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108604f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1086053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108605860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108605cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108606140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1086065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108606a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108606e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108607300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108607770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108607be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108608050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1086084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108608930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108609630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108609d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10860a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10860a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10860aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10860b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10860b7b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.714.887 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.714.891 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10bc04d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10bc051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10bc05630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10bc05aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10bc05f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10bc06380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10bc067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10bc06c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10bc070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10bc07540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10bc079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10bc080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10bc08bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10bc09370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10bc09b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10bc0a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10bc0a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10bc0b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10bc0b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10bc0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10bc0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10bc0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10bc0d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10bc0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10bc0e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10bc0e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10bc0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10bc0ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10bc0f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10bc0f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10bc0fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10bc0ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10bc103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10bc10670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10bc10ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10bc10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10bc113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10bc11830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10bc11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10bc12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10bc12580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10bc129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10bc12e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10bc132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10bc13740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10bc13bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10bc14020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10bc14490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10bc14900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10bc14d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10bc151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10bc15650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10bc15ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10bc15f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10bc163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10bc16810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10bc16d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10bc17280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10bc176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10bc17b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10bc17fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10bc18440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10bc188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10bc18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10bc19190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10bc19600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10bc19a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10bc19ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10bc1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10bc1a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10bc1ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10bc1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10bc1b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10bc1b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10bc1bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10bc1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10bc1c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10bc1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10bc1cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10bc1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10bc1d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10bc1dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10bc1e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10bc1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10bc1ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10bc1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10bc1f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10bc1f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10bc1fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10bc20080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10bc204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10bc20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10bc20dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10bc21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10bc216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10bc21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10bc21f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10bc22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10bc22870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10bc22ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10bc23150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10bc235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10bc23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10bc23ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10bc24310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10bc24780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10bc24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10bc25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10bc254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10bc25940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10bc25db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10bc26220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10bc26690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10bc26b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10bc26f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10bc273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10bc27850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10bc27cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10bc28130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10bc285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10bc28a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10bc28e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10bc292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10bc29760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10bc29bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10bc2a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10bc2a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10bc2a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10bc2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10bc2b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10bc2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10bc2bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10bc2bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10bc2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10bc2c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10bc2cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10bc2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10bc2d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10bc2d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10bc2de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10bc2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10bc2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10bc2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10bc2f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10bc2f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10bc2f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10bc2fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10bc301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10bc30650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10bc30ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10bc30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10bc313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10bc31810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10bc31c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10bc320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10bc32560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10bc329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10bc32e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10bc332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10bc33720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10bc33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10bc34000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10bc34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10bc348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10bc34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10bc351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10bc35df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10bc360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10bc36370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10bc367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10bc36c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10bc370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10bc37530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10bc379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10bc37e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10bc38280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10bc386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10bc38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10bc38fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10bc39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10bc398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10bc39d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10bc3a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10bc3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10bc3aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10bc3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10bc3b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10bc3b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10bc3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10bc3c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10bc3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10bc3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10bc3cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10bc3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10bc3d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10bc3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10bc3dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10bc3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10bc3e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10bc3ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10bc3f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10bc3f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10bc3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10bc40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10bc404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10bc40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10bc40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10bc41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10bc41730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10bc41c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10bc427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10bc42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10bc43030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10bc435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10bc43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10bc44170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10bc44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10bc44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10bc452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10bc45870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10bc45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10bc463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10bc469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10bc46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10bc47530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10bc47af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10bc480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10bc48670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10bc48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10bc491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10bc497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10bc49d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10bc4a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10bc4a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10bc4aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10bc4b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10bc4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10bc4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10bc4c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10bc4cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10bc4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10bc4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10bc4dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10bc4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10bc4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10bc4edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10bc4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10bc4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10bc4ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10bc504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10bc50ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10bc51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10bc51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10bc51bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10bc521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10bc52770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10bc52d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10bc532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10bc538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10bc53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10bc54430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10bc549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10bc54fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10bc55570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10bc55b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10bc560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10bc566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10bc56c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10bc57170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10bc57670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10bc57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10bc58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10bc58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10bc58a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10bc58f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10bc59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10bc59970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10bc59e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10bc5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10bc5a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10bc5ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10bc5b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10bc5b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10bc5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10bc5c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10bc5cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10bc5d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10bc5d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10bc5e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10bc5e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10bc5ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138620500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138625a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13861ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1386254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13862cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13862c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13862bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1386276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138622170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13862a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138647200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138627110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138621bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138624ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138623830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138646c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13862ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138626b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138621610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138624940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138629810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13862b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1386265b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138621060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138624390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138629260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13862aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138626000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138623de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13862a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138648f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13864aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138610dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13860b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138604540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13861b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13861a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138628220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13864b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138649540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1386124f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13865f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13865fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13865fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13865fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1386602b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138660570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138660830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138660af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138660db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138661070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138661330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1386615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1386618b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138661b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138661e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1386620f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1386623b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138662670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138662930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138662bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138662eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138663170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138663430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1386636f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1386639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138663c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138663f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1386641f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1386644b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138664770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138664a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138664cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138664fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138665270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138665530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1386657f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138665ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138665d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138666030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1386662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1386665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138666870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138666b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138666df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1386670b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138667370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138667630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1386678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138667bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138667e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138668130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1386683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1386686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138668970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138668c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138668ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1386691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138669470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138669730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1386699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138669cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138669f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13866a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13866a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13866a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13866aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13866ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13866aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13866b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13866b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13866b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13866baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13866bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13866c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13866c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13866c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13866c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13866cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13866ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13866d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13866d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13866d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13866d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13866dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13866deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13866e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13866e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13866e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13866e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13866ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13866ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13866f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13866f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13866f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13866fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13866fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13866ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138670270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138670530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1386707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138670ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138670d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138671030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1386712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1386715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138671870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138671b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138671df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1386720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138672370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138672630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1386728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138672bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138672e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138673130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1386733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1386736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138673970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138673c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138673ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1386741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138674470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138674730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1386749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138674cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138674f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138675230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1386754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1386757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138675a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138675d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138675ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1386762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138676570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138676830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138676af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138676db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138677070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138677330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1386775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1386778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138677b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138677e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1386780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1386783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138678670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138678930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138678bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138678eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138679170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138679430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1386796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1386799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138679c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138679f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13867a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13867a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13867a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13867af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13867b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13867bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13867c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13867c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13867cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13867d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13867d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13867dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13867e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13867e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13867ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13867f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13867f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13867fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1386802c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138680810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138680d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1386812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138681800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138681d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1386822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1386827f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138682d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138683290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1386837e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138683d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138684280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1386847d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138684d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138685270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1386857c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138685d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138686260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1386867b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138686d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138687250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1386877a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138687cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138688240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138688790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138688ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138689230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138689780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138689cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13868a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13868a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13868acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13868b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13868b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13868bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13868c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13868c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13868cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13868d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13868d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13868dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13868e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13868e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13868ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13868f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13868f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13868fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138690110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1386905b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138690a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138690ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138691390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138691830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138691cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138692170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138692610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138692ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138692f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1386933f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138693890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138693d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1386941d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138694720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138694e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138695560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138695c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1386963a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138696660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138696e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138697110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138697720 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.755s
user	0m0.281s
sys	0m0.312s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4709 (c1f958c0)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15170d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15170dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15170e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15170e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15170ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15170f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15170f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15170fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1517103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1517108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x151710db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1517112b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151712580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151712d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1517134b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151713bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1517142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151714a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1517151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151715900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151716020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151716740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151716fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151717700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1517179c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x151717fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151718c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151719180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151719440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1517198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151719ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15171a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15171a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15171ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15171b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15171b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15171ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15171beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15171c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15171c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15171cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15171d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15171d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15171d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15171dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15171e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15171edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15171f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15171f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x151720000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x151720610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x151720c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x151721230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x151721a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x151721ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151722360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151722620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151722c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151723420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1517236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151723b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x151724020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1517244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151724960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151724e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1517252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151725740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151725be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151726080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x151726520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1517269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151726e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1517273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151727900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151727e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1517283a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1517288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151728e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151729390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1517298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151729e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15172a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15172a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15172ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15172b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15172b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15172be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15172c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15172c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15172ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15172d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15172d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15172ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15172e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15172e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15172ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15171eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15172f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15172fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15172ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1517304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1517309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x151730f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x151731490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1517319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x151731f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151732480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1517329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x151732f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151733470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1517339c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x151733f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1517343b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151734850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151734cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151735190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151735630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151735ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151735f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151736410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1517368b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x151736d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1517371f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151737690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x151737b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151737fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151738470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x151738910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151738db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x151739250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1517396f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151739b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15173a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15173a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15173a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15173ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15173b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15173b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15173bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15173c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15173c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15173c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15173ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15173d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15173d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15173dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15173e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15173e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15173ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15173eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15173f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15173f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15173fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x151740150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1517405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x151740a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x151740f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1517413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151741870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x151741d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1517421b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151742650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151742af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151742f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151743430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1517438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151743d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x151744210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1517446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x151744b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151744ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151745490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x151745930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151745dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151746270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151746710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151746bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x151747050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1517474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151747990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x151747e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1517482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153004080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1530044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153004960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153004dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153005240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1530056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153005b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x153005f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x153006400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x153006870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x153006ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x153007150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1530075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x153007a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x153007ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x153008310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x153008780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x153008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x153009060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1530094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x153009940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x153009db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15300a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15300a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15300ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15300b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15300b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15300c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15300c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15300c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15300cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15300d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15300dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15300e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15300e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15300ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15300f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15300f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15300fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1530102b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153010860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153010e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1530113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153011970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153011f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1530124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153012a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153013030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1530135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153013b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153014140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1530146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153014ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153015250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153015800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153015db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153016360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153016910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153016ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x153017470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153017a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153017fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153018580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x153018b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1530190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153019690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153019c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15301a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15301a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15301ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15301b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15301b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15301be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15301c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15301c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15301cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15301d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15301dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15301e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15301e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15301ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15301f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15301f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15301fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1530202a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1530207a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153020ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1530211a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1530216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153021ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1530220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1530225a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153022aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153022fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1530234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1530239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153023ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1530243a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1530248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153024da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1530257b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153025ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1530265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153026d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153026fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1530277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153027a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x153028090 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.097.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153011c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15300f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15301e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15301c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153019f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153017ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15300ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153012790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15300d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x153018df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15301d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153018840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1530132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x153019950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153016620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1530121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15300e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15300eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x153025060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x153011680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x153010b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15301dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15301ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15300dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15301a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x153013e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15300cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x153028f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1530293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x153029840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x153029d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15302a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x153012d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15302a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15302acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15302b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15302b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15302bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15302c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15302c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15302cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15302d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15302d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15302daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15302dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15302e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15302e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15302ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15302f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15302f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15302fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15302feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153030320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153030790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x153030c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1530313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153031870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x153031b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153032140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153032930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153032dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153033270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x153033710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153033bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x153034050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1530344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153034990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153034e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1530352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x153035770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x153035c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1530360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x153036550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153036aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x153036ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x153037540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153037a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x153037fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x153038530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153038a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x153038fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153039520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x153039a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x153039fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15303a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15303aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15303afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15303b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15303ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15303bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15303c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15303ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15303cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15303d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15303da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15303df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15303e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15303ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15303ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15303f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15303fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15303ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1530404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153040a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153040f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1530414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1530419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153041f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x153042490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1530429e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153042f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153043480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1530439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153043e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153044310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1530447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153044c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1530450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153045590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x153045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153045ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153046370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153046810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153046cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153047150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1530475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x153047a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x153047f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1530483d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x153048870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x153048d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1530491b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153049650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153049af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153049f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15304a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15304a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15304ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15304b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15304b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15304bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15304bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15304c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15304c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15304cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15304d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15304d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15304dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15304e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15304e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15304e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15304ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15304f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15304f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15304fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1530500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153050550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1530509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153050e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153051330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1530517d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153051c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153052110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1530525b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153052a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153052ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153053390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153053830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153053cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153054170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153054610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153054ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153054f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1530553f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153055890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153055d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1530561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153056670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153056b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153056fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153057450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1530578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153057d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153058230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1530586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153058b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153059010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1530594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x153059950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x153059df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15305a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15305a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15305abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15305b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15305b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15305bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15305c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15305c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15305c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15305cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15305d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15305ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15305e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15305e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15305eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15305f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15305f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15305fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1530602a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153060740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153060ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153061440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153061990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153061ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153062430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153062980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x153062ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153063420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x153063970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153063ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153064410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153064960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153064eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153065400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153065950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153065ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1530663f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153066940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153066e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1530673e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x153067930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153067e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1530683d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153068920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x153068e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1530693c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153069910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153069e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15306a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15306a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15306ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15306b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15306b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15306be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15306c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15306c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15306ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15306d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15306d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15306de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15306e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15306e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15306ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15306f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15306f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15306fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153070350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1530708a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x153070df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153071340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153071890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153071de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153072330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153072880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153072dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153073320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153073870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153073d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1530741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153074650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153074af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153074f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153075430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1530758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153075d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153076210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1530766b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153076b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153076ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153077490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x153077930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153077dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153078320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153078a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153079160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153079880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153079fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15307a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15307aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15307ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15307b320 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1517202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1517208d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1517228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x151717c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15171e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15171f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15171f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15171e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15171db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151720ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15171fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15170cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151722ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151719e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15171a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151718550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x151718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151748590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x151748850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151748b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x151748dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151749090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151749350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x151749610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1517498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151749b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151749e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15174a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15174a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15174a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15174a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15174ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15174aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15174b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15174b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15174b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15174b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15174bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15174bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15174c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15174c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15174c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15174ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15174cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15174cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15174d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15174d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15174d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15174dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15174dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15174e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15174e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15174e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15174e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15174eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15174ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15174f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15174f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15174f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15174f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15174fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15174fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x151750410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1517506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151750990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151750c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151750f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1517511d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151751490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151751750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x153014f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1530193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1530138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x153015ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15301fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15301f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x153016bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15301aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x153018290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15300fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15301cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x153017730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15301eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x153014400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x153010570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x153017180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15305e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15305cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15307afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15305c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15305d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x153031df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15305ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153028c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15302a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153032400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15307a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153030ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15305f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15305d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15307b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15307ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15307bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15307bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15307c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15307c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15307c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15307cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15307cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15307d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15307d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15307d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15307d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15307db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15307de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15307e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15307e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15307e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15307e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15307ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15307ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15307f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15307f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15307f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15307f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15307fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15307ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1530801c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x153080480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x153080740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x153080a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x153080cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x153080f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x153081240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x153081500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1530817c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x153081a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x153081d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x153082000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1530822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x153082580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x153082840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x153082b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x153082dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153083080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153083340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153083600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1530838c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x153083b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153083e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153084100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1530843c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153084680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x153084940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153084c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x153084ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153085180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153085440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153085700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1530859c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153085c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153085f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x153086200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1530864c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153086780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153086a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x153086d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153086fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153087280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x153087540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153087800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x153087ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153087d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x153088040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153088300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1530885c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153088880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153088b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x153088e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1530890c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x153089380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x153089640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x153089900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x153089bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x153089e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15308a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15308a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15308a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15308a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15308ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15308af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15308b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15308b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15308b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15308ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15308bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15308bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15308c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15308c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15308c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15308ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15308cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15308d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15308d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15308d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15308d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15308db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15308e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15308e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15308e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15308e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15308ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15308ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15308f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15308f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15308f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15308f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15308fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15308ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1530901d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x153090490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153090750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x153090a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153090cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x153090f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153091250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153091510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1530917d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153091a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153091d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153092010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1530922d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153092590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153092850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153092b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153092dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153093090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x153093350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x153093610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1530938d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x153093b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x153093e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x153094110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1530943d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x153094690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x153094950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x153094c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x153094ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x153095190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x153095450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x153095710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1530959d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x153095c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x153095f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x153096210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1530964d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x153096790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x153096a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153096d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x153096fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153097290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153097550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153097810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x153097ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153097d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153098050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153098310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1530985d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153098890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153098b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x153098e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1530990d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153099390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153099650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153099910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153099bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153099e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15309a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15309a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15309a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15309a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15309ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15309af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15309b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15309b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15309b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15309ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15309bcd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.940s
user	0m0.229s
sys	0m0.187s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
