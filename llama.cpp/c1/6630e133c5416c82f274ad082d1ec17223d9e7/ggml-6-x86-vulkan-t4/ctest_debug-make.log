++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building CXX object ggml/src/ggml-vulkan/vulkan-shaders/CMakeFiles/vulkan-shaders-gen.dir/vulkan-shaders-gen.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.43.0") 
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Built target sha256
[  2%] Linking CXX executable ../../../../bin/vulkan-shaders-gen
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target xxhash
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Linking CXX shared library libggml-base.so
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target vulkan-shaders-gen
[  6%] Built target ggml-base
[  7%] Generate vulkan shaders
ggml_vulkan: Generating and compiling shaders to SPIR-V
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/ggml-vulkan/CMakeFiles/ggml-vulkan.dir/ggml-vulkan.cpp.o
[ 13%] Building CXX object ggml/src/ggml-vulkan/CMakeFiles/ggml-vulkan.dir/ggml-vulkan-shaders.cpp.o
[ 13%] Linking CXX shared library libggml-vulkan.so
[ 13%] Built target ggml-vulkan
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.so
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 17%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Built target llama-gguf-hash
[ 20%] Built target llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In member function ‘ggml_tensor* llama_model_loader::create_tensor_as_view(ggml_context*, ggml_tensor*, const std::string&, const std::initializer_list<long int>&, size_t, bool)’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:690:40: error: aggregate ‘std::array<long int, 4> dims’ has incomplete type and cannot be defined
  690 |     std::array<int64_t, GGML_MAX_DIMS> dims;
      |                                        ^~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In member function ‘void llama_model_loader::load_data_for(ggml_tensor*) const’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:762:13: error: ‘memcpy’ was not declared in this scope
  762 |             memcpy(cur->data, (uint8_t *)mapping->addr() + w.offs, ggml_nbytes(cur));
      |             ^~~~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:7:1: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    6 | #include <future>
  +++ |+#include <cstring>
    7 | 
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = int; long unsigned int N_MAX = 4; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<int, 4>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:363:145:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:351:19: error: no match for ‘operator[]’ (operand types are ‘std::array<int, 4>’ and ‘uint32_t’ {aka ‘unsigned int’})
  351 |             result[i] = value;
      |             ~~~~~~^
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = unsigned int; long unsigned int N_MAX = 512; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<unsigned int, 512>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:364:159:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:351:19: error: no match for ‘operator[]’ (operand types are ‘std::array<unsigned int, 512>’ and ‘uint32_t’ {aka ‘unsigned int’})
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_arr(const std::string&, std::array<T, N_MAX>&, bool) [with T = int; long unsigned int N_MAX = 4; std::string = std::__cxx11::basic_string<char>]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:340:27:   required from ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = int; long unsigned int N_MAX = 4; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<int, 4>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:363:145:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:268:95: error: invalid use of incomplete type ‘struct std::array<int, 4>’
  268 |         std::copy((const T*)arr_info.data, (const T *)arr_info.data + arr_info.length, result.begin());
      |                                                                                        ~~~~~~~^~~~~
In file included from /usr/include/c++/13/bits/memory_resource.h:47,
                 from /usr/include/c++/13/string:58,
                 from /home/ggml/work/llama.cpp/src/llama-impl.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-model-loader.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-model-loader.cpp:1:
/usr/include/c++/13/tuple:2019:45: note: declaration of ‘struct std::array<int, 4>’
 2019 |   template<typename _Tp, size_t _Nm> struct array;
      |                                             ^~~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_arr(const std::string&, std::array<T, N_MAX>&, bool) [with T = unsigned int; long unsigned int N_MAX = 512; std::string = std::__cxx11::basic_string<char>]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:340:27:   required from ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = unsigned int; long unsigned int N_MAX = 512; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<unsigned int, 512>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:364:159:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:268:95: error: invalid use of incomplete type ‘struct std::array<unsigned int, 512>’
  268 |         std::copy((const T*)arr_info.data, (const T *)arr_info.data + arr_info.length, result.begin());
      |                                                                                        ~~~~~~~^~~~~
/usr/include/c++/13/tuple:2019:45: note: declaration of ‘struct std::array<unsigned int, 512>’
 2019 |   template<typename _Tp, size_t _Nm> struct array;
      |                                             ^~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:244: src/CMakeFiles/llama.dir/llama-model-loader.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1857: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m49.936s
user	2m9.846s
sys	0m25.413s
