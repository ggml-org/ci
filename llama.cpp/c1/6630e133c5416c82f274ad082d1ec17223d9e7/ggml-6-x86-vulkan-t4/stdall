Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/cpu
Requirement already satisfied: numpy~=1.26.4 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)
Requirement already satisfied: sentencepiece~=0.2.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.0)
Requirement already satisfied: transformers<5.0.0,>=4.45.1 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.47.0)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.13.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 5)) (4.25.5)
Requirement already satisfied: torch~=2.2.1 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2.2.2+cpu)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.16.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.26.5)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (24.2)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (6.0.2)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.11.6)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.32.3)
Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.21.0)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (0.4.5)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (4.67.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (4.12.2)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.13.3)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.4.2)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.1.4)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (2024.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from jinja2->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (3.0.2)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.4.0)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.45.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_legacy_llama.txt (line 3)) (2024.8.30)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from sympy->torch~=2.2.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert_hf_to_gguf.txt (line 3)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (1.26.4)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (6.0.2)
Requirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (0.2.0)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.12/site-packages (from gguf==0.13.0) (4.67.1)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.13.0-py3-none-any.whl size=3396 sha256=860f734d9faba95b979f0fdf4248b4dd431b6188bebb734489ea7d6f3b88c2e9
  Stored in directory: /tmp/pip-ephem-wheel-cache-byzvdmze/wheels/b2/bf/09/4e7df5812cefd3c9f9cc38b7d737712d94c9c25193c894bb1c
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.13.0
    Uninstalling gguf-0.13.0:
      Successfully uninstalled gguf-0.13.0
Successfully installed gguf-0.13.0
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ tee /home/ggml/results/llama.cpp/c1/6630e133c5416c82f274ad082d1ec17223d9e7/ggml-6-x86-vulkan-t4/ctest_debug.log
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ gg_check_build_requirements
+ command -v cmake
+ command -v make
+ command -v ctest
+ tee -a /home/ggml/results/llama.cpp/c1/6630e133c5416c82f274ad082d1ec17223d9e7/ggml-6-x86-vulkan-t4/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DGGML_VULKAN=1 ..
-- The C compiler identification is GNU 13.3.0
-- The CXX compiler identification is GNU 13.3.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.43.0") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- x86 detected
-- Adding CPU backend variant ggml-cpu: -march=native 
-- Found Vulkan: /usr/lib/x86_64-linux-gnu/libvulkan.so (found version "1.3.296") found components: glslc glslangValidator 
-- Vulkan found
-- GL_NV_cooperative_matrix2 not supported by glslc
-- Including Vulkan backend
-- Configuring done (1.2s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m1.449s
user	0m0.868s
sys	0m0.586s
+ tee -a /home/ggml/results/llama.cpp/c1/6630e133c5416c82f274ad082d1ec17223d9e7/ggml-6-x86-vulkan-t4/ctest_debug-make.log
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building CXX object ggml/src/ggml-vulkan/vulkan-shaders/CMakeFiles/vulkan-shaders-gen.dir/vulkan-shaders-gen.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.43.0") 
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Built target sha256
[  2%] Linking CXX executable ../../../../bin/vulkan-shaders-gen
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target xxhash
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  5%] Linking CXX shared library libggml-base.so
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target vulkan-shaders-gen
[  6%] Built target ggml-base
[  7%] Generate vulkan shaders
ggml_vulkan: Generating and compiling shaders to SPIR-V
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/ggml-vulkan/CMakeFiles/ggml-vulkan.dir/ggml-vulkan.cpp.o
[ 13%] Building CXX object ggml/src/ggml-vulkan/CMakeFiles/ggml-vulkan.dir/ggml-vulkan-shaders.cpp.o
[ 13%] Linking CXX shared library libggml-vulkan.so
[ 13%] Built target ggml-vulkan
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.so
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 17%] Linking CXX executable ../../bin/llama-gguf
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Built target llama-gguf-hash
[ 20%] Built target llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In member function ‘ggml_tensor* llama_model_loader::create_tensor_as_view(ggml_context*, ggml_tensor*, const std::string&, const std::initializer_list<long int>&, size_t, bool)’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:690:40: error: aggregate ‘std::array<long int, 4> dims’ has incomplete type and cannot be defined
  690 |     std::array<int64_t, GGML_MAX_DIMS> dims;
      |                                        ^~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In member function ‘void llama_model_loader::load_data_for(ggml_tensor*) const’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:762:13: error: ‘memcpy’ was not declared in this scope
  762 |             memcpy(cur->data, (uint8_t *)mapping->addr() + w.offs, ggml_nbytes(cur));
      |             ^~~~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:7:1: note: ‘memcpy’ is defined in header ‘<cstring>’; did you forget to ‘#include <cstring>’?
    6 | #include <future>
  +++ |+#include <cstring>
    7 | 
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = int; long unsigned int N_MAX = 4; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<int, 4>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:363:145:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:351:19: error: no match for ‘operator[]’ (operand types are ‘std::array<int, 4>’ and ‘uint32_t’ {aka ‘unsigned int’})
  351 |             result[i] = value;
      |             ~~~~~~^
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = unsigned int; long unsigned int N_MAX = 512; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<unsigned int, 512>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:364:159:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:351:19: error: no match for ‘operator[]’ (operand types are ‘std::array<unsigned int, 512>’ and ‘uint32_t’ {aka ‘unsigned int’})
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_arr(const std::string&, std::array<T, N_MAX>&, bool) [with T = int; long unsigned int N_MAX = 4; std::string = std::__cxx11::basic_string<char>]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:340:27:   required from ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = int; long unsigned int N_MAX = 4; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<int, 4>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:363:145:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:268:95: error: invalid use of incomplete type ‘struct std::array<int, 4>’
  268 |         std::copy((const T*)arr_info.data, (const T *)arr_info.data + arr_info.length, result.begin());
      |                                                                                        ~~~~~~~^~~~~
In file included from /usr/include/c++/13/bits/memory_resource.h:47,
                 from /usr/include/c++/13/string:58,
                 from /home/ggml/work/llama.cpp/src/llama-impl.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-model-loader.h:5,
                 from /home/ggml/work/llama.cpp/src/llama-model-loader.cpp:1:
/usr/include/c++/13/tuple:2019:45: note: declaration of ‘struct std::array<int, 4>’
 2019 |   template<typename _Tp, size_t _Nm> struct array;
      |                                             ^~~~~
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp: In instantiation of ‘bool llama_model_loader::get_arr(const std::string&, std::array<T, N_MAX>&, bool) [with T = unsigned int; long unsigned int N_MAX = 512; std::string = std::__cxx11::basic_string<char>]’:
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:340:27:   required from ‘bool llama_model_loader::get_key_or_arr(const std::string&, std::array<T, N_MAX>&, uint32_t, bool) [with T = unsigned int; long unsigned int N_MAX = 512; std::string = std::__cxx11::basic_string<char>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:359:30:   required from ‘bool llama_model_loader::get_key_or_arr(llm_kv, T&, uint32_t, bool) [with T = std::array<unsigned int, 512>; uint32_t = unsigned int]’
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:364:159:   required from here
/home/ggml/work/llama.cpp/src/llama-model-loader.cpp:268:95: error: invalid use of incomplete type ‘struct std::array<unsigned int, 512>’
  268 |         std::copy((const T*)arr_info.data, (const T *)arr_info.data + arr_info.length, result.begin());
      |                                                                                        ~~~~~~~^~~~~
/usr/include/c++/13/tuple:2019:45: note: declaration of ‘struct std::array<unsigned int, 512>’
 2019 |   template<typename _Tp, size_t _Nm> struct array;
      |                                             ^~~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:244: src/CMakeFiles/llama.dir/llama-model-loader.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
make[1]: *** [CMakeFiles/Makefile2:1857: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m49.936s
user	2m9.846s
sys	0m25.413s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/c1/6630e133c5416c82f274ad082d1ec17223d9e7/ggml-6-x86-vulkan-t4/ctest_debug-ctest.log: No such file or directory
