Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.056s
user	0m1.029s
sys	0m1.456s
++ nproc
+ make -j10
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Built target xxhash
[  4%] Built target sha1
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Built target llava
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llama-simple-chat
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 45%] Linking CXX executable ../bin/test-chat
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-log
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-integration
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-arg-parser
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 61%] Linking CXX executable ../bin/test-backend-ops
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-barrier
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-gguf
[ 63%] Built target test-autorelease
[ 63%] Built target test-chat-template
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-barrier
[ 63%] Built target test-arg-parser
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 63%] Built target test-backend-ops
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 70%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched-bench
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-embedding
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-batched
[ 73%] Built target llama-infill
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-bench
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookup-create
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-quantize
[ 81%] Generating loading.html.hpp
[ 81%] Built target llama-passkey
[ 81%] Built target llama-parallel
[ 81%] Built target llama-cli
[ 82%] Generating index.html.gz.hpp
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Linking CXX executable ../../bin/llama-gen-docs
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-retrieval
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-speculative-simple
[ 91%] Built target llama-run
[ 91%] Built target llama-gen-docs
[ 91%] Built target llama-tokenize
[ 91%] Built target llama-speculative
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 94%] Built target llama-convert-llama2c-to-ggml
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.187s
user	0m6.459s
sys	0m10.515s

main: quantize time =  5985.17 ms
main:    total time =  5985.17 ms

main: quantize time =  1965.94 ms
main:    total time =  1965.94 ms

main: quantize time =  1975.74 ms
main:    total time =  1975.74 ms

main: quantize time =  1980.83 ms
main:    total time =  1980.83 ms

main: quantize time =  2274.95 ms
main:    total time =  2274.95 ms

main: quantize time =  5693.24 ms
main:    total time =  5693.24 ms

main: quantize time =  5995.44 ms
main:    total time =  5995.44 ms

main: quantize time =  7139.92 ms
main:    total time =  7139.92 ms

main: quantize time =  6829.47 ms
main:    total time =  6829.47 ms

main: quantize time =  4548.08 ms
main:    total time =  4548.08 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.191 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.365 I main: llama backend init
0.00.000.371 I main: load the model and apply lora adapter, if any
0.00.075.443 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.088.092 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.088.109 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.088.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.088.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.088.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.088.115 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.088.115 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.088.117 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.088.118 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.088.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.088.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.088.119 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.088.120 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.088.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.088.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.088.126 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.088.127 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.094.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.097.056 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.103.807 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.103.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.103.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.103.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.103.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.103.816 I llama_model_loader: - type  f32:  194 tensors
0.00.103.817 I llama_model_loader: - type  f16:   98 tensors
0.00.103.822 I print_info: file format = GGUF V3 (latest)
0.00.103.824 I print_info: file type   = all F32 (guessed)
0.00.103.827 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.122.111 I load: special tokens cache size = 25
0.00.132.702 I load: token to piece cache size = 0.2984 MB
0.00.132.707 I print_info: arch             = gptneox
0.00.132.707 I print_info: vocab_only       = 0
0.00.132.708 I print_info: n_ctx_train      = 2048
0.00.132.708 I print_info: n_embd           = 2048
0.00.132.708 I print_info: n_layer          = 24
0.00.132.714 I print_info: n_head           = 16
0.00.132.716 I print_info: n_head_kv        = 16
0.00.132.716 I print_info: n_rot            = 32
0.00.132.716 I print_info: n_swa            = 0
0.00.132.716 I print_info: n_embd_head_k    = 128
0.00.132.716 I print_info: n_embd_head_v    = 128
0.00.132.717 I print_info: n_gqa            = 1
0.00.132.718 I print_info: n_embd_k_gqa     = 2048
0.00.132.719 I print_info: n_embd_v_gqa     = 2048
0.00.132.720 I print_info: f_norm_eps       = 1.0e-05
0.00.132.722 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.132.725 I print_info: f_clamp_kqv      = 0.0e+00
0.00.132.725 I print_info: f_max_alibi_bias = 0.0e+00
0.00.132.725 I print_info: f_logit_scale    = 0.0e+00
0.00.132.726 I print_info: n_ff             = 8192
0.00.132.727 I print_info: n_expert         = 0
0.00.132.727 I print_info: n_expert_used    = 0
0.00.132.727 I print_info: causal attn      = 1
0.00.132.727 I print_info: pooling type     = 0
0.00.132.727 I print_info: rope type        = 2
0.00.132.727 I print_info: rope scaling     = linear
0.00.132.728 I print_info: freq_base_train  = 10000.0
0.00.132.731 I print_info: freq_scale_train = 1
0.00.132.731 I print_info: n_ctx_orig_yarn  = 2048
0.00.132.731 I print_info: rope_finetuned   = unknown
0.00.132.732 I print_info: ssm_d_conv       = 0
0.00.132.732 I print_info: ssm_d_inner      = 0
0.00.132.732 I print_info: ssm_d_state      = 0
0.00.132.732 I print_info: ssm_dt_rank      = 0
0.00.132.732 I print_info: ssm_dt_b_c_rms   = 0
0.00.132.733 I print_info: model type       = 1.4B
0.00.132.733 I print_info: model params     = 1.41 B
0.00.132.733 I print_info: general.name     = 1.4B
0.00.132.734 I print_info: vocab type       = BPE
0.00.132.734 I print_info: n_vocab          = 50304
0.00.132.739 I print_info: n_merges         = 50009
0.00.132.740 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.132.740 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.132.740 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.132.740 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.132.741 I print_info: LF token         = 187 'Ċ'
0.00.132.741 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.132.742 I print_info: max token length = 1024
0.00.132.742 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.197.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.197.968 I load_tensors: offloading output layer to GPU
0.00.197.968 I load_tensors: offloaded 25/25 layers to GPU
0.00.197.995 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.197.996 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.198.606 I llama_init_from_model: n_seq_max     = 1
0.00.198.607 I llama_init_from_model: n_ctx         = 2048
0.00.198.607 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.198.607 I llama_init_from_model: n_batch       = 2048
0.00.198.607 I llama_init_from_model: n_ubatch      = 512
0.00.198.607 I llama_init_from_model: flash_attn    = 0
0.00.198.608 I llama_init_from_model: freq_base     = 10000.0
0.00.198.608 I llama_init_from_model: freq_scale    = 1
0.00.198.609 I ggml_metal_init: allocating
0.00.198.644 I ggml_metal_init: found device: Apple M4
0.00.198.654 I ggml_metal_init: picking default device: Apple M4
0.00.199.306 I ggml_metal_init: using embedded metal library
0.00.211.004 I ggml_metal_init: GPU name:   Apple M4
0.00.211.006 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.211.006 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.211.006 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.211.007 I ggml_metal_init: simdgroup reduction   = true
0.00.211.007 I ggml_metal_init: simdgroup matrix mul. = true
0.00.211.007 I ggml_metal_init: has residency sets    = true
0.00.211.007 I ggml_metal_init: has bfloat            = true
0.00.211.007 I ggml_metal_init: use bfloat            = true
0.00.211.007 I ggml_metal_init: hasUnifiedMemory      = true
0.00.211.008 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.238.994 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.269.497 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.269.504 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.269.548 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.273.585 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.273.587 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.273.587 I llama_init_from_model: graph nodes  = 967
0.00.273.588 I llama_init_from_model: graph splits = 2
0.00.273.592 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.273.725 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.273.726 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.337.867 I main: llama threadpool init, n_threads = 4
0.00.337.911 I 
0.00.337.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.337.944 I 
0.00.338.126 I sampler seed: 1234
0.00.338.131 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.338.155 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.338.157 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.338.157 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.178.370 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.02.178.371 I llama_perf_context_print:        load time =     261.55 ms
0.02.178.372 I llama_perf_context_print: prompt eval time =      53.61 ms /     7 tokens (    7.66 ms per token,   130.57 tokens per second)
0.02.178.374 I llama_perf_context_print:        eval time =    1783.78 ms /    63 runs   (   28.31 ms per token,    35.32 tokens per second)
0.02.178.374 I llama_perf_context_print:       total time =    1841.37 ms /    70 tokens
0.02.178.599 I ggml_metal_free: deallocating

real	0m2.504s
user	0m0.134s
sys	0m0.160s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.092 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.009.923 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.214 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.220 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.223 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.226 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.228 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.228 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.229 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.229 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.229 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.230 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.232 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.233 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.233 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.269 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.271 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.271 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.271 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.272 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.272 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.272 I llama_model_loader: - type  f32:  194 tensors
0.00.040.273 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.274 I print_info: file format = GGUF V3 (latest)
0.00.040.274 I print_info: file type   = Q8_0
0.00.040.276 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.049.525 I load: special tokens cache size = 25
0.00.057.008 I load: token to piece cache size = 0.2984 MB
0.00.057.013 I print_info: arch             = gptneox
0.00.057.013 I print_info: vocab_only       = 0
0.00.057.013 I print_info: n_ctx_train      = 2048
0.00.057.013 I print_info: n_embd           = 2048
0.00.057.014 I print_info: n_layer          = 24
0.00.057.019 I print_info: n_head           = 16
0.00.057.020 I print_info: n_head_kv        = 16
0.00.057.020 I print_info: n_rot            = 32
0.00.057.021 I print_info: n_swa            = 0
0.00.057.021 I print_info: n_embd_head_k    = 128
0.00.057.021 I print_info: n_embd_head_v    = 128
0.00.057.022 I print_info: n_gqa            = 1
0.00.057.022 I print_info: n_embd_k_gqa     = 2048
0.00.057.023 I print_info: n_embd_v_gqa     = 2048
0.00.057.024 I print_info: f_norm_eps       = 1.0e-05
0.00.057.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.024 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.025 I print_info: f_logit_scale    = 0.0e+00
0.00.057.025 I print_info: n_ff             = 8192
0.00.057.026 I print_info: n_expert         = 0
0.00.057.026 I print_info: n_expert_used    = 0
0.00.057.026 I print_info: causal attn      = 1
0.00.057.026 I print_info: pooling type     = 0
0.00.057.026 I print_info: rope type        = 2
0.00.057.026 I print_info: rope scaling     = linear
0.00.057.027 I print_info: freq_base_train  = 10000.0
0.00.057.027 I print_info: freq_scale_train = 1
0.00.057.027 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.028 I print_info: rope_finetuned   = unknown
0.00.057.028 I print_info: ssm_d_conv       = 0
0.00.057.028 I print_info: ssm_d_inner      = 0
0.00.057.028 I print_info: ssm_d_state      = 0
0.00.057.028 I print_info: ssm_dt_rank      = 0
0.00.057.028 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.029 I print_info: model type       = 1.4B
0.00.057.033 I print_info: model params     = 1.41 B
0.00.057.033 I print_info: general.name     = 1.4B
0.00.057.034 I print_info: vocab type       = BPE
0.00.057.034 I print_info: n_vocab          = 50304
0.00.057.034 I print_info: n_merges         = 50009
0.00.057.034 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.034 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.035 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.035 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.035 I print_info: LF token         = 187 'Ċ'
0.00.057.035 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.036 I print_info: max token length = 1024
0.00.057.036 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.163.613 I load_tensors: offloading 24 repeating layers to GPU
0.01.163.618 I load_tensors: offloading output layer to GPU
0.01.163.620 I load_tensors: offloaded 25/25 layers to GPU
0.01.163.645 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.163.646 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.164.924 I llama_init_from_model: n_seq_max     = 1
0.01.164.926 I llama_init_from_model: n_ctx         = 2048
0.01.164.927 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.164.928 I llama_init_from_model: n_batch       = 2048
0.01.164.928 I llama_init_from_model: n_ubatch      = 512
0.01.164.928 I llama_init_from_model: flash_attn    = 0
0.01.164.929 I llama_init_from_model: freq_base     = 10000.0
0.01.164.930 I llama_init_from_model: freq_scale    = 1
0.01.164.931 I ggml_metal_init: allocating
0.01.164.946 I ggml_metal_init: found device: Apple M4
0.01.164.955 I ggml_metal_init: picking default device: Apple M4
0.01.166.255 I ggml_metal_init: using embedded metal library
0.01.172.171 I ggml_metal_init: GPU name:   Apple M4
0.01.172.175 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.172.176 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.172.176 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.172.177 I ggml_metal_init: simdgroup reduction   = true
0.01.172.177 I ggml_metal_init: simdgroup matrix mul. = true
0.01.172.177 I ggml_metal_init: has residency sets    = true
0.01.172.178 I ggml_metal_init: has bfloat            = true
0.01.172.178 I ggml_metal_init: use bfloat            = true
0.01.172.179 I ggml_metal_init: hasUnifiedMemory      = true
0.01.172.180 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.191.148 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.247.522 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.247.532 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.247.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.251.931 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.251.934 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.251.934 I llama_init_from_model: graph nodes  = 967
0.01.251.934 I llama_init_from_model: graph splits = 2
0.01.251.940 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.252.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.252.064 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.306.906 I main: llama threadpool init, n_threads = 4
0.01.306.950 I 
0.01.306.973 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.306.973 I 
0.01.307.116 I sampler seed: 1234
0.01.307.120 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.307.130 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.307.130 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.307.130 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.413.828 I llama_perf_sampler_print:    sampling time =       1.53 ms /    71 runs   (    0.02 ms per token, 46344.65 tokens per second)
0.02.413.828 I llama_perf_context_print:        load time =    1296.27 ms
0.02.413.829 I llama_perf_context_print: prompt eval time =      49.27 ms /     7 tokens (    7.04 ms per token,   142.08 tokens per second)
0.02.413.830 I llama_perf_context_print:        eval time =    1054.64 ms /    63 runs   (   16.74 ms per token,    59.74 tokens per second)
0.02.413.831 I llama_perf_context_print:       total time =    1107.63 ms /    70 tokens
0.02.414.070 I ggml_metal_free: deallocating

real	0m2.432s
user	0m0.114s
sys	0m0.267s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.003 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.950 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.027.955 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.956 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.957 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.957 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.957 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.959 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.960 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.960 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.961 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.961 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.962 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.965 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.965 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.854 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.930 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.793 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.794 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.794 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.795 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.795 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.795 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.796 I llama_model_loader: - type  f32:  194 tensors
0.00.036.796 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.796 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.797 I print_info: file format = GGUF V3 (latest)
0.00.036.797 I print_info: file type   = Q4_0
0.00.036.798 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.251 I load: special tokens cache size = 25
0.00.051.365 I load: token to piece cache size = 0.2984 MB
0.00.051.368 I print_info: arch             = gptneox
0.00.051.368 I print_info: vocab_only       = 0
0.00.051.368 I print_info: n_ctx_train      = 2048
0.00.051.368 I print_info: n_embd           = 2048
0.00.051.369 I print_info: n_layer          = 24
0.00.051.372 I print_info: n_head           = 16
0.00.051.373 I print_info: n_head_kv        = 16
0.00.051.373 I print_info: n_rot            = 32
0.00.051.373 I print_info: n_swa            = 0
0.00.051.373 I print_info: n_embd_head_k    = 128
0.00.051.374 I print_info: n_embd_head_v    = 128
0.00.051.375 I print_info: n_gqa            = 1
0.00.051.376 I print_info: n_embd_k_gqa     = 2048
0.00.051.377 I print_info: n_embd_v_gqa     = 2048
0.00.051.377 I print_info: f_norm_eps       = 1.0e-05
0.00.051.378 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.378 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.378 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.378 I print_info: f_logit_scale    = 0.0e+00
0.00.051.379 I print_info: n_ff             = 8192
0.00.051.380 I print_info: n_expert         = 0
0.00.051.380 I print_info: n_expert_used    = 0
0.00.051.382 I print_info: causal attn      = 1
0.00.051.382 I print_info: pooling type     = 0
0.00.051.384 I print_info: rope type        = 2
0.00.051.384 I print_info: rope scaling     = linear
0.00.051.384 I print_info: freq_base_train  = 10000.0
0.00.051.385 I print_info: freq_scale_train = 1
0.00.051.388 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.389 I print_info: rope_finetuned   = unknown
0.00.051.389 I print_info: ssm_d_conv       = 0
0.00.051.389 I print_info: ssm_d_inner      = 0
0.00.051.389 I print_info: ssm_d_state      = 0
0.00.051.389 I print_info: ssm_dt_rank      = 0
0.00.051.390 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.390 I print_info: model type       = 1.4B
0.00.051.390 I print_info: model params     = 1.41 B
0.00.051.390 I print_info: general.name     = 1.4B
0.00.051.391 I print_info: vocab type       = BPE
0.00.051.391 I print_info: n_vocab          = 50304
0.00.051.392 I print_info: n_merges         = 50009
0.00.051.392 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.392 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.393 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.393 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.393 I print_info: LF token         = 187 'Ċ'
0.00.051.393 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.393 I print_info: max token length = 1024
0.00.051.394 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.627.957 I load_tensors: offloading 24 repeating layers to GPU
0.00.627.972 I load_tensors: offloading output layer to GPU
0.00.627.973 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.005 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.628.009 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.629.473 I llama_init_from_model: n_seq_max     = 1
0.00.629.476 I llama_init_from_model: n_ctx         = 2048
0.00.629.477 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.629.478 I llama_init_from_model: n_batch       = 2048
0.00.629.478 I llama_init_from_model: n_ubatch      = 512
0.00.629.479 I llama_init_from_model: flash_attn    = 0
0.00.629.481 I llama_init_from_model: freq_base     = 10000.0
0.00.629.482 I llama_init_from_model: freq_scale    = 1
0.00.629.484 I ggml_metal_init: allocating
0.00.629.556 I ggml_metal_init: found device: Apple M4
0.00.629.569 I ggml_metal_init: picking default device: Apple M4
0.00.631.531 I ggml_metal_init: using embedded metal library
0.00.637.124 I ggml_metal_init: GPU name:   Apple M4
0.00.637.129 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.130 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.131 I ggml_metal_init: simdgroup reduction   = true
0.00.637.132 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.132 I ggml_metal_init: has residency sets    = true
0.00.637.132 I ggml_metal_init: has bfloat            = true
0.00.637.132 I ggml_metal_init: use bfloat            = true
0.00.637.134 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.656.873 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.713.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.713.199 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.713.234 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.717.618 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.717.621 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.717.621 I llama_init_from_model: graph nodes  = 967
0.00.717.621 I llama_init_from_model: graph splits = 2
0.00.717.627 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.717.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.738 I main: llama threadpool init, n_threads = 4
0.00.769.779 I 
0.00.769.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.801 I 
0.00.769.910 I sampler seed: 1234
0.00.769.915 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.925 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.926 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.926 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.465.036 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.465.037 I llama_perf_context_print:        load time =     759.02 ms
0.01.465.038 I llama_perf_context_print: prompt eval time =      49.02 ms /     7 tokens (    7.00 ms per token,   142.80 tokens per second)
0.01.465.039 I llama_perf_context_print:        eval time =     643.27 ms /    63 runs   (   10.21 ms per token,    97.94 tokens per second)
0.01.465.039 I llama_perf_context_print:       total time =     696.01 ms /    70 tokens
0.01.465.272 I ggml_metal_free: deallocating

real	0m1.482s
user	0m0.110s
sys	0m0.199s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.826 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.572 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.582 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.584 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.585 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.585 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.585 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.587 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.587 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.587 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.588 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.588 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.589 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.590 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.591 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.591 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.521 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.641 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.809 I llama_model_loader: - type  f32:  194 tensors
0.00.036.809 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.809 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.810 I print_info: file format = GGUF V3 (latest)
0.00.036.810 I print_info: file type   = Q4_1
0.00.036.811 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.759 I load: special tokens cache size = 25
0.00.053.334 I load: token to piece cache size = 0.2984 MB
0.00.053.337 I print_info: arch             = gptneox
0.00.053.337 I print_info: vocab_only       = 0
0.00.053.337 I print_info: n_ctx_train      = 2048
0.00.053.338 I print_info: n_embd           = 2048
0.00.053.338 I print_info: n_layer          = 24
0.00.053.340 I print_info: n_head           = 16
0.00.053.341 I print_info: n_head_kv        = 16
0.00.053.342 I print_info: n_rot            = 32
0.00.053.344 I print_info: n_swa            = 0
0.00.053.344 I print_info: n_embd_head_k    = 128
0.00.053.344 I print_info: n_embd_head_v    = 128
0.00.053.345 I print_info: n_gqa            = 1
0.00.053.345 I print_info: n_embd_k_gqa     = 2048
0.00.053.346 I print_info: n_embd_v_gqa     = 2048
0.00.053.347 I print_info: f_norm_eps       = 1.0e-05
0.00.053.347 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.347 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.348 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.348 I print_info: f_logit_scale    = 0.0e+00
0.00.053.348 I print_info: n_ff             = 8192
0.00.053.349 I print_info: n_expert         = 0
0.00.053.349 I print_info: n_expert_used    = 0
0.00.053.349 I print_info: causal attn      = 1
0.00.053.349 I print_info: pooling type     = 0
0.00.053.349 I print_info: rope type        = 2
0.00.053.350 I print_info: rope scaling     = linear
0.00.053.350 I print_info: freq_base_train  = 10000.0
0.00.053.352 I print_info: freq_scale_train = 1
0.00.053.352 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.352 I print_info: rope_finetuned   = unknown
0.00.053.352 I print_info: ssm_d_conv       = 0
0.00.053.352 I print_info: ssm_d_inner      = 0
0.00.053.353 I print_info: ssm_d_state      = 0
0.00.053.353 I print_info: ssm_dt_rank      = 0
0.00.053.353 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.353 I print_info: model type       = 1.4B
0.00.053.353 I print_info: model params     = 1.41 B
0.00.053.354 I print_info: general.name     = 1.4B
0.00.053.354 I print_info: vocab type       = BPE
0.00.053.354 I print_info: n_vocab          = 50304
0.00.053.354 I print_info: n_merges         = 50009
0.00.053.359 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.359 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.359 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.360 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.360 I print_info: LF token         = 187 'Ċ'
0.00.053.360 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.361 I print_info: max token length = 1024
0.00.053.361 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.709.732 I load_tensors: offloading 24 repeating layers to GPU
0.00.709.753 I load_tensors: offloading output layer to GPU
0.00.709.754 I load_tensors: offloaded 25/25 layers to GPU
0.00.709.790 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.709.791 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.711.504 I llama_init_from_model: n_seq_max     = 1
0.00.711.513 I llama_init_from_model: n_ctx         = 2048
0.00.711.513 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.711.514 I llama_init_from_model: n_batch       = 2048
0.00.711.515 I llama_init_from_model: n_ubatch      = 512
0.00.711.515 I llama_init_from_model: flash_attn    = 0
0.00.711.518 I llama_init_from_model: freq_base     = 10000.0
0.00.711.518 I llama_init_from_model: freq_scale    = 1
0.00.711.521 I ggml_metal_init: allocating
0.00.711.596 I ggml_metal_init: found device: Apple M4
0.00.711.612 I ggml_metal_init: picking default device: Apple M4
0.00.713.927 I ggml_metal_init: using embedded metal library
0.00.720.808 I ggml_metal_init: GPU name:   Apple M4
0.00.720.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.720.814 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.720.814 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.720.815 I ggml_metal_init: simdgroup reduction   = true
0.00.720.815 I ggml_metal_init: simdgroup matrix mul. = true
0.00.720.816 I ggml_metal_init: has residency sets    = true
0.00.720.816 I ggml_metal_init: has bfloat            = true
0.00.720.816 I ggml_metal_init: use bfloat            = true
0.00.720.817 I ggml_metal_init: hasUnifiedMemory      = true
0.00.720.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.738.439 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.795.080 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.795.089 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.795.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.799.251 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.799.253 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.799.254 I llama_init_from_model: graph nodes  = 967
0.00.799.254 I llama_init_from_model: graph splits = 2
0.00.799.260 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.799.383 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.799.384 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.854.626 I main: llama threadpool init, n_threads = 4
0.00.854.676 I 
0.00.854.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.854.701 I 
0.00.854.852 I sampler seed: 1234
0.00.854.856 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.854.867 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.854.868 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.854.869 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.591.493 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.591.494 I llama_perf_context_print:        load time =     845.08 ms
0.01.591.495 I llama_perf_context_print: prompt eval time =      48.81 ms /     7 tokens (    6.97 ms per token,   143.41 tokens per second)
0.01.591.495 I llama_perf_context_print:        eval time =     685.15 ms /    63 runs   (   10.88 ms per token,    91.95 tokens per second)
0.01.591.495 I llama_perf_context_print:       total time =     737.58 ms /    70 tokens
0.01.591.759 I ggml_metal_free: deallocating

real	0m1.608s
user	0m0.114s
sys	0m0.196s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.010.754 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.309 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.313 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.318 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.319 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.319 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.319 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.320 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.322 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.322 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.323 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.324 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.324 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.327 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.130 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.165 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.889 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.890 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.891 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.892 I llama_model_loader: - type  f32:  194 tensors
0.00.028.892 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.892 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.893 I print_info: file format = GGUF V3 (latest)
0.00.028.893 I print_info: file type   = Q5_0
0.00.028.894 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.012 I load: special tokens cache size = 25
0.00.042.761 I load: token to piece cache size = 0.2984 MB
0.00.042.764 I print_info: arch             = gptneox
0.00.042.764 I print_info: vocab_only       = 0
0.00.042.764 I print_info: n_ctx_train      = 2048
0.00.042.764 I print_info: n_embd           = 2048
0.00.042.764 I print_info: n_layer          = 24
0.00.042.767 I print_info: n_head           = 16
0.00.042.768 I print_info: n_head_kv        = 16
0.00.042.768 I print_info: n_rot            = 32
0.00.042.768 I print_info: n_swa            = 0
0.00.042.768 I print_info: n_embd_head_k    = 128
0.00.042.769 I print_info: n_embd_head_v    = 128
0.00.042.769 I print_info: n_gqa            = 1
0.00.042.770 I print_info: n_embd_k_gqa     = 2048
0.00.042.771 I print_info: n_embd_v_gqa     = 2048
0.00.042.771 I print_info: f_norm_eps       = 1.0e-05
0.00.042.772 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.772 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.774 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.775 I print_info: f_logit_scale    = 0.0e+00
0.00.042.776 I print_info: n_ff             = 8192
0.00.042.776 I print_info: n_expert         = 0
0.00.042.778 I print_info: n_expert_used    = 0
0.00.042.778 I print_info: causal attn      = 1
0.00.042.778 I print_info: pooling type     = 0
0.00.042.778 I print_info: rope type        = 2
0.00.042.779 I print_info: rope scaling     = linear
0.00.042.779 I print_info: freq_base_train  = 10000.0
0.00.042.779 I print_info: freq_scale_train = 1
0.00.042.779 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.780 I print_info: rope_finetuned   = unknown
0.00.042.780 I print_info: ssm_d_conv       = 0
0.00.042.780 I print_info: ssm_d_inner      = 0
0.00.042.780 I print_info: ssm_d_state      = 0
0.00.042.780 I print_info: ssm_dt_rank      = 0
0.00.042.780 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.781 I print_info: model type       = 1.4B
0.00.042.781 I print_info: model params     = 1.41 B
0.00.042.781 I print_info: general.name     = 1.4B
0.00.042.782 I print_info: vocab type       = BPE
0.00.042.782 I print_info: n_vocab          = 50304
0.00.042.786 I print_info: n_merges         = 50009
0.00.042.786 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.786 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.786 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.786 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.786 I print_info: LF token         = 187 'Ċ'
0.00.042.787 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.787 I print_info: max token length = 1024
0.00.042.787 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.701.456 I load_tensors: offloading 24 repeating layers to GPU
0.00.701.473 I load_tensors: offloading output layer to GPU
0.00.701.474 I load_tensors: offloaded 25/25 layers to GPU
0.00.701.509 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.701.510 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.702.934 I llama_init_from_model: n_seq_max     = 1
0.00.702.937 I llama_init_from_model: n_ctx         = 2048
0.00.702.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.702.938 I llama_init_from_model: n_batch       = 2048
0.00.702.938 I llama_init_from_model: n_ubatch      = 512
0.00.702.939 I llama_init_from_model: flash_attn    = 0
0.00.702.941 I llama_init_from_model: freq_base     = 10000.0
0.00.702.942 I llama_init_from_model: freq_scale    = 1
0.00.702.944 I ggml_metal_init: allocating
0.00.703.023 I ggml_metal_init: found device: Apple M4
0.00.703.037 I ggml_metal_init: picking default device: Apple M4
0.00.704.982 I ggml_metal_init: using embedded metal library
0.00.711.397 I ggml_metal_init: GPU name:   Apple M4
0.00.711.401 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.711.401 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.711.402 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.711.403 I ggml_metal_init: simdgroup reduction   = true
0.00.711.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.711.403 I ggml_metal_init: has residency sets    = true
0.00.711.403 I ggml_metal_init: has bfloat            = true
0.00.711.404 I ggml_metal_init: use bfloat            = true
0.00.711.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.711.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.729.117 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.783.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.783.381 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.783.418 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.787.927 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.787.930 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.787.930 I llama_init_from_model: graph nodes  = 967
0.00.787.930 I llama_init_from_model: graph splits = 2
0.00.787.935 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.788.062 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.788.062 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.045 I main: llama threadpool init, n_threads = 4
0.00.847.091 I 
0.00.847.114 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.116 I 
0.00.847.281 I sampler seed: 1234
0.00.847.286 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.847.328 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.847.332 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.847.332 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.632.575 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.632.576 I llama_perf_context_print:        load time =     835.57 ms
0.01.632.577 I llama_perf_context_print: prompt eval time =      47.61 ms /     7 tokens (    6.80 ms per token,   147.03 tokens per second)
0.01.632.581 I llama_perf_context_print:        eval time =     734.74 ms /    63 runs   (   11.66 ms per token,    85.75 tokens per second)
0.01.632.583 I llama_perf_context_print:       total time =     786.25 ms /    70 tokens
0.01.632.864 I ggml_metal_free: deallocating

real	0m1.652s
user	0m0.110s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.275 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.790 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.792 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.793 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.793 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.794 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.795 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.795 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.796 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.796 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.796 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.797 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.797 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.486 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.195 I llama_model_loader: - type  f32:  194 tensors
0.00.025.196 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.196 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.196 I print_info: file format = GGUF V3 (latest)
0.00.025.197 I print_info: file type   = Q5_1
0.00.025.198 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.944 I load: special tokens cache size = 25
0.00.038.940 I load: token to piece cache size = 0.2984 MB
0.00.038.943 I print_info: arch             = gptneox
0.00.038.943 I print_info: vocab_only       = 0
0.00.038.943 I print_info: n_ctx_train      = 2048
0.00.038.943 I print_info: n_embd           = 2048
0.00.038.943 I print_info: n_layer          = 24
0.00.038.946 I print_info: n_head           = 16
0.00.038.947 I print_info: n_head_kv        = 16
0.00.038.947 I print_info: n_rot            = 32
0.00.038.947 I print_info: n_swa            = 0
0.00.038.948 I print_info: n_embd_head_k    = 128
0.00.038.948 I print_info: n_embd_head_v    = 128
0.00.038.950 I print_info: n_gqa            = 1
0.00.038.951 I print_info: n_embd_k_gqa     = 2048
0.00.038.951 I print_info: n_embd_v_gqa     = 2048
0.00.038.952 I print_info: f_norm_eps       = 1.0e-05
0.00.038.953 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.953 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.957 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.957 I print_info: f_logit_scale    = 0.0e+00
0.00.038.958 I print_info: n_ff             = 8192
0.00.038.958 I print_info: n_expert         = 0
0.00.038.959 I print_info: n_expert_used    = 0
0.00.038.959 I print_info: causal attn      = 1
0.00.038.959 I print_info: pooling type     = 0
0.00.038.959 I print_info: rope type        = 2
0.00.038.960 I print_info: rope scaling     = linear
0.00.038.960 I print_info: freq_base_train  = 10000.0
0.00.038.960 I print_info: freq_scale_train = 1
0.00.038.960 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.961 I print_info: rope_finetuned   = unknown
0.00.038.961 I print_info: ssm_d_conv       = 0
0.00.038.961 I print_info: ssm_d_inner      = 0
0.00.038.961 I print_info: ssm_d_state      = 0
0.00.038.961 I print_info: ssm_dt_rank      = 0
0.00.038.961 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.962 I print_info: model type       = 1.4B
0.00.038.962 I print_info: model params     = 1.41 B
0.00.038.962 I print_info: general.name     = 1.4B
0.00.038.963 I print_info: vocab type       = BPE
0.00.038.963 I print_info: n_vocab          = 50304
0.00.038.963 I print_info: n_merges         = 50009
0.00.038.963 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.963 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.964 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.964 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.965 I print_info: LF token         = 187 'Ċ'
0.00.038.966 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.966 I print_info: max token length = 1024
0.00.038.966 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.632.531 I load_tensors: offloading 24 repeating layers to GPU
0.00.632.548 I load_tensors: offloading output layer to GPU
0.00.632.549 I load_tensors: offloaded 25/25 layers to GPU
0.00.632.585 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.632.586 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.634.107 I llama_init_from_model: n_seq_max     = 1
0.00.634.110 I llama_init_from_model: n_ctx         = 2048
0.00.634.110 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.634.111 I llama_init_from_model: n_batch       = 2048
0.00.634.111 I llama_init_from_model: n_ubatch      = 512
0.00.634.112 I llama_init_from_model: flash_attn    = 0
0.00.634.115 I llama_init_from_model: freq_base     = 10000.0
0.00.634.115 I llama_init_from_model: freq_scale    = 1
0.00.634.117 I ggml_metal_init: allocating
0.00.634.234 I ggml_metal_init: found device: Apple M4
0.00.634.247 I ggml_metal_init: picking default device: Apple M4
0.00.636.152 I ggml_metal_init: using embedded metal library
0.00.642.566 I ggml_metal_init: GPU name:   Apple M4
0.00.642.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.642.571 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.642.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.642.573 I ggml_metal_init: simdgroup reduction   = true
0.00.642.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.642.573 I ggml_metal_init: has residency sets    = true
0.00.642.573 I ggml_metal_init: has bfloat            = true
0.00.642.574 I ggml_metal_init: use bfloat            = true
0.00.642.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.642.576 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.178 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.714.786 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.714.794 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.714.831 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.719.247 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.719.249 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.719.250 I llama_init_from_model: graph nodes  = 967
0.00.719.250 I llama_init_from_model: graph splits = 2
0.00.719.255 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.719.380 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.778.823 I main: llama threadpool init, n_threads = 4
0.00.778.875 I 
0.00.778.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.778.902 I 
0.00.779.082 I sampler seed: 1234
0.00.779.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.779.108 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.779.108 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.779.109 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.619.985 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.619.986 I llama_perf_context_print:        load time =     768.82 ms
0.01.619.987 I llama_perf_context_print: prompt eval time =      51.99 ms /     7 tokens (    7.43 ms per token,   134.65 tokens per second)
0.01.619.991 I llama_perf_context_print:        eval time =     786.00 ms /    63 runs   (   12.48 ms per token,    80.15 tokens per second)
0.01.619.991 I llama_perf_context_print:       total time =     841.88 ms /    70 tokens
0.01.620.256 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.108s
sys	0m0.229s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.010.043 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.796 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.801 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.802 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.803 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.803 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.803 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.804 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.805 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.805 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.807 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.808 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.808 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.808 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.809 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.810 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.811 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.811 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.565 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.558 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.273 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.275 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.276 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.276 I llama_model_loader: - type  f32:  194 tensors
0.00.025.276 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.276 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.277 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.277 I print_info: file format = GGUF V3 (latest)
0.00.025.277 I print_info: file type   = Q2_K - Medium
0.00.025.278 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.990 I load: special tokens cache size = 25
0.00.039.108 I load: token to piece cache size = 0.2984 MB
0.00.039.111 I print_info: arch             = gptneox
0.00.039.111 I print_info: vocab_only       = 0
0.00.039.111 I print_info: n_ctx_train      = 2048
0.00.039.112 I print_info: n_embd           = 2048
0.00.039.112 I print_info: n_layer          = 24
0.00.039.114 I print_info: n_head           = 16
0.00.039.115 I print_info: n_head_kv        = 16
0.00.039.115 I print_info: n_rot            = 32
0.00.039.115 I print_info: n_swa            = 0
0.00.039.115 I print_info: n_embd_head_k    = 128
0.00.039.117 I print_info: n_embd_head_v    = 128
0.00.039.118 I print_info: n_gqa            = 1
0.00.039.118 I print_info: n_embd_k_gqa     = 2048
0.00.039.119 I print_info: n_embd_v_gqa     = 2048
0.00.039.124 I print_info: f_norm_eps       = 1.0e-05
0.00.039.125 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.125 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.125 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.125 I print_info: f_logit_scale    = 0.0e+00
0.00.039.126 I print_info: n_ff             = 8192
0.00.039.126 I print_info: n_expert         = 0
0.00.039.126 I print_info: n_expert_used    = 0
0.00.039.127 I print_info: causal attn      = 1
0.00.039.127 I print_info: pooling type     = 0
0.00.039.127 I print_info: rope type        = 2
0.00.039.132 I print_info: rope scaling     = linear
0.00.039.133 I print_info: freq_base_train  = 10000.0
0.00.039.133 I print_info: freq_scale_train = 1
0.00.039.133 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.134 I print_info: rope_finetuned   = unknown
0.00.039.134 I print_info: ssm_d_conv       = 0
0.00.039.134 I print_info: ssm_d_inner      = 0
0.00.039.134 I print_info: ssm_d_state      = 0
0.00.039.134 I print_info: ssm_dt_rank      = 0
0.00.039.134 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.135 I print_info: model type       = 1.4B
0.00.039.135 I print_info: model params     = 1.41 B
0.00.039.135 I print_info: general.name     = 1.4B
0.00.039.136 I print_info: vocab type       = BPE
0.00.039.136 I print_info: n_vocab          = 50304
0.00.039.136 I print_info: n_merges         = 50009
0.00.039.136 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.136 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.136 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.137 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: LF token         = 187 'Ċ'
0.00.039.139 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.139 I print_info: max token length = 1024
0.00.039.140 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.359.171 I load_tensors: offloading 24 repeating layers to GPU
0.00.359.179 I load_tensors: offloading output layer to GPU
0.00.359.180 I load_tensors: offloaded 25/25 layers to GPU
0.00.359.215 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.359.219 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.360.865 I llama_init_from_model: n_seq_max     = 1
0.00.360.868 I llama_init_from_model: n_ctx         = 2048
0.00.360.869 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.360.869 I llama_init_from_model: n_batch       = 2048
0.00.360.869 I llama_init_from_model: n_ubatch      = 512
0.00.360.870 I llama_init_from_model: flash_attn    = 0
0.00.360.872 I llama_init_from_model: freq_base     = 10000.0
0.00.360.872 I llama_init_from_model: freq_scale    = 1
0.00.360.874 I ggml_metal_init: allocating
0.00.360.948 I ggml_metal_init: found device: Apple M4
0.00.360.960 I ggml_metal_init: picking default device: Apple M4
0.00.363.171 I ggml_metal_init: using embedded metal library
0.00.369.251 I ggml_metal_init: GPU name:   Apple M4
0.00.369.259 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.369.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.369.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.369.262 I ggml_metal_init: simdgroup reduction   = true
0.00.369.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.369.262 I ggml_metal_init: has residency sets    = true
0.00.369.263 I ggml_metal_init: has bfloat            = true
0.00.369.263 I ggml_metal_init: use bfloat            = true
0.00.369.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.369.273 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.707 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.451.764 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.451.781 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.451.819 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.456.067 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.456.069 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.456.069 I llama_init_from_model: graph nodes  = 967
0.00.456.069 I llama_init_from_model: graph splits = 2
0.00.456.074 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.456.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.456.204 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.765 I main: llama threadpool init, n_threads = 4
0.00.513.809 I 
0.00.513.834 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.834 I 
0.00.514.011 I sampler seed: 1234
0.00.514.016 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.514.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.514.036 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.514.036 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.184.983 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55038.76 tokens per second)
0.01.184.984 I llama_perf_context_print:        load time =     503.00 ms
0.01.184.985 I llama_perf_context_print: prompt eval time =      35.46 ms /     7 tokens (    5.07 ms per token,   197.39 tokens per second)
0.01.184.985 I llama_perf_context_print:        eval time =     632.72 ms /    63 runs   (   10.04 ms per token,    99.57 tokens per second)
0.01.184.986 I llama_perf_context_print:       total time =     671.94 ms /    70 tokens
0.01.185.219 I ggml_metal_free: deallocating

real	0m1.202s
user	0m0.112s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.865 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.077 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.082 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.085 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.086 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.086 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.087 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.089 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.807 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.469 I llama_model_loader: - type  f32:  194 tensors
0.00.024.469 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.470 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.470 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.470 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.471 I print_info: file format = GGUF V3 (latest)
0.00.024.471 I print_info: file type   = Q3_K - Medium
0.00.024.472 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.232 I load: special tokens cache size = 25
0.00.038.281 I load: token to piece cache size = 0.2984 MB
0.00.038.284 I print_info: arch             = gptneox
0.00.038.284 I print_info: vocab_only       = 0
0.00.038.284 I print_info: n_ctx_train      = 2048
0.00.038.284 I print_info: n_embd           = 2048
0.00.038.285 I print_info: n_layer          = 24
0.00.038.287 I print_info: n_head           = 16
0.00.038.287 I print_info: n_head_kv        = 16
0.00.038.288 I print_info: n_rot            = 32
0.00.038.288 I print_info: n_swa            = 0
0.00.038.288 I print_info: n_embd_head_k    = 128
0.00.038.288 I print_info: n_embd_head_v    = 128
0.00.038.289 I print_info: n_gqa            = 1
0.00.038.290 I print_info: n_embd_k_gqa     = 2048
0.00.038.290 I print_info: n_embd_v_gqa     = 2048
0.00.038.291 I print_info: f_norm_eps       = 1.0e-05
0.00.038.291 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.292 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.292 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.292 I print_info: f_logit_scale    = 0.0e+00
0.00.038.293 I print_info: n_ff             = 8192
0.00.038.293 I print_info: n_expert         = 0
0.00.038.293 I print_info: n_expert_used    = 0
0.00.038.293 I print_info: causal attn      = 1
0.00.038.293 I print_info: pooling type     = 0
0.00.038.293 I print_info: rope type        = 2
0.00.038.294 I print_info: rope scaling     = linear
0.00.038.294 I print_info: freq_base_train  = 10000.0
0.00.038.294 I print_info: freq_scale_train = 1
0.00.038.295 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.295 I print_info: rope_finetuned   = unknown
0.00.038.295 I print_info: ssm_d_conv       = 0
0.00.038.295 I print_info: ssm_d_inner      = 0
0.00.038.295 I print_info: ssm_d_state      = 0
0.00.038.295 I print_info: ssm_dt_rank      = 0
0.00.038.295 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.296 I print_info: model type       = 1.4B
0.00.038.296 I print_info: model params     = 1.41 B
0.00.038.296 I print_info: general.name     = 1.4B
0.00.038.297 I print_info: vocab type       = BPE
0.00.038.297 I print_info: n_vocab          = 50304
0.00.038.297 I print_info: n_merges         = 50009
0.00.038.297 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.298 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.298 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.298 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.298 I print_info: LF token         = 187 'Ċ'
0.00.038.298 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.298 I print_info: max token length = 1024
0.00.038.299 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.454.043 I load_tensors: offloading 24 repeating layers to GPU
0.00.454.060 I load_tensors: offloading output layer to GPU
0.00.454.060 I load_tensors: offloaded 25/25 layers to GPU
0.00.454.094 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.454.095 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.455.552 I llama_init_from_model: n_seq_max     = 1
0.00.455.555 I llama_init_from_model: n_ctx         = 2048
0.00.455.556 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.455.556 I llama_init_from_model: n_batch       = 2048
0.00.455.557 I llama_init_from_model: n_ubatch      = 512
0.00.455.557 I llama_init_from_model: flash_attn    = 0
0.00.455.559 I llama_init_from_model: freq_base     = 10000.0
0.00.455.560 I llama_init_from_model: freq_scale    = 1
0.00.455.562 I ggml_metal_init: allocating
0.00.455.636 I ggml_metal_init: found device: Apple M4
0.00.455.650 I ggml_metal_init: picking default device: Apple M4
0.00.457.545 I ggml_metal_init: using embedded metal library
0.00.463.053 I ggml_metal_init: GPU name:   Apple M4
0.00.463.071 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.463.072 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.463.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.463.074 I ggml_metal_init: simdgroup reduction   = true
0.00.463.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.463.074 I ggml_metal_init: has residency sets    = true
0.00.463.074 I ggml_metal_init: has bfloat            = true
0.00.463.075 I ggml_metal_init: use bfloat            = true
0.00.463.077 I ggml_metal_init: hasUnifiedMemory      = true
0.00.463.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.483.718 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.542.090 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.542.098 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.542.136 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.546.709 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.546.711 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.546.711 I llama_init_from_model: graph nodes  = 967
0.00.546.711 I llama_init_from_model: graph splits = 2
0.00.546.717 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.546.859 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.546.859 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.751 I main: llama threadpool init, n_threads = 4
0.00.604.794 I 
0.00.604.817 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.818 I 
0.00.604.967 I sampler seed: 1234
0.00.604.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.604.983 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.604.983 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.604.983 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.353.147 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49859.55 tokens per second)
0.01.353.148 I llama_perf_context_print:        load time =     595.16 ms
0.01.353.149 I llama_perf_context_print: prompt eval time =      47.26 ms /     7 tokens (    6.75 ms per token,   148.10 tokens per second)
0.01.353.150 I llama_perf_context_print:        eval time =     697.86 ms /    63 runs   (   11.08 ms per token,    90.28 tokens per second)
0.01.353.150 I llama_perf_context_print:       total time =     749.12 ms /    70 tokens
0.01.353.374 I ggml_metal_free: deallocating

real	0m1.369s
user	0m0.111s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.939 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.545 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.556 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.561 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.564 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.567 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.323 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.330 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.064 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.066 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.066 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.067 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.067 I llama_model_loader: - type  f32:  194 tensors
0.00.025.068 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.068 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.068 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.069 I print_info: file format = GGUF V3 (latest)
0.00.025.069 I print_info: file type   = Q4_K - Medium
0.00.025.070 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.132 I load: special tokens cache size = 25
0.00.039.013 I load: token to piece cache size = 0.2984 MB
0.00.039.016 I print_info: arch             = gptneox
0.00.039.016 I print_info: vocab_only       = 0
0.00.039.016 I print_info: n_ctx_train      = 2048
0.00.039.016 I print_info: n_embd           = 2048
0.00.039.017 I print_info: n_layer          = 24
0.00.039.019 I print_info: n_head           = 16
0.00.039.020 I print_info: n_head_kv        = 16
0.00.039.020 I print_info: n_rot            = 32
0.00.039.020 I print_info: n_swa            = 0
0.00.039.020 I print_info: n_embd_head_k    = 128
0.00.039.021 I print_info: n_embd_head_v    = 128
0.00.039.021 I print_info: n_gqa            = 1
0.00.039.022 I print_info: n_embd_k_gqa     = 2048
0.00.039.023 I print_info: n_embd_v_gqa     = 2048
0.00.039.023 I print_info: f_norm_eps       = 1.0e-05
0.00.039.024 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.024 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.024 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.024 I print_info: f_logit_scale    = 0.0e+00
0.00.039.025 I print_info: n_ff             = 8192
0.00.039.025 I print_info: n_expert         = 0
0.00.039.025 I print_info: n_expert_used    = 0
0.00.039.025 I print_info: causal attn      = 1
0.00.039.025 I print_info: pooling type     = 0
0.00.039.026 I print_info: rope type        = 2
0.00.039.026 I print_info: rope scaling     = linear
0.00.039.026 I print_info: freq_base_train  = 10000.0
0.00.039.035 I print_info: freq_scale_train = 1
0.00.039.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.038 I print_info: rope_finetuned   = unknown
0.00.039.038 I print_info: ssm_d_conv       = 0
0.00.039.038 I print_info: ssm_d_inner      = 0
0.00.039.038 I print_info: ssm_d_state      = 0
0.00.039.039 I print_info: ssm_dt_rank      = 0
0.00.039.040 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.040 I print_info: model type       = 1.4B
0.00.039.040 I print_info: model params     = 1.41 B
0.00.039.041 I print_info: general.name     = 1.4B
0.00.039.041 I print_info: vocab type       = BPE
0.00.039.041 I print_info: n_vocab          = 50304
0.00.039.041 I print_info: n_merges         = 50009
0.00.039.042 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.043 I print_info: LF token         = 187 'Ċ'
0.00.039.045 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.045 I print_info: max token length = 1024
0.00.039.046 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.521.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.522.005 I load_tensors: offloading output layer to GPU
0.00.522.005 I load_tensors: offloaded 25/25 layers to GPU
0.00.522.034 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.522.037 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.523.592 I llama_init_from_model: n_seq_max     = 1
0.00.523.594 I llama_init_from_model: n_ctx         = 2048
0.00.523.594 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.523.595 I llama_init_from_model: n_batch       = 2048
0.00.523.595 I llama_init_from_model: n_ubatch      = 512
0.00.523.596 I llama_init_from_model: flash_attn    = 0
0.00.523.597 I llama_init_from_model: freq_base     = 10000.0
0.00.523.597 I llama_init_from_model: freq_scale    = 1
0.00.523.599 I ggml_metal_init: allocating
0.00.523.647 I ggml_metal_init: found device: Apple M4
0.00.523.661 I ggml_metal_init: picking default device: Apple M4
0.00.525.436 I ggml_metal_init: using embedded metal library
0.00.531.858 I ggml_metal_init: GPU name:   Apple M4
0.00.531.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.531.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.531.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.531.865 I ggml_metal_init: simdgroup reduction   = true
0.00.531.865 I ggml_metal_init: simdgroup matrix mul. = true
0.00.531.865 I ggml_metal_init: has residency sets    = true
0.00.531.865 I ggml_metal_init: has bfloat            = true
0.00.531.866 I ggml_metal_init: use bfloat            = true
0.00.531.866 I ggml_metal_init: hasUnifiedMemory      = true
0.00.531.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.549.370 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.447 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.605.454 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.605.497 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.609.839 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.609.841 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.609.842 I llama_init_from_model: graph nodes  = 967
0.00.609.842 I llama_init_from_model: graph splits = 2
0.00.609.847 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.609.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.609.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.790 I main: llama threadpool init, n_threads = 4
0.00.668.837 I 
0.00.668.864 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.865 I 
0.00.669.020 I sampler seed: 1234
0.00.669.024 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.669.035 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.669.035 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.669.037 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.436.023 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50605.84 tokens per second)
0.01.436.024 I llama_perf_context_print:        load time =     659.13 ms
0.01.436.024 I llama_perf_context_print: prompt eval time =      52.65 ms /     7 tokens (    7.52 ms per token,   132.95 tokens per second)
0.01.436.025 I llama_perf_context_print:        eval time =     711.42 ms /    63 runs   (   11.29 ms per token,    88.56 tokens per second)
0.01.436.025 I llama_perf_context_print:       total time =     767.95 ms /    70 tokens
0.01.436.298 I ggml_metal_free: deallocating

real	0m1.458s
user	0m0.110s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.010.463 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.761 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.766 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.768 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.769 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.769 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.774 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.774 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.775 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.776 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.776 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.777 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.777 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.778 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.780 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.780 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.780 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.413 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.442 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.263 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.265 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.265 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.266 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.266 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.266 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.267 I llama_model_loader: - type  f32:  194 tensors
0.00.026.267 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.268 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.268 I print_info: file format = GGUF V3 (latest)
0.00.026.269 I print_info: file type   = Q5_K - Medium
0.00.026.270 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.734 I load: special tokens cache size = 25
0.00.040.654 I load: token to piece cache size = 0.2984 MB
0.00.040.659 I print_info: arch             = gptneox
0.00.040.659 I print_info: vocab_only       = 0
0.00.040.659 I print_info: n_ctx_train      = 2048
0.00.040.660 I print_info: n_embd           = 2048
0.00.040.660 I print_info: n_layer          = 24
0.00.040.664 I print_info: n_head           = 16
0.00.040.665 I print_info: n_head_kv        = 16
0.00.040.665 I print_info: n_rot            = 32
0.00.040.665 I print_info: n_swa            = 0
0.00.040.665 I print_info: n_embd_head_k    = 128
0.00.040.665 I print_info: n_embd_head_v    = 128
0.00.040.666 I print_info: n_gqa            = 1
0.00.040.667 I print_info: n_embd_k_gqa     = 2048
0.00.040.668 I print_info: n_embd_v_gqa     = 2048
0.00.040.671 I print_info: f_norm_eps       = 1.0e-05
0.00.040.671 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.671 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.673 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.674 I print_info: f_logit_scale    = 0.0e+00
0.00.040.675 I print_info: n_ff             = 8192
0.00.040.675 I print_info: n_expert         = 0
0.00.040.675 I print_info: n_expert_used    = 0
0.00.040.675 I print_info: causal attn      = 1
0.00.040.675 I print_info: pooling type     = 0
0.00.040.675 I print_info: rope type        = 2
0.00.040.676 I print_info: rope scaling     = linear
0.00.040.676 I print_info: freq_base_train  = 10000.0
0.00.040.676 I print_info: freq_scale_train = 1
0.00.040.676 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.677 I print_info: rope_finetuned   = unknown
0.00.040.677 I print_info: ssm_d_conv       = 0
0.00.040.677 I print_info: ssm_d_inner      = 0
0.00.040.677 I print_info: ssm_d_state      = 0
0.00.040.677 I print_info: ssm_dt_rank      = 0
0.00.040.677 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.677 I print_info: model type       = 1.4B
0.00.040.678 I print_info: model params     = 1.41 B
0.00.040.678 I print_info: general.name     = 1.4B
0.00.040.678 I print_info: vocab type       = BPE
0.00.040.682 I print_info: n_vocab          = 50304
0.00.040.682 I print_info: n_merges         = 50009
0.00.040.684 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.684 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.684 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.684 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.685 I print_info: LF token         = 187 'Ċ'
0.00.040.685 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.685 I print_info: max token length = 1024
0.00.040.686 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.598.669 I load_tensors: offloading 24 repeating layers to GPU
0.00.598.687 I load_tensors: offloading output layer to GPU
0.00.598.687 I load_tensors: offloaded 25/25 layers to GPU
0.00.598.721 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.598.723 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.980 I llama_init_from_model: n_seq_max     = 1
0.00.599.984 I llama_init_from_model: n_ctx         = 2048
0.00.599.984 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.599.985 I llama_init_from_model: n_batch       = 2048
0.00.599.985 I llama_init_from_model: n_ubatch      = 512
0.00.599.986 I llama_init_from_model: flash_attn    = 0
0.00.599.988 I llama_init_from_model: freq_base     = 10000.0
0.00.599.988 I llama_init_from_model: freq_scale    = 1
0.00.599.991 I ggml_metal_init: allocating
0.00.600.055 I ggml_metal_init: found device: Apple M4
0.00.600.069 I ggml_metal_init: picking default device: Apple M4
0.00.601.711 I ggml_metal_init: using embedded metal library
0.00.608.098 I ggml_metal_init: GPU name:   Apple M4
0.00.608.103 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.103 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.104 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.104 I ggml_metal_init: simdgroup reduction   = true
0.00.608.105 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.105 I ggml_metal_init: has residency sets    = true
0.00.608.105 I ggml_metal_init: has bfloat            = true
0.00.608.105 I ggml_metal_init: use bfloat            = true
0.00.608.107 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.541 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.089 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.677.095 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.131 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.623 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.681.625 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.681.626 I llama_init_from_model: graph nodes  = 967
0.00.681.626 I llama_init_from_model: graph splits = 2
0.00.681.631 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.681.766 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.681.766 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.427 I main: llama threadpool init, n_threads = 4
0.00.746.475 I 
0.00.746.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.501 I 
0.00.746.689 I sampler seed: 1234
0.00.746.693 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.714 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.714 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.714 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.592.875 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.592.876 I llama_perf_context_print:        load time =     735.23 ms
0.01.592.877 I llama_perf_context_print: prompt eval time =      52.54 ms /     7 tokens (    7.51 ms per token,   133.23 tokens per second)
0.01.592.877 I llama_perf_context_print:        eval time =     790.72 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.592.879 I llama_perf_context_print:       total time =     847.18 ms /    70 tokens
0.01.593.149 I ggml_metal_free: deallocating

real	0m1.611s
user	0m0.108s
sys	0m0.207s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.724 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.779 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.780 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.784 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.784 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.785 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.787 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.787 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.402 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.036 I llama_model_loader: - type  f32:  194 tensors
0.00.025.036 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.036 I print_info: file format = GGUF V3 (latest)
0.00.025.037 I print_info: file type   = Q6_K
0.00.025.037 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.773 I load: special tokens cache size = 25
0.00.038.800 I load: token to piece cache size = 0.2984 MB
0.00.038.802 I print_info: arch             = gptneox
0.00.038.803 I print_info: vocab_only       = 0
0.00.038.803 I print_info: n_ctx_train      = 2048
0.00.038.803 I print_info: n_embd           = 2048
0.00.038.803 I print_info: n_layer          = 24
0.00.038.806 I print_info: n_head           = 16
0.00.038.807 I print_info: n_head_kv        = 16
0.00.038.807 I print_info: n_rot            = 32
0.00.038.807 I print_info: n_swa            = 0
0.00.038.807 I print_info: n_embd_head_k    = 128
0.00.038.807 I print_info: n_embd_head_v    = 128
0.00.038.808 I print_info: n_gqa            = 1
0.00.038.809 I print_info: n_embd_k_gqa     = 2048
0.00.038.810 I print_info: n_embd_v_gqa     = 2048
0.00.038.810 I print_info: f_norm_eps       = 1.0e-05
0.00.038.810 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.811 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.811 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.811 I print_info: f_logit_scale    = 0.0e+00
0.00.038.812 I print_info: n_ff             = 8192
0.00.038.812 I print_info: n_expert         = 0
0.00.038.812 I print_info: n_expert_used    = 0
0.00.038.812 I print_info: causal attn      = 1
0.00.038.812 I print_info: pooling type     = 0
0.00.038.812 I print_info: rope type        = 2
0.00.038.817 I print_info: rope scaling     = linear
0.00.038.817 I print_info: freq_base_train  = 10000.0
0.00.038.817 I print_info: freq_scale_train = 1
0.00.038.818 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.818 I print_info: rope_finetuned   = unknown
0.00.038.818 I print_info: ssm_d_conv       = 0
0.00.038.818 I print_info: ssm_d_inner      = 0
0.00.038.818 I print_info: ssm_d_state      = 0
0.00.038.818 I print_info: ssm_dt_rank      = 0
0.00.038.819 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.819 I print_info: model type       = 1.4B
0.00.038.819 I print_info: model params     = 1.41 B
0.00.038.819 I print_info: general.name     = 1.4B
0.00.038.820 I print_info: vocab type       = BPE
0.00.038.820 I print_info: n_vocab          = 50304
0.00.038.820 I print_info: n_merges         = 50009
0.00.038.821 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.821 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.821 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.821 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.821 I print_info: LF token         = 187 'Ċ'
0.00.038.822 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.822 I print_info: max token length = 1024
0.00.038.822 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.649.395 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.399 I load_tensors: offloading output layer to GPU
0.00.649.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.424 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.649.426 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.650.931 I llama_init_from_model: n_seq_max     = 1
0.00.650.933 I llama_init_from_model: n_ctx         = 2048
0.00.650.934 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.650.934 I llama_init_from_model: n_batch       = 2048
0.00.650.934 I llama_init_from_model: n_ubatch      = 512
0.00.650.935 I llama_init_from_model: flash_attn    = 0
0.00.650.936 I llama_init_from_model: freq_base     = 10000.0
0.00.650.936 I llama_init_from_model: freq_scale    = 1
0.00.650.938 I ggml_metal_init: allocating
0.00.650.989 I ggml_metal_init: found device: Apple M4
0.00.651.000 I ggml_metal_init: picking default device: Apple M4
0.00.652.562 I ggml_metal_init: using embedded metal library
0.00.658.459 I ggml_metal_init: GPU name:   Apple M4
0.00.658.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.658.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.658.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.658.464 I ggml_metal_init: simdgroup reduction   = true
0.00.658.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.658.465 I ggml_metal_init: has residency sets    = true
0.00.658.465 I ggml_metal_init: has bfloat            = true
0.00.658.465 I ggml_metal_init: use bfloat            = true
0.00.658.466 I ggml_metal_init: hasUnifiedMemory      = true
0.00.658.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.675.137 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.729.949 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.729.954 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.729.989 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.734.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.734.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.734.243 I llama_init_from_model: graph nodes  = 967
0.00.734.243 I llama_init_from_model: graph splits = 2
0.00.734.249 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.734.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.734.380 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.763 I main: llama threadpool init, n_threads = 4
0.00.796.811 I 
0.00.796.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.838 I 
0.00.797.006 I sampler seed: 1234
0.00.797.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.022 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.022 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.024 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.671.408 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.671.408 I llama_perf_context_print:        load time =     787.33 ms
0.01.671.409 I llama_perf_context_print: prompt eval time =      57.59 ms /     7 tokens (    8.23 ms per token,   121.55 tokens per second)
0.01.671.410 I llama_perf_context_print:        eval time =     813.85 ms /    63 runs   (   12.92 ms per token,    77.41 tokens per second)
0.01.671.410 I llama_perf_context_print:       total time =     875.35 ms /    70 tokens
0.01.671.605 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.106s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.685 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.884 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.040.479 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.040.485 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.492 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.495 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.498 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.499 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.499 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.501 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.503 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.504 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.504 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.512 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.976 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.978 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.978 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.979 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.980 I llama_model_loader: - type  f32:  194 tensors
0.00.055.980 I llama_model_loader: - type  f16:   98 tensors
0.00.055.981 I print_info: file format = GGUF V3 (latest)
0.00.055.981 I print_info: file type   = all F32 (guessed)
0.00.055.984 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.711 I load: special tokens cache size = 25
0.00.075.282 I load: token to piece cache size = 0.2984 MB
0.00.075.285 I print_info: arch             = gptneox
0.00.075.286 I print_info: vocab_only       = 0
0.00.075.286 I print_info: n_ctx_train      = 2048
0.00.075.286 I print_info: n_embd           = 2048
0.00.075.286 I print_info: n_layer          = 24
0.00.075.289 I print_info: n_head           = 16
0.00.075.290 I print_info: n_head_kv        = 16
0.00.075.290 I print_info: n_rot            = 32
0.00.075.290 I print_info: n_swa            = 0
0.00.075.291 I print_info: n_embd_head_k    = 128
0.00.075.291 I print_info: n_embd_head_v    = 128
0.00.075.292 I print_info: n_gqa            = 1
0.00.075.292 I print_info: n_embd_k_gqa     = 2048
0.00.075.293 I print_info: n_embd_v_gqa     = 2048
0.00.075.294 I print_info: f_norm_eps       = 1.0e-05
0.00.075.294 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.294 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.295 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.296 I print_info: f_logit_scale    = 0.0e+00
0.00.075.296 I print_info: n_ff             = 8192
0.00.075.296 I print_info: n_expert         = 0
0.00.075.297 I print_info: n_expert_used    = 0
0.00.075.297 I print_info: causal attn      = 1
0.00.075.297 I print_info: pooling type     = 0
0.00.075.297 I print_info: rope type        = 2
0.00.075.297 I print_info: rope scaling     = linear
0.00.075.298 I print_info: freq_base_train  = 10000.0
0.00.075.298 I print_info: freq_scale_train = 1
0.00.075.298 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.298 I print_info: rope_finetuned   = unknown
0.00.075.298 I print_info: ssm_d_conv       = 0
0.00.075.299 I print_info: ssm_d_inner      = 0
0.00.075.300 I print_info: ssm_d_state      = 0
0.00.075.300 I print_info: ssm_dt_rank      = 0
0.00.075.300 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.300 I print_info: model type       = 1.4B
0.00.075.301 I print_info: model params     = 1.41 B
0.00.075.301 I print_info: general.name     = 1.4B
0.00.075.301 I print_info: vocab type       = BPE
0.00.075.302 I print_info: n_vocab          = 50304
0.00.075.302 I print_info: n_merges         = 50009
0.00.075.302 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.302 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.303 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.303 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.303 I print_info: LF token         = 187 'Ċ'
0.00.075.304 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.304 I print_info: max token length = 1024
0.00.075.304 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.442.470 I load_tensors: offloading 24 repeating layers to GPU
0.01.442.476 I load_tensors: offloading output layer to GPU
0.01.442.478 I load_tensors: offloaded 25/25 layers to GPU
0.01.442.502 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.442.503 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.443.347 I llama_init_from_model: n_seq_max     = 1
0.01.443.348 I llama_init_from_model: n_ctx         = 128
0.01.443.348 I llama_init_from_model: n_ctx_per_seq = 128
0.01.443.349 I llama_init_from_model: n_batch       = 128
0.01.443.349 I llama_init_from_model: n_ubatch      = 128
0.01.443.349 I llama_init_from_model: flash_attn    = 0
0.01.443.350 I llama_init_from_model: freq_base     = 10000.0
0.01.443.350 I llama_init_from_model: freq_scale    = 1
0.01.443.350 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.443.352 I ggml_metal_init: allocating
0.01.443.379 I ggml_metal_init: found device: Apple M4
0.01.443.385 I ggml_metal_init: picking default device: Apple M4
0.01.444.418 I ggml_metal_init: using embedded metal library
0.01.448.408 I ggml_metal_init: GPU name:   Apple M4
0.01.448.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.448.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.448.412 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.448.412 I ggml_metal_init: simdgroup reduction   = true
0.01.448.412 I ggml_metal_init: simdgroup matrix mul. = true
0.01.448.412 I ggml_metal_init: has residency sets    = true
0.01.448.412 I ggml_metal_init: has bfloat            = true
0.01.448.412 I ggml_metal_init: use bfloat            = true
0.01.448.413 I ggml_metal_init: hasUnifiedMemory      = true
0.01.448.414 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.459.038 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.460.828 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.460.831 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.460.856 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.462.501 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.462.502 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.462.503 I llama_init_from_model: graph nodes  = 967
0.01.462.503 I llama_init_from_model: graph splits = 2
0.01.462.504 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.462.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.497.662 I 
0.01.497.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.497.697 I perplexity: tokenizing the input ..
0.01.502.634 I perplexity: tokenization took 4.935 ms
0.01.502.638 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.620.944 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.622.278 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.308 I llama_perf_context_print:        load time =    1472.77 ms
0.01.622.309 I llama_perf_context_print: prompt eval time =     118.04 ms /   128 tokens (    0.92 ms per token,  1084.36 tokens per second)
0.01.622.310 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.310 I llama_perf_context_print:       total time =     124.65 ms /   129 tokens
0.01.622.691 I ggml_metal_free: deallocating

real	0m1.812s
user	0m0.097s
sys	0m0.277s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.248 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.473 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.216 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.225 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.225 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.225 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.226 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.226 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.227 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.227 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.234 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.234 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.238 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.726 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.727 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.728 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.728 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.728 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.729 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.730 I llama_model_loader: - type  f32:  194 tensors
0.00.025.730 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.731 I print_info: file format = GGUF V3 (latest)
0.00.025.731 I print_info: file type   = Q8_0
0.00.025.733 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.952 I load: special tokens cache size = 25
0.00.039.924 I load: token to piece cache size = 0.2984 MB
0.00.039.929 I print_info: arch             = gptneox
0.00.039.929 I print_info: vocab_only       = 0
0.00.039.929 I print_info: n_ctx_train      = 2048
0.00.039.929 I print_info: n_embd           = 2048
0.00.039.930 I print_info: n_layer          = 24
0.00.039.934 I print_info: n_head           = 16
0.00.039.934 I print_info: n_head_kv        = 16
0.00.039.935 I print_info: n_rot            = 32
0.00.039.935 I print_info: n_swa            = 0
0.00.039.935 I print_info: n_embd_head_k    = 128
0.00.039.935 I print_info: n_embd_head_v    = 128
0.00.039.936 I print_info: n_gqa            = 1
0.00.039.937 I print_info: n_embd_k_gqa     = 2048
0.00.039.937 I print_info: n_embd_v_gqa     = 2048
0.00.039.938 I print_info: f_norm_eps       = 1.0e-05
0.00.039.938 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.939 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.939 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.939 I print_info: f_logit_scale    = 0.0e+00
0.00.039.940 I print_info: n_ff             = 8192
0.00.039.940 I print_info: n_expert         = 0
0.00.039.940 I print_info: n_expert_used    = 0
0.00.039.940 I print_info: causal attn      = 1
0.00.039.940 I print_info: pooling type     = 0
0.00.039.940 I print_info: rope type        = 2
0.00.039.940 I print_info: rope scaling     = linear
0.00.039.941 I print_info: freq_base_train  = 10000.0
0.00.039.941 I print_info: freq_scale_train = 1
0.00.039.941 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.941 I print_info: rope_finetuned   = unknown
0.00.039.941 I print_info: ssm_d_conv       = 0
0.00.039.941 I print_info: ssm_d_inner      = 0
0.00.039.945 I print_info: ssm_d_state      = 0
0.00.039.945 I print_info: ssm_dt_rank      = 0
0.00.039.945 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.945 I print_info: model type       = 1.4B
0.00.039.946 I print_info: model params     = 1.41 B
0.00.039.946 I print_info: general.name     = 1.4B
0.00.039.946 I print_info: vocab type       = BPE
0.00.039.946 I print_info: n_vocab          = 50304
0.00.039.947 I print_info: n_merges         = 50009
0.00.039.947 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.947 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.948 I print_info: LF token         = 187 'Ċ'
0.00.039.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.948 I print_info: max token length = 1024
0.00.039.951 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.931.576 I load_tensors: offloading 24 repeating layers to GPU
0.00.931.582 I load_tensors: offloading output layer to GPU
0.00.931.583 I load_tensors: offloaded 25/25 layers to GPU
0.00.931.612 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.931.615 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.933.121 I llama_init_from_model: n_seq_max     = 1
0.00.933.123 I llama_init_from_model: n_ctx         = 128
0.00.933.124 I llama_init_from_model: n_ctx_per_seq = 128
0.00.933.124 I llama_init_from_model: n_batch       = 128
0.00.933.124 I llama_init_from_model: n_ubatch      = 128
0.00.933.124 I llama_init_from_model: flash_attn    = 0
0.00.933.125 I llama_init_from_model: freq_base     = 10000.0
0.00.933.126 I llama_init_from_model: freq_scale    = 1
0.00.933.127 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.933.128 I ggml_metal_init: allocating
0.00.933.212 I ggml_metal_init: found device: Apple M4
0.00.933.224 I ggml_metal_init: picking default device: Apple M4
0.00.934.628 I ggml_metal_init: using embedded metal library
0.00.939.866 I ggml_metal_init: GPU name:   Apple M4
0.00.939.869 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.939.870 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.939.871 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.939.871 I ggml_metal_init: simdgroup reduction   = true
0.00.939.871 I ggml_metal_init: simdgroup matrix mul. = true
0.00.939.872 I ggml_metal_init: has residency sets    = true
0.00.939.872 I ggml_metal_init: has bfloat            = true
0.00.939.872 I ggml_metal_init: use bfloat            = true
0.00.939.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.939.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.955.021 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.958.447 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.958.456 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.958.515 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.961.810 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.961.811 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.961.812 I llama_init_from_model: graph nodes  = 967
0.00.961.812 I llama_init_from_model: graph splits = 2
0.00.961.815 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.961.815 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.992.002 I 
0.00.992.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.992.082 I perplexity: tokenizing the input ..
0.00.999.511 I perplexity: tokenization took 7.427 ms
0.00.999.519 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.139.183 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.140.589 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.140.619 I llama_perf_context_print:        load time =     981.52 ms
0.01.140.620 I llama_perf_context_print: prompt eval time =     138.72 ms /   128 tokens (    1.08 ms per token,   922.70 tokens per second)
0.01.140.621 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.140.621 I llama_perf_context_print:       total time =     148.62 ms /   129 tokens
0.01.140.993 I ggml_metal_free: deallocating

real	0m1.157s
user	0m0.076s
sys	0m0.176s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.246 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.694 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.683 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.688 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.695 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.696 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.696 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.698 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.698 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.698 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.699 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.699 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.700 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.702 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.702 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.702 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.346 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.345 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.012 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.014 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.014 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.015 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.015 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.015 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.016 I llama_model_loader: - type  f32:  194 tensors
0.00.025.016 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.016 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.017 I print_info: file format = GGUF V3 (latest)
0.00.025.017 I print_info: file type   = Q4_0
0.00.025.023 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.065 I load: special tokens cache size = 25
0.00.039.045 I load: token to piece cache size = 0.2984 MB
0.00.039.049 I print_info: arch             = gptneox
0.00.039.049 I print_info: vocab_only       = 0
0.00.039.050 I print_info: n_ctx_train      = 2048
0.00.039.050 I print_info: n_embd           = 2048
0.00.039.050 I print_info: n_layer          = 24
0.00.039.054 I print_info: n_head           = 16
0.00.039.055 I print_info: n_head_kv        = 16
0.00.039.055 I print_info: n_rot            = 32
0.00.039.055 I print_info: n_swa            = 0
0.00.039.055 I print_info: n_embd_head_k    = 128
0.00.039.057 I print_info: n_embd_head_v    = 128
0.00.039.058 I print_info: n_gqa            = 1
0.00.039.059 I print_info: n_embd_k_gqa     = 2048
0.00.039.059 I print_info: n_embd_v_gqa     = 2048
0.00.039.060 I print_info: f_norm_eps       = 1.0e-05
0.00.039.060 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.060 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.060 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.061 I print_info: f_logit_scale    = 0.0e+00
0.00.039.061 I print_info: n_ff             = 8192
0.00.039.061 I print_info: n_expert         = 0
0.00.039.061 I print_info: n_expert_used    = 0
0.00.039.062 I print_info: causal attn      = 1
0.00.039.062 I print_info: pooling type     = 0
0.00.039.062 I print_info: rope type        = 2
0.00.039.062 I print_info: rope scaling     = linear
0.00.039.062 I print_info: freq_base_train  = 10000.0
0.00.039.064 I print_info: freq_scale_train = 1
0.00.039.064 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.065 I print_info: rope_finetuned   = unknown
0.00.039.065 I print_info: ssm_d_conv       = 0
0.00.039.065 I print_info: ssm_d_inner      = 0
0.00.039.065 I print_info: ssm_d_state      = 0
0.00.039.065 I print_info: ssm_dt_rank      = 0
0.00.039.065 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.065 I print_info: model type       = 1.4B
0.00.039.066 I print_info: model params     = 1.41 B
0.00.039.066 I print_info: general.name     = 1.4B
0.00.039.067 I print_info: vocab type       = BPE
0.00.039.068 I print_info: n_vocab          = 50304
0.00.039.068 I print_info: n_merges         = 50009
0.00.039.068 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.069 I print_info: LF token         = 187 'Ċ'
0.00.039.070 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.070 I print_info: max token length = 1024
0.00.039.070 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.961 I load_tensors: offloading output layer to GPU
0.00.596.962 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.997 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.596.999 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.598.632 I llama_init_from_model: n_seq_max     = 1
0.00.598.634 I llama_init_from_model: n_ctx         = 128
0.00.598.635 I llama_init_from_model: n_ctx_per_seq = 128
0.00.598.636 I llama_init_from_model: n_batch       = 128
0.00.598.636 I llama_init_from_model: n_ubatch      = 128
0.00.598.636 I llama_init_from_model: flash_attn    = 0
0.00.598.639 I llama_init_from_model: freq_base     = 10000.0
0.00.598.639 I llama_init_from_model: freq_scale    = 1
0.00.598.640 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.598.645 I ggml_metal_init: allocating
0.00.598.789 I ggml_metal_init: found device: Apple M4
0.00.598.803 I ggml_metal_init: picking default device: Apple M4
0.00.600.740 I ggml_metal_init: using embedded metal library
0.00.606.333 I ggml_metal_init: GPU name:   Apple M4
0.00.606.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.343 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.344 I ggml_metal_init: simdgroup reduction   = true
0.00.606.344 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.345 I ggml_metal_init: has residency sets    = true
0.00.606.345 I ggml_metal_init: has bfloat            = true
0.00.606.345 I ggml_metal_init: use bfloat            = true
0.00.606.347 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.350 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.178 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.702 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.706 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.778 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.634.081 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.634.083 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.634.084 I llama_init_from_model: graph nodes  = 967
0.00.634.085 I llama_init_from_model: graph splits = 2
0.00.634.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.634.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.764 I 
0.00.658.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.849 I perplexity: tokenizing the input ..
0.00.665.773 I perplexity: tokenization took 6.919 ms
0.00.665.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.800.516 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.801.863 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.801.887 I llama_perf_context_print:        load time =     649.06 ms
0.00.801.888 I llama_perf_context_print: prompt eval time =     134.06 ms /   128 tokens (    1.05 ms per token,   954.76 tokens per second)
0.00.801.889 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.889 I llama_perf_context_print:       total time =     143.13 ms /   129 tokens
0.00.802.275 I ggml_metal_free: deallocating

real	0m0.818s
user	0m0.080s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.959 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.097 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.102 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.102 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.102 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.103 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.104 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.104 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.104 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.106 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.110 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.110 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.753 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.762 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.452 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.454 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.454 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.455 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.456 I llama_model_loader: - type  f32:  194 tensors
0.00.024.456 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.456 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.457 I print_info: file format = GGUF V3 (latest)
0.00.024.458 I print_info: file type   = Q4_1
0.00.024.459 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.556 I load: special tokens cache size = 25
0.00.038.722 I load: token to piece cache size = 0.2984 MB
0.00.038.726 I print_info: arch             = gptneox
0.00.038.726 I print_info: vocab_only       = 0
0.00.038.726 I print_info: n_ctx_train      = 2048
0.00.038.726 I print_info: n_embd           = 2048
0.00.038.726 I print_info: n_layer          = 24
0.00.038.731 I print_info: n_head           = 16
0.00.038.732 I print_info: n_head_kv        = 16
0.00.038.733 I print_info: n_rot            = 32
0.00.038.733 I print_info: n_swa            = 0
0.00.038.733 I print_info: n_embd_head_k    = 128
0.00.038.734 I print_info: n_embd_head_v    = 128
0.00.038.734 I print_info: n_gqa            = 1
0.00.038.735 I print_info: n_embd_k_gqa     = 2048
0.00.038.736 I print_info: n_embd_v_gqa     = 2048
0.00.038.736 I print_info: f_norm_eps       = 1.0e-05
0.00.038.737 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.737 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.737 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.737 I print_info: f_logit_scale    = 0.0e+00
0.00.038.738 I print_info: n_ff             = 8192
0.00.038.738 I print_info: n_expert         = 0
0.00.038.739 I print_info: n_expert_used    = 0
0.00.038.739 I print_info: causal attn      = 1
0.00.038.739 I print_info: pooling type     = 0
0.00.038.739 I print_info: rope type        = 2
0.00.038.739 I print_info: rope scaling     = linear
0.00.038.740 I print_info: freq_base_train  = 10000.0
0.00.038.740 I print_info: freq_scale_train = 1
0.00.038.740 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.740 I print_info: rope_finetuned   = unknown
0.00.038.742 I print_info: ssm_d_conv       = 0
0.00.038.742 I print_info: ssm_d_inner      = 0
0.00.038.743 I print_info: ssm_d_state      = 0
0.00.038.743 I print_info: ssm_dt_rank      = 0
0.00.038.743 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.743 I print_info: model type       = 1.4B
0.00.038.744 I print_info: model params     = 1.41 B
0.00.038.744 I print_info: general.name     = 1.4B
0.00.038.744 I print_info: vocab type       = BPE
0.00.038.744 I print_info: n_vocab          = 50304
0.00.038.745 I print_info: n_merges         = 50009
0.00.038.745 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.745 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.746 I print_info: LF token         = 187 'Ċ'
0.00.038.746 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.746 I print_info: max token length = 1024
0.00.038.747 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.582 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.597 I load_tensors: offloading output layer to GPU
0.00.645.597 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.635 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.645.637 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.647.287 I llama_init_from_model: n_seq_max     = 1
0.00.647.289 I llama_init_from_model: n_ctx         = 128
0.00.647.290 I llama_init_from_model: n_ctx_per_seq = 128
0.00.647.290 I llama_init_from_model: n_batch       = 128
0.00.647.290 I llama_init_from_model: n_ubatch      = 128
0.00.647.291 I llama_init_from_model: flash_attn    = 0
0.00.647.293 I llama_init_from_model: freq_base     = 10000.0
0.00.647.294 I llama_init_from_model: freq_scale    = 1
0.00.647.294 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.647.297 I ggml_metal_init: allocating
0.00.647.381 I ggml_metal_init: found device: Apple M4
0.00.647.398 I ggml_metal_init: picking default device: Apple M4
0.00.649.160 I ggml_metal_init: using embedded metal library
0.00.655.949 I ggml_metal_init: GPU name:   Apple M4
0.00.655.957 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.655.958 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.655.958 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.655.959 I ggml_metal_init: simdgroup reduction   = true
0.00.655.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.655.960 I ggml_metal_init: has residency sets    = true
0.00.655.960 I ggml_metal_init: has bfloat            = true
0.00.655.960 I ggml_metal_init: use bfloat            = true
0.00.655.961 I ggml_metal_init: hasUnifiedMemory      = true
0.00.655.969 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.673.978 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.598 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.677.602 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.650 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.789 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.680.791 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.680.791 I llama_init_from_model: graph nodes  = 967
0.00.680.792 I llama_init_from_model: graph splits = 2
0.00.680.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.795 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.530 I 
0.00.706.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.608 I perplexity: tokenizing the input ..
0.00.713.778 I perplexity: tokenization took 7.166 ms
0.00.713.786 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.850.310 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.851.647 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.851.677 I llama_perf_context_print:        load time =     697.56 ms
0.00.851.679 I llama_perf_context_print: prompt eval time =     135.60 ms /   128 tokens (    1.06 ms per token,   943.98 tokens per second)
0.00.851.680 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.680 I llama_perf_context_print:       total time =     145.15 ms /   129 tokens
0.00.852.079 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.815 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.821 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.821 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.824 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.825 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.826 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.829 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.509 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.525 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.212 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.214 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.214 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.215 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.215 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.216 I llama_model_loader: - type  f32:  194 tensors
0.00.025.216 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.216 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.217 I print_info: file format = GGUF V3 (latest)
0.00.025.218 I print_info: file type   = Q5_0
0.00.025.219 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.176 I load: special tokens cache size = 25
0.00.039.039 I load: token to piece cache size = 0.2984 MB
0.00.039.043 I print_info: arch             = gptneox
0.00.039.043 I print_info: vocab_only       = 0
0.00.039.044 I print_info: n_ctx_train      = 2048
0.00.039.044 I print_info: n_embd           = 2048
0.00.039.044 I print_info: n_layer          = 24
0.00.039.048 I print_info: n_head           = 16
0.00.039.049 I print_info: n_head_kv        = 16
0.00.039.049 I print_info: n_rot            = 32
0.00.039.049 I print_info: n_swa            = 0
0.00.039.049 I print_info: n_embd_head_k    = 128
0.00.039.049 I print_info: n_embd_head_v    = 128
0.00.039.050 I print_info: n_gqa            = 1
0.00.039.051 I print_info: n_embd_k_gqa     = 2048
0.00.039.051 I print_info: n_embd_v_gqa     = 2048
0.00.039.052 I print_info: f_norm_eps       = 1.0e-05
0.00.039.052 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.053 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.053 I print_info: f_logit_scale    = 0.0e+00
0.00.039.056 I print_info: n_ff             = 8192
0.00.039.056 I print_info: n_expert         = 0
0.00.039.056 I print_info: n_expert_used    = 0
0.00.039.057 I print_info: causal attn      = 1
0.00.039.057 I print_info: pooling type     = 0
0.00.039.057 I print_info: rope type        = 2
0.00.039.057 I print_info: rope scaling     = linear
0.00.039.058 I print_info: freq_base_train  = 10000.0
0.00.039.058 I print_info: freq_scale_train = 1
0.00.039.058 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.058 I print_info: rope_finetuned   = unknown
0.00.039.058 I print_info: ssm_d_conv       = 0
0.00.039.059 I print_info: ssm_d_inner      = 0
0.00.039.059 I print_info: ssm_d_state      = 0
0.00.039.059 I print_info: ssm_dt_rank      = 0
0.00.039.059 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.059 I print_info: model type       = 1.4B
0.00.039.060 I print_info: model params     = 1.41 B
0.00.039.060 I print_info: general.name     = 1.4B
0.00.039.061 I print_info: vocab type       = BPE
0.00.039.061 I print_info: n_vocab          = 50304
0.00.039.061 I print_info: n_merges         = 50009
0.00.039.062 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.063 I print_info: LF token         = 187 'Ċ'
0.00.039.064 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.064 I print_info: max token length = 1024
0.00.039.064 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.717.148 I load_tensors: offloading 24 repeating layers to GPU
0.00.717.164 I load_tensors: offloading output layer to GPU
0.00.717.164 I load_tensors: offloaded 25/25 layers to GPU
0.00.717.200 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.717.202 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.718.973 I llama_init_from_model: n_seq_max     = 1
0.00.718.977 I llama_init_from_model: n_ctx         = 128
0.00.718.977 I llama_init_from_model: n_ctx_per_seq = 128
0.00.718.977 I llama_init_from_model: n_batch       = 128
0.00.718.978 I llama_init_from_model: n_ubatch      = 128
0.00.718.978 I llama_init_from_model: flash_attn    = 0
0.00.718.981 I llama_init_from_model: freq_base     = 10000.0
0.00.718.981 I llama_init_from_model: freq_scale    = 1
0.00.718.982 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.718.984 I ggml_metal_init: allocating
0.00.719.047 I ggml_metal_init: found device: Apple M4
0.00.719.061 I ggml_metal_init: picking default device: Apple M4
0.00.720.557 I ggml_metal_init: using embedded metal library
0.00.726.989 I ggml_metal_init: GPU name:   Apple M4
0.00.726.994 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.726.995 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.726.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.726.999 I ggml_metal_init: simdgroup reduction   = true
0.00.726.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.727.000 I ggml_metal_init: has residency sets    = true
0.00.727.000 I ggml_metal_init: has bfloat            = true
0.00.727.000 I ggml_metal_init: use bfloat            = true
0.00.727.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.727.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.917 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.409 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.747.412 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.747.454 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.750.764 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.750.766 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.750.766 I llama_init_from_model: graph nodes  = 967
0.00.750.766 I llama_init_from_model: graph splits = 2
0.00.750.769 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.750.769 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.783.935 I 
0.00.784.020 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.028 I perplexity: tokenizing the input ..
0.00.791.092 I perplexity: tokenization took 7.06 ms
0.00.791.101 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.934.780 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.936.123 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.936.145 I llama_perf_context_print:        load time =     774.12 ms
0.00.936.147 I llama_perf_context_print: prompt eval time =     142.74 ms /   128 tokens (    1.12 ms per token,   896.73 tokens per second)
0.00.936.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.936.148 I llama_perf_context_print:       total time =     152.22 ms /   129 tokens
0.00.936.517 I ggml_metal_free: deallocating

real	0m0.953s
user	0m0.078s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.938 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.019.276 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.278 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.284 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.284 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.285 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.285 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.286 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.286 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.287 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.287 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.287 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.288 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.288 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.290 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.291 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.291 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.974 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.781 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.783 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.783 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.783 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.784 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.784 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.785 I llama_model_loader: - type  f32:  194 tensors
0.00.027.785 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.785 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.786 I print_info: file format = GGUF V3 (latest)
0.00.027.786 I print_info: file type   = Q5_1
0.00.027.787 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.036.082 I load: special tokens cache size = 25
0.00.042.165 I load: token to piece cache size = 0.2984 MB
0.00.042.169 I print_info: arch             = gptneox
0.00.042.169 I print_info: vocab_only       = 0
0.00.042.170 I print_info: n_ctx_train      = 2048
0.00.042.170 I print_info: n_embd           = 2048
0.00.042.170 I print_info: n_layer          = 24
0.00.042.175 I print_info: n_head           = 16
0.00.042.176 I print_info: n_head_kv        = 16
0.00.042.176 I print_info: n_rot            = 32
0.00.042.176 I print_info: n_swa            = 0
0.00.042.178 I print_info: n_embd_head_k    = 128
0.00.042.178 I print_info: n_embd_head_v    = 128
0.00.042.179 I print_info: n_gqa            = 1
0.00.042.179 I print_info: n_embd_k_gqa     = 2048
0.00.042.180 I print_info: n_embd_v_gqa     = 2048
0.00.042.181 I print_info: f_norm_eps       = 1.0e-05
0.00.042.181 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.182 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.182 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.183 I print_info: f_logit_scale    = 0.0e+00
0.00.042.183 I print_info: n_ff             = 8192
0.00.042.183 I print_info: n_expert         = 0
0.00.042.183 I print_info: n_expert_used    = 0
0.00.042.184 I print_info: causal attn      = 1
0.00.042.184 I print_info: pooling type     = 0
0.00.042.184 I print_info: rope type        = 2
0.00.042.184 I print_info: rope scaling     = linear
0.00.042.184 I print_info: freq_base_train  = 10000.0
0.00.042.185 I print_info: freq_scale_train = 1
0.00.042.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.185 I print_info: rope_finetuned   = unknown
0.00.042.185 I print_info: ssm_d_conv       = 0
0.00.042.185 I print_info: ssm_d_inner      = 0
0.00.042.186 I print_info: ssm_d_state      = 0
0.00.042.186 I print_info: ssm_dt_rank      = 0
0.00.042.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.186 I print_info: model type       = 1.4B
0.00.042.186 I print_info: model params     = 1.41 B
0.00.042.186 I print_info: general.name     = 1.4B
0.00.042.187 I print_info: vocab type       = BPE
0.00.042.187 I print_info: n_vocab          = 50304
0.00.042.187 I print_info: n_merges         = 50009
0.00.042.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.188 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.188 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.188 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.189 I print_info: LF token         = 187 'Ċ'
0.00.042.189 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.189 I print_info: max token length = 1024
0.00.042.192 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.688.634 I load_tensors: offloading 24 repeating layers to GPU
0.00.688.641 I load_tensors: offloading output layer to GPU
0.00.688.642 I load_tensors: offloaded 25/25 layers to GPU
0.00.688.667 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.688.670 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.690.142 I llama_init_from_model: n_seq_max     = 1
0.00.690.144 I llama_init_from_model: n_ctx         = 128
0.00.690.145 I llama_init_from_model: n_ctx_per_seq = 128
0.00.690.145 I llama_init_from_model: n_batch       = 128
0.00.690.145 I llama_init_from_model: n_ubatch      = 128
0.00.690.146 I llama_init_from_model: flash_attn    = 0
0.00.690.147 I llama_init_from_model: freq_base     = 10000.0
0.00.690.147 I llama_init_from_model: freq_scale    = 1
0.00.690.148 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.690.149 I ggml_metal_init: allocating
0.00.690.182 I ggml_metal_init: found device: Apple M4
0.00.690.191 I ggml_metal_init: picking default device: Apple M4
0.00.691.636 I ggml_metal_init: using embedded metal library
0.00.697.700 I ggml_metal_init: GPU name:   Apple M4
0.00.697.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.697.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.697.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.697.705 I ggml_metal_init: simdgroup reduction   = true
0.00.697.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.697.706 I ggml_metal_init: has residency sets    = true
0.00.697.706 I ggml_metal_init: has bfloat            = true
0.00.697.706 I ggml_metal_init: use bfloat            = true
0.00.697.707 I ggml_metal_init: hasUnifiedMemory      = true
0.00.697.709 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.699 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.718.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.718.159 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.718.209 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.336 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.721.338 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.721.338 I llama_init_from_model: graph nodes  = 967
0.00.721.339 I llama_init_from_model: graph splits = 2
0.00.721.341 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.721.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.362 I 
0.00.752.439 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.446 I perplexity: tokenizing the input ..
0.00.760.051 I perplexity: tokenization took 7.601 ms
0.00.760.064 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.902.499 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.903.845 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.903.868 I llama_perf_context_print:        load time =     743.42 ms
0.00.903.868 I llama_perf_context_print: prompt eval time =     141.46 ms /   128 tokens (    1.11 ms per token,   904.84 tokens per second)
0.00.903.869 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.903.869 I llama_perf_context_print:       total time =     151.51 ms /   129 tokens
0.00.904.228 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.079s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.120 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.353 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.131 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.026.137 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.139 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.141 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.141 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.142 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.143 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.143 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.144 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.144 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.145 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.146 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.147 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.147 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.275 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.676 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.805 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.808 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.035.809 I llama_model_loader: - type  f32:  194 tensors
0.00.035.809 I llama_model_loader: - type q2_K:   49 tensors
0.00.035.810 I llama_model_loader: - type q3_K:   48 tensors
0.00.035.810 I llama_model_loader: - type q6_K:    1 tensors
0.00.035.810 I print_info: file format = GGUF V3 (latest)
0.00.035.811 I print_info: file type   = Q2_K - Medium
0.00.035.812 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.897 I load: special tokens cache size = 25
0.00.053.337 I load: token to piece cache size = 0.2984 MB
0.00.053.341 I print_info: arch             = gptneox
0.00.053.342 I print_info: vocab_only       = 0
0.00.053.342 I print_info: n_ctx_train      = 2048
0.00.053.342 I print_info: n_embd           = 2048
0.00.053.342 I print_info: n_layer          = 24
0.00.053.347 I print_info: n_head           = 16
0.00.053.348 I print_info: n_head_kv        = 16
0.00.053.348 I print_info: n_rot            = 32
0.00.053.348 I print_info: n_swa            = 0
0.00.053.348 I print_info: n_embd_head_k    = 128
0.00.053.349 I print_info: n_embd_head_v    = 128
0.00.053.350 I print_info: n_gqa            = 1
0.00.053.351 I print_info: n_embd_k_gqa     = 2048
0.00.053.352 I print_info: n_embd_v_gqa     = 2048
0.00.053.352 I print_info: f_norm_eps       = 1.0e-05
0.00.053.353 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.353 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.353 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.353 I print_info: f_logit_scale    = 0.0e+00
0.00.053.354 I print_info: n_ff             = 8192
0.00.053.354 I print_info: n_expert         = 0
0.00.053.355 I print_info: n_expert_used    = 0
0.00.053.355 I print_info: causal attn      = 1
0.00.053.355 I print_info: pooling type     = 0
0.00.053.355 I print_info: rope type        = 2
0.00.053.357 I print_info: rope scaling     = linear
0.00.053.358 I print_info: freq_base_train  = 10000.0
0.00.053.358 I print_info: freq_scale_train = 1
0.00.053.358 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.359 I print_info: rope_finetuned   = unknown
0.00.053.360 I print_info: ssm_d_conv       = 0
0.00.053.360 I print_info: ssm_d_inner      = 0
0.00.053.363 I print_info: ssm_d_state      = 0
0.00.053.363 I print_info: ssm_dt_rank      = 0
0.00.053.363 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.363 I print_info: model type       = 1.4B
0.00.053.364 I print_info: model params     = 1.41 B
0.00.053.364 I print_info: general.name     = 1.4B
0.00.053.365 I print_info: vocab type       = BPE
0.00.053.365 I print_info: n_vocab          = 50304
0.00.053.365 I print_info: n_merges         = 50009
0.00.053.365 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.366 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.366 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.366 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.367 I print_info: LF token         = 187 'Ċ'
0.00.053.367 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.367 I print_info: max token length = 1024
0.00.053.368 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.425.294 I load_tensors: offloading 24 repeating layers to GPU
0.00.425.313 I load_tensors: offloading output layer to GPU
0.00.425.314 I load_tensors: offloaded 25/25 layers to GPU
0.00.425.349 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.425.350 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.426.896 I llama_init_from_model: n_seq_max     = 1
0.00.426.900 I llama_init_from_model: n_ctx         = 128
0.00.426.900 I llama_init_from_model: n_ctx_per_seq = 128
0.00.426.901 I llama_init_from_model: n_batch       = 128
0.00.426.901 I llama_init_from_model: n_ubatch      = 128
0.00.426.902 I llama_init_from_model: flash_attn    = 0
0.00.426.904 I llama_init_from_model: freq_base     = 10000.0
0.00.426.905 I llama_init_from_model: freq_scale    = 1
0.00.426.905 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.426.908 I ggml_metal_init: allocating
0.00.426.985 I ggml_metal_init: found device: Apple M4
0.00.426.997 I ggml_metal_init: picking default device: Apple M4
0.00.428.771 I ggml_metal_init: using embedded metal library
0.00.434.078 I ggml_metal_init: GPU name:   Apple M4
0.00.434.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.434.089 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.434.090 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.434.091 I ggml_metal_init: simdgroup reduction   = true
0.00.434.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.434.091 I ggml_metal_init: has residency sets    = true
0.00.434.092 I ggml_metal_init: has bfloat            = true
0.00.434.092 I ggml_metal_init: use bfloat            = true
0.00.434.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.434.097 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.455.842 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.459.474 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.459.490 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.459.551 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.462.912 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.462.915 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.462.915 I llama_init_from_model: graph nodes  = 967
0.00.462.916 I llama_init_from_model: graph splits = 2
0.00.462.919 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.462.919 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.495.477 I 
0.00.495.555 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.495.562 I perplexity: tokenizing the input ..
0.00.502.548 I perplexity: tokenization took 6.982 ms
0.00.502.554 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.648.466 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.649.882 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.649.898 I llama_perf_context_print:        load time =     480.11 ms
0.00.649.899 I llama_perf_context_print: prompt eval time =     144.99 ms /   128 tokens (    1.13 ms per token,   882.84 tokens per second)
0.00.649.900 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.649.900 I llama_perf_context_print:       total time =     154.43 ms /   129 tokens
0.00.650.258 I ggml_metal_free: deallocating

real	0m0.683s
user	0m0.086s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.022.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.337 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.338 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.338 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.338 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.339 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.340 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.340 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.340 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.341 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.341 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.342 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.343 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.345 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.346 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.004 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.978 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.693 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.693 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.694 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.694 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.030.695 I llama_model_loader: - type  f32:  194 tensors
0.00.030.695 I llama_model_loader: - type q3_K:   25 tensors
0.00.030.695 I llama_model_loader: - type q4_K:   71 tensors
0.00.030.696 I llama_model_loader: - type q5_K:    1 tensors
0.00.030.696 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.696 I print_info: file format = GGUF V3 (latest)
0.00.030.701 I print_info: file type   = Q3_K - Medium
0.00.030.703 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.038.655 I load: special tokens cache size = 25
0.00.044.879 I load: token to piece cache size = 0.2984 MB
0.00.044.883 I print_info: arch             = gptneox
0.00.044.883 I print_info: vocab_only       = 0
0.00.044.884 I print_info: n_ctx_train      = 2048
0.00.044.884 I print_info: n_embd           = 2048
0.00.044.884 I print_info: n_layer          = 24
0.00.044.888 I print_info: n_head           = 16
0.00.044.889 I print_info: n_head_kv        = 16
0.00.044.889 I print_info: n_rot            = 32
0.00.044.889 I print_info: n_swa            = 0
0.00.044.889 I print_info: n_embd_head_k    = 128
0.00.044.889 I print_info: n_embd_head_v    = 128
0.00.044.890 I print_info: n_gqa            = 1
0.00.044.890 I print_info: n_embd_k_gqa     = 2048
0.00.044.894 I print_info: n_embd_v_gqa     = 2048
0.00.044.894 I print_info: f_norm_eps       = 1.0e-05
0.00.044.895 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.895 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.895 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.895 I print_info: f_logit_scale    = 0.0e+00
0.00.044.896 I print_info: n_ff             = 8192
0.00.044.896 I print_info: n_expert         = 0
0.00.044.896 I print_info: n_expert_used    = 0
0.00.044.896 I print_info: causal attn      = 1
0.00.044.896 I print_info: pooling type     = 0
0.00.044.897 I print_info: rope type        = 2
0.00.044.897 I print_info: rope scaling     = linear
0.00.044.897 I print_info: freq_base_train  = 10000.0
0.00.044.898 I print_info: freq_scale_train = 1
0.00.044.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.898 I print_info: rope_finetuned   = unknown
0.00.044.898 I print_info: ssm_d_conv       = 0
0.00.044.898 I print_info: ssm_d_inner      = 0
0.00.044.898 I print_info: ssm_d_state      = 0
0.00.044.899 I print_info: ssm_dt_rank      = 0
0.00.044.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.899 I print_info: model type       = 1.4B
0.00.044.899 I print_info: model params     = 1.41 B
0.00.044.899 I print_info: general.name     = 1.4B
0.00.044.900 I print_info: vocab type       = BPE
0.00.044.900 I print_info: n_vocab          = 50304
0.00.044.900 I print_info: n_merges         = 50009
0.00.044.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.901 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.901 I print_info: LF token         = 187 'Ċ'
0.00.044.901 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.903 I print_info: max token length = 1024
0.00.044.905 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.842 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.858 I load_tensors: offloading output layer to GPU
0.00.599.859 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.891 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.599.893 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.601.573 I llama_init_from_model: n_seq_max     = 1
0.00.601.576 I llama_init_from_model: n_ctx         = 128
0.00.601.576 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.577 I llama_init_from_model: n_batch       = 128
0.00.601.578 I llama_init_from_model: n_ubatch      = 128
0.00.601.578 I llama_init_from_model: flash_attn    = 0
0.00.601.580 I llama_init_from_model: freq_base     = 10000.0
0.00.601.581 I llama_init_from_model: freq_scale    = 1
0.00.601.581 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.584 I ggml_metal_init: allocating
0.00.601.662 I ggml_metal_init: found device: Apple M4
0.00.601.676 I ggml_metal_init: picking default device: Apple M4
0.00.603.553 I ggml_metal_init: using embedded metal library
0.00.609.310 I ggml_metal_init: GPU name:   Apple M4
0.00.609.315 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.316 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.317 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.318 I ggml_metal_init: simdgroup reduction   = true
0.00.609.318 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.318 I ggml_metal_init: has residency sets    = true
0.00.609.319 I ggml_metal_init: has bfloat            = true
0.00.609.319 I ggml_metal_init: use bfloat            = true
0.00.609.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.324 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.987 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.632.537 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.632.540 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.584 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.635.901 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.635.903 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.635.903 I llama_init_from_model: graph nodes  = 967
0.00.635.904 I llama_init_from_model: graph splits = 2
0.00.635.907 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.907 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.000 I 
0.00.666.090 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.097 I perplexity: tokenizing the input ..
0.00.673.464 I perplexity: tokenization took 7.365 ms
0.00.673.476 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.558 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.820.884 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.820.909 I llama_perf_context_print:        load time =     657.07 ms
0.00.820.910 I llama_perf_context_print: prompt eval time =     145.15 ms /   128 tokens (    1.13 ms per token,   881.83 tokens per second)
0.00.820.911 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.911 I llama_perf_context_print:       total time =     154.91 ms /   129 tokens
0.00.821.295 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.080s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.718 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.079 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.025.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.088 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.088 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.089 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.089 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.089 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.090 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.091 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.091 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.091 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.092 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.092 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.093 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.094 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.095 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.095 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.706 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.711 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.361 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.362 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.362 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.033.363 I llama_model_loader: - type  f32:  194 tensors
0.00.033.364 I llama_model_loader: - type q4_K:   61 tensors
0.00.033.364 I llama_model_loader: - type q5_K:   24 tensors
0.00.033.364 I llama_model_loader: - type q6_K:   13 tensors
0.00.033.365 I print_info: file format = GGUF V3 (latest)
0.00.033.365 I print_info: file type   = Q4_K - Medium
0.00.033.366 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.041.360 I load: special tokens cache size = 25
0.00.048.006 I load: token to piece cache size = 0.2984 MB
0.00.048.009 I print_info: arch             = gptneox
0.00.048.009 I print_info: vocab_only       = 0
0.00.048.009 I print_info: n_ctx_train      = 2048
0.00.048.010 I print_info: n_embd           = 2048
0.00.048.010 I print_info: n_layer          = 24
0.00.048.014 I print_info: n_head           = 16
0.00.048.015 I print_info: n_head_kv        = 16
0.00.048.015 I print_info: n_rot            = 32
0.00.048.015 I print_info: n_swa            = 0
0.00.048.016 I print_info: n_embd_head_k    = 128
0.00.048.017 I print_info: n_embd_head_v    = 128
0.00.048.017 I print_info: n_gqa            = 1
0.00.048.018 I print_info: n_embd_k_gqa     = 2048
0.00.048.019 I print_info: n_embd_v_gqa     = 2048
0.00.048.019 I print_info: f_norm_eps       = 1.0e-05
0.00.048.019 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.019 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.020 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.020 I print_info: f_logit_scale    = 0.0e+00
0.00.048.020 I print_info: n_ff             = 8192
0.00.048.020 I print_info: n_expert         = 0
0.00.048.021 I print_info: n_expert_used    = 0
0.00.048.021 I print_info: causal attn      = 1
0.00.048.021 I print_info: pooling type     = 0
0.00.048.021 I print_info: rope type        = 2
0.00.048.021 I print_info: rope scaling     = linear
0.00.048.022 I print_info: freq_base_train  = 10000.0
0.00.048.024 I print_info: freq_scale_train = 1
0.00.048.024 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.025 I print_info: rope_finetuned   = unknown
0.00.048.025 I print_info: ssm_d_conv       = 0
0.00.048.025 I print_info: ssm_d_inner      = 0
0.00.048.025 I print_info: ssm_d_state      = 0
0.00.048.025 I print_info: ssm_dt_rank      = 0
0.00.048.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.025 I print_info: model type       = 1.4B
0.00.048.026 I print_info: model params     = 1.41 B
0.00.048.026 I print_info: general.name     = 1.4B
0.00.048.026 I print_info: vocab type       = BPE
0.00.048.026 I print_info: n_vocab          = 50304
0.00.048.027 I print_info: n_merges         = 50009
0.00.048.027 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.027 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.027 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.027 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.027 I print_info: LF token         = 187 'Ċ'
0.00.048.028 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.028 I print_info: max token length = 1024
0.00.048.028 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.981 I load_tensors: offloading output layer to GPU
0.00.588.982 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.020 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.589.025 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.590.841 I llama_init_from_model: n_seq_max     = 1
0.00.590.843 I llama_init_from_model: n_ctx         = 128
0.00.590.844 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.844 I llama_init_from_model: n_batch       = 128
0.00.590.845 I llama_init_from_model: n_ubatch      = 128
0.00.590.845 I llama_init_from_model: flash_attn    = 0
0.00.590.847 I llama_init_from_model: freq_base     = 10000.0
0.00.590.848 I llama_init_from_model: freq_scale    = 1
0.00.590.848 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.851 I ggml_metal_init: allocating
0.00.590.967 I ggml_metal_init: found device: Apple M4
0.00.590.981 I ggml_metal_init: picking default device: Apple M4
0.00.592.937 I ggml_metal_init: using embedded metal library
0.00.599.627 I ggml_metal_init: GPU name:   Apple M4
0.00.599.632 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.633 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.634 I ggml_metal_init: simdgroup reduction   = true
0.00.599.634 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.634 I ggml_metal_init: has residency sets    = true
0.00.599.635 I ggml_metal_init: has bfloat            = true
0.00.599.635 I ggml_metal_init: use bfloat            = true
0.00.599.636 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.638 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.873 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.413 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.417 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.456 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.649 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.651 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.651 I llama_init_from_model: graph nodes  = 967
0.00.624.651 I llama_init_from_model: graph splits = 2
0.00.624.655 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.655 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.639 I 
0.00.650.724 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.733 I perplexity: tokenizing the input ..
0.00.658.468 I perplexity: tokenization took 7.732 ms
0.00.658.476 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.152 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.793.509 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.793.532 I llama_perf_context_print:        load time =     641.91 ms
0.00.793.533 I llama_perf_context_print: prompt eval time =     132.79 ms /   128 tokens (    1.04 ms per token,   963.91 tokens per second)
0.00.793.533 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.533 I llama_perf_context_print:       total time =     142.90 ms /   129 tokens
0.00.793.909 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.081s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.562 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.565 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.565 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.567 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.568 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.072 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.072 I llama_model_loader: - type  f32:  194 tensors
0.00.025.073 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.073 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.073 I print_info: file format = GGUF V3 (latest)
0.00.025.074 I print_info: file type   = Q5_K - Medium
0.00.025.076 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.351 I load: special tokens cache size = 25
0.00.039.462 I load: token to piece cache size = 0.2984 MB
0.00.039.466 I print_info: arch             = gptneox
0.00.039.466 I print_info: vocab_only       = 0
0.00.039.467 I print_info: n_ctx_train      = 2048
0.00.039.467 I print_info: n_embd           = 2048
0.00.039.467 I print_info: n_layer          = 24
0.00.039.471 I print_info: n_head           = 16
0.00.039.472 I print_info: n_head_kv        = 16
0.00.039.472 I print_info: n_rot            = 32
0.00.039.473 I print_info: n_swa            = 0
0.00.039.473 I print_info: n_embd_head_k    = 128
0.00.039.473 I print_info: n_embd_head_v    = 128
0.00.039.474 I print_info: n_gqa            = 1
0.00.039.475 I print_info: n_embd_k_gqa     = 2048
0.00.039.475 I print_info: n_embd_v_gqa     = 2048
0.00.039.476 I print_info: f_norm_eps       = 1.0e-05
0.00.039.476 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.476 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.476 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.477 I print_info: f_logit_scale    = 0.0e+00
0.00.039.477 I print_info: n_ff             = 8192
0.00.039.477 I print_info: n_expert         = 0
0.00.039.478 I print_info: n_expert_used    = 0
0.00.039.478 I print_info: causal attn      = 1
0.00.039.478 I print_info: pooling type     = 0
0.00.039.478 I print_info: rope type        = 2
0.00.039.478 I print_info: rope scaling     = linear
0.00.039.479 I print_info: freq_base_train  = 10000.0
0.00.039.479 I print_info: freq_scale_train = 1
0.00.039.479 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.479 I print_info: rope_finetuned   = unknown
0.00.039.480 I print_info: ssm_d_conv       = 0
0.00.039.480 I print_info: ssm_d_inner      = 0
0.00.039.482 I print_info: ssm_d_state      = 0
0.00.039.482 I print_info: ssm_dt_rank      = 0
0.00.039.482 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.483 I print_info: model type       = 1.4B
0.00.039.483 I print_info: model params     = 1.41 B
0.00.039.483 I print_info: general.name     = 1.4B
0.00.039.484 I print_info: vocab type       = BPE
0.00.039.484 I print_info: n_vocab          = 50304
0.00.039.484 I print_info: n_merges         = 50009
0.00.039.484 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.484 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: LF token         = 187 'Ċ'
0.00.039.485 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.485 I print_info: max token length = 1024
0.00.039.486 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.971 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.977 I load_tensors: offloading output layer to GPU
0.00.600.978 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.005 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.601.008 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.602.596 I llama_init_from_model: n_seq_max     = 1
0.00.602.598 I llama_init_from_model: n_ctx         = 128
0.00.602.598 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.598 I llama_init_from_model: n_batch       = 128
0.00.602.599 I llama_init_from_model: n_ubatch      = 128
0.00.602.599 I llama_init_from_model: flash_attn    = 0
0.00.602.600 I llama_init_from_model: freq_base     = 10000.0
0.00.602.601 I llama_init_from_model: freq_scale    = 1
0.00.602.602 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.604 I ggml_metal_init: allocating
0.00.602.678 I ggml_metal_init: found device: Apple M4
0.00.602.691 I ggml_metal_init: picking default device: Apple M4
0.00.604.248 I ggml_metal_init: using embedded metal library
0.00.610.317 I ggml_metal_init: GPU name:   Apple M4
0.00.610.320 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.322 I ggml_metal_init: simdgroup reduction   = true
0.00.610.322 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.323 I ggml_metal_init: has residency sets    = true
0.00.610.323 I ggml_metal_init: has bfloat            = true
0.00.610.323 I ggml_metal_init: use bfloat            = true
0.00.610.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.328 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.844 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.557 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.560 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.619 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.979 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.980 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.981 I llama_init_from_model: graph nodes  = 967
0.00.633.981 I llama_init_from_model: graph splits = 2
0.00.633.984 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.984 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.884 I 
0.00.668.975 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.983 I perplexity: tokenizing the input ..
0.00.674.976 I perplexity: tokenization took 5.989 ms
0.00.674.983 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.163 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.813.574 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.813.600 I llama_perf_context_print:        load time =     659.00 ms
0.00.813.600 I llama_perf_context_print: prompt eval time =     136.28 ms /   128 tokens (    1.06 ms per token,   939.24 tokens per second)
0.00.813.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.601 I llama_perf_context_print:       total time =     144.72 ms /   129 tokens
0.00.813.957 I ggml_metal_free: deallocating

real	0m0.830s
user	0m0.076s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.406 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.201 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.217 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.002 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.745 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.747 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.747 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.747 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.748 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.748 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.749 I llama_model_loader: - type  f32:  194 tensors
0.00.024.749 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.750 I print_info: file format = GGUF V3 (latest)
0.00.024.751 I print_info: file type   = Q6_K
0.00.024.752 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.907 I load: special tokens cache size = 25
0.00.039.047 I load: token to piece cache size = 0.2984 MB
0.00.039.051 I print_info: arch             = gptneox
0.00.039.051 I print_info: vocab_only       = 0
0.00.039.051 I print_info: n_ctx_train      = 2048
0.00.039.052 I print_info: n_embd           = 2048
0.00.039.052 I print_info: n_layer          = 24
0.00.039.056 I print_info: n_head           = 16
0.00.039.059 I print_info: n_head_kv        = 16
0.00.039.059 I print_info: n_rot            = 32
0.00.039.060 I print_info: n_swa            = 0
0.00.039.060 I print_info: n_embd_head_k    = 128
0.00.039.060 I print_info: n_embd_head_v    = 128
0.00.039.061 I print_info: n_gqa            = 1
0.00.039.061 I print_info: n_embd_k_gqa     = 2048
0.00.039.062 I print_info: n_embd_v_gqa     = 2048
0.00.039.062 I print_info: f_norm_eps       = 1.0e-05
0.00.039.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.063 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.063 I print_info: f_logit_scale    = 0.0e+00
0.00.039.064 I print_info: n_ff             = 8192
0.00.039.065 I print_info: n_expert         = 0
0.00.039.067 I print_info: n_expert_used    = 0
0.00.039.067 I print_info: causal attn      = 1
0.00.039.067 I print_info: pooling type     = 0
0.00.039.068 I print_info: rope type        = 2
0.00.039.068 I print_info: rope scaling     = linear
0.00.039.068 I print_info: freq_base_train  = 10000.0
0.00.039.068 I print_info: freq_scale_train = 1
0.00.039.069 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.069 I print_info: rope_finetuned   = unknown
0.00.039.069 I print_info: ssm_d_conv       = 0
0.00.039.069 I print_info: ssm_d_inner      = 0
0.00.039.069 I print_info: ssm_d_state      = 0
0.00.039.069 I print_info: ssm_dt_rank      = 0
0.00.039.069 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.070 I print_info: model type       = 1.4B
0.00.039.070 I print_info: model params     = 1.41 B
0.00.039.070 I print_info: general.name     = 1.4B
0.00.039.071 I print_info: vocab type       = BPE
0.00.039.071 I print_info: n_vocab          = 50304
0.00.039.071 I print_info: n_merges         = 50009
0.00.039.071 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.071 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: LF token         = 187 'Ċ'
0.00.039.072 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.072 I print_info: max token length = 1024
0.00.039.073 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.954 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.974 I load_tensors: offloading output layer to GPU
0.00.630.974 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.004 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.631.006 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.632.525 I llama_init_from_model: n_seq_max     = 1
0.00.632.534 I llama_init_from_model: n_ctx         = 128
0.00.632.534 I llama_init_from_model: n_ctx_per_seq = 128
0.00.632.535 I llama_init_from_model: n_batch       = 128
0.00.632.535 I llama_init_from_model: n_ubatch      = 128
0.00.632.536 I llama_init_from_model: flash_attn    = 0
0.00.632.538 I llama_init_from_model: freq_base     = 10000.0
0.00.632.539 I llama_init_from_model: freq_scale    = 1
0.00.632.539 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.542 I ggml_metal_init: allocating
0.00.632.628 I ggml_metal_init: found device: Apple M4
0.00.632.643 I ggml_metal_init: picking default device: Apple M4
0.00.634.343 I ggml_metal_init: using embedded metal library
0.00.637.819 I ggml_metal_init: GPU name:   Apple M4
0.00.637.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.637.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.637.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.637.823 I ggml_metal_init: simdgroup reduction   = true
0.00.637.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.637.824 I ggml_metal_init: has residency sets    = true
0.00.637.824 I ggml_metal_init: has bfloat            = true
0.00.637.824 I ggml_metal_init: use bfloat            = true
0.00.637.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.637.828 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.647.689 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.649.423 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.649.426 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.649.452 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.037 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.039 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.039 I llama_init_from_model: graph nodes  = 967
0.00.651.039 I llama_init_from_model: graph splits = 2
0.00.651.041 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.680.563 I 
0.00.680.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.680.601 I perplexity: tokenizing the input ..
0.00.684.449 I perplexity: tokenization took 3.846 ms
0.00.684.454 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.814.810 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.816.252 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.816.272 I llama_perf_context_print:        load time =     671.15 ms
0.00.816.274 I llama_perf_context_print: prompt eval time =     130.12 ms /   128 tokens (    1.02 ms per token,   983.68 tokens per second)
0.00.816.275 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.275 I llama_perf_context_print:       total time =     135.71 ms /   129 tokens
0.00.816.595 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.065s
sys	0m0.144s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.215 I build: 4760 (36c258ee) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.270 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.406 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.413 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.416 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.416 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.416 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.417 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.418 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.418 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.419 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.420 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.048 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.812 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.813 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.815 I llama_model_loader: - type  f32:  194 tensors
0.00.036.815 I llama_model_loader: - type  f16:   98 tensors
0.00.036.816 I print_info: file format = GGUF V3 (latest)
0.00.036.816 I print_info: file type   = all F32 (guessed)
0.00.036.817 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.044.734 I load: special tokens cache size = 25
0.00.050.705 I load: token to piece cache size = 0.2984 MB
0.00.050.708 I print_info: arch             = gptneox
0.00.050.708 I print_info: vocab_only       = 0
0.00.050.708 I print_info: n_ctx_train      = 2048
0.00.050.709 I print_info: n_embd           = 2048
0.00.050.709 I print_info: n_layer          = 24
0.00.050.712 I print_info: n_head           = 16
0.00.050.713 I print_info: n_head_kv        = 16
0.00.050.713 I print_info: n_rot            = 32
0.00.050.713 I print_info: n_swa            = 0
0.00.050.713 I print_info: n_embd_head_k    = 128
0.00.050.713 I print_info: n_embd_head_v    = 128
0.00.050.714 I print_info: n_gqa            = 1
0.00.050.715 I print_info: n_embd_k_gqa     = 2048
0.00.050.715 I print_info: n_embd_v_gqa     = 2048
0.00.050.716 I print_info: f_norm_eps       = 1.0e-05
0.00.050.716 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.716 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.717 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.717 I print_info: f_logit_scale    = 0.0e+00
0.00.050.717 I print_info: n_ff             = 8192
0.00.050.717 I print_info: n_expert         = 0
0.00.050.718 I print_info: n_expert_used    = 0
0.00.050.718 I print_info: causal attn      = 1
0.00.050.718 I print_info: pooling type     = 0
0.00.050.718 I print_info: rope type        = 2
0.00.050.718 I print_info: rope scaling     = linear
0.00.050.721 I print_info: freq_base_train  = 10000.0
0.00.050.722 I print_info: freq_scale_train = 1
0.00.050.722 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.722 I print_info: rope_finetuned   = unknown
0.00.050.722 I print_info: ssm_d_conv       = 0
0.00.050.724 I print_info: ssm_d_inner      = 0
0.00.050.724 I print_info: ssm_d_state      = 0
0.00.050.724 I print_info: ssm_dt_rank      = 0
0.00.050.724 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.724 I print_info: model type       = 1.4B
0.00.050.725 I print_info: model params     = 1.41 B
0.00.050.725 I print_info: general.name     = 1.4B
0.00.050.725 I print_info: vocab type       = BPE
0.00.050.726 I print_info: n_vocab          = 50304
0.00.050.726 I print_info: n_merges         = 50009
0.00.050.726 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.726 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.726 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: LF token         = 187 'Ċ'
0.00.050.727 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.727 I print_info: max token length = 1024
0.00.050.728 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.403.114 I load_tensors: offloading 24 repeating layers to GPU
0.01.403.118 I load_tensors: offloading output layer to GPU
0.01.403.119 I load_tensors: offloaded 25/25 layers to GPU
0.01.403.138 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.403.139 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.403.629 I llama_init_from_model: n_seq_max     = 1
0.01.403.630 I llama_init_from_model: n_ctx         = 128
0.01.403.630 I llama_init_from_model: n_ctx_per_seq = 128
0.01.403.630 I llama_init_from_model: n_batch       = 128
0.01.403.630 I llama_init_from_model: n_ubatch      = 128
0.01.403.631 I llama_init_from_model: flash_attn    = 0
0.01.403.631 I llama_init_from_model: freq_base     = 10000.0
0.01.403.631 I llama_init_from_model: freq_scale    = 1
0.01.403.632 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.403.633 I ggml_metal_init: allocating
0.01.403.676 I ggml_metal_init: found device: Apple M4
0.01.403.682 I ggml_metal_init: picking default device: Apple M4
0.01.404.345 I ggml_metal_init: using embedded metal library
0.01.406.788 I ggml_metal_init: GPU name:   Apple M4
0.01.406.789 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.406.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.406.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.406.791 I ggml_metal_init: simdgroup reduction   = true
0.01.406.791 I ggml_metal_init: simdgroup matrix mul. = true
0.01.406.791 I ggml_metal_init: has residency sets    = true
0.01.406.791 I ggml_metal_init: has bfloat            = true
0.01.406.791 I ggml_metal_init: use bfloat            = true
0.01.406.792 I ggml_metal_init: hasUnifiedMemory      = true
0.01.406.793 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.416.914 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.418.644 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.418.646 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.418.675 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.420.229 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.420.230 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.420.230 I llama_init_from_model: graph nodes  = 967
0.01.420.230 I llama_init_from_model: graph splits = 2
0.01.420.232 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.420.232 I 
0.01.420.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.420.274 I compute_imatrix: tokenizing the input ..
0.01.424.186 I compute_imatrix: tokenization took 3.912 ms
0.01.424.188 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.696.476 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.699.126 I llama_perf_context_print:        load time =    1679.20 ms
0.01.699.127 I llama_perf_context_print: prompt eval time =     270.44 ms /   128 tokens (    2.11 ms per token,   473.30 tokens per second)
0.01.699.127 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.699.128 I llama_perf_context_print:       total time =    1681.85 ms /   129 tokens
0.01.699.667 I ggml_metal_free: deallocating

real	0m1.880s
user	0m0.111s
sys	0m0.214s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4760 (36c258ee)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116604a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116605160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116605710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116605cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116606270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116606820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116606dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116607380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116607930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116607e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116608330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116608830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116609350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116609b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11660a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11660aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11660b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11660b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11660bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11660c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11660ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11660d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11660dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11660e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11660ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11660ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11660f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1166101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116610700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1166109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116610e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116611120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1166119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116611ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1166121b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116612650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116612af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116612f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116613430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1166138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116613d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1166146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x116614b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116614e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116615420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116615a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116616350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116616960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116616f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116617580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116617b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1166181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1166187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116618fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x116619440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1166198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11661a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11661a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11661ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11661b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11661b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11661ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11661bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11661c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11661c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11661ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11661d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11661d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11661daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11661df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11661e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11661e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11661ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11661f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11661f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11661fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1166203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116620910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116620e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1166213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116621900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116621e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1166223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1166228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116622e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116623390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1166238e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x116623e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116624380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1166248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116624e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116625370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1166258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116625e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116626360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116616040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1166267d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1166274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116627a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116627f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1166284c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116628a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116628f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1166294b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116629a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116629f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11662a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11662a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11662af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11662b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11662b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11662bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11662c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11662c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11662cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11662d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11662d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11662d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11662de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11662e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11662e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11662ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11662f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11662f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11662f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11662fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116630330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1166307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x116630c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116631110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1166315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116631a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x116631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1166343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1166351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x116635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1166368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1166376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1166384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11663a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11663a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11663a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11663ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11663b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11663b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11663bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11663c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11663c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11663ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11663ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11663d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11663d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11663dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11663e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11663e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11663ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11663ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11663f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11663f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11663fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1166418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1166421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116642be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116643130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116643680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116643bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116643e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1166444a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116644ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1166450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1166458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116645d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116646010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116646620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116646c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116647420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1166478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116647d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116648200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1166489b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116648f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116649450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1166499a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116649ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11664a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11664a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11664aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11664b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11664b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11664bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11664c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11664c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11664cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11664d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11664d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11664deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11664e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11664e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11664eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11664f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11664f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11664fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1166503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x116650930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116650e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1166513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x116651920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116651e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1166523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x116652910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116652e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1166533b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x116653900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116653e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1166543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1166548f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116654e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116655390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1166558e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116655e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116656380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1166568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116656e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116657370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1166578c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116657e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116658360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1166588b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116658e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116659350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1166598a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116659df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11665a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11665a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11665ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11665b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11665b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11665bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11665c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11665c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11665ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11665cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11665d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11665d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11665dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11665e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11665e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11665eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11665ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11665f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11665f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11665fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116660500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116660c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116661340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116661a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116661d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116662510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1166627d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x116662de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.739.084 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.739.088 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116662a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116644760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116644150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116644d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116617e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116619e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1166468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11660f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116615cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116616610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1166150d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116617230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11660e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11661a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116626a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116661fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1166113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1166116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x116646ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116645380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11660f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11660fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11660fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116663240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x116663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1166637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116663a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x116663d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x116664000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1166642c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116664580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116664840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116664b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116664dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116665080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116665340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116665600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1166658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116665b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116665e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116666100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1166663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116666680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116666940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116666c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116666ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116667180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116667440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116667700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1166679c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116667c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116667f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116668200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1166684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116668a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x116668d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116668fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116669280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116669540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116669800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116669ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116669d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11666a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11666a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11666a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11666a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11666ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11666ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11666b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11666b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11666b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11666b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11666bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11666be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11666c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11666c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11666c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11666c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11666cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11666cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11666d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11666d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11666d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11666da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11666dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11666df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11666e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11666e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11666e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11666ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11666ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11666f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11666f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11666f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11666f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11666fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11666fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116670080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116670340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116670600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1166708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116670b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116670e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116671100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1166713c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116671680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116671940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116671c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116671ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116672180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x116672440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116672700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1166729c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116672c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116672f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116673200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1166734c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x116673780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116673a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x116673d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116673fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x116674280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116674540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x116674800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x116674ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x116674d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x116675040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x116675300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1166755c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x116675880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x116675b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x116675e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1166760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x116676380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x116676640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x116676900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x116676bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x116676e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x116677140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x116677400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1166776c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x116677980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x116677c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x116677f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1166781c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116678480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116678740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116678a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116678cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116678f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116679240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116679500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1166797c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116679a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116679d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11667a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11667a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11667a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11667a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11667ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11667adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11667b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11667b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11667b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11667b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11667bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11667be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11667c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11667c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11667c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11667c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11667cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11667cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11667d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11667d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11667d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11667d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11667dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11667df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11667e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11667e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11667e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11667ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11667ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11667efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11667f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11667f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11667f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11667fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11667fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x116680040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116680300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1166805c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116680880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116680b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116680e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1166810c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116681380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x116681640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116681900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116681bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116682100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116682640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116682900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116682da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116683240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1166836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116683e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116684150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116684410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x116684880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116684cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116685160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1166855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116685a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116685eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116686320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116686790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x116686c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116687070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1166874e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116687950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x116687dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116688230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1166886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116688b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116688f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1166893f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116689860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116689cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11668a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11668a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11668aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11668ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11668b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11668b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11668bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11668c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11668c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11668c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11668cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11668d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11668d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11668daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11668df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11668e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11668e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11668ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11668f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11668f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11668fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11668fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1166902e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116690750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116690bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116691030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1166914a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116691910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x116691d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1166921f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116692660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116692ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116692f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1166933b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116693820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116693c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116694100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116694570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1166949e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116694e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1166952c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116695730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116695ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116696010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116696480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1166968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x116696d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1166971d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116697640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x116697ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116698520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116698c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116699360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116699a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116699d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11669a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11669a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11669ae00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126607970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126607de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126608250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1266086c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126608b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126608fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126609410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126609880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126609cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12660a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12660a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12660ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12660b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12660bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12660c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12660ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12660d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12660dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12660e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12660eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12660f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12660f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1266100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1266111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1266114a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126611910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126611d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1266121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1266126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126612c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126613070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126613330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1266137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126613c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126614170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126614670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126614b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126615070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126615570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126615a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126615f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126616470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126616970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126616de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126617250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1266176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126617b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126617fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126618880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126618cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126619160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1266195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126619da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12661a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12661a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12661ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12661b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12661b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12661bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12661c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12661c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12661ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12661cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12661d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12661d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12661dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12661e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12661e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12661ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12661ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12661f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12661f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12661ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126620460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1266209b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126620f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126621450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1266219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126621ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126622440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126622ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126623430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126623980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126623ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126624420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126624970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126624ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126625410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126625960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126625eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126626400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126626950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126626ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1266273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126627940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126627e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1266283e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126628930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126628e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1266293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126629920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126629e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12662a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12662a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12662ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12662b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12662b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12662be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12662c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12662c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12662cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12662d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12662d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12662dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12662df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12662e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12662e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12662ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12662f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12662f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12662fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12662ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126630900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126630da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126631240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1266316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126631b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126632020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1266324c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126632960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1266332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126633740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126633be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126634080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126634520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1266349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126634e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1266357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126635c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1266360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126636580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126636a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126636ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126637800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126637ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126638140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1266385e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126638a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126638f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1266393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126639860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126639d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12663a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12663a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12663aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12663af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12663b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12663b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12663bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12663c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12663c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12663cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12663cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12663d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12663d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12663ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12663e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12663e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12663eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12663f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12663f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12663f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12663fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1266402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126640760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126640c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1266410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126641540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1266419e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126641e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126642320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1266427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126642c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126643100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1266435a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126643af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126644040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126644590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126644ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126644da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1266453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1266459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126645fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1266467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126646c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126646f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126647b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126648330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1266487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126648c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126649110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1266498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126649e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12664a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12664a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12664ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12664b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12664b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12664bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12664c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12664c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12664cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12664d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12664d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12664ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12664e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12664e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12664edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12664f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12664f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12664fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126650300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126650850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126650da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1266512f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126651840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126651d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1266522e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126652830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126652d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1266532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126653820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126653d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1266542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126654810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126654d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1266552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126655800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126655d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1266562a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1266567f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126656d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126657290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1266577e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126657d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126658280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1266587d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126658d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126659270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1266597c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126659d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12665a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12665a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12665ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12665b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12665b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12665bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12665c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12665c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12665cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12665d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12665d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12665d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12665de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12665e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12665e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12665ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12665f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12665f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12665f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12665fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1266607a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126660cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126661410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126661b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126662250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126662970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126662c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126663420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1266636e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126663cf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.793s
user	0m0.280s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4760 (36c258ee)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11de0b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11de0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11de0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11de0c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11de0ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11de0d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11de0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11de0df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11de0e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11de0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11de0eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11de0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11de0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11de106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11de10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11de115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11de11cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11de12410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11de12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11de13300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11de13a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11de14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11de14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11de15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11de15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11de15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11de160f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11de16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11de172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11de17560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11de17a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11de17cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11de18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11de18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11de18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11de191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11de19690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11de19b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11de19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11de1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11de1a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11de1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11de1b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11de1b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11de1b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11de1bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11de1c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11de1cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11de1d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11de1db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11de1e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11de1e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11de1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11de1f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11de1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11de1ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11de20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11de20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11de20d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11de21540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11de21800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11de21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11de22140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11de225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11de22a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11de22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11de233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11de23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11de23d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11de241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11de24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11de24ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11de24f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11de254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11de25a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11de25f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11de264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11de26a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11de26f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11de274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11de27a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11de27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11de284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11de289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11de28f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11de29490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11de299e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11de29f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11de2a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11de2a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11de2af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11de2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11de2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11de2bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11de2c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11de2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11de2cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11de1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11de2d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11de2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11de2e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11de2e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11de2eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11de2f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11de2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11de2fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11de30050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11de305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11de30af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11de31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11de31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11de31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11de32030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11de324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11de32970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11de32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11de332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11de33750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11de33bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11de34090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11de34530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11de349d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11de34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11de35310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11de357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11de35c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11de360f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11de36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11de36a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11de36ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11de37370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11de37810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11de37cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11de38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11de385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11de38a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11de38f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11de393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11de39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11de39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11de3a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11de3a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11de3aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11de3af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11de3b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11de3b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11de3bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11de3c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11de3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11de3cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11de3cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11de3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11de3d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11de3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11de3e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11de3e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11de3ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11de3f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11de3f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11de3f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11de3fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11de402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11de40770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11de40c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11de410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11de41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11de419f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11de41e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11de42330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11de427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11de42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11de43110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11de435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11de43a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11de43ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11de44390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11de44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11de44cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11de45170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11de45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11de45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11de45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11de463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11de46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11de46d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11de471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11de47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11de47b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11de47fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11de48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11de488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11de48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11de49230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11de49780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11de49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11de4a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11de4a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11de4aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11de4b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11de4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11de4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11de4c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11de4c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11de4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11de4d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11de4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11de4dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11de4e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11de4e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11de4eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11de4f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11de4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11de4fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11de50540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11de50a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11de50fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11de51530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11de51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11de51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11de52520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11de52a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11de52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11de53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11de53a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11de53fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11de54500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11de54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11de54fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11de554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11de55a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11de55f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11de564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11de56a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11de56f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11de574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11de57a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11de57f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11de584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11de58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11de58f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11de594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11de59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11de59f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11de5a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11de5a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11de5af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11de5b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11de5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11de5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11de5c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11de5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11de5cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11de5d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11de5d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11de5df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11de5e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11de5e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11de5ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11de5f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11de5f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11de5fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11de60440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11de60990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11de60ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11de61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11de61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11de61ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11de62370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11de62810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11de62cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11de63150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11de635f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11de63a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11de63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11de643d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11de64870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11de64d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11de651b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11de65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11de65af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11de65f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11de66430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11de66980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11de670a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11de677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11de67ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11de68600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11de688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11de690b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11de69370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11de69980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.098.269 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.273 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11df09630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11df09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11df09f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11df0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11df0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11df0ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11df0b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11df0b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11df0b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11df0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11df0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11df0ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11df0d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11df0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11df0e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11df0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11df0f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11df0fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11df10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11df10950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11df11070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11df11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11df11eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11df125d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11df12cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11df12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11df13270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11df136e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11df13b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11df13fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11df144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11df149d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11df14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11df15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11df15570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11df159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11df15f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11df16440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11df16940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11df16e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11df17340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11df17840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11df17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11df18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11df18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11df18bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11df19020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11df19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11df19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11df19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11df1a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11df1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11df1aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11df1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11df1b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11df1bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11df1c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11df1c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11df1c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11df1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11df1d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11df1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11df1deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11df1e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11df1e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11df1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11df1f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11df1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11df1fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11df1ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11df203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11df20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11df20cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11df21240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11df21790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11df21ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11df22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11df22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11df22cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11df23220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11df23770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11df23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11df24210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11df24760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11df24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11df25200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11df25750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11df25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11df261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11df26740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11df26c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11df271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11df27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11df27c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11df281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11df28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11df28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11df291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11df29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11df29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11df2a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11df2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11df2ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11df2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11df2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11df2bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11df2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11df2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11df2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11df2d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11df2d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11df2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11df2e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11df2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11df2eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11df2ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11df2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11df2f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11df2fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11df301d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11df30670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11df30b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11df30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11df31450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11df318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11df31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11df32230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11df326d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11df32b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11df33010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11df334b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11df33950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11df33df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11df34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11df34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11df34bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11df35070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11df35510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11df359b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11df35e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11df362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11df36790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11df36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11df370d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11df37570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11df37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11df37eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11df38350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11df387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11df38c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11df39130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11df395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11df39a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11df39f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11df3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11df3a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11df3acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11df3b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11df3b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11df3bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11df3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11df3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11df3c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11df3cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11df3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11df3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11df3db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11df3dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11df3e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11df3e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11df3edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11df3f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11df3f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11df3fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11df40030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11df404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11df40970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11df40e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11df412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11df41750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11df41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11df42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11df42530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11df429d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11df42e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11df43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11df437b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11df43c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11df440f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11df44590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11df44a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11df44ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11df45370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11df458c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11df45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11df46360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11df468b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11df46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11df47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11df47790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11df47da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11df48590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11df48a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11df48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11df49300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11df49910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11df4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11df4a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11df4aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11df4aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11df4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11df4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11df4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11df4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11df4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11df4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11df4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11df4dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11df4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11df4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11df4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11df4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11df4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11df4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11df500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11df50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11df50b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11df510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11df51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11df51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11df520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11df52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11df52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11df530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11df53610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11df53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11df540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11df54600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11df54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11df550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11df555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11df55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11df56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11df565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11df56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11df57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11df575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11df57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11df58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11df585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11df58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11df59060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11df595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11df59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11df5a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11df5a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11df5aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11df5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11df5b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11df5bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11df5c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11df5c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11df5cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11df5d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11df5d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11df5dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11df5e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11df5e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11df5e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11df5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11df5f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11df5f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11df5fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11df60070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11df60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11df609b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11df60e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11df612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11df61790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11df61c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11df620d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11df62570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11df62ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11df631e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11df63900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11df64020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11df64740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11df64a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11df651f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11df654b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11df65ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f805e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f806270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f8066e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f806b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f806fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f807430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f8078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f807d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f808180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f8085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f808a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f809100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f809c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f80a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f80abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f80b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f80ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f80c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f80c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f80d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f80d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f80de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f80e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f80ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f80f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f80f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f80f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f80fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f810230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f8106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f810b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f811040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f8114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f811770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f811be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f812050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f8124c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f812930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f812da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f813210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f813680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f813af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f813f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f8143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f814840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f814cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f815120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f815590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f815a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f815e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f8162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f816750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f816bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f817030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f8174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f817910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f817e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f818380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f8187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f818c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f8190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f819540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f8199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f819e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f81a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f81a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f81ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f81afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f81b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f81b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11f81bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11f81c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x159904080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1599044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x159904960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x159904dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x159905240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1599056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x159905b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x159905f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x159906400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x159906870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x159906ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x159907150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1599075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x159907a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x159907ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x159908310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x159908780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x159908bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x159909060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1599094d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x159909940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x159909db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15990a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15990a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15990ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15990af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15990b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15990b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15990bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15990c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15990c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15990ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15990d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15990d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15990d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15990de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15990e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15990e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15990eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15990f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15990f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15990f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15990fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1599101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x159910630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x159910aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x159910f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x159911380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1599117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x159911c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1599120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x159912540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1599129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x159912e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x159913290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x159913700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x159913b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x159913fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x159914450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1599148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x159914d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1599151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x159915610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x159915a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x159915ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x159916360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1599167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x159916c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1599170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x159917520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x159917990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x159917e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x159918270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1599186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x159918b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x159918fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x159919430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1599198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x159919d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15991a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15991a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15991aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15991aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15991b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15991b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15991bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15991c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15991c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15991c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15991cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15991d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15991d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15991db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15991dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15991e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15991e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15991ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15991f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15991f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15991fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15991feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x159920320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x159920790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x159920c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x159921070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1599214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x159921950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x159921dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x159922230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1599226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x159922b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x159922f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1599233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x159923860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x159923cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x159924140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1599245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x159924a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x159924e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x159925300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x159925770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x159925be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x159926050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1599264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x159926930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x159926da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x159927210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x159927680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x159927af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x159927f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1599283d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x159928840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x159928cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x159929120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x159929590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x159929a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x159929e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15992a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15992aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15992b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15992b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15992b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15992bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15992c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15992c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15992ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15992cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15992d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15992d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15992dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15992e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15992e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15992e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15992edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15992f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15992f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15992fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15992ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x159930420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x159930890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x159930d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x159931170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1599315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x159931a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x159931ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x159932330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1599327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x159932c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x159933080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1599334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x159933960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x159933dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x159934240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1599346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x159934b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x159934f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x159935400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x159935870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x159935ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x159936150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1599365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x159936a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x159936ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x159937310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x159937780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x159937bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x159938060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1599384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x159938940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x159938db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x159939220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x159939690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x159939b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x159939f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15993a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15993a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15993acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15993b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15993b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15993ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15993be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15993c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15993c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15993cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15993d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15993d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15993d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15993dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15993e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15993e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15993eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15993f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15993fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x159940390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x159940ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x159940d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1599411e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1599417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x159941df0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.960s
user	0m0.230s
sys	0m0.192s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
