### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.04 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.88 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.85 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.14 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.89 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.83 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 252.28 sec*proc (29 tests)

Total Test time (real) = 252.29 sec

real	4m12.322s
user	8m38.466s
sys	0m7.091s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.12 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.89 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.72 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.17 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.56 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.38 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.06 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.28 sec*proc (29 tests)

Total Test time (real) =  54.29 sec

real	0m54.304s
user	1m16.424s
sys	0m6.047s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.121 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.291 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.057 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.066 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.067 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.067 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.068 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.069 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.070 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.071 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.071 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.075 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.076 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.079 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.079 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.080 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.080 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.081 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.081 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.082 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.732 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.735 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.735 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.736 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.736 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.737 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.737 I llama_model_loader: - type  f32:  124 tensors
0.00.025.738 I llama_model_loader: - type  f16:   73 tensors
0.00.025.739 I print_info: file format = GGUF V3 (latest)
0.00.025.739 I print_info: file type   = F16
0.00.025.740 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.029.907 I load: special tokens cache size = 5
0.00.031.982 I load: token to piece cache size = 0.2032 MB
0.00.031.986 I print_info: arch             = bert
0.00.031.987 I print_info: vocab_only       = 0
0.00.031.987 I print_info: n_ctx_train      = 512
0.00.031.987 I print_info: n_embd           = 384
0.00.031.987 I print_info: n_layer          = 12
0.00.031.990 I print_info: n_head           = 12
0.00.031.991 I print_info: n_head_kv        = 12
0.00.031.991 I print_info: n_rot            = 32
0.00.031.992 I print_info: n_swa            = 0
0.00.031.992 I print_info: n_embd_head_k    = 32
0.00.031.992 I print_info: n_embd_head_v    = 32
0.00.031.993 I print_info: n_gqa            = 1
0.00.031.994 I print_info: n_embd_k_gqa     = 384
0.00.031.994 I print_info: n_embd_v_gqa     = 384
0.00.031.995 I print_info: f_norm_eps       = 1.0e-12
0.00.031.996 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.031.996 I print_info: f_clamp_kqv      = 0.0e+00
0.00.031.996 I print_info: f_max_alibi_bias = 0.0e+00
0.00.031.996 I print_info: f_logit_scale    = 0.0e+00
0.00.031.997 I print_info: n_ff             = 1536
0.00.031.997 I print_info: n_expert         = 0
0.00.031.998 I print_info: n_expert_used    = 0
0.00.031.998 I print_info: causal attn      = 0
0.00.031.998 I print_info: pooling type     = 2
0.00.031.998 I print_info: rope type        = 2
0.00.031.998 I print_info: rope scaling     = linear
0.00.032.001 I print_info: freq_base_train  = 10000.0
0.00.032.001 I print_info: freq_scale_train = 1
0.00.032.002 I print_info: n_ctx_orig_yarn  = 512
0.00.032.002 I print_info: rope_finetuned   = unknown
0.00.032.002 I print_info: ssm_d_conv       = 0
0.00.032.002 I print_info: ssm_d_inner      = 0
0.00.032.002 I print_info: ssm_d_state      = 0
0.00.032.003 I print_info: ssm_dt_rank      = 0
0.00.032.003 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.003 I print_info: model type       = 33M
0.00.032.003 I print_info: model params     = 33.21 M
0.00.032.004 I print_info: general.name     = Bge Small
0.00.032.004 I print_info: vocab type       = WPM
0.00.032.004 I print_info: n_vocab          = 30522
0.00.032.004 I print_info: n_merges         = 0
0.00.032.005 I print_info: BOS token        = 101 '[CLS]'
0.00.032.005 I print_info: UNK token        = 100 '[UNK]'
0.00.032.005 I print_info: SEP token        = 102 '[SEP]'
0.00.032.005 I print_info: PAD token        = 0 '[PAD]'
0.00.032.006 I print_info: MASK token       = 103 '[MASK]'
0.00.032.006 I print_info: LF token         = 0 '[PAD]'
0.00.032.006 I print_info: max token length = 21
0.00.032.007 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.035.009 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.011 I load_tensors: offloading output layer to GPU
0.00.035.011 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.035 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.037 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.318 I llama_init_from_model: n_seq_max     = 1
0.00.035.319 I llama_init_from_model: n_ctx         = 512
0.00.035.319 I llama_init_from_model: n_ctx_per_seq = 512
0.00.035.320 I llama_init_from_model: n_batch       = 2048
0.00.035.320 I llama_init_from_model: n_ubatch      = 2048
0.00.035.320 I llama_init_from_model: flash_attn    = 0
0.00.035.321 I llama_init_from_model: freq_base     = 10000.0
0.00.035.321 I llama_init_from_model: freq_scale    = 1
0.00.035.321 I ggml_metal_init: allocating
0.00.035.326 I ggml_metal_init: found device: Apple M4
0.00.035.331 I ggml_metal_init: picking default device: Apple M4
0.00.036.064 I ggml_metal_init: using embedded metal library
0.00.040.153 I ggml_metal_init: GPU name:   Apple M4
0.00.040.155 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.156 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.156 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.157 I ggml_metal_init: simdgroup reduction   = true
0.00.040.157 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.157 I ggml_metal_init: has residency sets    = true
0.00.040.157 I ggml_metal_init: has bfloat            = true
0.00.040.157 I ggml_metal_init: use bfloat            = true
0.00.040.158 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.158 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.723 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.393 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.395 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.396 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.053.622 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.053.624 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.053.624 I llama_init_from_model: graph nodes  = 429
0.00.053.624 I llama_init_from_model: graph splits = 2
0.00.053.626 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.626 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.318 I 
0.00.059.344 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.001 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.171 I llama_perf_context_print:        load time =      44.02 ms
0.00.065.172 I llama_perf_context_print: prompt eval time =       5.03 ms /     9 tokens (    0.56 ms per token,  1790.33 tokens per second)
0.00.065.173 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.173 I llama_perf_context_print:       total time =       5.85 ms /    10 tokens
0.00.065.316 I ggml_metal_free: deallocating

real	0m0.239s
user	0m0.047s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.047 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.328 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.012.169 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.175 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.176 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.176 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.176 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.177 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.178 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.178 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.178 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.178 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.181 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.181 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.184 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.184 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.184 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.185 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.628 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.261 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.262 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.262 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.263 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.263 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.263 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.263 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.264 I llama_model_loader: - type  f32:  124 tensors
0.00.015.264 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.265 I print_info: file format = GGUF V3 (latest)
0.00.015.265 I print_info: file type   = Q8_0
0.00.015.266 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.828 I load: special tokens cache size = 5
0.00.019.040 I load: token to piece cache size = 0.2032 MB
0.00.019.044 I print_info: arch             = bert
0.00.019.044 I print_info: vocab_only       = 0
0.00.019.044 I print_info: n_ctx_train      = 512
0.00.019.044 I print_info: n_embd           = 384
0.00.019.044 I print_info: n_layer          = 12
0.00.019.048 I print_info: n_head           = 12
0.00.019.048 I print_info: n_head_kv        = 12
0.00.019.048 I print_info: n_rot            = 32
0.00.019.049 I print_info: n_swa            = 0
0.00.019.049 I print_info: n_embd_head_k    = 32
0.00.019.049 I print_info: n_embd_head_v    = 32
0.00.019.049 I print_info: n_gqa            = 1
0.00.019.050 I print_info: n_embd_k_gqa     = 384
0.00.019.051 I print_info: n_embd_v_gqa     = 384
0.00.019.051 I print_info: f_norm_eps       = 1.0e-12
0.00.019.051 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.052 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.053 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.054 I print_info: f_logit_scale    = 0.0e+00
0.00.019.055 I print_info: n_ff             = 1536
0.00.019.055 I print_info: n_expert         = 0
0.00.019.055 I print_info: n_expert_used    = 0
0.00.019.055 I print_info: causal attn      = 0
0.00.019.055 I print_info: pooling type     = 2
0.00.019.056 I print_info: rope type        = 2
0.00.019.056 I print_info: rope scaling     = linear
0.00.019.058 I print_info: freq_base_train  = 10000.0
0.00.019.058 I print_info: freq_scale_train = 1
0.00.019.058 I print_info: n_ctx_orig_yarn  = 512
0.00.019.058 I print_info: rope_finetuned   = unknown
0.00.019.058 I print_info: ssm_d_conv       = 0
0.00.019.058 I print_info: ssm_d_inner      = 0
0.00.019.059 I print_info: ssm_d_state      = 0
0.00.019.059 I print_info: ssm_dt_rank      = 0
0.00.019.060 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.060 I print_info: model type       = 33M
0.00.019.060 I print_info: model params     = 33.21 M
0.00.019.061 I print_info: general.name     = Bge Small
0.00.019.061 I print_info: vocab type       = WPM
0.00.019.061 I print_info: n_vocab          = 30522
0.00.019.061 I print_info: n_merges         = 0
0.00.019.061 I print_info: BOS token        = 101 '[CLS]'
0.00.019.062 I print_info: UNK token        = 100 '[UNK]'
0.00.019.062 I print_info: SEP token        = 102 '[SEP]'
0.00.019.062 I print_info: PAD token        = 0 '[PAD]'
0.00.019.062 I print_info: MASK token       = 103 '[MASK]'
0.00.019.062 I print_info: LF token         = 0 '[PAD]'
0.00.019.062 I print_info: max token length = 21
0.00.019.063 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.796 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.797 I load_tensors: offloading output layer to GPU
0.00.020.797 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.804 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.804 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.979 I llama_init_from_model: n_seq_max     = 1
0.00.020.980 I llama_init_from_model: n_ctx         = 512
0.00.020.980 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.980 I llama_init_from_model: n_batch       = 2048
0.00.020.980 I llama_init_from_model: n_ubatch      = 2048
0.00.020.980 I llama_init_from_model: flash_attn    = 0
0.00.020.981 I llama_init_from_model: freq_base     = 10000.0
0.00.020.981 I llama_init_from_model: freq_scale    = 1
0.00.020.982 I ggml_metal_init: allocating
0.00.020.985 I ggml_metal_init: found device: Apple M4
0.00.020.989 I ggml_metal_init: picking default device: Apple M4
0.00.021.512 I ggml_metal_init: using embedded metal library
0.00.024.066 I ggml_metal_init: GPU name:   Apple M4
0.00.024.068 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.069 I ggml_metal_init: simdgroup reduction   = true
0.00.024.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.070 I ggml_metal_init: has residency sets    = true
0.00.024.070 I ggml_metal_init: has bfloat            = true
0.00.024.070 I ggml_metal_init: use bfloat            = true
0.00.024.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.504 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.101 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.104 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.106 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.036.105 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.036.106 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.036.107 I llama_init_from_model: graph nodes  = 429
0.00.036.107 I llama_init_from_model: graph splits = 2
0.00.036.108 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.108 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.040.190 I 
0.00.040.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.753 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.045.229 I llama_perf_context_print:        load time =      30.86 ms
0.00.045.230 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2071.82 tokens per second)
0.00.045.231 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.045.231 I llama_perf_context_print:       total time =       5.04 ms /    10 tokens
0.00.045.414 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.275 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.263 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.707 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.712 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.715 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.718 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.719 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.719 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.720 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.722 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.722 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.723 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.724 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.724 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.727 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.728 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.729 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.729 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.730 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.316 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.615 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.079 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.082 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.083 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.083 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.083 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.084 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.084 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.084 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.051.085 I llama_model_loader: - type  f32:   40 tensors
0.00.051.085 I llama_model_loader: - type  f16:   30 tensors
0.00.051.086 I print_info: file format = GGUF V3 (latest)
0.00.051.087 I print_info: file type   = F16
0.00.051.088 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.055.335 W load: empty token at index 5
0.00.060.636 W load: model vocab missing newline token, using special_pad_id instead
0.00.062.259 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.062.296 I load: special tokens cache size = 5
0.00.321.747 I load: token to piece cache size = 1.5060 MB
0.00.321.757 I print_info: arch             = jina-bert-v2
0.00.321.758 I print_info: vocab_only       = 0
0.00.321.758 I print_info: n_ctx_train      = 8192
0.00.321.760 I print_info: n_embd           = 384
0.00.321.760 I print_info: n_layer          = 4
0.00.321.767 I print_info: n_head           = 12
0.00.321.769 I print_info: n_head_kv        = 12
0.00.321.770 I print_info: n_rot            = 32
0.00.321.770 I print_info: n_swa            = 0
0.00.321.770 I print_info: n_embd_head_k    = 32
0.00.321.770 I print_info: n_embd_head_v    = 32
0.00.321.771 I print_info: n_gqa            = 1
0.00.321.771 I print_info: n_embd_k_gqa     = 384
0.00.321.772 I print_info: n_embd_v_gqa     = 384
0.00.321.773 I print_info: f_norm_eps       = 1.0e-12
0.00.321.774 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.321.774 I print_info: f_clamp_kqv      = 0.0e+00
0.00.321.774 I print_info: f_max_alibi_bias = 8.0e+00
0.00.321.774 I print_info: f_logit_scale    = 0.0e+00
0.00.321.775 I print_info: n_ff             = 1536
0.00.321.775 I print_info: n_expert         = 0
0.00.321.775 I print_info: n_expert_used    = 0
0.00.321.776 I print_info: causal attn      = 0
0.00.321.776 I print_info: pooling type     = -1
0.00.321.776 I print_info: rope type        = -1
0.00.321.776 I print_info: rope scaling     = linear
0.00.321.776 I print_info: freq_base_train  = 10000.0
0.00.321.777 I print_info: freq_scale_train = 1
0.00.321.777 I print_info: n_ctx_orig_yarn  = 8192
0.00.321.777 I print_info: rope_finetuned   = unknown
0.00.321.777 I print_info: ssm_d_conv       = 0
0.00.321.778 I print_info: ssm_d_inner      = 0
0.00.321.778 I print_info: ssm_d_state      = 0
0.00.321.778 I print_info: ssm_dt_rank      = 0
0.00.321.778 I print_info: ssm_dt_b_c_rms   = 0
0.00.321.778 I print_info: model type       = 33M
0.00.321.779 I print_info: model params     = 32.90 M
0.00.321.779 I print_info: general.name     = Jina Bert Implementation
0.00.321.780 I print_info: vocab type       = BPE
0.00.321.781 I print_info: n_vocab          = 61056
0.00.321.781 I print_info: n_merges         = 39382
0.00.321.781 I print_info: BOS token        = 0 '<s>'
0.00.321.781 I print_info: EOS token        = 2 '</s>'
0.00.321.782 I print_info: UNK token        = 3 '<unk>'
0.00.321.782 I print_info: SEP token        = 2 '</s>'
0.00.321.782 I print_info: PAD token        = 1 '<pad>'
0.00.321.782 I print_info: MASK token       = 4 '<mask>'
0.00.321.783 I print_info: EOG token        = 2 '</s>'
0.00.321.783 I print_info: max token length = 45
0.00.321.784 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.323.362 I load_tensors: offloading 4 repeating layers to GPU
0.00.323.363 I load_tensors: offloading output layer to GPU
0.00.323.364 I load_tensors: offloaded 5/5 layers to GPU
0.00.323.383 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.323.384 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.323.739 I llama_init_from_model: n_seq_max     = 1
0.00.323.740 I llama_init_from_model: n_ctx         = 8192
0.00.323.740 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.323.740 I llama_init_from_model: n_batch       = 2048
0.00.323.740 I llama_init_from_model: n_ubatch      = 2048
0.00.323.740 I llama_init_from_model: flash_attn    = 0
0.00.323.741 I llama_init_from_model: freq_base     = 10000.0
0.00.323.741 I llama_init_from_model: freq_scale    = 1
0.00.323.742 I ggml_metal_init: allocating
0.00.323.746 I ggml_metal_init: found device: Apple M4
0.00.323.749 I ggml_metal_init: picking default device: Apple M4
0.00.324.294 I ggml_metal_init: using embedded metal library
0.00.326.774 I ggml_metal_init: GPU name:   Apple M4
0.00.326.775 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.326.776 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.326.776 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.326.777 I ggml_metal_init: simdgroup reduction   = true
0.00.326.777 I ggml_metal_init: simdgroup matrix mul. = true
0.00.326.777 I ggml_metal_init: has residency sets    = true
0.00.326.777 I ggml_metal_init: has bfloat            = true
0.00.326.777 I ggml_metal_init: use bfloat            = true
0.00.326.777 I ggml_metal_init: hasUnifiedMemory      = true
0.00.326.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.336.173 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.339.115 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.339.117 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.339.118 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.344.970 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.344.971 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.344.971 I llama_init_from_model: graph nodes  = 154
0.00.344.971 I llama_init_from_model: graph splits = 2
0.00.344.973 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.344.973 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.352.162 I 
0.00.352.193 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.352.293 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.352.293 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.352.296 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.352.296 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.352.300 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.352.300 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.352.789 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.356.506 I llama_perf_context_print:        load time =     328.89 ms
0.00.356.508 I llama_perf_context_print: prompt eval time =       3.71 ms /    62 tokens (    0.06 ms per token, 16711.59 tokens per second)
0.00.356.510 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.356.510 I llama_perf_context_print:       total time =       4.35 ms /    63 tokens
0.00.356.776 I ggml_metal_free: deallocating

real	0m1.058s
user	0m0.328s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.224 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.388 I main: llama backend init
0.00.000.393 I main: load the model and apply lora adapter, if any
0.00.083.290 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.107.817 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.107.828 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.107.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.107.833 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.107.834 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.107.835 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.107.835 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.107.838 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.107.839 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.107.840 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.107.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.107.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.107.845 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.107.846 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.107.850 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.107.851 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.107.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.116.600 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.118.864 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.126.909 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.126.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.126.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.126.914 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.126.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.126.916 I llama_model_loader: - type  f32:  194 tensors
0.00.126.916 I llama_model_loader: - type  f16:   98 tensors
0.00.126.925 I print_info: file format = GGUF V3 (latest)
0.00.126.926 I print_info: file type   = all F32 (guessed)
0.00.126.928 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.142.308 I load: special tokens cache size = 25
0.00.151.197 I load: token to piece cache size = 0.2984 MB
0.00.151.202 I print_info: arch             = gptneox
0.00.151.202 I print_info: vocab_only       = 0
0.00.151.203 I print_info: n_ctx_train      = 2048
0.00.151.203 I print_info: n_embd           = 2048
0.00.151.203 I print_info: n_layer          = 24
0.00.151.206 I print_info: n_head           = 16
0.00.151.207 I print_info: n_head_kv        = 16
0.00.151.207 I print_info: n_rot            = 32
0.00.151.207 I print_info: n_swa            = 0
0.00.151.207 I print_info: n_embd_head_k    = 128
0.00.151.208 I print_info: n_embd_head_v    = 128
0.00.151.209 I print_info: n_gqa            = 1
0.00.151.210 I print_info: n_embd_k_gqa     = 2048
0.00.151.210 I print_info: n_embd_v_gqa     = 2048
0.00.151.211 I print_info: f_norm_eps       = 1.0e-05
0.00.151.212 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.151.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.151.212 I print_info: f_max_alibi_bias = 0.0e+00
0.00.151.212 I print_info: f_logit_scale    = 0.0e+00
0.00.151.213 I print_info: n_ff             = 8192
0.00.151.213 I print_info: n_expert         = 0
0.00.151.213 I print_info: n_expert_used    = 0
0.00.151.213 I print_info: causal attn      = 1
0.00.151.214 I print_info: pooling type     = 0
0.00.151.214 I print_info: rope type        = 2
0.00.151.214 I print_info: rope scaling     = linear
0.00.151.214 I print_info: freq_base_train  = 10000.0
0.00.151.215 I print_info: freq_scale_train = 1
0.00.151.215 I print_info: n_ctx_orig_yarn  = 2048
0.00.151.215 I print_info: rope_finetuned   = unknown
0.00.151.215 I print_info: ssm_d_conv       = 0
0.00.151.216 I print_info: ssm_d_inner      = 0
0.00.151.216 I print_info: ssm_d_state      = 0
0.00.151.216 I print_info: ssm_dt_rank      = 0
0.00.151.218 I print_info: ssm_dt_b_c_rms   = 0
0.00.151.218 I print_info: model type       = 1.4B
0.00.151.219 I print_info: model params     = 1.41 B
0.00.151.219 I print_info: general.name     = 1.4B
0.00.151.219 I print_info: vocab type       = BPE
0.00.151.220 I print_info: n_vocab          = 50304
0.00.151.220 I print_info: n_merges         = 50009
0.00.151.220 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.151.220 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.151.221 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.151.223 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.151.223 I print_info: LF token         = 187 ''
0.00.151.223 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.151.224 I print_info: max token length = 1024
0.00.151.224 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.193.397 I load_tensors: offloading 24 repeating layers to GPU
0.00.193.402 I load_tensors: offloading output layer to GPU
0.00.193.403 I load_tensors: offloaded 25/25 layers to GPU
0.00.193.428 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.193.430 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.193.876 I llama_init_from_model: n_seq_max     = 1
0.00.193.877 I llama_init_from_model: n_ctx         = 2048
0.00.193.878 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.193.878 I llama_init_from_model: n_batch       = 2048
0.00.193.878 I llama_init_from_model: n_ubatch      = 512
0.00.193.878 I llama_init_from_model: flash_attn    = 0
0.00.193.879 I llama_init_from_model: freq_base     = 10000.0
0.00.193.879 I llama_init_from_model: freq_scale    = 1
0.00.193.880 I ggml_metal_init: allocating
0.00.193.922 I ggml_metal_init: found device: Apple M4
0.00.193.930 I ggml_metal_init: picking default device: Apple M4
0.00.194.605 I ggml_metal_init: using embedded metal library
0.00.204.088 I ggml_metal_init: GPU name:   Apple M4
0.00.204.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.204.090 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.204.091 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.204.091 I ggml_metal_init: simdgroup reduction   = true
0.00.204.091 I ggml_metal_init: simdgroup matrix mul. = true
0.00.204.091 I ggml_metal_init: has residency sets    = true
0.00.204.091 I ggml_metal_init: has bfloat            = true
0.00.204.092 I ggml_metal_init: use bfloat            = true
0.00.204.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.204.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.230.288 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.256.654 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.256.661 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.256.683 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.260.214 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.260.217 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.260.217 I llama_init_from_model: graph nodes  = 967
0.00.260.217 I llama_init_from_model: graph splits = 2
0.00.260.223 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.260.349 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.260.350 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.318.329 I main: llama threadpool init, n_threads = 4
0.00.318.365 I 
0.00.318.397 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.318.397 I 
0.00.318.530 I sampler seed: 1234
0.00.318.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.318.557 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.318.559 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.318.559 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.121.786 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.02.121.787 I llama_perf_context_print:        load time =     234.19 ms
0.02.121.788 I llama_perf_context_print: prompt eval time =      43.57 ms /     7 tokens (    6.22 ms per token,   160.65 tokens per second)
0.02.121.789 I llama_perf_context_print:        eval time =    1756.89 ms /    63 runs   (   27.89 ms per token,    35.86 tokens per second)
0.02.121.789 I llama_perf_context_print:       total time =    1804.30 ms /    70 tokens
0.02.122.011 I ggml_metal_free: deallocating

real	0m2.418s
user	0m0.151s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.502 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.569 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.581 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.581 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.583 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.584 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.584 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.588 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.589 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.589 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.590 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.592 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.593 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.593 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.769 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.070 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.051.072 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.073 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.073 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.073 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.074 I llama_model_loader: - type  f32:  194 tensors
0.00.051.074 I llama_model_loader: - type  f16:   98 tensors
0.00.051.075 I print_info: file format = GGUF V3 (latest)
0.00.051.076 I print_info: file type   = all F32 (guessed)
0.00.051.077 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.447 I load: special tokens cache size = 25
0.00.069.850 I load: token to piece cache size = 0.2984 MB
0.00.069.853 I print_info: arch             = gptneox
0.00.069.853 I print_info: vocab_only       = 0
0.00.069.854 I print_info: n_ctx_train      = 2048
0.00.069.854 I print_info: n_embd           = 2048
0.00.069.854 I print_info: n_layer          = 24
0.00.069.857 I print_info: n_head           = 16
0.00.069.857 I print_info: n_head_kv        = 16
0.00.069.857 I print_info: n_rot            = 32
0.00.069.858 I print_info: n_swa            = 0
0.00.069.858 I print_info: n_embd_head_k    = 128
0.00.069.858 I print_info: n_embd_head_v    = 128
0.00.069.859 I print_info: n_gqa            = 1
0.00.069.860 I print_info: n_embd_k_gqa     = 2048
0.00.069.860 I print_info: n_embd_v_gqa     = 2048
0.00.069.862 I print_info: f_norm_eps       = 1.0e-05
0.00.069.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.863 I print_info: f_logit_scale    = 0.0e+00
0.00.069.864 I print_info: n_ff             = 8192
0.00.069.864 I print_info: n_expert         = 0
0.00.069.864 I print_info: n_expert_used    = 0
0.00.069.864 I print_info: causal attn      = 1
0.00.069.864 I print_info: pooling type     = 0
0.00.069.864 I print_info: rope type        = 2
0.00.069.865 I print_info: rope scaling     = linear
0.00.069.865 I print_info: freq_base_train  = 10000.0
0.00.069.865 I print_info: freq_scale_train = 1
0.00.069.865 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.865 I print_info: rope_finetuned   = unknown
0.00.069.866 I print_info: ssm_d_conv       = 0
0.00.069.866 I print_info: ssm_d_inner      = 0
0.00.069.866 I print_info: ssm_d_state      = 0
0.00.069.866 I print_info: ssm_dt_rank      = 0
0.00.069.866 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.867 I print_info: model type       = 1.4B
0.00.069.868 I print_info: model params     = 1.41 B
0.00.069.868 I print_info: general.name     = 1.4B
0.00.069.868 I print_info: vocab type       = BPE
0.00.069.868 I print_info: n_vocab          = 50304
0.00.069.868 I print_info: n_merges         = 50009
0.00.069.869 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.869 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.869 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.869 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.869 I print_info: LF token         = 187 ''
0.00.069.870 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.870 I print_info: max token length = 1024
0.00.069.870 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.566.723 I load_tensors: offloading 24 repeating layers to GPU
0.01.566.730 I load_tensors: offloading output layer to GPU
0.01.566.731 I load_tensors: offloaded 25/25 layers to GPU
0.01.566.759 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.566.762 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.567.364 I llama_init_from_model: n_seq_max     = 1
0.01.567.365 I llama_init_from_model: n_ctx         = 128
0.01.567.365 I llama_init_from_model: n_ctx_per_seq = 128
0.01.567.365 I llama_init_from_model: n_batch       = 128
0.01.567.365 I llama_init_from_model: n_ubatch      = 128
0.01.567.366 I llama_init_from_model: flash_attn    = 0
0.01.567.366 I llama_init_from_model: freq_base     = 10000.0
0.01.567.366 I llama_init_from_model: freq_scale    = 1
0.01.567.367 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.567.367 I ggml_metal_init: allocating
0.01.567.389 I ggml_metal_init: found device: Apple M4
0.01.567.394 I ggml_metal_init: picking default device: Apple M4
0.01.568.295 I ggml_metal_init: using embedded metal library
0.01.571.609 I ggml_metal_init: GPU name:   Apple M4
0.01.571.611 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.571.612 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.571.613 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.571.613 I ggml_metal_init: simdgroup reduction   = true
0.01.571.613 I ggml_metal_init: simdgroup matrix mul. = true
0.01.571.613 I ggml_metal_init: has residency sets    = true
0.01.571.613 I ggml_metal_init: has bfloat            = true
0.01.571.614 I ggml_metal_init: use bfloat            = true
0.01.571.614 I ggml_metal_init: hasUnifiedMemory      = true
0.01.571.615 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.581.423 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.582.945 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.582.947 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.582.979 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.584.388 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.584.389 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.584.389 I llama_init_from_model: graph nodes  = 967
0.01.584.389 I llama_init_from_model: graph splits = 2
0.01.584.391 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.584.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.616.971 I 
0.01.617.005 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.617.022 I perplexity: tokenizing the input ..
0.01.621.152 I perplexity: tokenization took 4.128 ms
0.01.621.174 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.739.084 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.740.327 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.740.337 I llama_perf_context_print:        load time =    1596.11 ms
0.01.740.338 I llama_perf_context_print: prompt eval time =     117.69 ms /   128 tokens (    0.92 ms per token,  1087.64 tokens per second)
0.01.740.343 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.740.343 I llama_perf_context_print:       total time =     123.37 ms /   129 tokens
0.01.740.737 I ggml_metal_free: deallocating

real	0m1.934s
user	0m0.096s
sys	0m0.367s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.202 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.310 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.311 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.312 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.313 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.313 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.320 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.320 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.164 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.162 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.007 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.008 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.008 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.009 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.009 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.010 I llama_model_loader: - type  f32:  194 tensors
0.00.031.010 I llama_model_loader: - type q8_0:   98 tensors
0.00.031.011 I print_info: file format = GGUF V3 (latest)
0.00.031.012 I print_info: file type   = Q8_0
0.00.031.015 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.039.366 I load: special tokens cache size = 25
0.00.045.513 I load: token to piece cache size = 0.2984 MB
0.00.045.518 I print_info: arch             = gptneox
0.00.045.518 I print_info: vocab_only       = 0
0.00.045.519 I print_info: n_ctx_train      = 2048
0.00.045.519 I print_info: n_embd           = 2048
0.00.045.519 I print_info: n_layer          = 24
0.00.045.524 I print_info: n_head           = 16
0.00.045.525 I print_info: n_head_kv        = 16
0.00.045.525 I print_info: n_rot            = 32
0.00.045.525 I print_info: n_swa            = 0
0.00.045.525 I print_info: n_embd_head_k    = 128
0.00.045.526 I print_info: n_embd_head_v    = 128
0.00.045.526 I print_info: n_gqa            = 1
0.00.045.527 I print_info: n_embd_k_gqa     = 2048
0.00.045.528 I print_info: n_embd_v_gqa     = 2048
0.00.045.528 I print_info: f_norm_eps       = 1.0e-05
0.00.045.529 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.045.529 I print_info: f_clamp_kqv      = 0.0e+00
0.00.045.529 I print_info: f_max_alibi_bias = 0.0e+00
0.00.045.530 I print_info: f_logit_scale    = 0.0e+00
0.00.045.530 I print_info: n_ff             = 8192
0.00.045.530 I print_info: n_expert         = 0
0.00.045.530 I print_info: n_expert_used    = 0
0.00.045.531 I print_info: causal attn      = 1
0.00.045.531 I print_info: pooling type     = 0
0.00.045.531 I print_info: rope type        = 2
0.00.045.531 I print_info: rope scaling     = linear
0.00.045.532 I print_info: freq_base_train  = 10000.0
0.00.045.532 I print_info: freq_scale_train = 1
0.00.045.532 I print_info: n_ctx_orig_yarn  = 2048
0.00.045.532 I print_info: rope_finetuned   = unknown
0.00.045.532 I print_info: ssm_d_conv       = 0
0.00.045.532 I print_info: ssm_d_inner      = 0
0.00.045.533 I print_info: ssm_d_state      = 0
0.00.045.533 I print_info: ssm_dt_rank      = 0
0.00.045.537 I print_info: ssm_dt_b_c_rms   = 0
0.00.045.537 I print_info: model type       = 1.4B
0.00.045.537 I print_info: model params     = 1.41 B
0.00.045.538 I print_info: general.name     = 1.4B
0.00.045.538 I print_info: vocab type       = BPE
0.00.045.538 I print_info: n_vocab          = 50304
0.00.045.538 I print_info: n_merges         = 50009
0.00.045.539 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.045.540 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.045.540 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.045.540 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.045.540 I print_info: LF token         = 187 ''
0.00.045.541 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.045.542 I print_info: max token length = 1024
0.00.045.543 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.136.873 I load_tensors: offloading 24 repeating layers to GPU
0.01.136.875 I load_tensors: offloading output layer to GPU
0.01.136.876 I load_tensors: offloaded 25/25 layers to GPU
0.01.136.895 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.136.897 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.137.586 I llama_init_from_model: n_seq_max     = 1
0.01.137.588 I llama_init_from_model: n_ctx         = 2048
0.01.137.588 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.137.588 I llama_init_from_model: n_batch       = 2048
0.01.137.589 I llama_init_from_model: n_ubatch      = 512
0.01.137.589 I llama_init_from_model: flash_attn    = 0
0.01.137.590 I llama_init_from_model: freq_base     = 10000.0
0.01.137.590 I llama_init_from_model: freq_scale    = 1
0.01.137.591 I ggml_metal_init: allocating
0.01.137.605 I ggml_metal_init: found device: Apple M4
0.01.137.611 I ggml_metal_init: picking default device: Apple M4
0.01.138.764 I ggml_metal_init: using embedded metal library
0.01.143.161 I ggml_metal_init: GPU name:   Apple M4
0.01.143.164 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.143.165 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.143.166 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.143.166 I ggml_metal_init: simdgroup reduction   = true
0.01.143.166 I ggml_metal_init: simdgroup matrix mul. = true
0.01.143.166 I ggml_metal_init: has residency sets    = true
0.01.143.166 I ggml_metal_init: has bfloat            = true
0.01.143.167 I ggml_metal_init: use bfloat            = true
0.01.143.167 I ggml_metal_init: hasUnifiedMemory      = true
0.01.143.168 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.155.854 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.187.285 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.187.291 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.187.313 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.192.009 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.192.011 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.192.012 I llama_init_from_model: graph nodes  = 967
0.01.192.012 I llama_init_from_model: graph splits = 2
0.01.192.017 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.192.148 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.192.149 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.238.652 I main: llama threadpool init, n_threads = 4
0.01.238.691 I 
0.01.238.711 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.238.712 I 
0.01.238.843 I sampler seed: 1234
0.01.238.847 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.238.857 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.238.857 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.238.857 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.325.464 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.02.325.468 I llama_perf_context_print:        load time =    1228.78 ms
0.02.325.469 I llama_perf_context_print: prompt eval time =      49.47 ms /     7 tokens (    7.07 ms per token,   141.51 tokens per second)
0.02.325.470 I llama_perf_context_print:        eval time =    1034.20 ms /    63 runs   (   16.42 ms per token,    60.92 tokens per second)
0.02.325.470 I llama_perf_context_print:       total time =    1087.48 ms /    70 tokens
0.02.325.728 I ggml_metal_free: deallocating

real	0m2.342s
user	0m0.104s
sys	0m0.335s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.122 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.541 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.547 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.549 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.550 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.550 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.551 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.552 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.552 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.553 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.555 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.555 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.555 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.558 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.558 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.511 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.379 I llama_model_loader: - type  f32:  194 tensors
0.00.025.380 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.380 I print_info: file format = GGUF V3 (latest)
0.00.025.381 I print_info: file type   = Q8_0
0.00.025.382 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.669 I load: special tokens cache size = 25
0.00.039.502 I load: token to piece cache size = 0.2984 MB
0.00.039.507 I print_info: arch             = gptneox
0.00.039.508 I print_info: vocab_only       = 0
0.00.039.508 I print_info: n_ctx_train      = 2048
0.00.039.508 I print_info: n_embd           = 2048
0.00.039.508 I print_info: n_layer          = 24
0.00.039.513 I print_info: n_head           = 16
0.00.039.514 I print_info: n_head_kv        = 16
0.00.039.517 I print_info: n_rot            = 32
0.00.039.517 I print_info: n_swa            = 0
0.00.039.517 I print_info: n_embd_head_k    = 128
0.00.039.517 I print_info: n_embd_head_v    = 128
0.00.039.518 I print_info: n_gqa            = 1
0.00.039.519 I print_info: n_embd_k_gqa     = 2048
0.00.039.520 I print_info: n_embd_v_gqa     = 2048
0.00.039.520 I print_info: f_norm_eps       = 1.0e-05
0.00.039.521 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.522 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.522 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.523 I print_info: f_logit_scale    = 0.0e+00
0.00.039.524 I print_info: n_ff             = 8192
0.00.039.524 I print_info: n_expert         = 0
0.00.039.524 I print_info: n_expert_used    = 0
0.00.039.524 I print_info: causal attn      = 1
0.00.039.525 I print_info: pooling type     = 0
0.00.039.525 I print_info: rope type        = 2
0.00.039.526 I print_info: rope scaling     = linear
0.00.039.526 I print_info: freq_base_train  = 10000.0
0.00.039.527 I print_info: freq_scale_train = 1
0.00.039.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.527 I print_info: rope_finetuned   = unknown
0.00.039.527 I print_info: ssm_d_conv       = 0
0.00.039.527 I print_info: ssm_d_inner      = 0
0.00.039.527 I print_info: ssm_d_state      = 0
0.00.039.528 I print_info: ssm_dt_rank      = 0
0.00.039.528 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.528 I print_info: model type       = 1.4B
0.00.039.528 I print_info: model params     = 1.41 B
0.00.039.529 I print_info: general.name     = 1.4B
0.00.039.529 I print_info: vocab type       = BPE
0.00.039.529 I print_info: n_vocab          = 50304
0.00.039.529 I print_info: n_merges         = 50009
0.00.039.530 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.530 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.531 I print_info: LF token         = 187 ''
0.00.039.531 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.531 I print_info: max token length = 1024
0.00.039.532 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.944.460 I load_tensors: offloading 24 repeating layers to GPU
0.00.944.468 I load_tensors: offloading output layer to GPU
0.00.944.468 I load_tensors: offloaded 25/25 layers to GPU
0.00.944.494 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.944.496 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.945.416 I llama_init_from_model: n_seq_max     = 1
0.00.945.417 I llama_init_from_model: n_ctx         = 128
0.00.945.418 I llama_init_from_model: n_ctx_per_seq = 128
0.00.945.418 I llama_init_from_model: n_batch       = 128
0.00.945.418 I llama_init_from_model: n_ubatch      = 128
0.00.945.418 I llama_init_from_model: flash_attn    = 0
0.00.945.419 I llama_init_from_model: freq_base     = 10000.0
0.00.945.419 I llama_init_from_model: freq_scale    = 1
0.00.945.420 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.945.420 I ggml_metal_init: allocating
0.00.945.485 I ggml_metal_init: found device: Apple M4
0.00.945.493 I ggml_metal_init: picking default device: Apple M4
0.00.946.664 I ggml_metal_init: using embedded metal library
0.00.951.024 I ggml_metal_init: GPU name:   Apple M4
0.00.951.026 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.951.027 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.951.027 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.951.028 I ggml_metal_init: simdgroup reduction   = true
0.00.951.028 I ggml_metal_init: simdgroup matrix mul. = true
0.00.951.028 I ggml_metal_init: has residency sets    = true
0.00.951.028 I ggml_metal_init: has bfloat            = true
0.00.951.029 I ggml_metal_init: use bfloat            = true
0.00.951.029 I ggml_metal_init: hasUnifiedMemory      = true
0.00.951.031 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.963.511 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.965.287 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.965.290 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.965.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.967.089 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.967.090 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.967.091 I llama_init_from_model: graph nodes  = 967
0.00.967.091 I llama_init_from_model: graph splits = 2
0.00.967.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.967.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.991.825 I 
0.00.991.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.991.869 I perplexity: tokenizing the input ..
0.00.996.755 I perplexity: tokenization took 4.885 ms
0.00.996.768 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.134.723 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.136.019 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.136.035 I llama_perf_context_print:        load time =     982.70 ms
0.01.136.035 I llama_perf_context_print: prompt eval time =     137.73 ms /   128 tokens (    1.08 ms per token,   929.34 tokens per second)
0.01.136.036 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.136.036 I llama_perf_context_print:       total time =     144.21 ms /   129 tokens
0.01.136.457 I ggml_metal_free: deallocating

real	0m1.150s
user	0m0.070s
sys	0m0.232s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.063 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.101 I main: llama backend init
0.00.000.103 I main: load the model and apply lora adapter, if any
0.00.010.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.088 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.094 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.095 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.095 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.095 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.101 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.103 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.103 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.105 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.105 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.106 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.106 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.107 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.112 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.112 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.113 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.946 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.787 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.789 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.789 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.790 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.790 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.791 I llama_model_loader: - type  f32:  194 tensors
0.00.026.791 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.791 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.792 I print_info: file format = GGUF V3 (latest)
0.00.026.793 I print_info: file type   = Q4_0
0.00.026.793 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.034 I load: special tokens cache size = 25
0.00.041.204 I load: token to piece cache size = 0.2984 MB
0.00.041.208 I print_info: arch             = gptneox
0.00.041.208 I print_info: vocab_only       = 0
0.00.041.209 I print_info: n_ctx_train      = 2048
0.00.041.209 I print_info: n_embd           = 2048
0.00.041.209 I print_info: n_layer          = 24
0.00.041.213 I print_info: n_head           = 16
0.00.041.214 I print_info: n_head_kv        = 16
0.00.041.216 I print_info: n_rot            = 32
0.00.041.217 I print_info: n_swa            = 0
0.00.041.217 I print_info: n_embd_head_k    = 128
0.00.041.217 I print_info: n_embd_head_v    = 128
0.00.041.218 I print_info: n_gqa            = 1
0.00.041.219 I print_info: n_embd_k_gqa     = 2048
0.00.041.219 I print_info: n_embd_v_gqa     = 2048
0.00.041.220 I print_info: f_norm_eps       = 1.0e-05
0.00.041.221 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.221 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.221 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.221 I print_info: f_logit_scale    = 0.0e+00
0.00.041.222 I print_info: n_ff             = 8192
0.00.041.222 I print_info: n_expert         = 0
0.00.041.222 I print_info: n_expert_used    = 0
0.00.041.222 I print_info: causal attn      = 1
0.00.041.226 I print_info: pooling type     = 0
0.00.041.227 I print_info: rope type        = 2
0.00.041.227 I print_info: rope scaling     = linear
0.00.041.228 I print_info: freq_base_train  = 10000.0
0.00.041.228 I print_info: freq_scale_train = 1
0.00.041.228 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.229 I print_info: rope_finetuned   = unknown
0.00.041.233 I print_info: ssm_d_conv       = 0
0.00.041.235 I print_info: ssm_d_inner      = 0
0.00.041.235 I print_info: ssm_d_state      = 0
0.00.041.235 I print_info: ssm_dt_rank      = 0
0.00.041.235 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.235 I print_info: model type       = 1.4B
0.00.041.236 I print_info: model params     = 1.41 B
0.00.041.236 I print_info: general.name     = 1.4B
0.00.041.237 I print_info: vocab type       = BPE
0.00.041.237 I print_info: n_vocab          = 50304
0.00.041.237 I print_info: n_merges         = 50009
0.00.041.238 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.238 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.238 I print_info: LF token         = 187 ''
0.00.041.239 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.239 I print_info: max token length = 1024
0.00.041.240 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.026 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.037 I load_tensors: offloading output layer to GPU
0.00.637.038 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.065 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.637.066 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.638.340 I llama_init_from_model: n_seq_max     = 1
0.00.638.344 I llama_init_from_model: n_ctx         = 2048
0.00.638.345 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.345 I llama_init_from_model: n_batch       = 2048
0.00.638.346 I llama_init_from_model: n_ubatch      = 512
0.00.638.346 I llama_init_from_model: flash_attn    = 0
0.00.638.347 I llama_init_from_model: freq_base     = 10000.0
0.00.638.348 I llama_init_from_model: freq_scale    = 1
0.00.638.349 I ggml_metal_init: allocating
0.00.638.367 I ggml_metal_init: found device: Apple M4
0.00.638.377 I ggml_metal_init: picking default device: Apple M4
0.00.639.855 I ggml_metal_init: using embedded metal library
0.00.646.185 I ggml_metal_init: GPU name:   Apple M4
0.00.646.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.190 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.191 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.191 I ggml_metal_init: simdgroup reduction   = true
0.00.646.191 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.192 I ggml_metal_init: has residency sets    = true
0.00.646.192 I ggml_metal_init: has bfloat            = true
0.00.646.192 I ggml_metal_init: use bfloat            = true
0.00.646.193 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.022 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.268 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.276 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.299 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.520 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.522 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.523 I llama_init_from_model: graph nodes  = 967
0.00.722.523 I llama_init_from_model: graph splits = 2
0.00.722.527 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.357 I main: llama threadpool init, n_threads = 4
0.00.772.399 I 
0.00.772.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.436 I 
0.00.772.563 I sampler seed: 1234
0.00.772.568 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.599 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.600 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.600 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.456.936 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48763.74 tokens per second)
0.01.456.937 I llama_perf_context_print:        load time =     760.62 ms
0.01.456.938 I llama_perf_context_print: prompt eval time =      49.85 ms /     7 tokens (    7.12 ms per token,   140.43 tokens per second)
0.01.456.938 I llama_perf_context_print:        eval time =     631.43 ms /    63 runs   (   10.02 ms per token,    99.77 tokens per second)
0.01.456.939 I llama_perf_context_print:       total time =     685.59 ms /    70 tokens
0.01.457.233 I ggml_metal_free: deallocating

real	0m1.474s
user	0m0.108s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.229 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.891 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.891 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.893 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.486 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.488 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.488 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.489 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.489 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.489 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.490 I llama_model_loader: - type  f32:  194 tensors
0.00.025.490 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.491 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.491 I print_info: file format = GGUF V3 (latest)
0.00.025.492 I print_info: file type   = Q4_0
0.00.025.493 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.330 I load: special tokens cache size = 25
0.00.039.346 I load: token to piece cache size = 0.2984 MB
0.00.039.351 I print_info: arch             = gptneox
0.00.039.352 I print_info: vocab_only       = 0
0.00.039.352 I print_info: n_ctx_train      = 2048
0.00.039.352 I print_info: n_embd           = 2048
0.00.039.352 I print_info: n_layer          = 24
0.00.039.356 I print_info: n_head           = 16
0.00.039.357 I print_info: n_head_kv        = 16
0.00.039.357 I print_info: n_rot            = 32
0.00.039.357 I print_info: n_swa            = 0
0.00.039.358 I print_info: n_embd_head_k    = 128
0.00.039.358 I print_info: n_embd_head_v    = 128
0.00.039.358 I print_info: n_gqa            = 1
0.00.039.359 I print_info: n_embd_k_gqa     = 2048
0.00.039.360 I print_info: n_embd_v_gqa     = 2048
0.00.039.361 I print_info: f_norm_eps       = 1.0e-05
0.00.039.361 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.361 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.362 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.362 I print_info: f_logit_scale    = 0.0e+00
0.00.039.362 I print_info: n_ff             = 8192
0.00.039.363 I print_info: n_expert         = 0
0.00.039.363 I print_info: n_expert_used    = 0
0.00.039.363 I print_info: causal attn      = 1
0.00.039.363 I print_info: pooling type     = 0
0.00.039.363 I print_info: rope type        = 2
0.00.039.363 I print_info: rope scaling     = linear
0.00.039.364 I print_info: freq_base_train  = 10000.0
0.00.039.364 I print_info: freq_scale_train = 1
0.00.039.364 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.364 I print_info: rope_finetuned   = unknown
0.00.039.365 I print_info: ssm_d_conv       = 0
0.00.039.365 I print_info: ssm_d_inner      = 0
0.00.039.365 I print_info: ssm_d_state      = 0
0.00.039.365 I print_info: ssm_dt_rank      = 0
0.00.039.365 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.365 I print_info: model type       = 1.4B
0.00.039.365 I print_info: model params     = 1.41 B
0.00.039.366 I print_info: general.name     = 1.4B
0.00.039.366 I print_info: vocab type       = BPE
0.00.039.366 I print_info: n_vocab          = 50304
0.00.039.366 I print_info: n_merges         = 50009
0.00.039.367 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.367 I print_info: LF token         = 187 ''
0.00.039.369 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.369 I print_info: max token length = 1024
0.00.039.369 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.981 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.994 I load_tensors: offloading output layer to GPU
0.00.630.995 I load_tensors: offloaded 25/25 layers to GPU
0.00.631.025 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.631.027 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.632.371 I llama_init_from_model: n_seq_max     = 1
0.00.632.373 I llama_init_from_model: n_ctx         = 128
0.00.632.374 I llama_init_from_model: n_ctx_per_seq = 128
0.00.632.374 I llama_init_from_model: n_batch       = 128
0.00.632.375 I llama_init_from_model: n_ubatch      = 128
0.00.632.375 I llama_init_from_model: flash_attn    = 0
0.00.632.377 I llama_init_from_model: freq_base     = 10000.0
0.00.632.378 I llama_init_from_model: freq_scale    = 1
0.00.632.378 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.632.381 I ggml_metal_init: allocating
0.00.632.447 I ggml_metal_init: found device: Apple M4
0.00.632.460 I ggml_metal_init: picking default device: Apple M4
0.00.634.240 I ggml_metal_init: using embedded metal library
0.00.640.432 I ggml_metal_init: GPU name:   Apple M4
0.00.640.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.440 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.442 I ggml_metal_init: simdgroup reduction   = true
0.00.640.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.443 I ggml_metal_init: has residency sets    = true
0.00.640.443 I ggml_metal_init: has bfloat            = true
0.00.640.444 I ggml_metal_init: use bfloat            = true
0.00.640.445 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.447 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.742 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.662.188 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.662.196 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.662.225 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.665.326 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.665.327 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.665.328 I llama_init_from_model: graph nodes  = 967
0.00.665.329 I llama_init_from_model: graph splits = 2
0.00.665.331 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.691.022 I 
0.00.691.097 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.691.115 I perplexity: tokenizing the input ..
0.00.697.788 I perplexity: tokenization took 6.67 ms
0.00.697.813 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.833.798 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.835.050 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.835.061 I llama_perf_context_print:        load time =     680.78 ms
0.00.835.062 I llama_perf_context_print: prompt eval time =     135.20 ms /   128 tokens (    1.06 ms per token,   946.75 tokens per second)
0.00.835.063 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.835.063 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.835.435 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.079s
sys	0m0.162s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.861 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.843 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.849 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.851 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.854 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.458 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.110 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.111 I llama_model_loader: - type  f32:  194 tensors
0.00.025.111 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.112 I print_info: file format = GGUF V3 (latest)
0.00.025.113 I print_info: file type   = Q4_1
0.00.025.113 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.186 I load: special tokens cache size = 25
0.00.039.336 I load: token to piece cache size = 0.2984 MB
0.00.039.339 I print_info: arch             = gptneox
0.00.039.339 I print_info: vocab_only       = 0
0.00.039.340 I print_info: n_ctx_train      = 2048
0.00.039.340 I print_info: n_embd           = 2048
0.00.039.340 I print_info: n_layer          = 24
0.00.039.343 I print_info: n_head           = 16
0.00.039.343 I print_info: n_head_kv        = 16
0.00.039.344 I print_info: n_rot            = 32
0.00.039.344 I print_info: n_swa            = 0
0.00.039.344 I print_info: n_embd_head_k    = 128
0.00.039.346 I print_info: n_embd_head_v    = 128
0.00.039.347 I print_info: n_gqa            = 1
0.00.039.348 I print_info: n_embd_k_gqa     = 2048
0.00.039.349 I print_info: n_embd_v_gqa     = 2048
0.00.039.349 I print_info: f_norm_eps       = 1.0e-05
0.00.039.351 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.351 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.351 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.351 I print_info: f_logit_scale    = 0.0e+00
0.00.039.352 I print_info: n_ff             = 8192
0.00.039.352 I print_info: n_expert         = 0
0.00.039.352 I print_info: n_expert_used    = 0
0.00.039.352 I print_info: causal attn      = 1
0.00.039.352 I print_info: pooling type     = 0
0.00.039.352 I print_info: rope type        = 2
0.00.039.353 I print_info: rope scaling     = linear
0.00.039.353 I print_info: freq_base_train  = 10000.0
0.00.039.353 I print_info: freq_scale_train = 1
0.00.039.354 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.359 I print_info: rope_finetuned   = unknown
0.00.039.359 I print_info: ssm_d_conv       = 0
0.00.039.359 I print_info: ssm_d_inner      = 0
0.00.039.359 I print_info: ssm_d_state      = 0
0.00.039.360 I print_info: ssm_dt_rank      = 0
0.00.039.360 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.362 I print_info: model type       = 1.4B
0.00.039.362 I print_info: model params     = 1.41 B
0.00.039.362 I print_info: general.name     = 1.4B
0.00.039.362 I print_info: vocab type       = BPE
0.00.039.363 I print_info: n_vocab          = 50304
0.00.039.363 I print_info: n_merges         = 50009
0.00.039.363 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.363 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.363 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.364 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.364 I print_info: LF token         = 187 ''
0.00.039.364 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.364 I print_info: max token length = 1024
0.00.039.365 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.584 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.587 I load_tensors: offloading output layer to GPU
0.00.637.588 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.608 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.637.609 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.638.809 I llama_init_from_model: n_seq_max     = 1
0.00.638.811 I llama_init_from_model: n_ctx         = 2048
0.00.638.812 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.638.812 I llama_init_from_model: n_batch       = 2048
0.00.638.812 I llama_init_from_model: n_ubatch      = 512
0.00.638.813 I llama_init_from_model: flash_attn    = 0
0.00.638.814 I llama_init_from_model: freq_base     = 10000.0
0.00.638.815 I llama_init_from_model: freq_scale    = 1
0.00.638.816 I ggml_metal_init: allocating
0.00.638.848 I ggml_metal_init: found device: Apple M4
0.00.638.859 I ggml_metal_init: picking default device: Apple M4
0.00.640.378 I ggml_metal_init: using embedded metal library
0.00.646.482 I ggml_metal_init: GPU name:   Apple M4
0.00.646.485 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.487 I ggml_metal_init: simdgroup reduction   = true
0.00.646.488 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.488 I ggml_metal_init: has residency sets    = true
0.00.646.488 I ggml_metal_init: has bfloat            = true
0.00.646.488 I ggml_metal_init: use bfloat            = true
0.00.646.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.491 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.723 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.324 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.330 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.409 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.722.268 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.722.270 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.722.271 I llama_init_from_model: graph nodes  = 967
0.00.722.271 I llama_init_from_model: graph splits = 2
0.00.722.276 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.722.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.722.401 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.147 I main: llama threadpool init, n_threads = 4
0.00.769.187 I 
0.00.769.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.212 I 
0.00.769.345 I sampler seed: 1234
0.00.769.350 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.383 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.386 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.386 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.496.856 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.496.856 I llama_perf_context_print:        load time =     759.57 ms
0.01.496.857 I llama_perf_context_print: prompt eval time =      47.23 ms /     7 tokens (    6.75 ms per token,   148.22 tokens per second)
0.01.496.858 I llama_perf_context_print:        eval time =     677.40 ms /    63 runs   (   10.75 ms per token,    93.00 tokens per second)
0.01.496.858 I llama_perf_context_print:       total time =     728.42 ms /    70 tokens
0.01.497.146 I ggml_metal_free: deallocating

real	0m1.513s
user	0m0.107s
sys	0m0.234s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.067 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.078 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.085 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.087 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.087 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.088 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.088 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.089 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.089 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.090 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.090 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.090 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.091 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.091 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.093 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.094 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.094 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.918 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.759 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.760 I print_info: file format = GGUF V3 (latest)
0.00.024.760 I print_info: file type   = Q4_1
0.00.024.762 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.120 I load: special tokens cache size = 25
0.00.039.224 I load: token to piece cache size = 0.2984 MB
0.00.039.228 I print_info: arch             = gptneox
0.00.039.228 I print_info: vocab_only       = 0
0.00.039.228 I print_info: n_ctx_train      = 2048
0.00.039.229 I print_info: n_embd           = 2048
0.00.039.229 I print_info: n_layer          = 24
0.00.039.233 I print_info: n_head           = 16
0.00.039.234 I print_info: n_head_kv        = 16
0.00.039.234 I print_info: n_rot            = 32
0.00.039.234 I print_info: n_swa            = 0
0.00.039.235 I print_info: n_embd_head_k    = 128
0.00.039.235 I print_info: n_embd_head_v    = 128
0.00.039.235 I print_info: n_gqa            = 1
0.00.039.236 I print_info: n_embd_k_gqa     = 2048
0.00.039.237 I print_info: n_embd_v_gqa     = 2048
0.00.039.238 I print_info: f_norm_eps       = 1.0e-05
0.00.039.238 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.239 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.239 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.240 I print_info: f_logit_scale    = 0.0e+00
0.00.039.242 I print_info: n_ff             = 8192
0.00.039.242 I print_info: n_expert         = 0
0.00.039.242 I print_info: n_expert_used    = 0
0.00.039.242 I print_info: causal attn      = 1
0.00.039.242 I print_info: pooling type     = 0
0.00.039.244 I print_info: rope type        = 2
0.00.039.244 I print_info: rope scaling     = linear
0.00.039.245 I print_info: freq_base_train  = 10000.0
0.00.039.245 I print_info: freq_scale_train = 1
0.00.039.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.245 I print_info: rope_finetuned   = unknown
0.00.039.245 I print_info: ssm_d_conv       = 0
0.00.039.245 I print_info: ssm_d_inner      = 0
0.00.039.245 I print_info: ssm_d_state      = 0
0.00.039.246 I print_info: ssm_dt_rank      = 0
0.00.039.246 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.246 I print_info: model type       = 1.4B
0.00.039.246 I print_info: model params     = 1.41 B
0.00.039.246 I print_info: general.name     = 1.4B
0.00.039.247 I print_info: vocab type       = BPE
0.00.039.247 I print_info: n_vocab          = 50304
0.00.039.247 I print_info: n_merges         = 50009
0.00.039.248 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.248 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.258 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.258 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.259 I print_info: LF token         = 187 ''
0.00.039.259 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.259 I print_info: max token length = 1024
0.00.039.260 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.634.748 I load_tensors: offloading 24 repeating layers to GPU
0.00.634.755 I load_tensors: offloading output layer to GPU
0.00.634.755 I load_tensors: offloaded 25/25 layers to GPU
0.00.634.781 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.634.783 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.635.937 I llama_init_from_model: n_seq_max     = 1
0.00.635.939 I llama_init_from_model: n_ctx         = 128
0.00.635.939 I llama_init_from_model: n_ctx_per_seq = 128
0.00.635.940 I llama_init_from_model: n_batch       = 128
0.00.635.940 I llama_init_from_model: n_ubatch      = 128
0.00.635.941 I llama_init_from_model: flash_attn    = 0
0.00.635.942 I llama_init_from_model: freq_base     = 10000.0
0.00.635.942 I llama_init_from_model: freq_scale    = 1
0.00.635.943 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.635.944 I ggml_metal_init: allocating
0.00.635.997 I ggml_metal_init: found device: Apple M4
0.00.636.009 I ggml_metal_init: picking default device: Apple M4
0.00.637.483 I ggml_metal_init: using embedded metal library
0.00.643.595 I ggml_metal_init: GPU name:   Apple M4
0.00.643.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.643.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.643.599 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.643.600 I ggml_metal_init: simdgroup reduction   = true
0.00.643.600 I ggml_metal_init: simdgroup matrix mul. = true
0.00.643.600 I ggml_metal_init: has residency sets    = true
0.00.643.600 I ggml_metal_init: has bfloat            = true
0.00.643.601 I ggml_metal_init: use bfloat            = true
0.00.643.601 I ggml_metal_init: hasUnifiedMemory      = true
0.00.643.603 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.660.082 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.663.540 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.663.544 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.663.572 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.666.820 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.666.822 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.666.823 I llama_init_from_model: graph nodes  = 967
0.00.666.823 I llama_init_from_model: graph splits = 2
0.00.666.826 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.666.827 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.089 I 
0.00.692.162 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.181 I perplexity: tokenizing the input ..
0.00.697.908 I perplexity: tokenization took 5.722 ms
0.00.697.924 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.822.771 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.824.014 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.824.030 I llama_perf_context_print:        load time =     683.01 ms
0.00.824.032 I llama_perf_context_print: prompt eval time =     123.95 ms /   128 tokens (    0.97 ms per token,  1032.63 tokens per second)
0.00.824.032 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.033 I llama_perf_context_print:       total time =     131.94 ms /   129 tokens
0.00.824.401 I ggml_metal_free: deallocating

real	0m0.838s
user	0m0.077s
sys	0m0.165s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.673 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.727 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.734 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.735 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.740 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.742 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.744 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.744 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.745 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.745 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.745 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.746 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.748 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.599 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.607 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.286 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.287 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.288 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.288 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.288 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.288 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.289 I llama_model_loader: - type  f32:  194 tensors
0.00.026.289 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.289 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.290 I print_info: file format = GGUF V3 (latest)
0.00.026.290 I print_info: file type   = Q5_0
0.00.026.291 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.255 I load: special tokens cache size = 25
0.00.040.288 I load: token to piece cache size = 0.2984 MB
0.00.040.291 I print_info: arch             = gptneox
0.00.040.291 I print_info: vocab_only       = 0
0.00.040.291 I print_info: n_ctx_train      = 2048
0.00.040.292 I print_info: n_embd           = 2048
0.00.040.292 I print_info: n_layer          = 24
0.00.040.294 I print_info: n_head           = 16
0.00.040.294 I print_info: n_head_kv        = 16
0.00.040.295 I print_info: n_rot            = 32
0.00.040.295 I print_info: n_swa            = 0
0.00.040.295 I print_info: n_embd_head_k    = 128
0.00.040.295 I print_info: n_embd_head_v    = 128
0.00.040.296 I print_info: n_gqa            = 1
0.00.040.296 I print_info: n_embd_k_gqa     = 2048
0.00.040.298 I print_info: n_embd_v_gqa     = 2048
0.00.040.298 I print_info: f_norm_eps       = 1.0e-05
0.00.040.299 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.299 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.299 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.299 I print_info: f_logit_scale    = 0.0e+00
0.00.040.300 I print_info: n_ff             = 8192
0.00.040.300 I print_info: n_expert         = 0
0.00.040.300 I print_info: n_expert_used    = 0
0.00.040.301 I print_info: causal attn      = 1
0.00.040.301 I print_info: pooling type     = 0
0.00.040.301 I print_info: rope type        = 2
0.00.040.301 I print_info: rope scaling     = linear
0.00.040.301 I print_info: freq_base_train  = 10000.0
0.00.040.302 I print_info: freq_scale_train = 1
0.00.040.302 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.302 I print_info: rope_finetuned   = unknown
0.00.040.302 I print_info: ssm_d_conv       = 0
0.00.040.302 I print_info: ssm_d_inner      = 0
0.00.040.303 I print_info: ssm_d_state      = 0
0.00.040.303 I print_info: ssm_dt_rank      = 0
0.00.040.303 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.303 I print_info: model type       = 1.4B
0.00.040.304 I print_info: model params     = 1.41 B
0.00.040.305 I print_info: general.name     = 1.4B
0.00.040.305 I print_info: vocab type       = BPE
0.00.040.306 I print_info: n_vocab          = 50304
0.00.040.306 I print_info: n_merges         = 50009
0.00.040.306 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.306 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.306 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: LF token         = 187 ''
0.00.040.307 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.307 I print_info: max token length = 1024
0.00.040.308 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.739.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.739.894 I load_tensors: offloading output layer to GPU
0.00.739.895 I load_tensors: offloaded 25/25 layers to GPU
0.00.739.913 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.739.915 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.740.879 I llama_init_from_model: n_seq_max     = 1
0.00.740.881 I llama_init_from_model: n_ctx         = 2048
0.00.740.882 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.740.882 I llama_init_from_model: n_batch       = 2048
0.00.740.883 I llama_init_from_model: n_ubatch      = 512
0.00.740.883 I llama_init_from_model: flash_attn    = 0
0.00.740.884 I llama_init_from_model: freq_base     = 10000.0
0.00.740.885 I llama_init_from_model: freq_scale    = 1
0.00.740.886 I ggml_metal_init: allocating
0.00.740.901 I ggml_metal_init: found device: Apple M4
0.00.740.910 I ggml_metal_init: picking default device: Apple M4
0.00.742.430 I ggml_metal_init: using embedded metal library
0.00.748.418 I ggml_metal_init: GPU name:   Apple M4
0.00.748.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.748.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.748.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.748.424 I ggml_metal_init: simdgroup reduction   = true
0.00.748.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.748.424 I ggml_metal_init: has residency sets    = true
0.00.748.425 I ggml_metal_init: has bfloat            = true
0.00.748.425 I ggml_metal_init: use bfloat            = true
0.00.748.426 I ggml_metal_init: hasUnifiedMemory      = true
0.00.748.427 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.765.759 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.811.284 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.811.292 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.811.314 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.816.083 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.816.085 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.816.085 I llama_init_from_model: graph nodes  = 967
0.00.816.085 I llama_init_from_model: graph splits = 2
0.00.816.091 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.816.225 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.816.226 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.867.693 I main: llama threadpool init, n_threads = 4
0.00.867.732 I 
0.00.867.751 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.867.752 I 
0.00.867.869 I sampler seed: 1234
0.00.867.874 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.867.911 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.867.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.867.915 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.653.384 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50247.70 tokens per second)
0.01.653.385 I llama_perf_context_print:        load time =     857.32 ms
0.01.653.387 I llama_perf_context_print: prompt eval time =      48.76 ms /     7 tokens (    6.97 ms per token,   143.58 tokens per second)
0.01.653.388 I llama_perf_context_print:        eval time =     733.79 ms /    63 runs   (   11.65 ms per token,    85.86 tokens per second)
0.01.653.388 I llama_perf_context_print:       total time =     786.39 ms /    70 tokens
0.01.653.608 I ggml_metal_free: deallocating

real	0m1.670s
user	0m0.108s
sys	0m0.248s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.906 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.682 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.687 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.689 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.690 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.690 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.690 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.691 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.692 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.692 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.692 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.693 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.694 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.694 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.696 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.696 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.578 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.413 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.414 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.415 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.415 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.415 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.416 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.416 I llama_model_loader: - type  f32:  194 tensors
0.00.026.416 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.417 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.417 I print_info: file format = GGUF V3 (latest)
0.00.026.418 I print_info: file type   = Q5_0
0.00.026.419 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.702 I load: special tokens cache size = 25
0.00.040.728 I load: token to piece cache size = 0.2984 MB
0.00.040.733 I print_info: arch             = gptneox
0.00.040.733 I print_info: vocab_only       = 0
0.00.040.733 I print_info: n_ctx_train      = 2048
0.00.040.734 I print_info: n_embd           = 2048
0.00.040.734 I print_info: n_layer          = 24
0.00.040.738 I print_info: n_head           = 16
0.00.040.738 I print_info: n_head_kv        = 16
0.00.040.739 I print_info: n_rot            = 32
0.00.040.739 I print_info: n_swa            = 0
0.00.040.739 I print_info: n_embd_head_k    = 128
0.00.040.739 I print_info: n_embd_head_v    = 128
0.00.040.740 I print_info: n_gqa            = 1
0.00.040.741 I print_info: n_embd_k_gqa     = 2048
0.00.040.741 I print_info: n_embd_v_gqa     = 2048
0.00.040.742 I print_info: f_norm_eps       = 1.0e-05
0.00.040.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.746 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.746 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.746 I print_info: f_logit_scale    = 0.0e+00
0.00.040.747 I print_info: n_ff             = 8192
0.00.040.749 I print_info: n_expert         = 0
0.00.040.749 I print_info: n_expert_used    = 0
0.00.040.749 I print_info: causal attn      = 1
0.00.040.749 I print_info: pooling type     = 0
0.00.040.749 I print_info: rope type        = 2
0.00.040.749 I print_info: rope scaling     = linear
0.00.040.750 I print_info: freq_base_train  = 10000.0
0.00.040.750 I print_info: freq_scale_train = 1
0.00.040.750 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.750 I print_info: rope_finetuned   = unknown
0.00.040.751 I print_info: ssm_d_conv       = 0
0.00.040.751 I print_info: ssm_d_inner      = 0
0.00.040.751 I print_info: ssm_d_state      = 0
0.00.040.751 I print_info: ssm_dt_rank      = 0
0.00.040.751 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.751 I print_info: model type       = 1.4B
0.00.040.752 I print_info: model params     = 1.41 B
0.00.040.752 I print_info: general.name     = 1.4B
0.00.040.752 I print_info: vocab type       = BPE
0.00.040.752 I print_info: n_vocab          = 50304
0.00.040.753 I print_info: n_merges         = 50009
0.00.040.753 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.753 I print_info: LF token         = 187 ''
0.00.040.754 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.755 I print_info: max token length = 1024
0.00.040.755 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.736.859 I load_tensors: offloading 24 repeating layers to GPU
0.00.736.865 I load_tensors: offloading output layer to GPU
0.00.736.866 I load_tensors: offloaded 25/25 layers to GPU
0.00.736.892 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.736.894 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.737.989 I llama_init_from_model: n_seq_max     = 1
0.00.737.991 I llama_init_from_model: n_ctx         = 128
0.00.737.992 I llama_init_from_model: n_ctx_per_seq = 128
0.00.737.992 I llama_init_from_model: n_batch       = 128
0.00.737.993 I llama_init_from_model: n_ubatch      = 128
0.00.737.993 I llama_init_from_model: flash_attn    = 0
0.00.737.994 I llama_init_from_model: freq_base     = 10000.0
0.00.737.995 I llama_init_from_model: freq_scale    = 1
0.00.737.995 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.737.997 I ggml_metal_init: allocating
0.00.738.049 I ggml_metal_init: found device: Apple M4
0.00.738.061 I ggml_metal_init: picking default device: Apple M4
0.00.739.430 I ggml_metal_init: using embedded metal library
0.00.744.905 I ggml_metal_init: GPU name:   Apple M4
0.00.744.909 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.744.909 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.744.910 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.744.910 I ggml_metal_init: simdgroup reduction   = true
0.00.744.911 I ggml_metal_init: simdgroup matrix mul. = true
0.00.744.911 I ggml_metal_init: has residency sets    = true
0.00.744.911 I ggml_metal_init: has bfloat            = true
0.00.744.911 I ggml_metal_init: use bfloat            = true
0.00.744.912 I ggml_metal_init: hasUnifiedMemory      = true
0.00.744.914 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.760.977 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.764.180 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.764.183 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.764.207 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.767.228 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.767.230 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.767.230 I llama_init_from_model: graph nodes  = 967
0.00.767.230 I llama_init_from_model: graph splits = 2
0.00.767.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.767.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.691 I 
0.00.792.754 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.771 I perplexity: tokenizing the input ..
0.00.799.377 I perplexity: tokenization took 6.603 ms
0.00.799.398 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.935.275 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.936.556 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.936.573 I llama_perf_context_print:        load time =     781.78 ms
0.00.936.574 I llama_perf_context_print: prompt eval time =     135.05 ms /   128 tokens (    1.06 ms per token,   947.80 tokens per second)
0.00.936.575 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.936.575 I llama_perf_context_print:       total time =     143.89 ms /   129 tokens
0.00.936.958 I ggml_metal_free: deallocating

real	0m0.953s
user	0m0.077s
sys	0m0.193s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.662 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.599 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.495 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.467 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.316 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.317 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.317 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.318 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.318 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.318 I llama_model_loader: - type  f32:  194 tensors
0.00.025.319 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.319 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.320 I print_info: file format = GGUF V3 (latest)
0.00.025.320 I print_info: file type   = Q5_1
0.00.025.321 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.272 I load: special tokens cache size = 25
0.00.039.317 I load: token to piece cache size = 0.2984 MB
0.00.039.320 I print_info: arch             = gptneox
0.00.039.321 I print_info: vocab_only       = 0
0.00.039.321 I print_info: n_ctx_train      = 2048
0.00.039.321 I print_info: n_embd           = 2048
0.00.039.321 I print_info: n_layer          = 24
0.00.039.325 I print_info: n_head           = 16
0.00.039.326 I print_info: n_head_kv        = 16
0.00.039.326 I print_info: n_rot            = 32
0.00.039.326 I print_info: n_swa            = 0
0.00.039.326 I print_info: n_embd_head_k    = 128
0.00.039.326 I print_info: n_embd_head_v    = 128
0.00.039.329 I print_info: n_gqa            = 1
0.00.039.330 I print_info: n_embd_k_gqa     = 2048
0.00.039.331 I print_info: n_embd_v_gqa     = 2048
0.00.039.331 I print_info: f_norm_eps       = 1.0e-05
0.00.039.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.332 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.332 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.332 I print_info: f_logit_scale    = 0.0e+00
0.00.039.334 I print_info: n_ff             = 8192
0.00.039.334 I print_info: n_expert         = 0
0.00.039.334 I print_info: n_expert_used    = 0
0.00.039.335 I print_info: causal attn      = 1
0.00.039.335 I print_info: pooling type     = 0
0.00.039.335 I print_info: rope type        = 2
0.00.039.335 I print_info: rope scaling     = linear
0.00.039.336 I print_info: freq_base_train  = 10000.0
0.00.039.336 I print_info: freq_scale_train = 1
0.00.039.336 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.336 I print_info: rope_finetuned   = unknown
0.00.039.337 I print_info: ssm_d_conv       = 0
0.00.039.337 I print_info: ssm_d_inner      = 0
0.00.039.337 I print_info: ssm_d_state      = 0
0.00.039.337 I print_info: ssm_dt_rank      = 0
0.00.039.337 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.337 I print_info: model type       = 1.4B
0.00.039.338 I print_info: model params     = 1.41 B
0.00.039.338 I print_info: general.name     = 1.4B
0.00.039.338 I print_info: vocab type       = BPE
0.00.039.339 I print_info: n_vocab          = 50304
0.00.039.339 I print_info: n_merges         = 50009
0.00.039.341 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.341 I print_info: LF token         = 187 ''
0.00.039.342 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.342 I print_info: max token length = 1024
0.00.039.343 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.823.162 I load_tensors: offloading 24 repeating layers to GPU
0.00.823.167 I load_tensors: offloading output layer to GPU
0.00.823.168 I load_tensors: offloaded 25/25 layers to GPU
0.00.823.187 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.823.190 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.824.088 I llama_init_from_model: n_seq_max     = 1
0.00.824.090 I llama_init_from_model: n_ctx         = 2048
0.00.824.091 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.824.091 I llama_init_from_model: n_batch       = 2048
0.00.824.091 I llama_init_from_model: n_ubatch      = 512
0.00.824.092 I llama_init_from_model: flash_attn    = 0
0.00.824.093 I llama_init_from_model: freq_base     = 10000.0
0.00.824.093 I llama_init_from_model: freq_scale    = 1
0.00.824.095 I ggml_metal_init: allocating
0.00.824.109 I ggml_metal_init: found device: Apple M4
0.00.824.117 I ggml_metal_init: picking default device: Apple M4
0.00.825.428 I ggml_metal_init: using embedded metal library
0.00.831.438 I ggml_metal_init: GPU name:   Apple M4
0.00.831.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.831.443 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.831.444 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.831.444 I ggml_metal_init: simdgroup reduction   = true
0.00.831.445 I ggml_metal_init: simdgroup matrix mul. = true
0.00.831.445 I ggml_metal_init: has residency sets    = true
0.00.831.445 I ggml_metal_init: has bfloat            = true
0.00.831.445 I ggml_metal_init: use bfloat            = true
0.00.831.446 I ggml_metal_init: hasUnifiedMemory      = true
0.00.831.448 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.848.043 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.905.685 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.905.693 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.905.718 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.910.837 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.910.839 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.910.840 I llama_init_from_model: graph nodes  = 967
0.00.910.840 I llama_init_from_model: graph splits = 2
0.00.910.845 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.910.979 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.910.980 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.963.305 I main: llama threadpool init, n_threads = 4
0.00.963.352 I 
0.00.963.374 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.963.376 I 
0.00.963.493 I sampler seed: 1234
0.00.963.497 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.963.536 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.963.540 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.963.540 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.807.036 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.807.037 I llama_perf_context_print:        load time =     953.93 ms
0.01.807.038 I llama_perf_context_print: prompt eval time =      53.31 ms /     7 tokens (    7.62 ms per token,   131.30 tokens per second)
0.01.807.038 I llama_perf_context_print:        eval time =     787.16 ms /    63 runs   (   12.49 ms per token,    80.03 tokens per second)
0.01.807.039 I llama_perf_context_print:       total time =     844.44 ms /    70 tokens
0.01.807.293 I ggml_metal_free: deallocating

real	0m1.825s
user	0m0.108s
sys	0m0.271s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.005 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.006 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.006 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.006 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.009 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.009 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.009 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.010 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.010 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.010 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.011 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.013 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.013 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.014 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.631 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.688 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.375 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.376 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.377 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.378 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.378 I llama_model_loader: - type  f32:  194 tensors
0.00.025.379 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.379 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.380 I print_info: file format = GGUF V3 (latest)
0.00.025.380 I print_info: file type   = Q5_1
0.00.025.381 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.208 I load: special tokens cache size = 25
0.00.039.211 I load: token to piece cache size = 0.2984 MB
0.00.039.215 I print_info: arch             = gptneox
0.00.039.216 I print_info: vocab_only       = 0
0.00.039.216 I print_info: n_ctx_train      = 2048
0.00.039.216 I print_info: n_embd           = 2048
0.00.039.216 I print_info: n_layer          = 24
0.00.039.221 I print_info: n_head           = 16
0.00.039.222 I print_info: n_head_kv        = 16
0.00.039.223 I print_info: n_rot            = 32
0.00.039.223 I print_info: n_swa            = 0
0.00.039.224 I print_info: n_embd_head_k    = 128
0.00.039.226 I print_info: n_embd_head_v    = 128
0.00.039.226 I print_info: n_gqa            = 1
0.00.039.227 I print_info: n_embd_k_gqa     = 2048
0.00.039.228 I print_info: n_embd_v_gqa     = 2048
0.00.039.228 I print_info: f_norm_eps       = 1.0e-05
0.00.039.228 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.229 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.229 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.229 I print_info: f_logit_scale    = 0.0e+00
0.00.039.229 I print_info: n_ff             = 8192
0.00.039.230 I print_info: n_expert         = 0
0.00.039.230 I print_info: n_expert_used    = 0
0.00.039.230 I print_info: causal attn      = 1
0.00.039.230 I print_info: pooling type     = 0
0.00.039.230 I print_info: rope type        = 2
0.00.039.230 I print_info: rope scaling     = linear
0.00.039.231 I print_info: freq_base_train  = 10000.0
0.00.039.231 I print_info: freq_scale_train = 1
0.00.039.231 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.232 I print_info: rope_finetuned   = unknown
0.00.039.232 I print_info: ssm_d_conv       = 0
0.00.039.232 I print_info: ssm_d_inner      = 0
0.00.039.232 I print_info: ssm_d_state      = 0
0.00.039.232 I print_info: ssm_dt_rank      = 0
0.00.039.232 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.232 I print_info: model type       = 1.4B
0.00.039.233 I print_info: model params     = 1.41 B
0.00.039.233 I print_info: general.name     = 1.4B
0.00.039.234 I print_info: vocab type       = BPE
0.00.039.234 I print_info: n_vocab          = 50304
0.00.039.234 I print_info: n_merges         = 50009
0.00.039.235 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.235 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.235 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.235 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.235 I print_info: LF token         = 187 ''
0.00.039.236 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.236 I print_info: max token length = 1024
0.00.039.236 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.779.224 I load_tensors: offloading 24 repeating layers to GPU
0.00.779.235 I load_tensors: offloading output layer to GPU
0.00.779.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.779.259 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.779.261 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.780.544 I llama_init_from_model: n_seq_max     = 1
0.00.780.547 I llama_init_from_model: n_ctx         = 128
0.00.780.548 I llama_init_from_model: n_ctx_per_seq = 128
0.00.780.549 I llama_init_from_model: n_batch       = 128
0.00.780.549 I llama_init_from_model: n_ubatch      = 128
0.00.780.549 I llama_init_from_model: flash_attn    = 0
0.00.780.552 I llama_init_from_model: freq_base     = 10000.0
0.00.780.553 I llama_init_from_model: freq_scale    = 1
0.00.780.555 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.780.558 I ggml_metal_init: allocating
0.00.780.609 I ggml_metal_init: found device: Apple M4
0.00.780.622 I ggml_metal_init: picking default device: Apple M4
0.00.782.090 I ggml_metal_init: using embedded metal library
0.00.787.890 I ggml_metal_init: GPU name:   Apple M4
0.00.787.898 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.787.899 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.787.900 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.787.901 I ggml_metal_init: simdgroup reduction   = true
0.00.787.901 I ggml_metal_init: simdgroup matrix mul. = true
0.00.787.902 I ggml_metal_init: has residency sets    = true
0.00.787.902 I ggml_metal_init: has bfloat            = true
0.00.787.902 I ggml_metal_init: use bfloat            = true
0.00.787.904 I ggml_metal_init: hasUnifiedMemory      = true
0.00.787.908 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.807.745 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.811.307 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.811.314 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.811.353 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.814.547 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.814.549 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.814.549 I llama_init_from_model: graph nodes  = 967
0.00.814.550 I llama_init_from_model: graph splits = 2
0.00.814.553 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.814.557 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.841.011 I 
0.00.841.091 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.841.112 I perplexity: tokenizing the input ..
0.00.847.953 I perplexity: tokenization took 6.838 ms
0.00.847.970 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.983.617 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.984.879 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.984.889 I llama_perf_context_print:        load time =     831.15 ms
0.00.984.890 I llama_perf_context_print: prompt eval time =     134.73 ms /   128 tokens (    1.05 ms per token,   950.06 tokens per second)
0.00.984.891 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.984.891 I llama_perf_context_print:       total time =     143.88 ms /   129 tokens
0.00.985.287 I ggml_metal_free: deallocating

real	0m0.999s
user	0m0.080s
sys	0m0.177s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.010.279 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.674 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.679 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.681 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.682 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.683 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.683 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.683 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.685 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.685 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.686 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.686 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.687 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.688 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.688 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.443 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.247 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.250 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.250 I llama_model_loader: - type  f32:  194 tensors
0.00.025.251 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.251 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.251 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.252 I print_info: file format = GGUF V3 (latest)
0.00.025.252 I print_info: file type   = Q2_K - Medium
0.00.025.253 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.904 I load: special tokens cache size = 25
0.00.038.956 I load: token to piece cache size = 0.2984 MB
0.00.038.962 I print_info: arch             = gptneox
0.00.038.962 I print_info: vocab_only       = 0
0.00.038.962 I print_info: n_ctx_train      = 2048
0.00.038.962 I print_info: n_embd           = 2048
0.00.038.962 I print_info: n_layer          = 24
0.00.038.965 I print_info: n_head           = 16
0.00.038.966 I print_info: n_head_kv        = 16
0.00.038.966 I print_info: n_rot            = 32
0.00.038.966 I print_info: n_swa            = 0
0.00.038.966 I print_info: n_embd_head_k    = 128
0.00.038.968 I print_info: n_embd_head_v    = 128
0.00.038.968 I print_info: n_gqa            = 1
0.00.038.969 I print_info: n_embd_k_gqa     = 2048
0.00.038.972 I print_info: n_embd_v_gqa     = 2048
0.00.038.973 I print_info: f_norm_eps       = 1.0e-05
0.00.038.973 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.973 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.973 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.973 I print_info: f_logit_scale    = 0.0e+00
0.00.038.974 I print_info: n_ff             = 8192
0.00.038.979 I print_info: n_expert         = 0
0.00.038.979 I print_info: n_expert_used    = 0
0.00.038.979 I print_info: causal attn      = 1
0.00.038.979 I print_info: pooling type     = 0
0.00.038.980 I print_info: rope type        = 2
0.00.038.980 I print_info: rope scaling     = linear
0.00.038.981 I print_info: freq_base_train  = 10000.0
0.00.038.981 I print_info: freq_scale_train = 1
0.00.038.981 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.982 I print_info: rope_finetuned   = unknown
0.00.038.982 I print_info: ssm_d_conv       = 0
0.00.038.982 I print_info: ssm_d_inner      = 0
0.00.038.982 I print_info: ssm_d_state      = 0
0.00.038.982 I print_info: ssm_dt_rank      = 0
0.00.038.992 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.993 I print_info: model type       = 1.4B
0.00.038.994 I print_info: model params     = 1.41 B
0.00.038.994 I print_info: general.name     = 1.4B
0.00.038.995 I print_info: vocab type       = BPE
0.00.038.995 I print_info: n_vocab          = 50304
0.00.038.995 I print_info: n_merges         = 50009
0.00.038.995 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.995 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: LF token         = 187 ''
0.00.038.996 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.996 I print_info: max token length = 1024
0.00.038.997 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.401.402 I load_tensors: offloading 24 repeating layers to GPU
0.00.401.415 I load_tensors: offloading output layer to GPU
0.00.401.416 I load_tensors: offloaded 25/25 layers to GPU
0.00.401.451 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.401.452 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.402.909 I llama_init_from_model: n_seq_max     = 1
0.00.402.912 I llama_init_from_model: n_ctx         = 2048
0.00.402.913 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.402.913 I llama_init_from_model: n_batch       = 2048
0.00.402.914 I llama_init_from_model: n_ubatch      = 512
0.00.402.914 I llama_init_from_model: flash_attn    = 0
0.00.402.917 I llama_init_from_model: freq_base     = 10000.0
0.00.402.917 I llama_init_from_model: freq_scale    = 1
0.00.402.919 I ggml_metal_init: allocating
0.00.402.991 I ggml_metal_init: found device: Apple M4
0.00.403.006 I ggml_metal_init: picking default device: Apple M4
0.00.404.978 I ggml_metal_init: using embedded metal library
0.00.411.035 I ggml_metal_init: GPU name:   Apple M4
0.00.411.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.411.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.411.047 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.411.048 I ggml_metal_init: simdgroup reduction   = true
0.00.411.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.411.048 I ggml_metal_init: has residency sets    = true
0.00.411.048 I ggml_metal_init: has bfloat            = true
0.00.411.049 I ggml_metal_init: use bfloat            = true
0.00.411.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.411.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.948 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.485.679 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.485.690 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.485.715 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.491.911 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.491.914 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.491.914 I llama_init_from_model: graph nodes  = 967
0.00.491.914 I llama_init_from_model: graph splits = 2
0.00.491.919 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.492.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.492.053 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.542.459 I main: llama threadpool init, n_threads = 4
0.00.542.504 I 
0.00.542.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.542.529 I 
0.00.542.667 I sampler seed: 1234
0.00.542.671 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.542.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.542.689 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.542.689 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.236.536 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.236.537 I llama_perf_context_print:        load time =     531.48 ms
0.01.236.537 I llama_perf_context_print: prompt eval time =      44.52 ms /     7 tokens (    6.36 ms per token,   157.24 tokens per second)
0.01.236.538 I llama_perf_context_print:        eval time =     646.46 ms /    63 runs   (   10.26 ms per token,    97.45 tokens per second)
0.01.236.538 I llama_perf_context_print:       total time =     694.77 ms /    70 tokens
0.01.236.764 I ggml_metal_free: deallocating

real	0m1.254s
user	0m0.111s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.540 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.380 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.380 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.381 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.381 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.382 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.383 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.383 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.384 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.386 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.387 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.387 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.134 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.901 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.902 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.902 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.902 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.903 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.903 I llama_model_loader: - type  f32:  194 tensors
0.00.026.904 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.904 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.904 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.905 I print_info: file format = GGUF V3 (latest)
0.00.026.905 I print_info: file type   = Q2_K - Medium
0.00.026.906 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.688 I load: special tokens cache size = 25
0.00.040.876 I load: token to piece cache size = 0.2984 MB
0.00.040.881 I print_info: arch             = gptneox
0.00.040.881 I print_info: vocab_only       = 0
0.00.040.881 I print_info: n_ctx_train      = 2048
0.00.040.882 I print_info: n_embd           = 2048
0.00.040.882 I print_info: n_layer          = 24
0.00.040.886 I print_info: n_head           = 16
0.00.040.886 I print_info: n_head_kv        = 16
0.00.040.887 I print_info: n_rot            = 32
0.00.040.887 I print_info: n_swa            = 0
0.00.040.887 I print_info: n_embd_head_k    = 128
0.00.040.887 I print_info: n_embd_head_v    = 128
0.00.040.888 I print_info: n_gqa            = 1
0.00.040.889 I print_info: n_embd_k_gqa     = 2048
0.00.040.893 I print_info: n_embd_v_gqa     = 2048
0.00.040.893 I print_info: f_norm_eps       = 1.0e-05
0.00.040.894 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.894 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.894 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.894 I print_info: f_logit_scale    = 0.0e+00
0.00.040.895 I print_info: n_ff             = 8192
0.00.040.895 I print_info: n_expert         = 0
0.00.040.895 I print_info: n_expert_used    = 0
0.00.040.895 I print_info: causal attn      = 1
0.00.040.896 I print_info: pooling type     = 0
0.00.040.897 I print_info: rope type        = 2
0.00.040.898 I print_info: rope scaling     = linear
0.00.040.898 I print_info: freq_base_train  = 10000.0
0.00.040.898 I print_info: freq_scale_train = 1
0.00.040.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.899 I print_info: rope_finetuned   = unknown
0.00.040.899 I print_info: ssm_d_conv       = 0
0.00.040.899 I print_info: ssm_d_inner      = 0
0.00.040.899 I print_info: ssm_d_state      = 0
0.00.040.899 I print_info: ssm_dt_rank      = 0
0.00.040.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.900 I print_info: model type       = 1.4B
0.00.040.900 I print_info: model params     = 1.41 B
0.00.040.900 I print_info: general.name     = 1.4B
0.00.040.900 I print_info: vocab type       = BPE
0.00.040.901 I print_info: n_vocab          = 50304
0.00.040.901 I print_info: n_merges         = 50009
0.00.040.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.901 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.901 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.902 I print_info: LF token         = 187 ''
0.00.040.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.902 I print_info: max token length = 1024
0.00.040.903 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.405.108 I load_tensors: offloading 24 repeating layers to GPU
0.00.405.120 I load_tensors: offloading output layer to GPU
0.00.405.121 I load_tensors: offloaded 25/25 layers to GPU
0.00.405.151 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.405.152 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.406.692 I llama_init_from_model: n_seq_max     = 1
0.00.406.694 I llama_init_from_model: n_ctx         = 128
0.00.406.694 I llama_init_from_model: n_ctx_per_seq = 128
0.00.406.695 I llama_init_from_model: n_batch       = 128
0.00.406.695 I llama_init_from_model: n_ubatch      = 128
0.00.406.696 I llama_init_from_model: flash_attn    = 0
0.00.406.697 I llama_init_from_model: freq_base     = 10000.0
0.00.406.698 I llama_init_from_model: freq_scale    = 1
0.00.406.698 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.406.700 I ggml_metal_init: allocating
0.00.406.761 I ggml_metal_init: found device: Apple M4
0.00.406.775 I ggml_metal_init: picking default device: Apple M4
0.00.408.587 I ggml_metal_init: using embedded metal library
0.00.415.621 I ggml_metal_init: GPU name:   Apple M4
0.00.415.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.415.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.415.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.415.628 I ggml_metal_init: simdgroup reduction   = true
0.00.415.628 I ggml_metal_init: simdgroup matrix mul. = true
0.00.415.628 I ggml_metal_init: has residency sets    = true
0.00.415.629 I ggml_metal_init: has bfloat            = true
0.00.415.629 I ggml_metal_init: use bfloat            = true
0.00.415.630 I ggml_metal_init: hasUnifiedMemory      = true
0.00.415.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.434.514 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.438.053 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.438.058 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.438.092 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.441.302 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.441.304 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.441.305 I llama_init_from_model: graph nodes  = 967
0.00.441.305 I llama_init_from_model: graph splits = 2
0.00.441.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.441.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.469.915 I 
0.00.469.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.016 I perplexity: tokenizing the input ..
0.00.476.578 I perplexity: tokenization took 6.559 ms
0.00.476.596 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.609.202 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.610.506 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.610.518 I llama_perf_context_print:        load time =     458.37 ms
0.00.610.519 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.16 tokens per second)
0.00.610.520 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.610.520 I llama_perf_context_print:       total time =     140.61 ms /   129 tokens
0.00.610.916 I ggml_metal_free: deallocating

real	0m0.627s
user	0m0.080s
sys	0m0.140s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.445 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.181 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.188 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.189 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.190 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.191 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.192 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.910 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.894 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.646 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.647 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.647 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.648 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.648 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.648 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.649 I llama_model_loader: - type  f32:  194 tensors
0.00.024.649 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.649 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.650 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.650 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.650 I print_info: file format = GGUF V3 (latest)
0.00.024.651 I print_info: file type   = Q3_K - Medium
0.00.024.651 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.275 I load: special tokens cache size = 25
0.00.038.285 I load: token to piece cache size = 0.2984 MB
0.00.038.291 I print_info: arch             = gptneox
0.00.038.291 I print_info: vocab_only       = 0
0.00.038.291 I print_info: n_ctx_train      = 2048
0.00.038.291 I print_info: n_embd           = 2048
0.00.038.291 I print_info: n_layer          = 24
0.00.038.294 I print_info: n_head           = 16
0.00.038.295 I print_info: n_head_kv        = 16
0.00.038.295 I print_info: n_rot            = 32
0.00.038.295 I print_info: n_swa            = 0
0.00.038.295 I print_info: n_embd_head_k    = 128
0.00.038.295 I print_info: n_embd_head_v    = 128
0.00.038.296 I print_info: n_gqa            = 1
0.00.038.297 I print_info: n_embd_k_gqa     = 2048
0.00.038.298 I print_info: n_embd_v_gqa     = 2048
0.00.038.298 I print_info: f_norm_eps       = 1.0e-05
0.00.038.299 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.299 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.299 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.299 I print_info: f_logit_scale    = 0.0e+00
0.00.038.300 I print_info: n_ff             = 8192
0.00.038.300 I print_info: n_expert         = 0
0.00.038.300 I print_info: n_expert_used    = 0
0.00.038.301 I print_info: causal attn      = 1
0.00.038.302 I print_info: pooling type     = 0
0.00.038.302 I print_info: rope type        = 2
0.00.038.303 I print_info: rope scaling     = linear
0.00.038.303 I print_info: freq_base_train  = 10000.0
0.00.038.303 I print_info: freq_scale_train = 1
0.00.038.304 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.306 I print_info: rope_finetuned   = unknown
0.00.038.306 I print_info: ssm_d_conv       = 0
0.00.038.306 I print_info: ssm_d_inner      = 0
0.00.038.306 I print_info: ssm_d_state      = 0
0.00.038.306 I print_info: ssm_dt_rank      = 0
0.00.038.307 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.307 I print_info: model type       = 1.4B
0.00.038.307 I print_info: model params     = 1.41 B
0.00.038.308 I print_info: general.name     = 1.4B
0.00.038.308 I print_info: vocab type       = BPE
0.00.038.308 I print_info: n_vocab          = 50304
0.00.038.309 I print_info: n_merges         = 50009
0.00.038.309 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.309 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.310 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.312 I print_info: LF token         = 187 ''
0.00.038.312 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.312 I print_info: max token length = 1024
0.00.038.312 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.490.244 I load_tensors: offloading 24 repeating layers to GPU
0.00.490.249 I load_tensors: offloading output layer to GPU
0.00.490.251 I load_tensors: offloaded 25/25 layers to GPU
0.00.490.274 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.490.277 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.491.498 I llama_init_from_model: n_seq_max     = 1
0.00.491.500 I llama_init_from_model: n_ctx         = 2048
0.00.491.500 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.491.501 I llama_init_from_model: n_batch       = 2048
0.00.491.501 I llama_init_from_model: n_ubatch      = 512
0.00.491.502 I llama_init_from_model: flash_attn    = 0
0.00.491.503 I llama_init_from_model: freq_base     = 10000.0
0.00.491.503 I llama_init_from_model: freq_scale    = 1
0.00.491.505 I ggml_metal_init: allocating
0.00.491.526 I ggml_metal_init: found device: Apple M4
0.00.491.536 I ggml_metal_init: picking default device: Apple M4
0.00.493.060 I ggml_metal_init: using embedded metal library
0.00.499.084 I ggml_metal_init: GPU name:   Apple M4
0.00.499.088 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.499.088 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.499.089 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.499.090 I ggml_metal_init: simdgroup reduction   = true
0.00.499.090 I ggml_metal_init: simdgroup matrix mul. = true
0.00.499.090 I ggml_metal_init: has residency sets    = true
0.00.499.091 I ggml_metal_init: has bfloat            = true
0.00.499.091 I ggml_metal_init: use bfloat            = true
0.00.499.092 I ggml_metal_init: hasUnifiedMemory      = true
0.00.499.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.516.382 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.570.970 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.570.978 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.571.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.588.293 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.588.296 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.588.296 I llama_init_from_model: graph nodes  = 967
0.00.588.296 I llama_init_from_model: graph splits = 2
0.00.588.301 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.588.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.588.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.637.473 I main: llama threadpool init, n_threads = 4
0.00.637.525 I 
0.00.637.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.637.550 I 
0.00.637.677 I sampler seed: 1234
0.00.637.681 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.637.690 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.637.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.637.691 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.385.109 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53828.66 tokens per second)
0.01.385.111 I llama_perf_context_print:        load time =     628.32 ms
0.01.385.112 I llama_perf_context_print: prompt eval time =      50.26 ms /     7 tokens (    7.18 ms per token,   139.29 tokens per second)
0.01.385.113 I llama_perf_context_print:        eval time =     694.20 ms /    63 runs   (   11.02 ms per token,    90.75 tokens per second)
0.01.385.113 I llama_perf_context_print:       total time =     748.34 ms /    70 tokens
0.01.385.360 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.107s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.013 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.373 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.379 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.381 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.384 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.384 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.385 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.385 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.386 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.386 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.387 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.387 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.387 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.388 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.390 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.390 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.390 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.910 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.912 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.913 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.026.914 I llama_model_loader: - type  f32:  194 tensors
0.00.026.914 I llama_model_loader: - type q3_K:   25 tensors
0.00.026.915 I llama_model_loader: - type q4_K:   71 tensors
0.00.026.915 I llama_model_loader: - type q5_K:    1 tensors
0.00.026.915 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.916 I print_info: file format = GGUF V3 (latest)
0.00.026.916 I print_info: file type   = Q3_K - Medium
0.00.026.917 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.761 I load: special tokens cache size = 25
0.00.040.801 I load: token to piece cache size = 0.2984 MB
0.00.040.805 I print_info: arch             = gptneox
0.00.040.805 I print_info: vocab_only       = 0
0.00.040.805 I print_info: n_ctx_train      = 2048
0.00.040.805 I print_info: n_embd           = 2048
0.00.040.806 I print_info: n_layer          = 24
0.00.040.810 I print_info: n_head           = 16
0.00.040.810 I print_info: n_head_kv        = 16
0.00.040.811 I print_info: n_rot            = 32
0.00.040.811 I print_info: n_swa            = 0
0.00.040.811 I print_info: n_embd_head_k    = 128
0.00.040.811 I print_info: n_embd_head_v    = 128
0.00.040.812 I print_info: n_gqa            = 1
0.00.040.813 I print_info: n_embd_k_gqa     = 2048
0.00.040.813 I print_info: n_embd_v_gqa     = 2048
0.00.040.814 I print_info: f_norm_eps       = 1.0e-05
0.00.040.814 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.815 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.815 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.815 I print_info: f_logit_scale    = 0.0e+00
0.00.040.816 I print_info: n_ff             = 8192
0.00.040.816 I print_info: n_expert         = 0
0.00.040.816 I print_info: n_expert_used    = 0
0.00.040.816 I print_info: causal attn      = 1
0.00.040.816 I print_info: pooling type     = 0
0.00.040.816 I print_info: rope type        = 2
0.00.040.817 I print_info: rope scaling     = linear
0.00.040.817 I print_info: freq_base_train  = 10000.0
0.00.040.817 I print_info: freq_scale_train = 1
0.00.040.818 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.818 I print_info: rope_finetuned   = unknown
0.00.040.818 I print_info: ssm_d_conv       = 0
0.00.040.821 I print_info: ssm_d_inner      = 0
0.00.040.821 I print_info: ssm_d_state      = 0
0.00.040.821 I print_info: ssm_dt_rank      = 0
0.00.040.821 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.821 I print_info: model type       = 1.4B
0.00.040.822 I print_info: model params     = 1.41 B
0.00.040.822 I print_info: general.name     = 1.4B
0.00.040.822 I print_info: vocab type       = BPE
0.00.040.823 I print_info: n_vocab          = 50304
0.00.040.823 I print_info: n_merges         = 50009
0.00.040.823 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.823 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: LF token         = 187 ''
0.00.040.824 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.824 I print_info: max token length = 1024
0.00.040.825 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.490.883 I load_tensors: offloading 24 repeating layers to GPU
0.00.490.886 I load_tensors: offloading output layer to GPU
0.00.490.887 I load_tensors: offloaded 25/25 layers to GPU
0.00.490.912 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.490.915 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.492.168 I llama_init_from_model: n_seq_max     = 1
0.00.492.170 I llama_init_from_model: n_ctx         = 128
0.00.492.170 I llama_init_from_model: n_ctx_per_seq = 128
0.00.492.171 I llama_init_from_model: n_batch       = 128
0.00.492.171 I llama_init_from_model: n_ubatch      = 128
0.00.492.172 I llama_init_from_model: flash_attn    = 0
0.00.492.173 I llama_init_from_model: freq_base     = 10000.0
0.00.492.173 I llama_init_from_model: freq_scale    = 1
0.00.492.174 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.492.176 I ggml_metal_init: allocating
0.00.492.216 I ggml_metal_init: found device: Apple M4
0.00.492.228 I ggml_metal_init: picking default device: Apple M4
0.00.493.751 I ggml_metal_init: using embedded metal library
0.00.499.683 I ggml_metal_init: GPU name:   Apple M4
0.00.499.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.499.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.499.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.499.688 I ggml_metal_init: simdgroup reduction   = true
0.00.499.689 I ggml_metal_init: simdgroup matrix mul. = true
0.00.499.689 I ggml_metal_init: has residency sets    = true
0.00.499.689 I ggml_metal_init: has bfloat            = true
0.00.499.689 I ggml_metal_init: use bfloat            = true
0.00.499.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.499.692 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.517.205 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.520.645 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.520.648 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.520.703 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.523.669 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.523.671 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.523.671 I llama_init_from_model: graph nodes  = 967
0.00.523.672 I llama_init_from_model: graph splits = 2
0.00.523.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.523.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.894 I 
0.00.552.961 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.552.979 I perplexity: tokenizing the input ..
0.00.558.902 I perplexity: tokenization took 5.92 ms
0.00.558.922 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.694.388 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.695.663 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.695.680 I llama_perf_context_print:        load time =     542.87 ms
0.00.695.681 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.39 tokens per second)
0.00.695.682 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.695.682 I llama_perf_context_print:       total time =     142.79 ms /   129 tokens
0.00.696.082 I ggml_metal_free: deallocating

real	0m0.710s
user	0m0.079s
sys	0m0.143s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.979 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.737 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.748 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.749 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.749 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.752 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.752 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.753 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.753 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.755 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.443 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.171 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.172 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.173 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.173 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.173 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.174 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.174 I llama_model_loader: - type  f32:  194 tensors
0.00.025.174 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.175 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.175 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.175 I print_info: file format = GGUF V3 (latest)
0.00.025.176 I print_info: file type   = Q4_K - Medium
0.00.025.176 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.813 I load: special tokens cache size = 25
0.00.038.792 I load: token to piece cache size = 0.2984 MB
0.00.038.795 I print_info: arch             = gptneox
0.00.038.795 I print_info: vocab_only       = 0
0.00.038.795 I print_info: n_ctx_train      = 2048
0.00.038.795 I print_info: n_embd           = 2048
0.00.038.796 I print_info: n_layer          = 24
0.00.038.798 I print_info: n_head           = 16
0.00.038.799 I print_info: n_head_kv        = 16
0.00.038.808 I print_info: n_rot            = 32
0.00.038.810 I print_info: n_swa            = 0
0.00.038.810 I print_info: n_embd_head_k    = 128
0.00.038.810 I print_info: n_embd_head_v    = 128
0.00.038.813 I print_info: n_gqa            = 1
0.00.038.814 I print_info: n_embd_k_gqa     = 2048
0.00.038.814 I print_info: n_embd_v_gqa     = 2048
0.00.038.815 I print_info: f_norm_eps       = 1.0e-05
0.00.038.816 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.816 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.817 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.817 I print_info: f_logit_scale    = 0.0e+00
0.00.038.818 I print_info: n_ff             = 8192
0.00.038.818 I print_info: n_expert         = 0
0.00.038.818 I print_info: n_expert_used    = 0
0.00.038.818 I print_info: causal attn      = 1
0.00.038.819 I print_info: pooling type     = 0
0.00.038.819 I print_info: rope type        = 2
0.00.038.819 I print_info: rope scaling     = linear
0.00.038.820 I print_info: freq_base_train  = 10000.0
0.00.038.820 I print_info: freq_scale_train = 1
0.00.038.820 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.820 I print_info: rope_finetuned   = unknown
0.00.038.821 I print_info: ssm_d_conv       = 0
0.00.038.821 I print_info: ssm_d_inner      = 0
0.00.038.821 I print_info: ssm_d_state      = 0
0.00.038.821 I print_info: ssm_dt_rank      = 0
0.00.038.821 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.821 I print_info: model type       = 1.4B
0.00.038.821 I print_info: model params     = 1.41 B
0.00.038.822 I print_info: general.name     = 1.4B
0.00.038.822 I print_info: vocab type       = BPE
0.00.038.822 I print_info: n_vocab          = 50304
0.00.038.822 I print_info: n_merges         = 50009
0.00.038.823 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.823 I print_info: LF token         = 187 ''
0.00.038.824 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.825 I print_info: max token length = 1024
0.00.038.826 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.578.526 I load_tensors: offloading 24 repeating layers to GPU
0.00.578.529 I load_tensors: offloading output layer to GPU
0.00.578.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.578.551 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.578.553 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.579.797 I llama_init_from_model: n_seq_max     = 1
0.00.579.800 I llama_init_from_model: n_ctx         = 2048
0.00.579.800 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.579.800 I llama_init_from_model: n_batch       = 2048
0.00.579.801 I llama_init_from_model: n_ubatch      = 512
0.00.579.801 I llama_init_from_model: flash_attn    = 0
0.00.579.802 I llama_init_from_model: freq_base     = 10000.0
0.00.579.803 I llama_init_from_model: freq_scale    = 1
0.00.579.804 I ggml_metal_init: allocating
0.00.579.835 I ggml_metal_init: found device: Apple M4
0.00.579.844 I ggml_metal_init: picking default device: Apple M4
0.00.581.190 I ggml_metal_init: using embedded metal library
0.00.586.862 I ggml_metal_init: GPU name:   Apple M4
0.00.586.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.865 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.867 I ggml_metal_init: simdgroup reduction   = true
0.00.586.867 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.867 I ggml_metal_init: has residency sets    = true
0.00.586.867 I ggml_metal_init: has bfloat            = true
0.00.586.868 I ggml_metal_init: use bfloat            = true
0.00.586.869 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.603.584 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.659.758 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.659.765 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.659.786 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.664.813 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.664.815 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.664.816 I llama_init_from_model: graph nodes  = 967
0.00.664.816 I llama_init_from_model: graph splits = 2
0.00.664.821 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.664.950 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.664.951 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.588 I main: llama threadpool init, n_threads = 4
0.00.715.632 I 
0.00.715.656 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.656 I 
0.00.715.786 I sampler seed: 1234
0.00.715.790 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.799 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.800 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.800 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.470.729 I llama_perf_sampler_print:    sampling time =       1.44 ms /    71 runs   (    0.02 ms per token, 49408.49 tokens per second)
0.01.470.729 I llama_perf_context_print:        load time =     705.86 ms
0.01.470.730 I llama_perf_context_print: prompt eval time =      50.59 ms /     7 tokens (    7.23 ms per token,   138.38 tokens per second)
0.01.470.731 I llama_perf_context_print:        eval time =     701.36 ms /    63 runs   (   11.13 ms per token,    89.83 tokens per second)
0.01.470.731 I llama_perf_context_print:       total time =     755.89 ms /    70 tokens
0.01.470.970 I ggml_metal_free: deallocating

real	0m1.489s
user	0m0.106s
sys	0m0.232s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.589 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.589 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.594 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.447 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.256 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.258 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.258 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.259 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.259 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.260 I llama_model_loader: - type  f32:  194 tensors
0.00.025.260 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.261 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.261 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.262 I print_info: file format = GGUF V3 (latest)
0.00.025.262 I print_info: file type   = Q4_K - Medium
0.00.025.263 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.385 I load: special tokens cache size = 25
0.00.039.550 I load: token to piece cache size = 0.2984 MB
0.00.039.554 I print_info: arch             = gptneox
0.00.039.554 I print_info: vocab_only       = 0
0.00.039.554 I print_info: n_ctx_train      = 2048
0.00.039.554 I print_info: n_embd           = 2048
0.00.039.554 I print_info: n_layer          = 24
0.00.039.559 I print_info: n_head           = 16
0.00.039.559 I print_info: n_head_kv        = 16
0.00.039.560 I print_info: n_rot            = 32
0.00.039.560 I print_info: n_swa            = 0
0.00.039.560 I print_info: n_embd_head_k    = 128
0.00.039.560 I print_info: n_embd_head_v    = 128
0.00.039.561 I print_info: n_gqa            = 1
0.00.039.563 I print_info: n_embd_k_gqa     = 2048
0.00.039.564 I print_info: n_embd_v_gqa     = 2048
0.00.039.565 I print_info: f_norm_eps       = 1.0e-05
0.00.039.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.565 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.565 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.566 I print_info: f_logit_scale    = 0.0e+00
0.00.039.568 I print_info: n_ff             = 8192
0.00.039.569 I print_info: n_expert         = 0
0.00.039.569 I print_info: n_expert_used    = 0
0.00.039.569 I print_info: causal attn      = 1
0.00.039.569 I print_info: pooling type     = 0
0.00.039.569 I print_info: rope type        = 2
0.00.039.575 I print_info: rope scaling     = linear
0.00.039.576 I print_info: freq_base_train  = 10000.0
0.00.039.576 I print_info: freq_scale_train = 1
0.00.039.577 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.577 I print_info: rope_finetuned   = unknown
0.00.039.577 I print_info: ssm_d_conv       = 0
0.00.039.577 I print_info: ssm_d_inner      = 0
0.00.039.577 I print_info: ssm_d_state      = 0
0.00.039.577 I print_info: ssm_dt_rank      = 0
0.00.039.578 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.578 I print_info: model type       = 1.4B
0.00.039.578 I print_info: model params     = 1.41 B
0.00.039.579 I print_info: general.name     = 1.4B
0.00.039.579 I print_info: vocab type       = BPE
0.00.039.580 I print_info: n_vocab          = 50304
0.00.039.580 I print_info: n_merges         = 50009
0.00.039.580 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.580 I print_info: LF token         = 187 ''
0.00.039.581 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.581 I print_info: max token length = 1024
0.00.039.581 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.577.435 I load_tensors: offloading 24 repeating layers to GPU
0.00.577.441 I load_tensors: offloading output layer to GPU
0.00.577.442 I load_tensors: offloaded 25/25 layers to GPU
0.00.577.464 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.577.466 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.578.700 I llama_init_from_model: n_seq_max     = 1
0.00.578.702 I llama_init_from_model: n_ctx         = 128
0.00.578.703 I llama_init_from_model: n_ctx_per_seq = 128
0.00.578.703 I llama_init_from_model: n_batch       = 128
0.00.578.704 I llama_init_from_model: n_ubatch      = 128
0.00.578.704 I llama_init_from_model: flash_attn    = 0
0.00.578.706 I llama_init_from_model: freq_base     = 10000.0
0.00.578.706 I llama_init_from_model: freq_scale    = 1
0.00.578.707 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.578.708 I ggml_metal_init: allocating
0.00.578.726 I ggml_metal_init: found device: Apple M4
0.00.578.736 I ggml_metal_init: picking default device: Apple M4
0.00.580.079 I ggml_metal_init: using embedded metal library
0.00.586.012 I ggml_metal_init: GPU name:   Apple M4
0.00.586.015 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.586.016 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.586.016 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.586.017 I ggml_metal_init: simdgroup reduction   = true
0.00.586.017 I ggml_metal_init: simdgroup matrix mul. = true
0.00.586.018 I ggml_metal_init: has residency sets    = true
0.00.586.018 I ggml_metal_init: has bfloat            = true
0.00.586.018 I ggml_metal_init: use bfloat            = true
0.00.586.019 I ggml_metal_init: hasUnifiedMemory      = true
0.00.586.021 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.602.567 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.913 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.605.919 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.605.970 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.827 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.608.829 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.608.829 I llama_init_from_model: graph nodes  = 967
0.00.608.829 I llama_init_from_model: graph splits = 2
0.00.608.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.608.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.798 I 
0.00.633.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.878 I perplexity: tokenizing the input ..
0.00.640.597 I perplexity: tokenization took 6.716 ms
0.00.640.628 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.867 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.776.133 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.776.147 I llama_perf_context_print:        load time =     624.06 ms
0.00.776.147 I llama_perf_context_print: prompt eval time =     133.81 ms /   128 tokens (    1.05 ms per token,   956.57 tokens per second)
0.00.776.148 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.149 I llama_perf_context_print:       total time =     142.35 ms /   129 tokens
0.00.776.502 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.077s
sys	0m0.164s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.010.103 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.587 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.589 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.589 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.590 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.590 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.591 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.591 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.592 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.592 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.592 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.593 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.593 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.595 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.596 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.223 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.224 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.224 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.225 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.225 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.225 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.226 I llama_model_loader: - type  f32:  194 tensors
0.00.026.226 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.226 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.226 I print_info: file format = GGUF V3 (latest)
0.00.026.227 I print_info: file type   = Q5_K - Medium
0.00.026.227 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.854 I load: special tokens cache size = 25
0.00.039.767 I load: token to piece cache size = 0.2984 MB
0.00.039.770 I print_info: arch             = gptneox
0.00.039.770 I print_info: vocab_only       = 0
0.00.039.770 I print_info: n_ctx_train      = 2048
0.00.039.771 I print_info: n_embd           = 2048
0.00.039.771 I print_info: n_layer          = 24
0.00.039.774 I print_info: n_head           = 16
0.00.039.774 I print_info: n_head_kv        = 16
0.00.039.774 I print_info: n_rot            = 32
0.00.039.775 I print_info: n_swa            = 0
0.00.039.776 I print_info: n_embd_head_k    = 128
0.00.039.776 I print_info: n_embd_head_v    = 128
0.00.039.777 I print_info: n_gqa            = 1
0.00.039.778 I print_info: n_embd_k_gqa     = 2048
0.00.039.778 I print_info: n_embd_v_gqa     = 2048
0.00.039.779 I print_info: f_norm_eps       = 1.0e-05
0.00.039.780 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.780 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.780 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.780 I print_info: f_logit_scale    = 0.0e+00
0.00.039.781 I print_info: n_ff             = 8192
0.00.039.781 I print_info: n_expert         = 0
0.00.039.781 I print_info: n_expert_used    = 0
0.00.039.781 I print_info: causal attn      = 1
0.00.039.781 I print_info: pooling type     = 0
0.00.039.781 I print_info: rope type        = 2
0.00.039.783 I print_info: rope scaling     = linear
0.00.039.784 I print_info: freq_base_train  = 10000.0
0.00.039.784 I print_info: freq_scale_train = 1
0.00.039.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.784 I print_info: rope_finetuned   = unknown
0.00.039.785 I print_info: ssm_d_conv       = 0
0.00.039.785 I print_info: ssm_d_inner      = 0
0.00.039.785 I print_info: ssm_d_state      = 0
0.00.039.785 I print_info: ssm_dt_rank      = 0
0.00.039.785 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.786 I print_info: model type       = 1.4B
0.00.039.786 I print_info: model params     = 1.41 B
0.00.039.786 I print_info: general.name     = 1.4B
0.00.039.787 I print_info: vocab type       = BPE
0.00.039.787 I print_info: n_vocab          = 50304
0.00.039.787 I print_info: n_merges         = 50009
0.00.039.787 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.787 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.788 I print_info: LF token         = 187 ''
0.00.039.788 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.790 I print_info: max token length = 1024
0.00.039.790 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.680.880 I load_tensors: offloading 24 repeating layers to GPU
0.00.680.883 I load_tensors: offloading output layer to GPU
0.00.680.884 I load_tensors: offloaded 25/25 layers to GPU
0.00.680.902 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.680.904 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.681.927 I llama_init_from_model: n_seq_max     = 1
0.00.681.929 I llama_init_from_model: n_ctx         = 2048
0.00.681.929 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.681.930 I llama_init_from_model: n_batch       = 2048
0.00.681.930 I llama_init_from_model: n_ubatch      = 512
0.00.681.930 I llama_init_from_model: flash_attn    = 0
0.00.681.931 I llama_init_from_model: freq_base     = 10000.0
0.00.681.931 I llama_init_from_model: freq_scale    = 1
0.00.681.932 I ggml_metal_init: allocating
0.00.681.952 I ggml_metal_init: found device: Apple M4
0.00.681.961 I ggml_metal_init: picking default device: Apple M4
0.00.683.204 I ggml_metal_init: using embedded metal library
0.00.689.047 I ggml_metal_init: GPU name:   Apple M4
0.00.689.050 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.052 I ggml_metal_init: simdgroup reduction   = true
0.00.689.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.053 I ggml_metal_init: has residency sets    = true
0.00.689.053 I ggml_metal_init: has bfloat            = true
0.00.689.053 I ggml_metal_init: use bfloat            = true
0.00.689.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.705.054 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.757.527 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.757.533 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.757.554 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.762.262 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.762.264 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.762.264 I llama_init_from_model: graph nodes  = 967
0.00.762.264 I llama_init_from_model: graph splits = 2
0.00.762.269 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.762.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.762.404 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.337 I main: llama threadpool init, n_threads = 4
0.00.817.379 I 
0.00.817.400 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.400 I 
0.00.817.522 I sampler seed: 1234
0.00.817.527 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.817.536 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.817.536 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.817.536 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.658.656 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53263.32 tokens per second)
0.01.658.657 I llama_perf_context_print:        load time =     806.54 ms
0.01.658.659 I llama_perf_context_print: prompt eval time =      51.22 ms /     7 tokens (    7.32 ms per token,   136.67 tokens per second)
0.01.658.660 I llama_perf_context_print:        eval time =     786.91 ms /    63 runs   (   12.49 ms per token,    80.06 tokens per second)
0.01.658.660 I llama_perf_context_print:       total time =     842.02 ms /    70 tokens
0.01.658.883 I ggml_metal_free: deallocating

real	0m1.675s
user	0m0.105s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.696 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.518 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.525 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.526 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.531 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.532 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.532 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.532 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.533 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.534 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.534 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.534 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.535 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.537 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.537 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.539 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.539 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.363 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.157 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.158 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.159 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.160 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.160 I llama_model_loader: - type  f32:  194 tensors
0.00.025.161 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.161 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.162 I print_info: file format = GGUF V3 (latest)
0.00.025.162 I print_info: file type   = Q5_K - Medium
0.00.025.163 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.400 I load: special tokens cache size = 25
0.00.039.489 I load: token to piece cache size = 0.2984 MB
0.00.039.496 I print_info: arch             = gptneox
0.00.039.496 I print_info: vocab_only       = 0
0.00.039.496 I print_info: n_ctx_train      = 2048
0.00.039.496 I print_info: n_embd           = 2048
0.00.039.497 I print_info: n_layer          = 24
0.00.039.501 I print_info: n_head           = 16
0.00.039.503 I print_info: n_head_kv        = 16
0.00.039.503 I print_info: n_rot            = 32
0.00.039.504 I print_info: n_swa            = 0
0.00.039.504 I print_info: n_embd_head_k    = 128
0.00.039.504 I print_info: n_embd_head_v    = 128
0.00.039.504 I print_info: n_gqa            = 1
0.00.039.505 I print_info: n_embd_k_gqa     = 2048
0.00.039.506 I print_info: n_embd_v_gqa     = 2048
0.00.039.506 I print_info: f_norm_eps       = 1.0e-05
0.00.039.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.506 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.507 I print_info: f_logit_scale    = 0.0e+00
0.00.039.507 I print_info: n_ff             = 8192
0.00.039.508 I print_info: n_expert         = 0
0.00.039.508 I print_info: n_expert_used    = 0
0.00.039.508 I print_info: causal attn      = 1
0.00.039.508 I print_info: pooling type     = 0
0.00.039.510 I print_info: rope type        = 2
0.00.039.510 I print_info: rope scaling     = linear
0.00.039.511 I print_info: freq_base_train  = 10000.0
0.00.039.511 I print_info: freq_scale_train = 1
0.00.039.511 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.512 I print_info: rope_finetuned   = unknown
0.00.039.512 I print_info: ssm_d_conv       = 0
0.00.039.512 I print_info: ssm_d_inner      = 0
0.00.039.512 I print_info: ssm_d_state      = 0
0.00.039.512 I print_info: ssm_dt_rank      = 0
0.00.039.512 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.514 I print_info: model type       = 1.4B
0.00.039.514 I print_info: model params     = 1.41 B
0.00.039.514 I print_info: general.name     = 1.4B
0.00.039.515 I print_info: vocab type       = BPE
0.00.039.515 I print_info: n_vocab          = 50304
0.00.039.515 I print_info: n_merges         = 50009
0.00.039.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.516 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: LF token         = 187 ''
0.00.039.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.518 I print_info: max token length = 1024
0.00.039.518 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.676.151 I load_tensors: offloading 24 repeating layers to GPU
0.00.676.157 I load_tensors: offloading output layer to GPU
0.00.676.158 I load_tensors: offloaded 25/25 layers to GPU
0.00.676.182 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.676.185 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.677.248 I llama_init_from_model: n_seq_max     = 1
0.00.677.250 I llama_init_from_model: n_ctx         = 128
0.00.677.251 I llama_init_from_model: n_ctx_per_seq = 128
0.00.677.251 I llama_init_from_model: n_batch       = 128
0.00.677.251 I llama_init_from_model: n_ubatch      = 128
0.00.677.252 I llama_init_from_model: flash_attn    = 0
0.00.677.252 I llama_init_from_model: freq_base     = 10000.0
0.00.677.253 I llama_init_from_model: freq_scale    = 1
0.00.677.254 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.677.255 I ggml_metal_init: allocating
0.00.677.288 I ggml_metal_init: found device: Apple M4
0.00.677.298 I ggml_metal_init: picking default device: Apple M4
0.00.678.509 I ggml_metal_init: using embedded metal library
0.00.683.956 I ggml_metal_init: GPU name:   Apple M4
0.00.683.960 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.683.960 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.683.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.683.961 I ggml_metal_init: simdgroup reduction   = true
0.00.683.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.683.962 I ggml_metal_init: has residency sets    = true
0.00.683.962 I ggml_metal_init: has bfloat            = true
0.00.683.962 I ggml_metal_init: use bfloat            = true
0.00.683.963 I ggml_metal_init: hasUnifiedMemory      = true
0.00.683.965 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.699.078 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.702.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.702.369 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.702.403 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.705.451 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.705.452 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.705.453 I llama_init_from_model: graph nodes  = 967
0.00.705.453 I llama_init_from_model: graph splits = 2
0.00.705.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.705.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.166 I 
0.00.740.239 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.740.258 I perplexity: tokenizing the input ..
0.00.747.169 I perplexity: tokenization took 6.907 ms
0.00.747.191 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.600 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.900.920 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.900.935 I llama_perf_context_print:        load time =     730.46 ms
0.00.900.936 I llama_perf_context_print: prompt eval time =     151.49 ms /   128 tokens (    1.18 ms per token,   844.96 tokens per second)
0.00.900.937 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.937 I llama_perf_context_print:       total time =     160.77 ms /   129 tokens
0.00.901.349 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.077s
sys	0m0.200s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.004 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.481 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.483 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.488 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.488 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.492 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.492 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.492 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.493 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.493 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.493 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.494 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.495 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.496 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.496 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.199 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.176 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.869 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.870 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.871 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.872 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.872 I llama_model_loader: - type  f32:  194 tensors
0.00.024.872 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.873 I print_info: file format = GGUF V3 (latest)
0.00.024.873 I print_info: file type   = Q6_K
0.00.024.874 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.476 I load: special tokens cache size = 25
0.00.038.583 I load: token to piece cache size = 0.2984 MB
0.00.038.586 I print_info: arch             = gptneox
0.00.038.587 I print_info: vocab_only       = 0
0.00.038.587 I print_info: n_ctx_train      = 2048
0.00.038.587 I print_info: n_embd           = 2048
0.00.038.587 I print_info: n_layer          = 24
0.00.038.590 I print_info: n_head           = 16
0.00.038.591 I print_info: n_head_kv        = 16
0.00.038.591 I print_info: n_rot            = 32
0.00.038.591 I print_info: n_swa            = 0
0.00.038.591 I print_info: n_embd_head_k    = 128
0.00.038.591 I print_info: n_embd_head_v    = 128
0.00.038.592 I print_info: n_gqa            = 1
0.00.038.593 I print_info: n_embd_k_gqa     = 2048
0.00.038.594 I print_info: n_embd_v_gqa     = 2048
0.00.038.594 I print_info: f_norm_eps       = 1.0e-05
0.00.038.595 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.595 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.595 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.595 I print_info: f_logit_scale    = 0.0e+00
0.00.038.596 I print_info: n_ff             = 8192
0.00.038.596 I print_info: n_expert         = 0
0.00.038.596 I print_info: n_expert_used    = 0
0.00.038.596 I print_info: causal attn      = 1
0.00.038.596 I print_info: pooling type     = 0
0.00.038.597 I print_info: rope type        = 2
0.00.038.597 I print_info: rope scaling     = linear
0.00.038.597 I print_info: freq_base_train  = 10000.0
0.00.038.598 I print_info: freq_scale_train = 1
0.00.038.598 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.598 I print_info: rope_finetuned   = unknown
0.00.038.598 I print_info: ssm_d_conv       = 0
0.00.038.598 I print_info: ssm_d_inner      = 0
0.00.038.598 I print_info: ssm_d_state      = 0
0.00.038.599 I print_info: ssm_dt_rank      = 0
0.00.038.599 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.599 I print_info: model type       = 1.4B
0.00.038.599 I print_info: model params     = 1.41 B
0.00.038.599 I print_info: general.name     = 1.4B
0.00.038.600 I print_info: vocab type       = BPE
0.00.038.600 I print_info: n_vocab          = 50304
0.00.038.600 I print_info: n_merges         = 50009
0.00.038.602 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.602 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.602 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.602 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.602 I print_info: LF token         = 187 ''
0.00.038.603 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.603 I print_info: max token length = 1024
0.00.038.603 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.747.838 I load_tensors: offloading 24 repeating layers to GPU
0.00.747.842 I load_tensors: offloading output layer to GPU
0.00.747.843 I load_tensors: offloaded 25/25 layers to GPU
0.00.747.864 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.747.867 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.748.861 I llama_init_from_model: n_seq_max     = 1
0.00.748.862 I llama_init_from_model: n_ctx         = 2048
0.00.748.863 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.748.863 I llama_init_from_model: n_batch       = 2048
0.00.748.863 I llama_init_from_model: n_ubatch      = 512
0.00.748.863 I llama_init_from_model: flash_attn    = 0
0.00.748.864 I llama_init_from_model: freq_base     = 10000.0
0.00.748.865 I llama_init_from_model: freq_scale    = 1
0.00.748.866 I ggml_metal_init: allocating
0.00.748.879 I ggml_metal_init: found device: Apple M4
0.00.748.885 I ggml_metal_init: picking default device: Apple M4
0.00.750.133 I ggml_metal_init: using embedded metal library
0.00.755.231 I ggml_metal_init: GPU name:   Apple M4
0.00.755.234 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.755.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.755.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.755.237 I ggml_metal_init: simdgroup reduction   = true
0.00.755.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.755.238 I ggml_metal_init: has residency sets    = true
0.00.755.238 I ggml_metal_init: has bfloat            = true
0.00.755.238 I ggml_metal_init: use bfloat            = true
0.00.755.239 I ggml_metal_init: hasUnifiedMemory      = true
0.00.755.240 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.770.114 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.821.595 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.821.601 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.821.672 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.827.014 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.827.016 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.827.016 I llama_init_from_model: graph nodes  = 967
0.00.827.017 I llama_init_from_model: graph splits = 2
0.00.827.021 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.827.146 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.827.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.883.118 I main: llama threadpool init, n_threads = 4
0.00.883.164 I 
0.00.883.199 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.883.199 I 
0.00.883.342 I sampler seed: 1234
0.00.883.346 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.883.377 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.883.378 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.883.378 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.748.778 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53463.86 tokens per second)
0.01.748.779 I llama_perf_context_print:        load time =     873.10 ms
0.01.748.780 I llama_perf_context_print: prompt eval time =      54.07 ms /     7 tokens (    7.72 ms per token,   129.46 tokens per second)
0.01.748.780 I llama_perf_context_print:        eval time =     808.30 ms /    63 runs   (   12.83 ms per token,    77.94 tokens per second)
0.01.748.781 I llama_perf_context_print:       total time =     866.67 ms /    70 tokens
0.01.749.035 I ggml_metal_free: deallocating

real	0m1.765s
user	0m0.104s
sys	0m0.281s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4691 (369be559) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.922 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.865 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.871 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.871 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.879 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.880 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.882 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.882 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.681 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.641 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.439 I llama_model_loader: - type  f32:  194 tensors
0.00.025.439 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.440 I print_info: file format = GGUF V3 (latest)
0.00.025.440 I print_info: file type   = Q6_K
0.00.025.441 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.975 I load: special tokens cache size = 25
0.00.038.849 I load: token to piece cache size = 0.2984 MB
0.00.038.852 I print_info: arch             = gptneox
0.00.038.852 I print_info: vocab_only       = 0
0.00.038.852 I print_info: n_ctx_train      = 2048
0.00.038.852 I print_info: n_embd           = 2048
0.00.038.852 I print_info: n_layer          = 24
0.00.038.856 I print_info: n_head           = 16
0.00.038.856 I print_info: n_head_kv        = 16
0.00.038.857 I print_info: n_rot            = 32
0.00.038.857 I print_info: n_swa            = 0
0.00.038.857 I print_info: n_embd_head_k    = 128
0.00.038.857 I print_info: n_embd_head_v    = 128
0.00.038.858 I print_info: n_gqa            = 1
0.00.038.859 I print_info: n_embd_k_gqa     = 2048
0.00.038.859 I print_info: n_embd_v_gqa     = 2048
0.00.038.860 I print_info: f_norm_eps       = 1.0e-05
0.00.038.860 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.860 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.861 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.861 I print_info: f_logit_scale    = 0.0e+00
0.00.038.862 I print_info: n_ff             = 8192
0.00.038.866 I print_info: n_expert         = 0
0.00.038.867 I print_info: n_expert_used    = 0
0.00.038.867 I print_info: causal attn      = 1
0.00.038.867 I print_info: pooling type     = 0
0.00.038.867 I print_info: rope type        = 2
0.00.038.867 I print_info: rope scaling     = linear
0.00.038.868 I print_info: freq_base_train  = 10000.0
0.00.038.868 I print_info: freq_scale_train = 1
0.00.038.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.869 I print_info: rope_finetuned   = unknown
0.00.038.869 I print_info: ssm_d_conv       = 0
0.00.038.869 I print_info: ssm_d_inner      = 0
0.00.038.869 I print_info: ssm_d_state      = 0
0.00.038.869 I print_info: ssm_dt_rank      = 0
0.00.038.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.870 I print_info: model type       = 1.4B
0.00.038.870 I print_info: model params     = 1.41 B
0.00.038.870 I print_info: general.name     = 1.4B
0.00.038.871 I print_info: vocab type       = BPE
0.00.038.871 I print_info: n_vocab          = 50304
0.00.038.871 I print_info: n_merges         = 50009
0.00.038.873 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.873 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: LF token         = 187 ''
0.00.038.874 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.874 I print_info: max token length = 1024
0.00.038.875 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.692.314 I load_tensors: offloading 24 repeating layers to GPU
0.00.692.320 I load_tensors: offloading output layer to GPU
0.00.692.322 I load_tensors: offloaded 25/25 layers to GPU
0.00.692.347 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.692.349 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.693.317 I llama_init_from_model: n_seq_max     = 1
0.00.693.318 I llama_init_from_model: n_ctx         = 128
0.00.693.319 I llama_init_from_model: n_ctx_per_seq = 128
0.00.693.319 I llama_init_from_model: n_batch       = 128
0.00.693.320 I llama_init_from_model: n_ubatch      = 128
0.00.693.320 I llama_init_from_model: flash_attn    = 0
0.00.693.321 I llama_init_from_model: freq_base     = 10000.0
0.00.693.321 I llama_init_from_model: freq_scale    = 1
0.00.693.322 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.693.323 I ggml_metal_init: allocating
0.00.693.339 I ggml_metal_init: found device: Apple M4
0.00.693.346 I ggml_metal_init: picking default device: Apple M4
0.00.694.576 I ggml_metal_init: using embedded metal library
0.00.699.932 I ggml_metal_init: GPU name:   Apple M4
0.00.699.935 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.936 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.936 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.938 I ggml_metal_init: simdgroup reduction   = true
0.00.699.939 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.939 I ggml_metal_init: has residency sets    = true
0.00.699.939 I ggml_metal_init: has bfloat            = true
0.00.699.939 I ggml_metal_init: use bfloat            = true
0.00.699.940 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.944 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.714.085 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.716.282 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.716.285 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.716.305 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.718.445 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.718.447 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.718.447 I llama_init_from_model: graph nodes  = 967
0.00.718.447 I llama_init_from_model: graph splits = 2
0.00.718.449 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.718.449 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.343 I 
0.00.747.375 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.747.384 I perplexity: tokenizing the input ..
0.00.753.065 I perplexity: tokenization took 5.68 ms
0.00.753.078 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.892.259 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.893.526 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.893.541 I llama_perf_context_print:        load time =     737.42 ms
0.00.893.542 I llama_perf_context_print: prompt eval time =     138.96 ms /   128 tokens (    1.09 ms per token,   921.15 tokens per second)
0.00.893.543 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.893.543 I llama_perf_context_print:       total time =     146.20 ms /   129 tokens
0.00.893.963 I ggml_metal_free: deallocating

real	0m0.908s
user	0m0.072s
sys	0m0.195s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4691 (369be559)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a2084a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a208bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a209160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a209710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a209cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a20a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a20a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a20add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a20b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a20b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a20bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a20c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a20cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a20d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a20dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a20e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a20eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a20f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a20f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a2101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a2108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a210ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a211710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a211fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a2126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a212990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a212fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a213c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a214150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a214410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a2148b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a214b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a215400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a215940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a215c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a2160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a216540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a2169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a216e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a217320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a2177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a217c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a218100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a2185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a218860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a218e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a219480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a219da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a21a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a21a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a21afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a21b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a21bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a21c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a21c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a21ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a21d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a21d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a21dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a21e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a21e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a21eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a21eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a21f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a21f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a21fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a220270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a220710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a220bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a221050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a2214f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a221990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a221e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a222380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a2228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a222e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a223370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a2238c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a223e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a224360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a2248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a224e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a225350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a2258a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a225df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a226340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a226890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a226de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a227330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a227880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a227dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a228320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a228870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a228dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a229310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a229860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a229db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a219a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a22a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a22a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a22af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a22b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a22b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a22bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a22c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a22c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a22cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a22d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a22d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a22def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a22e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a22e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a22eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a22f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a22f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a22fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a230160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a230600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a230aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a230f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a2313e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a231880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a231d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a2321c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a232660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a232b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a232fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a233440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a2338e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a233d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a234220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a2346c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a234b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a235000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a2354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a235940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a235de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a236280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a236720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a236bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a237060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a237500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a2379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a237e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a2382e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a238780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a238c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a2390c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a239560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a239a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a239ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a23a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a23a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a23ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a23b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a23b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a23ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a23bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a23c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a23c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a23cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a23d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a23d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a23dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a23df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a23e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a23e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a23ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a23f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a23f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a23fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a23ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a240460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a240900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a240da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a241240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a2416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a241b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a242020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a2424c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a242960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a242e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a2432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a243740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a243be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a244080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a244520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a2449c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a244e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a245300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a2457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a245c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a2460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a246630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a246b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a2470d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a247620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a2478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a247ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a248500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a248b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a249300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a2497a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a249a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a24a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a24a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a24ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a24b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a24b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a24bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a24c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a24c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a24cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a24d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a24d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a24de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a24e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a24e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a24ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a24f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a24f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a24fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a2503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a250910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a250e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a2513b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a251900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a251e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a2523a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a2528f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a252e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a253390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a2538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a253e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a254380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a2548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a254e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a255370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a2558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a255e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a256360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a2568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a256e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a257350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a2578a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a257df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a258340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a258890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a258de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a259330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a259880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a259dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a25a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a25a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a25adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a25b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a25b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a25bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a25c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a25c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a25cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a25d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a25d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a25dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a25e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a25e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a25ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a25f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a25f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a25fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a260000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a2604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a260940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a260de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a261280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a261720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a261bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a262060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a262500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a2629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a262e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a2632e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a263830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a263f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a264670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a264d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a2654b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a265770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a265f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a266220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a266830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.752.716 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.752.721 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12e304dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12e305240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12e3056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12e305b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12e305f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12e306400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12e306870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12e306ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12e307150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12e3075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12e307a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12e308120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12e308c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12e3093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12e309c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12e30a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12e30aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12e30b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12e30b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12e30bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12e30c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12e30cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12e30d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12e30dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12e30e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12e30e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12e30e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12e30ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12e30f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12e30f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12e30fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12e30ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12e310430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12e3106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12e310b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12e310fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12e311440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12e3118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12e311d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12e312190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12e312600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12e312a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12e312ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12e313350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12e3137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12e313c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12e3140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12e314510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12e314980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12e314df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12e315260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12e3156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12e315b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12e315fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12e316420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12e316890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12e316e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12e317300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12e317770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12e317be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12e318050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12e3184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12e318930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12e318da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12e319210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12e319680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12e319af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12e319f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12e31a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12e31a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12e31acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12e31b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12e31b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12e31ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12e31be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12e31c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12e31c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12e31cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12e31d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12e31d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12e31d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12e31dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12e31e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12e31e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12e31ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12e31ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12e31f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12e31f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12e31fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12e320100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12e320570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12e3209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12e320e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12e3212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12e321730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12e321ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12e322010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12e322480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12e3228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12e322d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12e3231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12e323640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12e323ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12e323f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12e324390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12e324800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12e324c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12e3250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12e325550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12e3259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12e325e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12e3262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12e326710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12e326b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12e326ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12e327460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12e3278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12e327d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12e3281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12e328620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12e328a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12e328f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12e329370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12e3297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12e329c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12e32a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12e32a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12e32a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12e32ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12e32b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12e32b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12e32bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12e32bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12e32c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12e32c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12e32cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12e32d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12e32d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12e32da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12e32dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12e32e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12e32e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12e32ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12e32f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12e32f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12e32f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12e32fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12e330260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12e3306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12e330b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12e330fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12e331420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12e331890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12e331d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12e332170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12e3325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12e332a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12e332ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12e333330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12e3337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12e333c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12e334080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12e3344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12e334960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12e334dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12e335240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12e335e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12e336130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12e3363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12e336860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12e336cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12e337140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12e3375b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12e337a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12e337e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12e338300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12e338770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12e338be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12e339050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12e3394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12e339930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12e339da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12e33a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12e33a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12e33aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12e33af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12e33b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12e33b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12e33bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12e33c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12e33c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12e33ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12e33ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12e33d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12e33d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12e33dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12e33e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12e33e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12e33e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12e33ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12e33f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12e33f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12e33fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12e3400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12e340540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12e3409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12e340e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12e341290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12e3417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12e341cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12e342830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12e342af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12e3430b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12e343670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12e343c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12e3441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12e3447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12e344d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12e345330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12e3458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12e345eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12e346470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12e346a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12e346ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12e3475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12e347b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12e348130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12e3486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12e348cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12e349270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12e349830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12e349df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12e34a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12e34a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12e34af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12e34b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12e34bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12e34c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12e34c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12e34cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12e34d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12e34d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12e34dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12e34e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12e34e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12e34ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12e34f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12e34f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12e34ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12e350570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12e350b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12e3510f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12e3516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12e351c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12e352230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12e3527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12e352db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12e353370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12e353930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12e353ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12e3544b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12e354a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12e355030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12e3555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12e355bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12e356170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12e356730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12e356cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12e3571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12e3576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12e357bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12e3580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12e3585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12e358af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12e358ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12e3594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12e3599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12e359ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12e35a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12e35a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12e35adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12e35b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12e35b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12e35c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12e35c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12e35d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12e35d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12e35da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12e35e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12e35e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12e35eae0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a2664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a2481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a247ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a2487c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a21b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a21b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a21d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a24a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a212c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a219740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a21a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a21a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a218b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a21ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a211c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a21dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a22a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a265a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a214e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a2150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a24a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a248dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a213260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a213520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a2137e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a266c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a266f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a267210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a2674d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a267790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a267a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a267d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a267fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a268290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a268550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a268810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a268ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a268d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a269050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a269310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a2695d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a269890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a269b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a269e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a26a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a26a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a26a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a26a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a26abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a26ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a26b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a26b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a26b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a26b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a26bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a26bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a26c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a26c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a26c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a26ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a26ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a26cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a26d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a26d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a26d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a26da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a26dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a26e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a26e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a26e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a26e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a26eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a26edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a26f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a26f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a26f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a26f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a26fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a26fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a270110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a2703d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a270690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a270950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a270c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a270ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a271190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a271450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a271710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a2719d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a271c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a271f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a272210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a2724d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a272790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a272a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a272d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a272fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a273290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a273550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a273810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a273ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a273d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a274050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a274310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a2745d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a274890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a274b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a274e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a2750d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a275390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a275650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a275910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a275bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a275e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a276150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a276410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a2766d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a276990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a276c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a276f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a2771d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a277490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a277750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a277a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a277cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a277f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a278250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a278510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a2787d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a278a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a278d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a279010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a2792d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a279590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a279850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a279b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a279dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a27a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a27a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a27a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a27a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a27ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a27ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a27b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a27b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a27b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a27b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a27bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a27bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a27c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a27c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a27c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a27c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a27cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a27cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a27d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a27d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a27d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a27da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a27dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a27dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a27e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a27e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a27e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a27ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a27ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a27f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a27f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a27f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a27f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a27fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a27fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a2800d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a280390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a280650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a280910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a280bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a280e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a281150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a281410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a2816d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a281990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a281c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a281f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a2821d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a282490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a282750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a282a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a282cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a282f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a283250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a283510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a2837d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a283a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a283d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a284010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a2842d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a284590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a284850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a284b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a284dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a285090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a285350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a285610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a2858d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a285b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a285e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a286110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a286510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a2869b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a287160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a287420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a2876e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a287b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a287fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a288430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a2888a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a288d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a289180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a2895f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a289a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a289ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a28a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a28a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a28ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a28b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a28b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a28b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a28bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a28c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a28c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a28cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a28cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a28d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a28d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a28dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a28e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a28e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a28ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a28eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a28f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a28f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a28fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a290070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a2904e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a290950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a290dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a291230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a2916a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a291b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a291f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a2923f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a292860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a292cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a293140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a2935b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a293a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a293e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a294300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a294770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a294be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a295050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a2954c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a295930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a295da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a296210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a296680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a296af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a296f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a2973d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a297840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a297cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a298120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a298590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a298a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a298e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a2992e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a299750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a299bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a29a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a29a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a29a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a29ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a29b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a29bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a29c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a29cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a29d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a29d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a29dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a29e0d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.786s
user	0m0.277s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4691 (369be559)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138e0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138e0ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138e0f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138e0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138e0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138e10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138e10820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138e10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138e11380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138e11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138e11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138e12280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138e12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138e13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138e13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138e14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138e14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138e152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138e159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138e161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138e168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138e16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138e17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138e17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138e186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138e18990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138e18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138e19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138e1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138e1a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138e1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138e1ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138e1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138e1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138e1bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138e1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138e1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138e1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138e1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138e1d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138e1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138e1dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138e1e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138e1e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138e1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138e1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138e1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138e1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138e209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138e20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138e215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138e21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138e22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138e22e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138e235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138e23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138e243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138e246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138e24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138e24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138e25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138e25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138e25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138e26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138e26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138e27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138e274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138e27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138e28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138e28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138e29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138e298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138e29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138e2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138e2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138e2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138e2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138e2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138e2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138e2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138e2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138e2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138e2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138e2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138e2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138e2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138e1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138e30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138e309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138e30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138e31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138e319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138e31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138e32460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138e329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138e32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138e33450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138e339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138e33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138e34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138e34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138e35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138e35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138e35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138e36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138e36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138e36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138e36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138e373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138e37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138e37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138e381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138e38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138e38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138e38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138e39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138e398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138e39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138e3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138e3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138e3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138e3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138e3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138e3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138e3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138e3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138e3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138e3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138e3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138e3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138e3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138e3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138e3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138e3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138e3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138e3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138e3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138e3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138e40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138e407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138e40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138e41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138e415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138e41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138e41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138e423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138e42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138e42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138e43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138e43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138e43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138e43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138e44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138e448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138e44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138e451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138e45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138e45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138e45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138e46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138e46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138e46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138e47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138e476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138e47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138e48020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138e48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138e48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138e492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138e49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138e49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138e4a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138e4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138e4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138e4ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138e4b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138e4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138e4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138e4c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138e4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138e4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138e4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138e4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138e4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138e4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138e4e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138e4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138e4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138e4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138e4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138e50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138e50680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138e50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138e51310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138e517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138e51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138e52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138e52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138e533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138e53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138e53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138e543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138e54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138e54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138e55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138e563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138e56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138e57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138e57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138e583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138e588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138e58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138e59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138e598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138e59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138e5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138e5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138e5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138e5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138e5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138e5be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138e5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138e5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138e5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138e5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138e5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138e5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138e5e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138e5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138e5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138e5f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138e5f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138e5fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138e60320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138e60870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138e60dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138e61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138e61860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138e61db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138e62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138e62850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138e62da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138e632f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138e63840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138e63d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138e642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138e64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138e64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138e65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138e656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138e65b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138e66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138e664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138e66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138e66de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138e67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138e67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138e67bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138e68060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138e68500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138e689a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138e68e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138e692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138e69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138e69f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138e6a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138e6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138e6b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138e6b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138e6bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138e6c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138e6c830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.095.498 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.501 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138e6c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138e4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138e4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138e4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138e218a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138e21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138e238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138e50330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138e18c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138e1f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138e20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138e20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138e1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138e20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138e17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138e0dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138e224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138e23ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138e304e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138e6ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138e1ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138e1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138e50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x138e4edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x138e19260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x138e19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x138e197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x138e6cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x138e6cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x138e6d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x138e6d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x138e6d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x138e6da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x138e6dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x138e6dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x138e6e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x138e6e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x138e6e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x138e6ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x138e6ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x138e6f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x138e6f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x138e6f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x138e6f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x138e6fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x138e6fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x138e700d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x138e70390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x138e70650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x138e70910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x138e70bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x138e70e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x138e71150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x138e71410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x138e716d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x138e71990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x138e71c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x138e71f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x138e721d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x138e72490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x138e72750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x138e72a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x138e72cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x138e72f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x138e73250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x138e73510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x138e737d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x138e73a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x138e73d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x138e74010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x138e742d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x138e74590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x138e74850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x138e74b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x138e74dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x138e75090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x138e75350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x138e75610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x138e758d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x138e75b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x138e75e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x138e76110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x138e763d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x138e76690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x138e76950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x138e76c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x138e76ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x138e77190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x138e77450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x138e77710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x138e779d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x138e77c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x138e77f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x138e78210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x138e784d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x138e78790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x138e78a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x138e78d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x138e78fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x138e79290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x138e79550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x138e79810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x138e79ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x138e79d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x138e7a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x138e7a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x138e7a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x138e7a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x138e7ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x138e7ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x138e7b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x138e7b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x138e7b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x138e7b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x138e7bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x138e7be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x138e7c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x138e7c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x138e7c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x138e7c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x138e7cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x138e7cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x138e7d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x138e7d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x138e7d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x138e7da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x138e7dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x138e7df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x138e7e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x138e7e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x138e7e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x138e7ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x138e7ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x138e7f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x138e7f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x138e7f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x138e7f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x138e7fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x138e7fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x138e80090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x138e80350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x138e80610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x138e808d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x138e80b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x138e80e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x138e81110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x138e813d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x138e81690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x138e81950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x138e81c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x138e81ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x138e82190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x138e82450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x138e82710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x138e829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x138e82c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x138e82f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x138e83210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x138e834d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x138e83790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x138e83a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x138e83d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x138e83fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x138e84290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x138e84550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x138e84810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x138e84ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x138e84d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x138e85050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x138e85310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x138e855d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x138e85890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x138e85b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x138e85e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x138e860d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x138e86390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x138e86650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x138e86910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x138e86bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x138e86e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x138e87150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x138e87410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x138e876d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x138e87990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x138e87c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x138e87f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x138e881d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x138e88490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x138e88750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x138e88a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x138e88cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x138e88f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x138e89250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x138e89510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x138e897d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x138e89a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x138e89d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x138e8a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x138e8a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x138e8a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x138e8a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x138e8ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x138e8add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x138e8b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x138e8b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x138e8b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x138e8b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x138e8bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x138e8be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x138e8c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x138e8c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x138e8c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x138e8cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x138e8cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x138e8d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x138e8d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x138e8d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x138e8da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x138e8dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x138e8dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x138e8e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x138e8e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x138e8e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x138e8eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x138e8ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x138e8f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x138e8f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x138e8f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x138e8f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x138e8fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x138e8fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x138e900a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x138e90360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x138e90620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x138e908e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x138e90ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x138e90e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x138e91120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x138e913e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x138e916a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x138e91960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x138e91c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x138e91ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x138e92430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x138e92980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x138e92ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x138e93420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x138e93970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x138e93ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x138e94410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x138e94960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x138e94eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x138e95400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x138e95950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x138e95ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x138e963f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x138e96940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x138e96e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x138e973e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x138e97930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x138e97e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x138e983d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x138e98920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x138e98e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x138e993c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x138e99910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x138e99e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x138e9a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x138e9a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x138e9a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x138e9ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x138e9b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x138e9b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x138e9bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x138e9c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x138e9c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x138e9cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x138e9d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x138e9d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x138e9dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x138e9dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x138e9e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x138e9e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x138e9f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x138e9fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x138ea0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x138ea0950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x138ea0c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x138ea1400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x138ea16c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x138ea1cd0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a00a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a00a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a00af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a00b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a00be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a00c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a00cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a00dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a00dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a00e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a00e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a00edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a00f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a0133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a0149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a0177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a0189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a0196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a01a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a01a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a01b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a01bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a01c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a01d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a01ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a01e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a01e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a01eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a01efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a01f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a01f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a01fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a0217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a0224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a0253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a0269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a0272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a0291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a02a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a02a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a02ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a02b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a02b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a02b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a02be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a02c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a02c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a02cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a02d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a02d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a02d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a02dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a02e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a02e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a02eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a02ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a02f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a02f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a02fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a0300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a0309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a0328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a0331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a0347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a0350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a035990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a0366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a0378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a0385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a0397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a03a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a03a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a03ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a03b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a03b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a03bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a03bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a03c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a03c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a03ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a03d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a03da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a03e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a03e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a03ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a03f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a03f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a03f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a03fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a0406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a040f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a041dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a042080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a0424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a0436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a0493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a04a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a04aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a04b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a04b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a04bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a04c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a04c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a04c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a04cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a04d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a04d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a04df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a04e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a04e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a04eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a04f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a04f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a04fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a0533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a0540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a0549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a0552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a0568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a0576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a0579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a058420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a058a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.958s
user	0m0.229s
sys	0m0.151s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.42 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.86 sec*proc (2 tests)

Total Test time (real) =   1.87 sec
        1.90 real         0.51 user         0.28 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.13 user         0.08 sys
```
