Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.679s
user	0m0.910s
sys	0m1.283s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Built target build_info
[  3%] Built target sha1
[  3%] Built target sha256
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  4%] Built target xxhash
[  4%] Linking CXX shared library ../../bin/libggml-base.dylib
[  4%] Built target ggml-base
[  4%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  5%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[  9%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 10%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 11%] Built target ggml-blas
[ 11%] Built target ggml-cpu
[ 12%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 12%] Built target ggml-metal
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library ../../bin/libggml.dylib
[ 13%] Built target ggml
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Linking CXX shared library ../bin/libllama.dylib
[ 24%] Built target llama-gguf-hash
[ 24%] Built target llama-gguf
[ 24%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 31%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-simple
[ 35%] Built target llama-simple-chat
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-sampling
[ 48%] Built target test-llama-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Built target test-chat
[ 50%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Built target test-log
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Built target test-gguf
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Built target test-arg-parser
[ 62%] Built target test-barrier
[ 62%] Built target test-chat-template
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Built target test-quantize-fns
[ 65%] Linking CXX executable ../../bin/llama-batched-bench
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 68%] Built target test-quantize-perf
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target test-rope
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-batched-bench
[ 71%] Built target llama-eval-callback
[ 71%] Built target llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Built target llama-imatrix
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Built target llama-infill
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-bench
[ 81%] Built target llama-lookahead
[ 81%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Built target llama-lookup
[ 83%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup-stats
[ 83%] Generating loading.html.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-create
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Built target llama-cli
[ 84%] Generating index.html.gz.hpp
[ 84%] Built target llama-parallel
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 85%] Built target llama-passkey
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Built target llama-perplexity
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Built target llama-quantize
[ 90%] Built target llama-retrieval
[ 90%] Linking CXX executable ../../bin/llama-run
[ 91%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 92%] Built target llama-save-load-state
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 92%] Built target llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-speculative
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-run
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Built target llama-gen-docs
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Built target llama-cvector-generator
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.118s
user	0m6.575s
sys	0m9.970s

main: quantize time =  3667.33 ms
main:    total time =  3667.33 ms

main: quantize time =  2435.37 ms
main:    total time =  2435.37 ms

main: quantize time =  2033.89 ms
main:    total time =  2033.89 ms

main: quantize time =  1968.03 ms
main:    total time =  1968.03 ms

main: quantize time =  1833.12 ms
main:    total time =  1833.12 ms

main: quantize time =  5228.31 ms
main:    total time =  5228.31 ms

main: quantize time =  5951.02 ms
main:    total time =  5951.02 ms

main: quantize time =  7260.40 ms
main:    total time =  7260.40 ms

main: quantize time =  6230.26 ms
main:    total time =  6230.26 ms

main: quantize time =  4373.12 ms
main:    total time =  4373.12 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.230 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.387 I main: llama backend init
0.00.000.393 I main: load the model and apply lora adapter, if any
0.00.054.105 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.066.752 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.066.772 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.066.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.066.777 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.066.778 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.066.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.066.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.066.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.066.781 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.066.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.066.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.066.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.066.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.066.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.066.789 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.066.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.066.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.743 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.059 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.946 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.953 I llama_model_loader: - type  f32:  194 tensors
0.00.085.954 I llama_model_loader: - type  f16:   98 tensors
0.00.085.955 I print_info: file format = GGUF V3 (latest)
0.00.085.957 I print_info: file type   = all F32 (guessed)
0.00.085.959 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.099.463 I load: special tokens cache size = 25
0.00.108.128 I load: token to piece cache size = 0.2984 MB
0.00.108.152 I print_info: arch             = gptneox
0.00.108.153 I print_info: vocab_only       = 0
0.00.108.153 I print_info: n_ctx_train      = 2048
0.00.108.153 I print_info: n_embd           = 2048
0.00.108.153 I print_info: n_layer          = 24
0.00.108.156 I print_info: n_head           = 16
0.00.108.157 I print_info: n_head_kv        = 16
0.00.108.157 I print_info: n_rot            = 32
0.00.108.158 I print_info: n_swa            = 0
0.00.108.158 I print_info: n_embd_head_k    = 128
0.00.108.158 I print_info: n_embd_head_v    = 128
0.00.108.159 I print_info: n_gqa            = 1
0.00.108.160 I print_info: n_embd_k_gqa     = 2048
0.00.108.161 I print_info: n_embd_v_gqa     = 2048
0.00.108.161 I print_info: f_norm_eps       = 1.0e-05
0.00.108.162 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.162 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.162 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.162 I print_info: f_logit_scale    = 0.0e+00
0.00.108.163 I print_info: n_ff             = 8192
0.00.108.163 I print_info: n_expert         = 0
0.00.108.163 I print_info: n_expert_used    = 0
0.00.108.163 I print_info: causal attn      = 1
0.00.108.164 I print_info: pooling type     = 0
0.00.108.164 I print_info: rope type        = 2
0.00.108.164 I print_info: rope scaling     = linear
0.00.108.164 I print_info: freq_base_train  = 10000.0
0.00.108.165 I print_info: freq_scale_train = 1
0.00.108.165 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.165 I print_info: rope_finetuned   = unknown
0.00.108.165 I print_info: ssm_d_conv       = 0
0.00.108.166 I print_info: ssm_d_inner      = 0
0.00.108.166 I print_info: ssm_d_state      = 0
0.00.108.166 I print_info: ssm_dt_rank      = 0
0.00.108.166 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.166 I print_info: model type       = 1.4B
0.00.108.167 I print_info: model params     = 1.41 B
0.00.108.168 I print_info: general.name     = 1.4B
0.00.108.169 I print_info: vocab type       = BPE
0.00.108.169 I print_info: n_vocab          = 50304
0.00.108.169 I print_info: n_merges         = 50009
0.00.108.170 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.170 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.170 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.170 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.171 I print_info: LF token         = 187 'Ċ'
0.00.108.171 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.171 I print_info: max token length = 1024
0.00.108.172 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.142.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.142.689 I load_tensors: offloading output layer to GPU
0.00.142.689 I load_tensors: offloaded 25/25 layers to GPU
0.00.142.709 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.142.710 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.143.049 I llama_context_unified: n_seq_max     = 1
0.00.143.050 I llama_context_unified: n_ctx         = 2048
0.00.143.050 I llama_context_unified: n_ctx_per_seq = 2048
0.00.143.050 I llama_context_unified: n_batch       = 2048
0.00.143.051 I llama_context_unified: n_ubatch      = 512
0.00.143.051 I llama_context_unified: flash_attn    = 0
0.00.143.051 I llama_context_unified: freq_base     = 10000.0
0.00.143.051 I llama_context_unified: freq_scale    = 1
0.00.143.052 I ggml_metal_init: allocating
0.00.143.066 I ggml_metal_init: found device: Apple M4
0.00.143.070 I ggml_metal_init: picking default device: Apple M4
0.00.143.649 I ggml_metal_init: using embedded metal library
0.00.169.440 I ggml_metal_init: GPU name:   Apple M4
0.00.169.442 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.169.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.169.443 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.169.443 I ggml_metal_init: simdgroup reduction   = true
0.00.169.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.169.443 I ggml_metal_init: has residency sets    = true
0.00.169.443 I ggml_metal_init: has bfloat            = true
0.00.169.444 I ggml_metal_init: use bfloat            = true
0.00.169.444 I ggml_metal_init: hasUnifiedMemory      = true
0.00.169.445 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.316.579 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.344.761 I init:      Metal KV buffer size =   384.00 MiB
0.00.344.768 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.344.792 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.349.699 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.349.702 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.349.702 I llama_context_unified: graph nodes  = 967
0.00.349.702 I llama_context_unified: graph splits = 2
0.00.349.706 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.349.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.349.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.416.912 I main: llama threadpool init, n_threads = 4
0.00.416.950 I 
0.00.416.978 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.416.979 I 
0.00.417.022 I sampler seed: 1234
0.00.417.026 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.417.051 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.417.053 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.417.053 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.251.135 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.02.251.136 I llama_perf_context_print:        load time =     361.96 ms
0.02.251.136 I llama_perf_context_print: prompt eval time =      43.91 ms /     7 tokens (    6.27 ms per token,   159.42 tokens per second)
0.02.251.137 I llama_perf_context_print:        eval time =    1787.39 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.251.138 I llama_perf_context_print:       total time =    1835.05 ms /    70 tokens
0.02.254.951 I ggml_metal_free: deallocating

real	0m2.584s
user	0m0.132s
sys	0m0.133s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.009.959 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.183 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.183 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.183 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.184 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.184 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.185 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.185 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.186 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.188 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.189 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.191 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.133 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.018 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.018 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.019 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.019 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.019 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.020 I llama_model_loader: - type  f32:  194 tensors
0.00.035.020 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.021 I print_info: file format = GGUF V3 (latest)
0.00.035.021 I print_info: file type   = Q8_0
0.00.035.023 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.627 I load: special tokens cache size = 25
0.00.050.128 I load: token to piece cache size = 0.2984 MB
0.00.050.143 I print_info: arch             = gptneox
0.00.050.144 I print_info: vocab_only       = 0
0.00.050.144 I print_info: n_ctx_train      = 2048
0.00.050.145 I print_info: n_embd           = 2048
0.00.050.145 I print_info: n_layer          = 24
0.00.050.150 I print_info: n_head           = 16
0.00.050.151 I print_info: n_head_kv        = 16
0.00.050.151 I print_info: n_rot            = 32
0.00.050.151 I print_info: n_swa            = 0
0.00.050.151 I print_info: n_embd_head_k    = 128
0.00.050.151 I print_info: n_embd_head_v    = 128
0.00.050.152 I print_info: n_gqa            = 1
0.00.050.153 I print_info: n_embd_k_gqa     = 2048
0.00.050.153 I print_info: n_embd_v_gqa     = 2048
0.00.050.154 I print_info: f_norm_eps       = 1.0e-05
0.00.050.154 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.154 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.155 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.155 I print_info: f_logit_scale    = 0.0e+00
0.00.050.155 I print_info: n_ff             = 8192
0.00.050.156 I print_info: n_expert         = 0
0.00.050.156 I print_info: n_expert_used    = 0
0.00.050.156 I print_info: causal attn      = 1
0.00.050.156 I print_info: pooling type     = 0
0.00.050.156 I print_info: rope type        = 2
0.00.050.156 I print_info: rope scaling     = linear
0.00.050.157 I print_info: freq_base_train  = 10000.0
0.00.050.157 I print_info: freq_scale_train = 1
0.00.050.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.157 I print_info: rope_finetuned   = unknown
0.00.050.157 I print_info: ssm_d_conv       = 0
0.00.050.157 I print_info: ssm_d_inner      = 0
0.00.050.157 I print_info: ssm_d_state      = 0
0.00.050.158 I print_info: ssm_dt_rank      = 0
0.00.050.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.158 I print_info: model type       = 1.4B
0.00.050.158 I print_info: model params     = 1.41 B
0.00.050.158 I print_info: general.name     = 1.4B
0.00.050.159 I print_info: vocab type       = BPE
0.00.050.159 I print_info: n_vocab          = 50304
0.00.050.162 I print_info: n_merges         = 50009
0.00.050.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.162 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.162 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.162 I print_info: LF token         = 187 'Ċ'
0.00.050.163 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.163 I print_info: max token length = 1024
0.00.050.163 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.315.924 I load_tensors: offloading 24 repeating layers to GPU
0.01.315.930 I load_tensors: offloading output layer to GPU
0.01.315.931 I load_tensors: offloaded 25/25 layers to GPU
0.01.315.956 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.315.961 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.316.758 I llama_context_unified: n_seq_max     = 1
0.01.316.760 I llama_context_unified: n_ctx         = 2048
0.01.316.760 I llama_context_unified: n_ctx_per_seq = 2048
0.01.316.761 I llama_context_unified: n_batch       = 2048
0.01.316.761 I llama_context_unified: n_ubatch      = 512
0.01.316.761 I llama_context_unified: flash_attn    = 0
0.01.316.762 I llama_context_unified: freq_base     = 10000.0
0.01.316.763 I llama_context_unified: freq_scale    = 1
0.01.316.764 I ggml_metal_init: allocating
0.01.316.782 I ggml_metal_init: found device: Apple M4
0.01.316.790 I ggml_metal_init: picking default device: Apple M4
0.01.318.008 I ggml_metal_init: using embedded metal library
0.01.323.314 I ggml_metal_init: GPU name:   Apple M4
0.01.323.318 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.323.319 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.323.319 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.323.320 I ggml_metal_init: simdgroup reduction   = true
0.01.323.320 I ggml_metal_init: simdgroup matrix mul. = true
0.01.323.320 I ggml_metal_init: has residency sets    = true
0.01.323.320 I ggml_metal_init: has bfloat            = true
0.01.323.321 I ggml_metal_init: use bfloat            = true
0.01.323.321 I ggml_metal_init: hasUnifiedMemory      = true
0.01.323.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.339.527 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.394.569 I init:      Metal KV buffer size =   384.00 MiB
0.01.394.578 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.394.603 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.399.095 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.01.399.096 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.01.399.097 I llama_context_unified: graph nodes  = 967
0.01.399.097 I llama_context_unified: graph splits = 2
0.01.399.102 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.399.242 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.399.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.453.964 I main: llama threadpool init, n_threads = 4
0.01.454.004 I 
0.01.454.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.454.027 I 
0.01.454.175 I sampler seed: 1234
0.01.454.179 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.454.225 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.454.226 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.454.229 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.562.351 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50035.24 tokens per second)
0.02.562.352 I llama_perf_context_print:        load time =    1443.32 ms
0.02.562.354 I llama_perf_context_print: prompt eval time =      49.03 ms /     7 tokens (    7.00 ms per token,   142.77 tokens per second)
0.02.562.355 I llama_perf_context_print:        eval time =    1056.40 ms /    63 runs   (   16.77 ms per token,    59.64 tokens per second)
0.02.562.356 I llama_perf_context_print:       total time =    1109.07 ms /    70 tokens
0.02.566.430 I ggml_metal_free: deallocating

real	0m2.584s
user	0m0.109s
sys	0m0.255s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.012.209 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.517 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.518 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.518 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.520 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.520 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.520 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.521 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.521 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.521 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.522 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.523 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.446 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.475 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.304 I llama_model_loader: - type  f32:  194 tensors
0.00.030.304 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.304 I llama_model_loader: - type q6_K:    1 tensors
0.00.030.305 I print_info: file format = GGUF V3 (latest)
0.00.030.305 I print_info: file type   = Q4_0
0.00.030.306 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.038.708 I load: special tokens cache size = 25
0.00.044.862 I load: token to piece cache size = 0.2984 MB
0.00.044.878 I print_info: arch             = gptneox
0.00.044.879 I print_info: vocab_only       = 0
0.00.044.879 I print_info: n_ctx_train      = 2048
0.00.044.879 I print_info: n_embd           = 2048
0.00.044.879 I print_info: n_layer          = 24
0.00.044.884 I print_info: n_head           = 16
0.00.044.886 I print_info: n_head_kv        = 16
0.00.044.886 I print_info: n_rot            = 32
0.00.044.886 I print_info: n_swa            = 0
0.00.044.886 I print_info: n_embd_head_k    = 128
0.00.044.886 I print_info: n_embd_head_v    = 128
0.00.044.887 I print_info: n_gqa            = 1
0.00.044.888 I print_info: n_embd_k_gqa     = 2048
0.00.044.889 I print_info: n_embd_v_gqa     = 2048
0.00.044.889 I print_info: f_norm_eps       = 1.0e-05
0.00.044.890 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.044.890 I print_info: f_clamp_kqv      = 0.0e+00
0.00.044.890 I print_info: f_max_alibi_bias = 0.0e+00
0.00.044.892 I print_info: f_logit_scale    = 0.0e+00
0.00.044.892 I print_info: n_ff             = 8192
0.00.044.893 I print_info: n_expert         = 0
0.00.044.893 I print_info: n_expert_used    = 0
0.00.044.893 I print_info: causal attn      = 1
0.00.044.894 I print_info: pooling type     = 0
0.00.044.894 I print_info: rope type        = 2
0.00.044.894 I print_info: rope scaling     = linear
0.00.044.894 I print_info: freq_base_train  = 10000.0
0.00.044.898 I print_info: freq_scale_train = 1
0.00.044.898 I print_info: n_ctx_orig_yarn  = 2048
0.00.044.898 I print_info: rope_finetuned   = unknown
0.00.044.899 I print_info: ssm_d_conv       = 0
0.00.044.899 I print_info: ssm_d_inner      = 0
0.00.044.900 I print_info: ssm_d_state      = 0
0.00.044.900 I print_info: ssm_dt_rank      = 0
0.00.044.900 I print_info: ssm_dt_b_c_rms   = 0
0.00.044.900 I print_info: model type       = 1.4B
0.00.044.900 I print_info: model params     = 1.41 B
0.00.044.901 I print_info: general.name     = 1.4B
0.00.044.901 I print_info: vocab type       = BPE
0.00.044.901 I print_info: n_vocab          = 50304
0.00.044.901 I print_info: n_merges         = 50009
0.00.044.901 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.044.902 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.044.902 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.044.902 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.044.902 I print_info: LF token         = 187 'Ċ'
0.00.044.902 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.044.903 I print_info: max token length = 1024
0.00.044.903 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.645.294 I load_tensors: offloading 24 repeating layers to GPU
0.00.645.312 I load_tensors: offloading output layer to GPU
0.00.645.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.645.346 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.645.347 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.646.829 I llama_context_unified: n_seq_max     = 1
0.00.646.832 I llama_context_unified: n_ctx         = 2048
0.00.646.833 I llama_context_unified: n_ctx_per_seq = 2048
0.00.646.833 I llama_context_unified: n_batch       = 2048
0.00.646.834 I llama_context_unified: n_ubatch      = 512
0.00.646.834 I llama_context_unified: flash_attn    = 0
0.00.646.837 I llama_context_unified: freq_base     = 10000.0
0.00.646.838 I llama_context_unified: freq_scale    = 1
0.00.646.840 I ggml_metal_init: allocating
0.00.646.912 I ggml_metal_init: found device: Apple M4
0.00.646.925 I ggml_metal_init: picking default device: Apple M4
0.00.648.713 I ggml_metal_init: using embedded metal library
0.00.654.411 I ggml_metal_init: GPU name:   Apple M4
0.00.654.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.432 I ggml_metal_init: simdgroup reduction   = true
0.00.654.432 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.432 I ggml_metal_init: has residency sets    = true
0.00.654.433 I ggml_metal_init: has bfloat            = true
0.00.654.433 I ggml_metal_init: use bfloat            = true
0.00.654.436 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.441 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.255 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.853 I init:      Metal KV buffer size =   384.00 MiB
0.00.735.859 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.883 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.740.337 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.740.339 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.740.340 I llama_context_unified: graph nodes  = 967
0.00.740.340 I llama_context_unified: graph splits = 2
0.00.740.346 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.805 I main: llama threadpool init, n_threads = 4
0.00.796.854 I 
0.00.796.875 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.875 I 
0.00.797.034 I sampler seed: 1234
0.00.797.038 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.070 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.073 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.074 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.486.368 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.486.369 I llama_perf_context_print:        load time =     783.89 ms
0.01.486.369 I llama_perf_context_print: prompt eval time =      47.70 ms /     7 tokens (    6.81 ms per token,   146.76 tokens per second)
0.01.486.370 I llama_perf_context_print:        eval time =     638.63 ms /    63 runs   (   10.14 ms per token,    98.65 tokens per second)
0.01.486.370 I llama_perf_context_print:       total time =     690.27 ms /    70 tokens
0.01.490.113 I ggml_metal_free: deallocating

real	0m1.512s
user	0m0.111s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.630 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.603 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.615 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.617 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.618 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.619 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.620 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.620 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.409 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.137 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.138 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.139 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.139 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.139 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.140 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.140 I llama_model_loader: - type  f32:  194 tensors
0.00.025.140 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.141 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.141 I print_info: file format = GGUF V3 (latest)
0.00.025.142 I print_info: file type   = Q4_1
0.00.025.146 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.048 I load: special tokens cache size = 25
0.00.039.059 I load: token to piece cache size = 0.2984 MB
0.00.039.072 I print_info: arch             = gptneox
0.00.039.074 I print_info: vocab_only       = 0
0.00.039.074 I print_info: n_ctx_train      = 2048
0.00.039.074 I print_info: n_embd           = 2048
0.00.039.074 I print_info: n_layer          = 24
0.00.039.077 I print_info: n_head           = 16
0.00.039.078 I print_info: n_head_kv        = 16
0.00.039.078 I print_info: n_rot            = 32
0.00.039.078 I print_info: n_swa            = 0
0.00.039.078 I print_info: n_embd_head_k    = 128
0.00.039.078 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.080 I print_info: n_embd_k_gqa     = 2048
0.00.039.080 I print_info: n_embd_v_gqa     = 2048
0.00.039.081 I print_info: f_norm_eps       = 1.0e-05
0.00.039.081 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.082 I print_info: f_logit_scale    = 0.0e+00
0.00.039.083 I print_info: n_ff             = 8192
0.00.039.083 I print_info: n_expert         = 0
0.00.039.084 I print_info: n_expert_used    = 0
0.00.039.085 I print_info: causal attn      = 1
0.00.039.085 I print_info: pooling type     = 0
0.00.039.085 I print_info: rope type        = 2
0.00.039.085 I print_info: rope scaling     = linear
0.00.039.085 I print_info: freq_base_train  = 10000.0
0.00.039.086 I print_info: freq_scale_train = 1
0.00.039.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.086 I print_info: rope_finetuned   = unknown
0.00.039.086 I print_info: ssm_d_conv       = 0
0.00.039.086 I print_info: ssm_d_inner      = 0
0.00.039.086 I print_info: ssm_d_state      = 0
0.00.039.086 I print_info: ssm_dt_rank      = 0
0.00.039.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.086 I print_info: model type       = 1.4B
0.00.039.087 I print_info: model params     = 1.41 B
0.00.039.090 I print_info: general.name     = 1.4B
0.00.039.090 I print_info: vocab type       = BPE
0.00.039.091 I print_info: n_vocab          = 50304
0.00.039.091 I print_info: n_merges         = 50009
0.00.039.091 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: LF token         = 187 'Ċ'
0.00.039.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: max token length = 1024
0.00.039.093 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.666.660 I load_tensors: offloading 24 repeating layers to GPU
0.00.666.672 I load_tensors: offloading output layer to GPU
0.00.666.673 I load_tensors: offloaded 25/25 layers to GPU
0.00.666.708 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.666.709 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.668.103 I llama_context_unified: n_seq_max     = 1
0.00.668.106 I llama_context_unified: n_ctx         = 2048
0.00.668.107 I llama_context_unified: n_ctx_per_seq = 2048
0.00.668.107 I llama_context_unified: n_batch       = 2048
0.00.668.108 I llama_context_unified: n_ubatch      = 512
0.00.668.108 I llama_context_unified: flash_attn    = 0
0.00.668.111 I llama_context_unified: freq_base     = 10000.0
0.00.668.111 I llama_context_unified: freq_scale    = 1
0.00.668.115 I ggml_metal_init: allocating
0.00.668.172 I ggml_metal_init: found device: Apple M4
0.00.668.184 I ggml_metal_init: picking default device: Apple M4
0.00.669.926 I ggml_metal_init: using embedded metal library
0.00.676.372 I ggml_metal_init: GPU name:   Apple M4
0.00.676.377 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.676.378 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.676.379 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.676.380 I ggml_metal_init: simdgroup reduction   = true
0.00.676.380 I ggml_metal_init: simdgroup matrix mul. = true
0.00.676.380 I ggml_metal_init: has residency sets    = true
0.00.676.380 I ggml_metal_init: has bfloat            = true
0.00.676.381 I ggml_metal_init: use bfloat            = true
0.00.676.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.676.384 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.694.763 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.750.404 I init:      Metal KV buffer size =   384.00 MiB
0.00.750.411 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.750.433 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.754.810 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.754.813 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.754.813 I llama_context_unified: graph nodes  = 967
0.00.754.813 I llama_context_unified: graph splits = 2
0.00.754.819 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.754.943 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.754.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.217 I main: llama threadpool init, n_threads = 4
0.00.809.264 I 
0.00.809.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.289 I 
0.00.809.443 I sampler seed: 1234
0.00.809.447 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.458 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.459 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.459 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.541.984 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.541.985 I llama_perf_context_print:        load time =     799.89 ms
0.01.541.986 I llama_perf_context_print: prompt eval time =      46.25 ms /     7 tokens (    6.61 ms per token,   151.36 tokens per second)
0.01.541.987 I llama_perf_context_print:        eval time =     683.40 ms /    63 runs   (   10.85 ms per token,    92.19 tokens per second)
0.01.541.987 I llama_perf_context_print:       total time =     733.46 ms /    70 tokens
0.01.545.778 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.107s
sys	0m0.202s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.571 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.576 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.577 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.578 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.578 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.580 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.580 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.581 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.581 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.582 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.583 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.584 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.584 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.585 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.587 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.587 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.588 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.393 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.149 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.150 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.027.151 I llama_model_loader: - type  f32:  194 tensors
0.00.027.152 I llama_model_loader: - type q5_0:   97 tensors
0.00.027.152 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.152 I print_info: file format = GGUF V3 (latest)
0.00.027.153 I print_info: file type   = Q5_0
0.00.027.154 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.035.335 I load: special tokens cache size = 25
0.00.041.609 I load: token to piece cache size = 0.2984 MB
0.00.041.623 I print_info: arch             = gptneox
0.00.041.624 I print_info: vocab_only       = 0
0.00.041.625 I print_info: n_ctx_train      = 2048
0.00.041.625 I print_info: n_embd           = 2048
0.00.041.625 I print_info: n_layer          = 24
0.00.041.628 I print_info: n_head           = 16
0.00.041.629 I print_info: n_head_kv        = 16
0.00.041.629 I print_info: n_rot            = 32
0.00.041.631 I print_info: n_swa            = 0
0.00.041.631 I print_info: n_embd_head_k    = 128
0.00.041.631 I print_info: n_embd_head_v    = 128
0.00.041.632 I print_info: n_gqa            = 1
0.00.041.633 I print_info: n_embd_k_gqa     = 2048
0.00.041.634 I print_info: n_embd_v_gqa     = 2048
0.00.041.635 I print_info: f_norm_eps       = 1.0e-05
0.00.041.636 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.636 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.636 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.636 I print_info: f_logit_scale    = 0.0e+00
0.00.041.637 I print_info: n_ff             = 8192
0.00.041.637 I print_info: n_expert         = 0
0.00.041.637 I print_info: n_expert_used    = 0
0.00.041.637 I print_info: causal attn      = 1
0.00.041.637 I print_info: pooling type     = 0
0.00.041.638 I print_info: rope type        = 2
0.00.041.640 I print_info: rope scaling     = linear
0.00.041.640 I print_info: freq_base_train  = 10000.0
0.00.041.640 I print_info: freq_scale_train = 1
0.00.041.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.641 I print_info: rope_finetuned   = unknown
0.00.041.641 I print_info: ssm_d_conv       = 0
0.00.041.644 I print_info: ssm_d_inner      = 0
0.00.041.644 I print_info: ssm_d_state      = 0
0.00.041.644 I print_info: ssm_dt_rank      = 0
0.00.041.644 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.644 I print_info: model type       = 1.4B
0.00.041.645 I print_info: model params     = 1.41 B
0.00.041.645 I print_info: general.name     = 1.4B
0.00.041.646 I print_info: vocab type       = BPE
0.00.041.646 I print_info: n_vocab          = 50304
0.00.041.647 I print_info: n_merges         = 50009
0.00.041.648 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.648 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.648 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.648 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.648 I print_info: LF token         = 187 'Ċ'
0.00.041.648 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.649 I print_info: max token length = 1024
0.00.041.649 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.725.171 I load_tensors: offloading 24 repeating layers to GPU
0.00.725.184 I load_tensors: offloading output layer to GPU
0.00.725.185 I load_tensors: offloaded 25/25 layers to GPU
0.00.725.216 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.725.229 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.726.767 I llama_context_unified: n_seq_max     = 1
0.00.726.770 I llama_context_unified: n_ctx         = 2048
0.00.726.771 I llama_context_unified: n_ctx_per_seq = 2048
0.00.726.771 I llama_context_unified: n_batch       = 2048
0.00.726.771 I llama_context_unified: n_ubatch      = 512
0.00.726.772 I llama_context_unified: flash_attn    = 0
0.00.726.774 I llama_context_unified: freq_base     = 10000.0
0.00.726.775 I llama_context_unified: freq_scale    = 1
0.00.726.778 I ggml_metal_init: allocating
0.00.726.864 I ggml_metal_init: found device: Apple M4
0.00.726.901 I ggml_metal_init: picking default device: Apple M4
0.00.728.377 I ggml_metal_init: using embedded metal library
0.00.734.805 I ggml_metal_init: GPU name:   Apple M4
0.00.734.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.734.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.734.810 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.734.810 I ggml_metal_init: simdgroup reduction   = true
0.00.734.811 I ggml_metal_init: simdgroup matrix mul. = true
0.00.734.811 I ggml_metal_init: has residency sets    = true
0.00.734.811 I ggml_metal_init: has bfloat            = true
0.00.734.811 I ggml_metal_init: use bfloat            = true
0.00.734.812 I ggml_metal_init: hasUnifiedMemory      = true
0.00.734.821 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.752.104 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.809.732 I init:      Metal KV buffer size =   384.00 MiB
0.00.809.738 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.809.761 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.814.462 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.814.464 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.814.464 I llama_context_unified: graph nodes  = 967
0.00.814.465 I llama_context_unified: graph splits = 2
0.00.814.471 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.814.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.814.589 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.875.130 I main: llama threadpool init, n_threads = 4
0.00.875.175 I 
0.00.875.196 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.875.197 I 
0.00.875.348 I sampler seed: 1234
0.00.875.353 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.875.364 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.875.365 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.875.365 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.672.281 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.672.282 I llama_perf_context_print:        load time =     864.76 ms
0.01.672.283 I llama_perf_context_print: prompt eval time =      53.24 ms /     7 tokens (    7.61 ms per token,   131.49 tokens per second)
0.01.672.284 I llama_perf_context_print:        eval time =     740.67 ms /    63 runs   (   11.76 ms per token,    85.06 tokens per second)
0.01.672.284 I llama_perf_context_print:       total time =     797.85 ms /    70 tokens
0.01.676.170 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.110s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.273 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.053 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.059 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.060 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.065 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.065 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.066 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.067 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.067 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.068 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.001 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.016 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.867 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.868 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.869 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.870 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.870 I llama_model_loader: - type  f32:  194 tensors
0.00.025.871 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.871 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.872 I print_info: file format = GGUF V3 (latest)
0.00.025.872 I print_info: file type   = Q5_1
0.00.025.873 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.972 I load: special tokens cache size = 25
0.00.040.109 I load: token to piece cache size = 0.2984 MB
0.00.040.118 I print_info: arch             = gptneox
0.00.040.119 I print_info: vocab_only       = 0
0.00.040.120 I print_info: n_ctx_train      = 2048
0.00.040.120 I print_info: n_embd           = 2048
0.00.040.120 I print_info: n_layer          = 24
0.00.040.123 I print_info: n_head           = 16
0.00.040.123 I print_info: n_head_kv        = 16
0.00.040.124 I print_info: n_rot            = 32
0.00.040.124 I print_info: n_swa            = 0
0.00.040.124 I print_info: n_embd_head_k    = 128
0.00.040.124 I print_info: n_embd_head_v    = 128
0.00.040.125 I print_info: n_gqa            = 1
0.00.040.126 I print_info: n_embd_k_gqa     = 2048
0.00.040.126 I print_info: n_embd_v_gqa     = 2048
0.00.040.127 I print_info: f_norm_eps       = 1.0e-05
0.00.040.132 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.132 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.132 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.132 I print_info: f_logit_scale    = 0.0e+00
0.00.040.133 I print_info: n_ff             = 8192
0.00.040.133 I print_info: n_expert         = 0
0.00.040.133 I print_info: n_expert_used    = 0
0.00.040.134 I print_info: causal attn      = 1
0.00.040.134 I print_info: pooling type     = 0
0.00.040.134 I print_info: rope type        = 2
0.00.040.134 I print_info: rope scaling     = linear
0.00.040.134 I print_info: freq_base_train  = 10000.0
0.00.040.135 I print_info: freq_scale_train = 1
0.00.040.135 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.136 I print_info: rope_finetuned   = unknown
0.00.040.136 I print_info: ssm_d_conv       = 0
0.00.040.136 I print_info: ssm_d_inner      = 0
0.00.040.137 I print_info: ssm_d_state      = 0
0.00.040.137 I print_info: ssm_dt_rank      = 0
0.00.040.137 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.137 I print_info: model type       = 1.4B
0.00.040.137 I print_info: model params     = 1.41 B
0.00.040.137 I print_info: general.name     = 1.4B
0.00.040.138 I print_info: vocab type       = BPE
0.00.040.138 I print_info: n_vocab          = 50304
0.00.040.138 I print_info: n_merges         = 50009
0.00.040.139 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.140 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.140 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.140 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.141 I print_info: LF token         = 187 'Ċ'
0.00.040.141 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.141 I print_info: max token length = 1024
0.00.040.141 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.611.992 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.999 I load_tensors: offloading output layer to GPU
0.00.612.000 I load_tensors: offloaded 25/25 layers to GPU
0.00.612.024 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.612.027 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.613.353 I llama_context_unified: n_seq_max     = 1
0.00.613.356 I llama_context_unified: n_ctx         = 2048
0.00.613.356 I llama_context_unified: n_ctx_per_seq = 2048
0.00.613.356 I llama_context_unified: n_batch       = 2048
0.00.613.357 I llama_context_unified: n_ubatch      = 512
0.00.613.357 I llama_context_unified: flash_attn    = 0
0.00.613.358 I llama_context_unified: freq_base     = 10000.0
0.00.613.359 I llama_context_unified: freq_scale    = 1
0.00.613.360 I ggml_metal_init: allocating
0.00.613.379 I ggml_metal_init: found device: Apple M4
0.00.613.388 I ggml_metal_init: picking default device: Apple M4
0.00.614.928 I ggml_metal_init: using embedded metal library
0.00.621.082 I ggml_metal_init: GPU name:   Apple M4
0.00.621.086 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.087 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.088 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.089 I ggml_metal_init: simdgroup reduction   = true
0.00.621.089 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.089 I ggml_metal_init: has residency sets    = true
0.00.621.089 I ggml_metal_init: has bfloat            = true
0.00.621.090 I ggml_metal_init: use bfloat            = true
0.00.621.091 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.092 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.638.208 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.689.622 I init:      Metal KV buffer size =   384.00 MiB
0.00.689.629 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.689.652 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.694.200 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.694.202 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.694.202 I llama_context_unified: graph nodes  = 967
0.00.694.202 I llama_context_unified: graph splits = 2
0.00.694.209 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.694.340 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.694.341 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.839 I main: llama threadpool init, n_threads = 4
0.00.752.879 I 
0.00.752.900 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.901 I 
0.00.753.051 I sampler seed: 1234
0.00.753.055 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.066 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.067 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.067 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.594.918 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.594.919 I llama_perf_context_print:        load time =     742.85 ms
0.01.594.920 I llama_perf_context_print: prompt eval time =      50.70 ms /     7 tokens (    7.24 ms per token,   138.05 tokens per second)
0.01.594.921 I llama_perf_context_print:        eval time =     788.22 ms /    63 runs   (   12.51 ms per token,    79.93 tokens per second)
0.01.594.922 I llama_perf_context_print:       total time =     842.79 ms /    70 tokens
0.01.598.948 I ggml_metal_free: deallocating

real	0m1.615s
user	0m0.108s
sys	0m0.219s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.734 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.161 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.174 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.174 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.919 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.665 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.668 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.668 I llama_model_loader: - type  f32:  194 tensors
0.00.024.668 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.669 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.669 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.669 I print_info: file format = GGUF V3 (latest)
0.00.024.670 I print_info: file type   = Q2_K - Medium
0.00.024.670 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.549 I load: special tokens cache size = 25
0.00.038.688 I load: token to piece cache size = 0.2984 MB
0.00.038.703 I print_info: arch             = gptneox
0.00.038.704 I print_info: vocab_only       = 0
0.00.038.704 I print_info: n_ctx_train      = 2048
0.00.038.704 I print_info: n_embd           = 2048
0.00.038.704 I print_info: n_layer          = 24
0.00.038.707 I print_info: n_head           = 16
0.00.038.708 I print_info: n_head_kv        = 16
0.00.038.708 I print_info: n_rot            = 32
0.00.038.708 I print_info: n_swa            = 0
0.00.038.709 I print_info: n_embd_head_k    = 128
0.00.038.709 I print_info: n_embd_head_v    = 128
0.00.038.709 I print_info: n_gqa            = 1
0.00.038.710 I print_info: n_embd_k_gqa     = 2048
0.00.038.712 I print_info: n_embd_v_gqa     = 2048
0.00.038.713 I print_info: f_norm_eps       = 1.0e-05
0.00.038.713 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.713 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.713 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.713 I print_info: f_logit_scale    = 0.0e+00
0.00.038.714 I print_info: n_ff             = 8192
0.00.038.714 I print_info: n_expert         = 0
0.00.038.714 I print_info: n_expert_used    = 0
0.00.038.715 I print_info: causal attn      = 1
0.00.038.715 I print_info: pooling type     = 0
0.00.038.717 I print_info: rope type        = 2
0.00.038.717 I print_info: rope scaling     = linear
0.00.038.717 I print_info: freq_base_train  = 10000.0
0.00.038.718 I print_info: freq_scale_train = 1
0.00.038.718 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.718 I print_info: rope_finetuned   = unknown
0.00.038.718 I print_info: ssm_d_conv       = 0
0.00.038.718 I print_info: ssm_d_inner      = 0
0.00.038.718 I print_info: ssm_d_state      = 0
0.00.038.718 I print_info: ssm_dt_rank      = 0
0.00.038.718 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.719 I print_info: model type       = 1.4B
0.00.038.719 I print_info: model params     = 1.41 B
0.00.038.719 I print_info: general.name     = 1.4B
0.00.038.720 I print_info: vocab type       = BPE
0.00.038.720 I print_info: n_vocab          = 50304
0.00.038.720 I print_info: n_merges         = 50009
0.00.038.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.720 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.722 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.722 I print_info: LF token         = 187 'Ċ'
0.00.038.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.722 I print_info: max token length = 1024
0.00.038.723 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.346.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.346.922 I load_tensors: offloading output layer to GPU
0.00.346.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.346.956 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.346.957 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.348.503 I llama_context_unified: n_seq_max     = 1
0.00.348.505 I llama_context_unified: n_ctx         = 2048
0.00.348.506 I llama_context_unified: n_ctx_per_seq = 2048
0.00.348.506 I llama_context_unified: n_batch       = 2048
0.00.348.506 I llama_context_unified: n_ubatch      = 512
0.00.348.507 I llama_context_unified: flash_attn    = 0
0.00.348.509 I llama_context_unified: freq_base     = 10000.0
0.00.348.510 I llama_context_unified: freq_scale    = 1
0.00.348.512 I ggml_metal_init: allocating
0.00.348.589 I ggml_metal_init: found device: Apple M4
0.00.348.603 I ggml_metal_init: picking default device: Apple M4
0.00.350.419 I ggml_metal_init: using embedded metal library
0.00.356.110 I ggml_metal_init: GPU name:   Apple M4
0.00.356.123 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.356.124 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.356.124 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.356.125 I ggml_metal_init: simdgroup reduction   = true
0.00.356.125 I ggml_metal_init: simdgroup matrix mul. = true
0.00.356.126 I ggml_metal_init: has residency sets    = true
0.00.356.126 I ggml_metal_init: has bfloat            = true
0.00.356.126 I ggml_metal_init: use bfloat            = true
0.00.356.130 I ggml_metal_init: hasUnifiedMemory      = true
0.00.356.134 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.377.305 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.439.551 I init:      Metal KV buffer size =   384.00 MiB
0.00.439.561 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.439.586 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.443.957 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.443.959 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.443.960 I llama_context_unified: graph nodes  = 967
0.00.443.960 I llama_context_unified: graph splits = 2
0.00.443.966 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.444.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.444.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.940 I main: llama threadpool init, n_threads = 4
0.00.502.981 I 
0.00.503.003 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.003 I 
0.00.503.183 I sampler seed: 1234
0.00.503.187 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.503.221 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.503.224 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.503.224 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.173.594 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.173.594 I llama_perf_context_print:        load time =     492.48 ms
0.01.173.595 I llama_perf_context_print: prompt eval time =      35.74 ms /     7 tokens (    5.11 ms per token,   195.85 tokens per second)
0.01.173.596 I llama_perf_context_print:        eval time =     631.75 ms /    63 runs   (   10.03 ms per token,    99.72 tokens per second)
0.01.173.596 I llama_perf_context_print:       total time =     671.38 ms /    70 tokens
0.01.177.494 I ggml_metal_free: deallocating

real	0m1.196s
user	0m0.111s
sys	0m0.177s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.240 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.816 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.823 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.823 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.828 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.828 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.829 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.836 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.638 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.423 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.425 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.425 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.425 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.425 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.426 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.426 I llama_model_loader: - type  f32:  194 tensors
0.00.025.427 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.427 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.427 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.427 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.428 I print_info: file format = GGUF V3 (latest)
0.00.025.428 I print_info: file type   = Q3_K - Medium
0.00.025.429 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.261 I load: special tokens cache size = 25
0.00.039.363 I load: token to piece cache size = 0.2984 MB
0.00.039.377 I print_info: arch             = gptneox
0.00.039.378 I print_info: vocab_only       = 0
0.00.039.378 I print_info: n_ctx_train      = 2048
0.00.039.378 I print_info: n_embd           = 2048
0.00.039.379 I print_info: n_layer          = 24
0.00.039.381 I print_info: n_head           = 16
0.00.039.382 I print_info: n_head_kv        = 16
0.00.039.382 I print_info: n_rot            = 32
0.00.039.383 I print_info: n_swa            = 0
0.00.039.383 I print_info: n_embd_head_k    = 128
0.00.039.383 I print_info: n_embd_head_v    = 128
0.00.039.384 I print_info: n_gqa            = 1
0.00.039.384 I print_info: n_embd_k_gqa     = 2048
0.00.039.385 I print_info: n_embd_v_gqa     = 2048
0.00.039.386 I print_info: f_norm_eps       = 1.0e-05
0.00.039.386 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.388 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.388 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.388 I print_info: f_logit_scale    = 0.0e+00
0.00.039.389 I print_info: n_ff             = 8192
0.00.039.389 I print_info: n_expert         = 0
0.00.039.389 I print_info: n_expert_used    = 0
0.00.039.391 I print_info: causal attn      = 1
0.00.039.392 I print_info: pooling type     = 0
0.00.039.392 I print_info: rope type        = 2
0.00.039.392 I print_info: rope scaling     = linear
0.00.039.398 I print_info: freq_base_train  = 10000.0
0.00.039.400 I print_info: freq_scale_train = 1
0.00.039.400 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.400 I print_info: rope_finetuned   = unknown
0.00.039.400 I print_info: ssm_d_conv       = 0
0.00.039.400 I print_info: ssm_d_inner      = 0
0.00.039.401 I print_info: ssm_d_state      = 0
0.00.039.401 I print_info: ssm_dt_rank      = 0
0.00.039.401 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.401 I print_info: model type       = 1.4B
0.00.039.402 I print_info: model params     = 1.41 B
0.00.039.403 I print_info: general.name     = 1.4B
0.00.039.403 I print_info: vocab type       = BPE
0.00.039.403 I print_info: n_vocab          = 50304
0.00.039.403 I print_info: n_merges         = 50009
0.00.039.404 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.404 I print_info: LF token         = 187 'Ċ'
0.00.039.405 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.405 I print_info: max token length = 1024
0.00.039.405 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.462.140 I load_tensors: offloading 24 repeating layers to GPU
0.00.462.151 I load_tensors: offloading output layer to GPU
0.00.462.151 I load_tensors: offloaded 25/25 layers to GPU
0.00.462.179 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.462.181 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.463.754 I llama_context_unified: n_seq_max     = 1
0.00.463.758 I llama_context_unified: n_ctx         = 2048
0.00.463.758 I llama_context_unified: n_ctx_per_seq = 2048
0.00.463.759 I llama_context_unified: n_batch       = 2048
0.00.463.759 I llama_context_unified: n_ubatch      = 512
0.00.463.760 I llama_context_unified: flash_attn    = 0
0.00.463.760 I llama_context_unified: freq_base     = 10000.0
0.00.463.761 I llama_context_unified: freq_scale    = 1
0.00.463.763 I ggml_metal_init: allocating
0.00.463.808 I ggml_metal_init: found device: Apple M4
0.00.463.820 I ggml_metal_init: picking default device: Apple M4
0.00.465.605 I ggml_metal_init: using embedded metal library
0.00.471.655 I ggml_metal_init: GPU name:   Apple M4
0.00.471.660 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.471.660 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.471.661 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.471.662 I ggml_metal_init: simdgroup reduction   = true
0.00.471.662 I ggml_metal_init: simdgroup matrix mul. = true
0.00.471.662 I ggml_metal_init: has residency sets    = true
0.00.471.663 I ggml_metal_init: has bfloat            = true
0.00.471.663 I ggml_metal_init: use bfloat            = true
0.00.471.664 I ggml_metal_init: hasUnifiedMemory      = true
0.00.471.670 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.491.235 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.548.349 I init:      Metal KV buffer size =   384.00 MiB
0.00.548.355 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.548.376 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.553.831 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.553.833 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.553.833 I llama_context_unified: graph nodes  = 967
0.00.553.833 I llama_context_unified: graph splits = 2
0.00.553.839 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.553.969 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.553.970 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.609.339 I main: llama threadpool init, n_threads = 4
0.00.609.382 I 
0.00.609.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.609.405 I 
0.00.609.580 I sampler seed: 1234
0.00.609.585 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.609.596 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.609.597 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.609.597 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.767 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52167.52 tokens per second)
0.01.342.767 I llama_perf_context_print:        load time =     599.38 ms
0.01.342.768 I llama_perf_context_print: prompt eval time =      40.20 ms /     7 tokens (    5.74 ms per token,   174.14 tokens per second)
0.01.342.769 I llama_perf_context_print:        eval time =     690.06 ms /    63 runs   (   10.95 ms per token,    91.30 tokens per second)
0.01.342.769 I llama_perf_context_print:       total time =     734.15 ms /    70 tokens
0.01.346.395 I ggml_metal_free: deallocating

real	0m1.362s
user	0m0.110s
sys	0m0.203s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.532 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.056 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.058 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.058 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.059 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.059 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.059 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.062 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.902 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.664 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.665 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.666 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.667 I llama_model_loader: - type  f32:  194 tensors
0.00.023.667 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.667 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.668 I llama_model_loader: - type q6_K:   13 tensors
0.00.023.668 I print_info: file format = GGUF V3 (latest)
0.00.023.669 I print_info: file type   = Q4_K - Medium
0.00.023.673 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.031.633 I load: special tokens cache size = 25
0.00.037.708 I load: token to piece cache size = 0.2984 MB
0.00.037.722 I print_info: arch             = gptneox
0.00.037.723 I print_info: vocab_only       = 0
0.00.037.723 I print_info: n_ctx_train      = 2048
0.00.037.723 I print_info: n_embd           = 2048
0.00.037.723 I print_info: n_layer          = 24
0.00.037.726 I print_info: n_head           = 16
0.00.037.727 I print_info: n_head_kv        = 16
0.00.037.727 I print_info: n_rot            = 32
0.00.037.727 I print_info: n_swa            = 0
0.00.037.728 I print_info: n_embd_head_k    = 128
0.00.037.729 I print_info: n_embd_head_v    = 128
0.00.037.731 I print_info: n_gqa            = 1
0.00.037.731 I print_info: n_embd_k_gqa     = 2048
0.00.037.732 I print_info: n_embd_v_gqa     = 2048
0.00.037.733 I print_info: f_norm_eps       = 1.0e-05
0.00.037.733 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.733 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.733 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.734 I print_info: f_logit_scale    = 0.0e+00
0.00.037.734 I print_info: n_ff             = 8192
0.00.037.734 I print_info: n_expert         = 0
0.00.037.734 I print_info: n_expert_used    = 0
0.00.037.734 I print_info: causal attn      = 1
0.00.037.735 I print_info: pooling type     = 0
0.00.037.735 I print_info: rope type        = 2
0.00.037.735 I print_info: rope scaling     = linear
0.00.037.735 I print_info: freq_base_train  = 10000.0
0.00.037.735 I print_info: freq_scale_train = 1
0.00.037.736 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.736 I print_info: rope_finetuned   = unknown
0.00.037.736 I print_info: ssm_d_conv       = 0
0.00.037.736 I print_info: ssm_d_inner      = 0
0.00.037.736 I print_info: ssm_d_state      = 0
0.00.037.736 I print_info: ssm_dt_rank      = 0
0.00.037.736 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.736 I print_info: model type       = 1.4B
0.00.037.737 I print_info: model params     = 1.41 B
0.00.037.737 I print_info: general.name     = 1.4B
0.00.037.737 I print_info: vocab type       = BPE
0.00.037.738 I print_info: n_vocab          = 50304
0.00.037.738 I print_info: n_merges         = 50009
0.00.037.739 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.739 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.740 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.740 I print_info: LF token         = 187 'Ċ'
0.00.037.740 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.740 I print_info: max token length = 1024
0.00.037.741 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.522.512 I load_tensors: offloading 24 repeating layers to GPU
0.00.522.526 I load_tensors: offloading output layer to GPU
0.00.522.527 I load_tensors: offloaded 25/25 layers to GPU
0.00.522.560 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.522.562 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.524.237 I llama_context_unified: n_seq_max     = 1
0.00.524.242 I llama_context_unified: n_ctx         = 2048
0.00.524.243 I llama_context_unified: n_ctx_per_seq = 2048
0.00.524.243 I llama_context_unified: n_batch       = 2048
0.00.524.244 I llama_context_unified: n_ubatch      = 512
0.00.524.244 I llama_context_unified: flash_attn    = 0
0.00.524.246 I llama_context_unified: freq_base     = 10000.0
0.00.524.247 I llama_context_unified: freq_scale    = 1
0.00.524.250 I ggml_metal_init: allocating
0.00.524.341 I ggml_metal_init: found device: Apple M4
0.00.524.355 I ggml_metal_init: picking default device: Apple M4
0.00.526.206 I ggml_metal_init: using embedded metal library
0.00.532.964 I ggml_metal_init: GPU name:   Apple M4
0.00.532.969 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.532.970 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.532.971 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.532.972 I ggml_metal_init: simdgroup reduction   = true
0.00.532.972 I ggml_metal_init: simdgroup matrix mul. = true
0.00.532.973 I ggml_metal_init: has residency sets    = true
0.00.532.973 I ggml_metal_init: has bfloat            = true
0.00.532.973 I ggml_metal_init: use bfloat            = true
0.00.532.974 I ggml_metal_init: hasUnifiedMemory      = true
0.00.532.978 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.551.046 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.604.236 I init:      Metal KV buffer size =   384.00 MiB
0.00.604.242 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.604.265 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.608.264 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.608.266 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.608.266 I llama_context_unified: graph nodes  = 967
0.00.608.266 I llama_context_unified: graph splits = 2
0.00.608.273 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.608.400 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.608.400 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.270 I main: llama threadpool init, n_threads = 4
0.00.657.312 I 
0.00.657.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.331 I 
0.00.657.509 I sampler seed: 1234
0.00.657.514 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.657.525 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.657.525 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.657.525 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.417.272 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 49067.04 tokens per second)
0.01.417.272 I llama_perf_context_print:        load time =     648.05 ms
0.01.417.273 I llama_perf_context_print: prompt eval time =      46.73 ms /     7 tokens (    6.68 ms per token,   149.80 tokens per second)
0.01.417.274 I llama_perf_context_print:        eval time =     710.52 ms /    63 runs   (   11.28 ms per token,    88.67 tokens per second)
0.01.417.274 I llama_perf_context_print:       total time =     760.69 ms /    70 tokens
0.01.419.978 I ggml_metal_free: deallocating

real	0m1.434s
user	0m0.108s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.300 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.992 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.998 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.000 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.001 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.001 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.004 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.004 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.006 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.014 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.015 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.016 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.017 I llama_model_loader: - type  f32:  194 tensors
0.00.025.018 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.018 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.019 I print_info: file format = GGUF V3 (latest)
0.00.025.019 I print_info: file type   = Q5_K - Medium
0.00.025.020 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.862 I load: special tokens cache size = 25
0.00.038.940 I load: token to piece cache size = 0.2984 MB
0.00.038.954 I print_info: arch             = gptneox
0.00.038.955 I print_info: vocab_only       = 0
0.00.038.955 I print_info: n_ctx_train      = 2048
0.00.038.955 I print_info: n_embd           = 2048
0.00.038.955 I print_info: n_layer          = 24
0.00.038.958 I print_info: n_head           = 16
0.00.038.958 I print_info: n_head_kv        = 16
0.00.038.959 I print_info: n_rot            = 32
0.00.038.959 I print_info: n_swa            = 0
0.00.038.959 I print_info: n_embd_head_k    = 128
0.00.038.959 I print_info: n_embd_head_v    = 128
0.00.038.960 I print_info: n_gqa            = 1
0.00.038.960 I print_info: n_embd_k_gqa     = 2048
0.00.038.961 I print_info: n_embd_v_gqa     = 2048
0.00.038.961 I print_info: f_norm_eps       = 1.0e-05
0.00.038.962 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.962 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.962 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.962 I print_info: f_logit_scale    = 0.0e+00
0.00.038.963 I print_info: n_ff             = 8192
0.00.038.963 I print_info: n_expert         = 0
0.00.038.963 I print_info: n_expert_used    = 0
0.00.038.964 I print_info: causal attn      = 1
0.00.038.965 I print_info: pooling type     = 0
0.00.038.965 I print_info: rope type        = 2
0.00.038.969 I print_info: rope scaling     = linear
0.00.038.969 I print_info: freq_base_train  = 10000.0
0.00.038.970 I print_info: freq_scale_train = 1
0.00.038.970 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.970 I print_info: rope_finetuned   = unknown
0.00.038.970 I print_info: ssm_d_conv       = 0
0.00.038.970 I print_info: ssm_d_inner      = 0
0.00.038.970 I print_info: ssm_d_state      = 0
0.00.038.970 I print_info: ssm_dt_rank      = 0
0.00.038.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.971 I print_info: model type       = 1.4B
0.00.038.971 I print_info: model params     = 1.41 B
0.00.038.971 I print_info: general.name     = 1.4B
0.00.038.971 I print_info: vocab type       = BPE
0.00.038.972 I print_info: n_vocab          = 50304
0.00.038.972 I print_info: n_merges         = 50009
0.00.038.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.973 I print_info: LF token         = 187 'Ċ'
0.00.038.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.973 I print_info: max token length = 1024
0.00.038.974 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.082 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.086 I load_tensors: offloading output layer to GPU
0.00.597.087 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.103 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.105 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.598.111 I llama_context_unified: n_seq_max     = 1
0.00.598.115 I llama_context_unified: n_ctx         = 2048
0.00.598.116 I llama_context_unified: n_ctx_per_seq = 2048
0.00.598.116 I llama_context_unified: n_batch       = 2048
0.00.598.117 I llama_context_unified: n_ubatch      = 512
0.00.598.117 I llama_context_unified: flash_attn    = 0
0.00.598.119 I llama_context_unified: freq_base     = 10000.0
0.00.598.120 I llama_context_unified: freq_scale    = 1
0.00.598.122 I ggml_metal_init: allocating
0.00.598.193 I ggml_metal_init: found device: Apple M4
0.00.598.215 I ggml_metal_init: picking default device: Apple M4
0.00.599.872 I ggml_metal_init: using embedded metal library
0.00.612.941 I ggml_metal_init: GPU name:   Apple M4
0.00.612.951 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.953 I ggml_metal_init: simdgroup reduction   = true
0.00.612.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.954 I ggml_metal_init: has residency sets    = true
0.00.612.954 I ggml_metal_init: has bfloat            = true
0.00.612.955 I ggml_metal_init: use bfloat            = true
0.00.612.956 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.958 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.249 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.304 I init:      Metal KV buffer size =   384.00 MiB
0.00.666.310 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.666.332 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.671.478 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.671.480 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.671.481 I llama_context_unified: graph nodes  = 967
0.00.671.481 I llama_context_unified: graph splits = 2
0.00.671.487 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.671.616 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.671.617 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.633 I main: llama threadpool init, n_threads = 4
0.00.732.674 I 
0.00.732.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.694 I 
0.00.732.887 I sampler seed: 1234
0.00.732.892 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.904 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.904 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.580.059 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49894.59 tokens per second)
0.01.580.060 I llama_perf_context_print:        load time =     722.60 ms
0.01.580.062 I llama_perf_context_print: prompt eval time =      51.15 ms /     7 tokens (    7.31 ms per token,   136.85 tokens per second)
0.01.580.062 I llama_perf_context_print:        eval time =     793.57 ms /    63 runs   (   12.60 ms per token,    79.39 tokens per second)
0.01.580.063 I llama_perf_context_print:       total time =     848.16 ms /    70 tokens
0.01.583.750 I ggml_metal_free: deallocating

real	0m1.600s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.811 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.350 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.351 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.087 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.132 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.890 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.891 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.891 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.892 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.892 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.892 I llama_model_loader: - type  f32:  194 tensors
0.00.024.893 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.893 I print_info: file format = GGUF V3 (latest)
0.00.024.894 I print_info: file type   = Q6_K
0.00.024.895 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.810 I load: special tokens cache size = 25
0.00.038.536 I load: token to piece cache size = 0.2984 MB
0.00.038.551 I print_info: arch             = gptneox
0.00.038.552 I print_info: vocab_only       = 0
0.00.038.552 I print_info: n_ctx_train      = 2048
0.00.038.552 I print_info: n_embd           = 2048
0.00.038.552 I print_info: n_layer          = 24
0.00.038.555 I print_info: n_head           = 16
0.00.038.556 I print_info: n_head_kv        = 16
0.00.038.556 I print_info: n_rot            = 32
0.00.038.556 I print_info: n_swa            = 0
0.00.038.556 I print_info: n_embd_head_k    = 128
0.00.038.556 I print_info: n_embd_head_v    = 128
0.00.038.557 I print_info: n_gqa            = 1
0.00.038.558 I print_info: n_embd_k_gqa     = 2048
0.00.038.558 I print_info: n_embd_v_gqa     = 2048
0.00.038.559 I print_info: f_norm_eps       = 1.0e-05
0.00.038.560 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.560 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.560 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.560 I print_info: f_logit_scale    = 0.0e+00
0.00.038.561 I print_info: n_ff             = 8192
0.00.038.561 I print_info: n_expert         = 0
0.00.038.561 I print_info: n_expert_used    = 0
0.00.038.561 I print_info: causal attn      = 1
0.00.038.561 I print_info: pooling type     = 0
0.00.038.562 I print_info: rope type        = 2
0.00.038.562 I print_info: rope scaling     = linear
0.00.038.562 I print_info: freq_base_train  = 10000.0
0.00.038.562 I print_info: freq_scale_train = 1
0.00.038.563 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.563 I print_info: rope_finetuned   = unknown
0.00.038.563 I print_info: ssm_d_conv       = 0
0.00.038.563 I print_info: ssm_d_inner      = 0
0.00.038.563 I print_info: ssm_d_state      = 0
0.00.038.563 I print_info: ssm_dt_rank      = 0
0.00.038.563 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.564 I print_info: model type       = 1.4B
0.00.038.564 I print_info: model params     = 1.41 B
0.00.038.564 I print_info: general.name     = 1.4B
0.00.038.567 I print_info: vocab type       = BPE
0.00.038.567 I print_info: n_vocab          = 50304
0.00.038.567 I print_info: n_merges         = 50009
0.00.038.567 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.567 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.567 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.567 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.568 I print_info: LF token         = 187 'Ċ'
0.00.038.568 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.568 I print_info: max token length = 1024
0.00.038.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.800.790 I load_tensors: offloading 24 repeating layers to GPU
0.00.800.806 I load_tensors: offloading output layer to GPU
0.00.800.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.800.839 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.800.841 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.801.764 I llama_context_unified: n_seq_max     = 1
0.00.801.766 I llama_context_unified: n_ctx         = 2048
0.00.801.767 I llama_context_unified: n_ctx_per_seq = 2048
0.00.801.767 I llama_context_unified: n_batch       = 2048
0.00.801.767 I llama_context_unified: n_ubatch      = 512
0.00.801.767 I llama_context_unified: flash_attn    = 0
0.00.801.769 I llama_context_unified: freq_base     = 10000.0
0.00.801.769 I llama_context_unified: freq_scale    = 1
0.00.801.770 I ggml_metal_init: allocating
0.00.801.812 I ggml_metal_init: found device: Apple M4
0.00.801.820 I ggml_metal_init: picking default device: Apple M4
0.00.802.797 I ggml_metal_init: using embedded metal library
0.00.807.667 I ggml_metal_init: GPU name:   Apple M4
0.00.807.670 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.807.670 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.807.671 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.807.671 I ggml_metal_init: simdgroup reduction   = true
0.00.807.671 I ggml_metal_init: simdgroup matrix mul. = true
0.00.807.672 I ggml_metal_init: has residency sets    = true
0.00.807.672 I ggml_metal_init: has bfloat            = true
0.00.807.672 I ggml_metal_init: use bfloat            = true
0.00.807.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.807.675 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.821.864 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.861.166 I init:      Metal KV buffer size =   384.00 MiB
0.00.861.173 I llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.861.208 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.865.483 I llama_context_unified:      Metal compute buffer size =   102.25 MiB
0.00.865.485 I llama_context_unified:        CPU compute buffer size =     8.01 MiB
0.00.865.485 I llama_context_unified: graph nodes  = 967
0.00.865.486 I llama_context_unified: graph splits = 2
0.00.865.491 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.865.619 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.865.620 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.933.806 I main: llama threadpool init, n_threads = 4
0.00.933.849 I 
0.00.933.870 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.933.871 I 
0.00.934.047 I sampler seed: 1234
0.00.934.051 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.934.085 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.934.088 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.934.088 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.811.938 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52437.22 tokens per second)
0.01.811.939 I llama_perf_context_print:        load time =     924.30 ms
0.01.811.939 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.50 tokens per second)
0.01.811.940 I llama_perf_context_print:        eval time =     820.42 ms /    63 runs   (   13.02 ms per token,    76.79 tokens per second)
0.01.811.940 I llama_perf_context_print:       total time =     878.83 ms /    70 tokens
0.01.815.817 I ggml_metal_free: deallocating

real	0m1.832s
user	0m0.103s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.760 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.201 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.038.643 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.649 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.651 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.651 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.659 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.660 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.660 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.665 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.666 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.667 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.671 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.673 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.047.827 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.049.874 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.196 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.197 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.198 I llama_model_loader: - type  f32:  194 tensors
0.00.057.199 I llama_model_loader: - type  f16:   98 tensors
0.00.057.199 I print_info: file format = GGUF V3 (latest)
0.00.057.200 I print_info: file type   = all F32 (guessed)
0.00.057.202 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.070.027 I load: special tokens cache size = 25
0.00.078.498 I load: token to piece cache size = 0.2984 MB
0.00.078.513 I print_info: arch             = gptneox
0.00.078.515 I print_info: vocab_only       = 0
0.00.078.515 I print_info: n_ctx_train      = 2048
0.00.078.515 I print_info: n_embd           = 2048
0.00.078.515 I print_info: n_layer          = 24
0.00.078.519 I print_info: n_head           = 16
0.00.078.519 I print_info: n_head_kv        = 16
0.00.078.520 I print_info: n_rot            = 32
0.00.078.520 I print_info: n_swa            = 0
0.00.078.520 I print_info: n_embd_head_k    = 128
0.00.078.520 I print_info: n_embd_head_v    = 128
0.00.078.521 I print_info: n_gqa            = 1
0.00.078.522 I print_info: n_embd_k_gqa     = 2048
0.00.078.522 I print_info: n_embd_v_gqa     = 2048
0.00.078.523 I print_info: f_norm_eps       = 1.0e-05
0.00.078.524 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.524 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.524 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.524 I print_info: f_logit_scale    = 0.0e+00
0.00.078.525 I print_info: n_ff             = 8192
0.00.078.525 I print_info: n_expert         = 0
0.00.078.525 I print_info: n_expert_used    = 0
0.00.078.525 I print_info: causal attn      = 1
0.00.078.525 I print_info: pooling type     = 0
0.00.078.526 I print_info: rope type        = 2
0.00.078.526 I print_info: rope scaling     = linear
0.00.078.526 I print_info: freq_base_train  = 10000.0
0.00.078.526 I print_info: freq_scale_train = 1
0.00.078.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.527 I print_info: rope_finetuned   = unknown
0.00.078.527 I print_info: ssm_d_conv       = 0
0.00.078.527 I print_info: ssm_d_inner      = 0
0.00.078.527 I print_info: ssm_d_state      = 0
0.00.078.528 I print_info: ssm_dt_rank      = 0
0.00.078.528 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.528 I print_info: model type       = 1.4B
0.00.078.528 I print_info: model params     = 1.41 B
0.00.078.530 I print_info: general.name     = 1.4B
0.00.078.530 I print_info: vocab type       = BPE
0.00.078.530 I print_info: n_vocab          = 50304
0.00.078.530 I print_info: n_merges         = 50009
0.00.078.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.531 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.531 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.531 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.532 I print_info: LF token         = 187 'Ċ'
0.00.078.532 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.532 I print_info: max token length = 1024
0.00.078.533 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.161.079 I load_tensors: offloading 24 repeating layers to GPU
0.01.161.086 I load_tensors: offloading output layer to GPU
0.01.161.086 I load_tensors: offloaded 25/25 layers to GPU
0.01.161.115 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.161.116 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.161.838 I llama_context_unified: n_seq_max     = 1
0.01.161.840 I llama_context_unified: n_ctx         = 128
0.01.161.840 I llama_context_unified: n_ctx_per_seq = 128
0.01.161.840 I llama_context_unified: n_batch       = 128
0.01.161.840 I llama_context_unified: n_ubatch      = 128
0.01.161.841 I llama_context_unified: flash_attn    = 0
0.01.161.841 I llama_context_unified: freq_base     = 10000.0
0.01.161.841 I llama_context_unified: freq_scale    = 1
0.01.161.842 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.161.843 I ggml_metal_init: allocating
0.01.161.886 I ggml_metal_init: found device: Apple M4
0.01.161.892 I ggml_metal_init: picking default device: Apple M4
0.01.162.927 I ggml_metal_init: using embedded metal library
0.01.166.825 I ggml_metal_init: GPU name:   Apple M4
0.01.166.828 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.166.829 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.166.829 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.166.830 I ggml_metal_init: simdgroup reduction   = true
0.01.166.830 I ggml_metal_init: simdgroup matrix mul. = true
0.01.166.830 I ggml_metal_init: has residency sets    = true
0.01.166.830 I ggml_metal_init: has bfloat            = true
0.01.166.830 I ggml_metal_init: use bfloat            = true
0.01.166.831 I ggml_metal_init: hasUnifiedMemory      = true
0.01.166.832 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.177.482 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.179.289 I init:      Metal KV buffer size =    24.00 MiB
0.01.179.294 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.179.307 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.180.881 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.01.180.883 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.01.180.883 I llama_context_unified: graph nodes  = 967
0.01.180.883 I llama_context_unified: graph splits = 2
0.01.180.885 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.180.885 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.216.096 I 
0.01.216.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.216.159 I perplexity: tokenizing the input ..
0.01.221.315 I perplexity: tokenization took 5.154 ms
0.01.221.336 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.340.048 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.341.404 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.341.419 I llama_perf_context_print:        load time =    1191.88 ms
0.01.341.420 I llama_perf_context_print: prompt eval time =     118.45 ms /   128 tokens (    0.93 ms per token,  1080.66 tokens per second)
0.01.341.421 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.341.421 I llama_perf_context_print:       total time =     125.32 ms /   129 tokens
0.01.341.970 I ggml_metal_free: deallocating

real	0m1.528s
user	0m0.100s
sys	0m0.242s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.190 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.426 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.432 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.434 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.434 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.438 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.439 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.439 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.440 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.440 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.440 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.441 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.442 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.443 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.444 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.261 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.246 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.016 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.017 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.017 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.018 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.018 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.019 I llama_model_loader: - type  f32:  194 tensors
0.00.025.019 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.020 I print_info: file format = GGUF V3 (latest)
0.00.025.020 I print_info: file type   = Q8_0
0.00.025.021 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.191 I load: special tokens cache size = 25
0.00.039.334 I load: token to piece cache size = 0.2984 MB
0.00.039.352 I print_info: arch             = gptneox
0.00.039.353 I print_info: vocab_only       = 0
0.00.039.353 I print_info: n_ctx_train      = 2048
0.00.039.353 I print_info: n_embd           = 2048
0.00.039.353 I print_info: n_layer          = 24
0.00.039.357 I print_info: n_head           = 16
0.00.039.358 I print_info: n_head_kv        = 16
0.00.039.358 I print_info: n_rot            = 32
0.00.039.358 I print_info: n_swa            = 0
0.00.039.358 I print_info: n_embd_head_k    = 128
0.00.039.358 I print_info: n_embd_head_v    = 128
0.00.039.359 I print_info: n_gqa            = 1
0.00.039.360 I print_info: n_embd_k_gqa     = 2048
0.00.039.360 I print_info: n_embd_v_gqa     = 2048
0.00.039.361 I print_info: f_norm_eps       = 1.0e-05
0.00.039.361 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.361 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.361 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.361 I print_info: f_logit_scale    = 0.0e+00
0.00.039.362 I print_info: n_ff             = 8192
0.00.039.362 I print_info: n_expert         = 0
0.00.039.362 I print_info: n_expert_used    = 0
0.00.039.362 I print_info: causal attn      = 1
0.00.039.362 I print_info: pooling type     = 0
0.00.039.362 I print_info: rope type        = 2
0.00.039.363 I print_info: rope scaling     = linear
0.00.039.363 I print_info: freq_base_train  = 10000.0
0.00.039.363 I print_info: freq_scale_train = 1
0.00.039.363 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.364 I print_info: rope_finetuned   = unknown
0.00.039.364 I print_info: ssm_d_conv       = 0
0.00.039.364 I print_info: ssm_d_inner      = 0
0.00.039.364 I print_info: ssm_d_state      = 0
0.00.039.364 I print_info: ssm_dt_rank      = 0
0.00.039.364 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.364 I print_info: model type       = 1.4B
0.00.039.366 I print_info: model params     = 1.41 B
0.00.039.368 I print_info: general.name     = 1.4B
0.00.039.369 I print_info: vocab type       = BPE
0.00.039.369 I print_info: n_vocab          = 50304
0.00.039.369 I print_info: n_merges         = 50009
0.00.039.369 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.371 I print_info: LF token         = 187 'Ċ'
0.00.039.371 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.372 I print_info: max token length = 1024
0.00.039.372 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.825.533 I load_tensors: offloading 24 repeating layers to GPU
0.00.825.540 I load_tensors: offloading output layer to GPU
0.00.825.541 I load_tensors: offloaded 25/25 layers to GPU
0.00.825.568 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.825.571 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.826.976 I llama_context_unified: n_seq_max     = 1
0.00.826.978 I llama_context_unified: n_ctx         = 128
0.00.826.978 I llama_context_unified: n_ctx_per_seq = 128
0.00.826.978 I llama_context_unified: n_batch       = 128
0.00.826.979 I llama_context_unified: n_ubatch      = 128
0.00.826.979 I llama_context_unified: flash_attn    = 0
0.00.826.980 I llama_context_unified: freq_base     = 10000.0
0.00.826.981 I llama_context_unified: freq_scale    = 1
0.00.826.981 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.826.982 I ggml_metal_init: allocating
0.00.827.041 I ggml_metal_init: found device: Apple M4
0.00.827.051 I ggml_metal_init: picking default device: Apple M4
0.00.828.265 I ggml_metal_init: using embedded metal library
0.00.833.437 I ggml_metal_init: GPU name:   Apple M4
0.00.833.441 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.833.442 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.833.442 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.833.443 I ggml_metal_init: simdgroup reduction   = true
0.00.833.443 I ggml_metal_init: simdgroup matrix mul. = true
0.00.833.443 I ggml_metal_init: has residency sets    = true
0.00.833.443 I ggml_metal_init: has bfloat            = true
0.00.833.444 I ggml_metal_init: use bfloat            = true
0.00.833.445 I ggml_metal_init: hasUnifiedMemory      = true
0.00.833.446 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.848.565 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.851.927 I init:      Metal KV buffer size =    24.00 MiB
0.00.851.930 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.851.995 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.855.025 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.855.027 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.855.027 I llama_context_unified: graph nodes  = 967
0.00.855.028 I llama_context_unified: graph splits = 2
0.00.855.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.855.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.881.481 I 
0.00.881.558 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.881.578 I perplexity: tokenizing the input ..
0.00.888.768 I perplexity: tokenization took 7.186 ms
0.00.888.790 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.027.418 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.028.759 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.028.778 I llama_perf_context_print:        load time =     872.28 ms
0.01.028.780 I llama_perf_context_print: prompt eval time =     137.69 ms /   128 tokens (    1.08 ms per token,   929.59 tokens per second)
0.01.028.782 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.028.782 I llama_perf_context_print:       total time =     147.30 ms /   129 tokens
0.01.029.382 I ggml_metal_free: deallocating

real	0m1.043s
user	0m0.076s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.016 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.394 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.400 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.401 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.404 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.404 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.404 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.405 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.408 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.410 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.293 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.325 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.188 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.188 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.189 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.189 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.190 I llama_model_loader: - type  f32:  194 tensors
0.00.026.190 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.190 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.191 I print_info: file format = GGUF V3 (latest)
0.00.026.191 I print_info: file type   = Q4_0
0.00.026.193 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.860 I load: special tokens cache size = 25
0.00.041.079 I load: token to piece cache size = 0.2984 MB
0.00.041.096 I print_info: arch             = gptneox
0.00.041.097 I print_info: vocab_only       = 0
0.00.041.097 I print_info: n_ctx_train      = 2048
0.00.041.097 I print_info: n_embd           = 2048
0.00.041.097 I print_info: n_layer          = 24
0.00.041.101 I print_info: n_head           = 16
0.00.041.102 I print_info: n_head_kv        = 16
0.00.041.102 I print_info: n_rot            = 32
0.00.041.103 I print_info: n_swa            = 0
0.00.041.104 I print_info: n_embd_head_k    = 128
0.00.041.104 I print_info: n_embd_head_v    = 128
0.00.041.105 I print_info: n_gqa            = 1
0.00.041.105 I print_info: n_embd_k_gqa     = 2048
0.00.041.108 I print_info: n_embd_v_gqa     = 2048
0.00.041.108 I print_info: f_norm_eps       = 1.0e-05
0.00.041.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.109 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.109 I print_info: f_logit_scale    = 0.0e+00
0.00.041.111 I print_info: n_ff             = 8192
0.00.041.111 I print_info: n_expert         = 0
0.00.041.111 I print_info: n_expert_used    = 0
0.00.041.111 I print_info: causal attn      = 1
0.00.041.111 I print_info: pooling type     = 0
0.00.041.111 I print_info: rope type        = 2
0.00.041.112 I print_info: rope scaling     = linear
0.00.041.112 I print_info: freq_base_train  = 10000.0
0.00.041.112 I print_info: freq_scale_train = 1
0.00.041.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.113 I print_info: rope_finetuned   = unknown
0.00.041.113 I print_info: ssm_d_conv       = 0
0.00.041.113 I print_info: ssm_d_inner      = 0
0.00.041.113 I print_info: ssm_d_state      = 0
0.00.041.113 I print_info: ssm_dt_rank      = 0
0.00.041.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.113 I print_info: model type       = 1.4B
0.00.041.114 I print_info: model params     = 1.41 B
0.00.041.114 I print_info: general.name     = 1.4B
0.00.041.114 I print_info: vocab type       = BPE
0.00.041.114 I print_info: n_vocab          = 50304
0.00.041.115 I print_info: n_merges         = 50009
0.00.041.115 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.115 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.115 I print_info: LF token         = 187 'Ċ'
0.00.041.116 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.116 I print_info: max token length = 1024
0.00.041.117 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.835 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.848 I load_tensors: offloading output layer to GPU
0.00.607.849 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.879 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.607.880 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.609.406 I llama_context_unified: n_seq_max     = 1
0.00.609.411 I llama_context_unified: n_ctx         = 128
0.00.609.412 I llama_context_unified: n_ctx_per_seq = 128
0.00.609.412 I llama_context_unified: n_batch       = 128
0.00.609.412 I llama_context_unified: n_ubatch      = 128
0.00.609.413 I llama_context_unified: flash_attn    = 0
0.00.609.415 I llama_context_unified: freq_base     = 10000.0
0.00.609.416 I llama_context_unified: freq_scale    = 1
0.00.609.416 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.418 I ggml_metal_init: allocating
0.00.609.501 I ggml_metal_init: found device: Apple M4
0.00.609.515 I ggml_metal_init: picking default device: Apple M4
0.00.611.288 I ggml_metal_init: using embedded metal library
0.00.616.605 I ggml_metal_init: GPU name:   Apple M4
0.00.616.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.619 I ggml_metal_init: simdgroup reduction   = true
0.00.616.620 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.620 I ggml_metal_init: has residency sets    = true
0.00.616.620 I ggml_metal_init: has bfloat            = true
0.00.616.620 I ggml_metal_init: use bfloat            = true
0.00.616.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.773 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.640.482 I init:      Metal KV buffer size =    24.00 MiB
0.00.640.488 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.555 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.643.642 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.643.644 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.643.644 I llama_context_unified: graph nodes  = 967
0.00.643.644 I llama_context_unified: graph splits = 2
0.00.643.649 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.649 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.718 I 
0.00.670.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.818 I perplexity: tokenizing the input ..
0.00.675.720 I perplexity: tokenization took 4.901 ms
0.00.675.730 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.798.884 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.800.260 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.800.278 I llama_perf_context_print:        load time =     660.69 ms
0.00.800.279 I llama_perf_context_print: prompt eval time =     122.90 ms /   128 tokens (    0.96 ms per token,  1041.51 tokens per second)
0.00.800.279 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.280 I llama_perf_context_print:       total time =     129.56 ms /   129 tokens
0.00.800.822 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.221 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.227 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.229 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.229 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.230 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.231 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.235 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.240 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.240 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.240 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.216 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.096 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.097 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.097 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.097 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.098 I llama_model_loader: - type  f32:  194 tensors
0.00.025.098 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.099 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.099 I print_info: file format = GGUF V3 (latest)
0.00.025.100 I print_info: file type   = Q4_1
0.00.025.101 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.510 I load: special tokens cache size = 25
0.00.039.437 I load: token to piece cache size = 0.2984 MB
0.00.039.455 I print_info: arch             = gptneox
0.00.039.456 I print_info: vocab_only       = 0
0.00.039.456 I print_info: n_ctx_train      = 2048
0.00.039.456 I print_info: n_embd           = 2048
0.00.039.456 I print_info: n_layer          = 24
0.00.039.460 I print_info: n_head           = 16
0.00.039.461 I print_info: n_head_kv        = 16
0.00.039.461 I print_info: n_rot            = 32
0.00.039.461 I print_info: n_swa            = 0
0.00.039.461 I print_info: n_embd_head_k    = 128
0.00.039.461 I print_info: n_embd_head_v    = 128
0.00.039.462 I print_info: n_gqa            = 1
0.00.039.463 I print_info: n_embd_k_gqa     = 2048
0.00.039.465 I print_info: n_embd_v_gqa     = 2048
0.00.039.466 I print_info: f_norm_eps       = 1.0e-05
0.00.039.466 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.467 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.467 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.467 I print_info: f_logit_scale    = 0.0e+00
0.00.039.468 I print_info: n_ff             = 8192
0.00.039.468 I print_info: n_expert         = 0
0.00.039.468 I print_info: n_expert_used    = 0
0.00.039.469 I print_info: causal attn      = 1
0.00.039.469 I print_info: pooling type     = 0
0.00.039.469 I print_info: rope type        = 2
0.00.039.469 I print_info: rope scaling     = linear
0.00.039.470 I print_info: freq_base_train  = 10000.0
0.00.039.470 I print_info: freq_scale_train = 1
0.00.039.470 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.471 I print_info: rope_finetuned   = unknown
0.00.039.472 I print_info: ssm_d_conv       = 0
0.00.039.472 I print_info: ssm_d_inner      = 0
0.00.039.472 I print_info: ssm_d_state      = 0
0.00.039.472 I print_info: ssm_dt_rank      = 0
0.00.039.472 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.472 I print_info: model type       = 1.4B
0.00.039.473 I print_info: model params     = 1.41 B
0.00.039.473 I print_info: general.name     = 1.4B
0.00.039.473 I print_info: vocab type       = BPE
0.00.039.473 I print_info: n_vocab          = 50304
0.00.039.473 I print_info: n_merges         = 50009
0.00.039.473 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.474 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.474 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.474 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.474 I print_info: LF token         = 187 'Ċ'
0.00.039.474 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.475 I print_info: max token length = 1024
0.00.039.475 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.664.120 I load_tensors: offloading 24 repeating layers to GPU
0.00.664.135 I load_tensors: offloading output layer to GPU
0.00.664.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.664.169 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.664.170 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.665.927 I llama_context_unified: n_seq_max     = 1
0.00.665.929 I llama_context_unified: n_ctx         = 128
0.00.665.930 I llama_context_unified: n_ctx_per_seq = 128
0.00.665.930 I llama_context_unified: n_batch       = 128
0.00.665.931 I llama_context_unified: n_ubatch      = 128
0.00.665.931 I llama_context_unified: flash_attn    = 0
0.00.665.933 I llama_context_unified: freq_base     = 10000.0
0.00.665.934 I llama_context_unified: freq_scale    = 1
0.00.665.935 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.937 I ggml_metal_init: allocating
0.00.666.033 I ggml_metal_init: found device: Apple M4
0.00.666.048 I ggml_metal_init: picking default device: Apple M4
0.00.667.868 I ggml_metal_init: using embedded metal library
0.00.674.629 I ggml_metal_init: GPU name:   Apple M4
0.00.674.637 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.638 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.639 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.639 I ggml_metal_init: simdgroup reduction   = true
0.00.674.640 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.640 I ggml_metal_init: has residency sets    = true
0.00.674.640 I ggml_metal_init: has bfloat            = true
0.00.674.640 I ggml_metal_init: use bfloat            = true
0.00.674.641 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.646 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.669 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.696.117 I init:      Metal KV buffer size =    24.00 MiB
0.00.696.121 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.696.147 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.699.367 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.699.370 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.699.370 I llama_context_unified: graph nodes  = 967
0.00.699.371 I llama_context_unified: graph splits = 2
0.00.699.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.376 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.362 I 
0.00.723.427 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.723.447 I perplexity: tokenizing the input ..
0.00.730.282 I perplexity: tokenization took 6.831 ms
0.00.730.300 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.858.660 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.859.961 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.859.976 I llama_perf_context_print:        load time =     714.51 ms
0.00.859.977 I llama_perf_context_print: prompt eval time =     127.49 ms /   128 tokens (    1.00 ms per token,  1003.98 tokens per second)
0.00.859.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.859.978 I llama_perf_context_print:       total time =     136.62 ms /   129 tokens
0.00.860.530 I ggml_metal_free: deallocating

real	0m0.874s
user	0m0.080s
sys	0m0.124s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.013.071 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.020.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.521 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.525 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.525 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.525 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.526 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.526 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.527 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.527 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.528 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.528 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.529 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.531 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.531 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.531 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.379 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.260 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.262 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.262 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.263 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.263 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.263 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.264 I llama_model_loader: - type  f32:  194 tensors
0.00.029.264 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.265 I llama_model_loader: - type q6_K:    1 tensors
0.00.029.265 I print_info: file format = GGUF V3 (latest)
0.00.029.266 I print_info: file type   = Q5_0
0.00.029.267 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.037.690 I load: special tokens cache size = 25
0.00.043.783 I load: token to piece cache size = 0.2984 MB
0.00.043.800 I print_info: arch             = gptneox
0.00.043.800 I print_info: vocab_only       = 0
0.00.043.801 I print_info: n_ctx_train      = 2048
0.00.043.801 I print_info: n_embd           = 2048
0.00.043.801 I print_info: n_layer          = 24
0.00.043.805 I print_info: n_head           = 16
0.00.043.806 I print_info: n_head_kv        = 16
0.00.043.806 I print_info: n_rot            = 32
0.00.043.806 I print_info: n_swa            = 0
0.00.043.806 I print_info: n_embd_head_k    = 128
0.00.043.806 I print_info: n_embd_head_v    = 128
0.00.043.807 I print_info: n_gqa            = 1
0.00.043.807 I print_info: n_embd_k_gqa     = 2048
0.00.043.809 I print_info: n_embd_v_gqa     = 2048
0.00.043.810 I print_info: f_norm_eps       = 1.0e-05
0.00.043.810 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.810 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.810 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.810 I print_info: f_logit_scale    = 0.0e+00
0.00.043.811 I print_info: n_ff             = 8192
0.00.043.811 I print_info: n_expert         = 0
0.00.043.811 I print_info: n_expert_used    = 0
0.00.043.812 I print_info: causal attn      = 1
0.00.043.812 I print_info: pooling type     = 0
0.00.043.812 I print_info: rope type        = 2
0.00.043.812 I print_info: rope scaling     = linear
0.00.043.812 I print_info: freq_base_train  = 10000.0
0.00.043.813 I print_info: freq_scale_train = 1
0.00.043.813 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.813 I print_info: rope_finetuned   = unknown
0.00.043.813 I print_info: ssm_d_conv       = 0
0.00.043.813 I print_info: ssm_d_inner      = 0
0.00.043.813 I print_info: ssm_d_state      = 0
0.00.043.813 I print_info: ssm_dt_rank      = 0
0.00.043.814 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.814 I print_info: model type       = 1.4B
0.00.043.814 I print_info: model params     = 1.41 B
0.00.043.816 I print_info: general.name     = 1.4B
0.00.043.816 I print_info: vocab type       = BPE
0.00.043.817 I print_info: n_vocab          = 50304
0.00.043.817 I print_info: n_merges         = 50009
0.00.043.817 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.817 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.817 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.817 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.818 I print_info: LF token         = 187 'Ċ'
0.00.043.818 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.818 I print_info: max token length = 1024
0.00.043.818 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.737.158 I load_tensors: offloading 24 repeating layers to GPU
0.00.737.177 I load_tensors: offloading output layer to GPU
0.00.737.177 I load_tensors: offloaded 25/25 layers to GPU
0.00.737.209 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.737.211 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.738.670 I llama_context_unified: n_seq_max     = 1
0.00.738.678 I llama_context_unified: n_ctx         = 128
0.00.738.678 I llama_context_unified: n_ctx_per_seq = 128
0.00.738.679 I llama_context_unified: n_batch       = 128
0.00.738.679 I llama_context_unified: n_ubatch      = 128
0.00.738.679 I llama_context_unified: flash_attn    = 0
0.00.738.681 I llama_context_unified: freq_base     = 10000.0
0.00.738.682 I llama_context_unified: freq_scale    = 1
0.00.738.683 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.738.685 I ggml_metal_init: allocating
0.00.738.772 I ggml_metal_init: found device: Apple M4
0.00.738.786 I ggml_metal_init: picking default device: Apple M4
0.00.740.453 I ggml_metal_init: using embedded metal library
0.00.744.793 I ggml_metal_init: GPU name:   Apple M4
0.00.744.799 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.744.800 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.744.800 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.744.801 I ggml_metal_init: simdgroup reduction   = true
0.00.744.801 I ggml_metal_init: simdgroup matrix mul. = true
0.00.744.801 I ggml_metal_init: has residency sets    = true
0.00.744.801 I ggml_metal_init: has bfloat            = true
0.00.744.801 I ggml_metal_init: use bfloat            = true
0.00.744.802 I ggml_metal_init: hasUnifiedMemory      = true
0.00.744.804 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.272 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.970 I init:      Metal KV buffer size =    24.00 MiB
0.00.756.973 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.756.991 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.758.566 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.758.568 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.758.568 I llama_context_unified: graph nodes  = 967
0.00.758.568 I llama_context_unified: graph splits = 2
0.00.758.570 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.758.571 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.786.370 I 
0.00.786.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.786.419 I perplexity: tokenizing the input ..
0.00.790.399 I perplexity: tokenization took 3.977 ms
0.00.790.416 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.932.205 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.933.427 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.933.442 I llama_perf_context_print:        load time =     773.30 ms
0.00.933.443 I llama_perf_context_print: prompt eval time =     141.56 ms /   128 tokens (    1.11 ms per token,   904.20 tokens per second)
0.00.933.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.933.444 I llama_perf_context_print:       total time =     147.07 ms /   129 tokens
0.00.933.992 I ggml_metal_free: deallocating

real	0m0.955s
user	0m0.066s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.719 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.480 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.483 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.486 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.488 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.488 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.180 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.181 I llama_model_loader: - type  f32:  194 tensors
0.00.024.182 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.182 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.183 I print_info: file format = GGUF V3 (latest)
0.00.024.183 I print_info: file type   = Q5_1
0.00.024.184 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.031.975 I load: special tokens cache size = 25
0.00.038.143 I load: token to piece cache size = 0.2984 MB
0.00.038.157 I print_info: arch             = gptneox
0.00.038.158 I print_info: vocab_only       = 0
0.00.038.158 I print_info: n_ctx_train      = 2048
0.00.038.159 I print_info: n_embd           = 2048
0.00.038.159 I print_info: n_layer          = 24
0.00.038.161 I print_info: n_head           = 16
0.00.038.162 I print_info: n_head_kv        = 16
0.00.038.162 I print_info: n_rot            = 32
0.00.038.163 I print_info: n_swa            = 0
0.00.038.163 I print_info: n_embd_head_k    = 128
0.00.038.163 I print_info: n_embd_head_v    = 128
0.00.038.164 I print_info: n_gqa            = 1
0.00.038.165 I print_info: n_embd_k_gqa     = 2048
0.00.038.165 I print_info: n_embd_v_gqa     = 2048
0.00.038.166 I print_info: f_norm_eps       = 1.0e-05
0.00.038.166 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.171 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.172 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.172 I print_info: f_logit_scale    = 0.0e+00
0.00.038.174 I print_info: n_ff             = 8192
0.00.038.174 I print_info: n_expert         = 0
0.00.038.176 I print_info: n_expert_used    = 0
0.00.038.176 I print_info: causal attn      = 1
0.00.038.176 I print_info: pooling type     = 0
0.00.038.176 I print_info: rope type        = 2
0.00.038.177 I print_info: rope scaling     = linear
0.00.038.177 I print_info: freq_base_train  = 10000.0
0.00.038.177 I print_info: freq_scale_train = 1
0.00.038.177 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.178 I print_info: rope_finetuned   = unknown
0.00.038.178 I print_info: ssm_d_conv       = 0
0.00.038.178 I print_info: ssm_d_inner      = 0
0.00.038.178 I print_info: ssm_d_state      = 0
0.00.038.178 I print_info: ssm_dt_rank      = 0
0.00.038.179 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.179 I print_info: model type       = 1.4B
0.00.038.180 I print_info: model params     = 1.41 B
0.00.038.180 I print_info: general.name     = 1.4B
0.00.038.180 I print_info: vocab type       = BPE
0.00.038.181 I print_info: n_vocab          = 50304
0.00.038.181 I print_info: n_merges         = 50009
0.00.038.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.183 I print_info: LF token         = 187 'Ċ'
0.00.038.183 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.183 I print_info: max token length = 1024
0.00.038.183 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.648.686 I load_tensors: offloading 24 repeating layers to GPU
0.00.648.689 I load_tensors: offloading output layer to GPU
0.00.648.690 I load_tensors: offloaded 25/25 layers to GPU
0.00.648.713 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.648.714 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.650.314 I llama_context_unified: n_seq_max     = 1
0.00.650.316 I llama_context_unified: n_ctx         = 128
0.00.650.317 I llama_context_unified: n_ctx_per_seq = 128
0.00.650.317 I llama_context_unified: n_batch       = 128
0.00.650.318 I llama_context_unified: n_ubatch      = 128
0.00.650.318 I llama_context_unified: flash_attn    = 0
0.00.650.319 I llama_context_unified: freq_base     = 10000.0
0.00.650.319 I llama_context_unified: freq_scale    = 1
0.00.650.320 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.650.322 I ggml_metal_init: allocating
0.00.650.373 I ggml_metal_init: found device: Apple M4
0.00.650.385 I ggml_metal_init: picking default device: Apple M4
0.00.651.854 I ggml_metal_init: using embedded metal library
0.00.657.817 I ggml_metal_init: GPU name:   Apple M4
0.00.657.820 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.820 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.821 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.822 I ggml_metal_init: simdgroup reduction   = true
0.00.657.822 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.822 I ggml_metal_init: has residency sets    = true
0.00.657.823 I ggml_metal_init: has bfloat            = true
0.00.657.823 I ggml_metal_init: use bfloat            = true
0.00.657.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.674.011 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.467 I init:      Metal KV buffer size =    24.00 MiB
0.00.677.471 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.677.496 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.680.841 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.680.843 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.680.843 I llama_context_unified: graph nodes  = 967
0.00.680.844 I llama_context_unified: graph splits = 2
0.00.680.847 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.680.850 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.631 I 
0.00.710.712 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.733 I perplexity: tokenizing the input ..
0.00.718.255 I perplexity: tokenization took 7.518 ms
0.00.718.278 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.854.450 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.855.788 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.855.803 I llama_perf_context_print:        load time =     701.90 ms
0.00.855.804 I llama_perf_context_print: prompt eval time =     135.24 ms /   128 tokens (    1.06 ms per token,   946.44 tokens per second)
0.00.855.805 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.805 I llama_perf_context_print:       total time =     145.18 ms /   129 tokens
0.00.856.343 I ggml_metal_free: deallocating

real	0m0.869s
user	0m0.078s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.905 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.368 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.372 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.372 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.372 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.374 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.376 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.377 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.379 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.379 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.379 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.323 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.294 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.296 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.296 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.297 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.297 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.298 I llama_model_loader: - type  f32:  194 tensors
0.00.025.298 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.298 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.299 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.299 I print_info: file format = GGUF V3 (latest)
0.00.025.299 I print_info: file type   = Q2_K - Medium
0.00.025.301 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.388 I load: special tokens cache size = 25
0.00.039.484 I load: token to piece cache size = 0.2984 MB
0.00.039.501 I print_info: arch             = gptneox
0.00.039.501 I print_info: vocab_only       = 0
0.00.039.502 I print_info: n_ctx_train      = 2048
0.00.039.502 I print_info: n_embd           = 2048
0.00.039.502 I print_info: n_layer          = 24
0.00.039.506 I print_info: n_head           = 16
0.00.039.506 I print_info: n_head_kv        = 16
0.00.039.506 I print_info: n_rot            = 32
0.00.039.507 I print_info: n_swa            = 0
0.00.039.507 I print_info: n_embd_head_k    = 128
0.00.039.507 I print_info: n_embd_head_v    = 128
0.00.039.507 I print_info: n_gqa            = 1
0.00.039.508 I print_info: n_embd_k_gqa     = 2048
0.00.039.509 I print_info: n_embd_v_gqa     = 2048
0.00.039.509 I print_info: f_norm_eps       = 1.0e-05
0.00.039.510 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.510 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.510 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.514 I print_info: f_logit_scale    = 0.0e+00
0.00.039.514 I print_info: n_ff             = 8192
0.00.039.515 I print_info: n_expert         = 0
0.00.039.515 I print_info: n_expert_used    = 0
0.00.039.515 I print_info: causal attn      = 1
0.00.039.515 I print_info: pooling type     = 0
0.00.039.515 I print_info: rope type        = 2
0.00.039.515 I print_info: rope scaling     = linear
0.00.039.516 I print_info: freq_base_train  = 10000.0
0.00.039.516 I print_info: freq_scale_train = 1
0.00.039.516 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.516 I print_info: rope_finetuned   = unknown
0.00.039.516 I print_info: ssm_d_conv       = 0
0.00.039.517 I print_info: ssm_d_inner      = 0
0.00.039.517 I print_info: ssm_d_state      = 0
0.00.039.517 I print_info: ssm_dt_rank      = 0
0.00.039.517 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.517 I print_info: model type       = 1.4B
0.00.039.517 I print_info: model params     = 1.41 B
0.00.039.518 I print_info: general.name     = 1.4B
0.00.039.518 I print_info: vocab type       = BPE
0.00.039.518 I print_info: n_vocab          = 50304
0.00.039.519 I print_info: n_merges         = 50009
0.00.039.521 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.521 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: LF token         = 187 'Ċ'
0.00.039.522 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.522 I print_info: max token length = 1024
0.00.039.523 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.343.553 I load_tensors: offloading 24 repeating layers to GPU
0.00.343.566 I load_tensors: offloading output layer to GPU
0.00.343.566 I load_tensors: offloaded 25/25 layers to GPU
0.00.343.595 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.343.597 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.345.150 I llama_context_unified: n_seq_max     = 1
0.00.345.159 I llama_context_unified: n_ctx         = 128
0.00.345.159 I llama_context_unified: n_ctx_per_seq = 128
0.00.345.160 I llama_context_unified: n_batch       = 128
0.00.345.160 I llama_context_unified: n_ubatch      = 128
0.00.345.160 I llama_context_unified: flash_attn    = 0
0.00.345.162 I llama_context_unified: freq_base     = 10000.0
0.00.345.162 I llama_context_unified: freq_scale    = 1
0.00.345.163 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.345.165 I ggml_metal_init: allocating
0.00.345.245 I ggml_metal_init: found device: Apple M4
0.00.345.258 I ggml_metal_init: picking default device: Apple M4
0.00.347.089 I ggml_metal_init: using embedded metal library
0.00.352.664 I ggml_metal_init: GPU name:   Apple M4
0.00.352.684 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.352.685 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.352.686 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.352.687 I ggml_metal_init: simdgroup reduction   = true
0.00.352.687 I ggml_metal_init: simdgroup matrix mul. = true
0.00.352.688 I ggml_metal_init: has residency sets    = true
0.00.352.688 I ggml_metal_init: has bfloat            = true
0.00.352.688 I ggml_metal_init: use bfloat            = true
0.00.352.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.352.694 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.374.252 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.378.073 I init:      Metal KV buffer size =    24.00 MiB
0.00.378.077 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.378.107 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.381.602 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.381.604 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.381.605 I llama_context_unified: graph nodes  = 967
0.00.381.606 I llama_context_unified: graph splits = 2
0.00.381.610 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.381.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.415.575 I 
0.00.415.660 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.415.680 I perplexity: tokenizing the input ..
0.00.421.820 I perplexity: tokenization took 6.137 ms
0.00.421.835 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.562.804 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.148 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.164 I llama_perf_context_print:        load time =     405.66 ms
0.00.564.166 I llama_perf_context_print: prompt eval time =     140.68 ms /   128 tokens (    1.10 ms per token,   909.87 tokens per second)
0.00.564.166 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.167 I llama_perf_context_print:       total time =     148.59 ms /   129 tokens
0.00.564.704 I ggml_metal_free: deallocating

real	0m0.580s
user	0m0.080s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.729 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.762 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.768 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.774 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.774 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.775 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.775 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.776 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.777 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.777 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.777 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.778 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.778 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.781 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.781 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.698 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.703 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.555 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.556 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.556 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.557 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.557 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.558 I llama_model_loader: - type  f32:  194 tensors
0.00.024.558 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.558 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.558 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.559 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.559 I print_info: file format = GGUF V3 (latest)
0.00.024.563 I print_info: file type   = Q3_K - Medium
0.00.024.565 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.378 I load: special tokens cache size = 25
0.00.038.558 I load: token to piece cache size = 0.2984 MB
0.00.038.573 I print_info: arch             = gptneox
0.00.038.574 I print_info: vocab_only       = 0
0.00.038.574 I print_info: n_ctx_train      = 2048
0.00.038.575 I print_info: n_embd           = 2048
0.00.038.575 I print_info: n_layer          = 24
0.00.038.578 I print_info: n_head           = 16
0.00.038.579 I print_info: n_head_kv        = 16
0.00.038.579 I print_info: n_rot            = 32
0.00.038.579 I print_info: n_swa            = 0
0.00.038.580 I print_info: n_embd_head_k    = 128
0.00.038.580 I print_info: n_embd_head_v    = 128
0.00.038.580 I print_info: n_gqa            = 1
0.00.038.582 I print_info: n_embd_k_gqa     = 2048
0.00.038.583 I print_info: n_embd_v_gqa     = 2048
0.00.038.583 I print_info: f_norm_eps       = 1.0e-05
0.00.038.584 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.584 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.584 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.584 I print_info: f_logit_scale    = 0.0e+00
0.00.038.584 I print_info: n_ff             = 8192
0.00.038.586 I print_info: n_expert         = 0
0.00.038.586 I print_info: n_expert_used    = 0
0.00.038.586 I print_info: causal attn      = 1
0.00.038.586 I print_info: pooling type     = 0
0.00.038.586 I print_info: rope type        = 2
0.00.038.587 I print_info: rope scaling     = linear
0.00.038.587 I print_info: freq_base_train  = 10000.0
0.00.038.587 I print_info: freq_scale_train = 1
0.00.038.587 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.588 I print_info: rope_finetuned   = unknown
0.00.038.588 I print_info: ssm_d_conv       = 0
0.00.038.588 I print_info: ssm_d_inner      = 0
0.00.038.588 I print_info: ssm_d_state      = 0
0.00.038.588 I print_info: ssm_dt_rank      = 0
0.00.038.588 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.588 I print_info: model type       = 1.4B
0.00.038.589 I print_info: model params     = 1.41 B
0.00.038.590 I print_info: general.name     = 1.4B
0.00.038.590 I print_info: vocab type       = BPE
0.00.038.590 I print_info: n_vocab          = 50304
0.00.038.590 I print_info: n_merges         = 50009
0.00.038.591 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.591 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.591 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.591 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.591 I print_info: LF token         = 187 'Ċ'
0.00.038.592 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.592 I print_info: max token length = 1024
0.00.038.592 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.442.964 I load_tensors: offloading 24 repeating layers to GPU
0.00.442.976 I load_tensors: offloading output layer to GPU
0.00.442.977 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.008 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.010 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.444.616 I llama_context_unified: n_seq_max     = 1
0.00.444.618 I llama_context_unified: n_ctx         = 128
0.00.444.619 I llama_context_unified: n_ctx_per_seq = 128
0.00.444.619 I llama_context_unified: n_batch       = 128
0.00.444.620 I llama_context_unified: n_ubatch      = 128
0.00.444.620 I llama_context_unified: flash_attn    = 0
0.00.444.623 I llama_context_unified: freq_base     = 10000.0
0.00.444.623 I llama_context_unified: freq_scale    = 1
0.00.444.624 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.444.627 I ggml_metal_init: allocating
0.00.444.689 I ggml_metal_init: found device: Apple M4
0.00.444.703 I ggml_metal_init: picking default device: Apple M4
0.00.446.451 I ggml_metal_init: using embedded metal library
0.00.451.956 I ggml_metal_init: GPU name:   Apple M4
0.00.451.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.451.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.451.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.451.976 I ggml_metal_init: simdgroup reduction   = true
0.00.451.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.451.976 I ggml_metal_init: has residency sets    = true
0.00.451.977 I ggml_metal_init: has bfloat            = true
0.00.451.977 I ggml_metal_init: use bfloat            = true
0.00.451.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.451.982 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.417 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.476.109 I init:      Metal KV buffer size =    24.00 MiB
0.00.476.119 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.476.174 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.479.562 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.479.564 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.479.565 I llama_context_unified: graph nodes  = 967
0.00.479.565 I llama_context_unified: graph splits = 2
0.00.479.569 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.479.573 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.128 I 
0.00.507.215 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.507.235 I perplexity: tokenizing the input ..
0.00.514.446 I perplexity: tokenization took 7.21 ms
0.00.514.457 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.655.837 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.657.164 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.657.176 I llama_perf_context_print:        load time =     498.39 ms
0.00.657.177 I llama_perf_context_print: prompt eval time =     141.15 ms /   128 tokens (    1.10 ms per token,   906.84 tokens per second)
0.00.657.178 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.657.178 I llama_perf_context_print:       total time =     150.05 ms /   129 tokens
0.00.657.722 I ggml_metal_free: deallocating

real	0m0.671s
user	0m0.079s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.682 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.171 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.179 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.179 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.180 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.180 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.180 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.181 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.181 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.184 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.185 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.185 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.191 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.105 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.912 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.913 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.913 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.914 I llama_model_loader: - type  f32:  194 tensors
0.00.025.915 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.915 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.915 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.916 I print_info: file format = GGUF V3 (latest)
0.00.025.916 I print_info: file type   = Q4_K - Medium
0.00.025.918 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.775 I load: special tokens cache size = 25
0.00.039.765 I load: token to piece cache size = 0.2984 MB
0.00.039.781 I print_info: arch             = gptneox
0.00.039.782 I print_info: vocab_only       = 0
0.00.039.782 I print_info: n_ctx_train      = 2048
0.00.039.782 I print_info: n_embd           = 2048
0.00.039.782 I print_info: n_layer          = 24
0.00.039.786 I print_info: n_head           = 16
0.00.039.787 I print_info: n_head_kv        = 16
0.00.039.787 I print_info: n_rot            = 32
0.00.039.788 I print_info: n_swa            = 0
0.00.039.788 I print_info: n_embd_head_k    = 128
0.00.039.788 I print_info: n_embd_head_v    = 128
0.00.039.789 I print_info: n_gqa            = 1
0.00.039.789 I print_info: n_embd_k_gqa     = 2048
0.00.039.790 I print_info: n_embd_v_gqa     = 2048
0.00.039.791 I print_info: f_norm_eps       = 1.0e-05
0.00.039.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.798 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.798 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.798 I print_info: f_logit_scale    = 0.0e+00
0.00.039.801 I print_info: n_ff             = 8192
0.00.039.801 I print_info: n_expert         = 0
0.00.039.801 I print_info: n_expert_used    = 0
0.00.039.801 I print_info: causal attn      = 1
0.00.039.801 I print_info: pooling type     = 0
0.00.039.802 I print_info: rope type        = 2
0.00.039.802 I print_info: rope scaling     = linear
0.00.039.802 I print_info: freq_base_train  = 10000.0
0.00.039.802 I print_info: freq_scale_train = 1
0.00.039.803 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.803 I print_info: rope_finetuned   = unknown
0.00.039.805 I print_info: ssm_d_conv       = 0
0.00.039.805 I print_info: ssm_d_inner      = 0
0.00.039.805 I print_info: ssm_d_state      = 0
0.00.039.805 I print_info: ssm_dt_rank      = 0
0.00.039.805 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.805 I print_info: model type       = 1.4B
0.00.039.806 I print_info: model params     = 1.41 B
0.00.039.806 I print_info: general.name     = 1.4B
0.00.039.806 I print_info: vocab type       = BPE
0.00.039.807 I print_info: n_vocab          = 50304
0.00.039.807 I print_info: n_merges         = 50009
0.00.039.807 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.807 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.808 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: LF token         = 187 'Ċ'
0.00.039.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: max token length = 1024
0.00.039.810 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.548.011 I load_tensors: offloading 24 repeating layers to GPU
0.00.548.025 I load_tensors: offloading output layer to GPU
0.00.548.026 I load_tensors: offloaded 25/25 layers to GPU
0.00.548.058 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.548.059 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.549.755 I llama_context_unified: n_seq_max     = 1
0.00.549.758 I llama_context_unified: n_ctx         = 128
0.00.549.758 I llama_context_unified: n_ctx_per_seq = 128
0.00.549.758 I llama_context_unified: n_batch       = 128
0.00.549.759 I llama_context_unified: n_ubatch      = 128
0.00.549.759 I llama_context_unified: flash_attn    = 0
0.00.549.762 I llama_context_unified: freq_base     = 10000.0
0.00.549.762 I llama_context_unified: freq_scale    = 1
0.00.549.763 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.549.765 I ggml_metal_init: allocating
0.00.549.840 I ggml_metal_init: found device: Apple M4
0.00.549.853 I ggml_metal_init: picking default device: Apple M4
0.00.551.649 I ggml_metal_init: using embedded metal library
0.00.558.283 I ggml_metal_init: GPU name:   Apple M4
0.00.558.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.291 I ggml_metal_init: simdgroup reduction   = true
0.00.558.291 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.291 I ggml_metal_init: has residency sets    = true
0.00.558.292 I ggml_metal_init: has bfloat            = true
0.00.558.292 I ggml_metal_init: use bfloat            = true
0.00.558.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.303 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.745 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.580.336 I init:      Metal KV buffer size =    24.00 MiB
0.00.580.340 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.580.379 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.583.678 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.583.680 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.583.680 I llama_context_unified: graph nodes  = 967
0.00.583.680 I llama_context_unified: graph splits = 2
0.00.583.684 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.583.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.582 I 
0.00.615.653 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.671 I perplexity: tokenizing the input ..
0.00.621.011 I perplexity: tokenization took 5.339 ms
0.00.621.023 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.757.019 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.758.359 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.758.380 I llama_perf_context_print:        load time =     606.89 ms
0.00.758.381 I llama_perf_context_print: prompt eval time =     135.76 ms /   128 tokens (    1.06 ms per token,   942.82 tokens per second)
0.00.758.382 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.758.382 I llama_perf_context_print:       total time =     142.80 ms /   129 tokens
0.00.758.978 I ggml_metal_free: deallocating

real	0m0.772s
user	0m0.079s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.029 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.782 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.788 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.795 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.796 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.797 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.797 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.798 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.798 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.798 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.799 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.799 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.801 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.801 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.801 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.530 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.271 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.272 I llama_model_loader: - type  f32:  194 tensors
0.00.025.272 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.272 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.273 I print_info: file format = GGUF V3 (latest)
0.00.025.273 I print_info: file type   = Q5_K - Medium
0.00.025.275 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.272 I load: special tokens cache size = 25
0.00.039.522 I load: token to piece cache size = 0.2984 MB
0.00.039.538 I print_info: arch             = gptneox
0.00.039.539 I print_info: vocab_only       = 0
0.00.039.539 I print_info: n_ctx_train      = 2048
0.00.039.539 I print_info: n_embd           = 2048
0.00.039.540 I print_info: n_layer          = 24
0.00.039.544 I print_info: n_head           = 16
0.00.039.544 I print_info: n_head_kv        = 16
0.00.039.545 I print_info: n_rot            = 32
0.00.039.545 I print_info: n_swa            = 0
0.00.039.545 I print_info: n_embd_head_k    = 128
0.00.039.545 I print_info: n_embd_head_v    = 128
0.00.039.546 I print_info: n_gqa            = 1
0.00.039.546 I print_info: n_embd_k_gqa     = 2048
0.00.039.549 I print_info: n_embd_v_gqa     = 2048
0.00.039.549 I print_info: f_norm_eps       = 1.0e-05
0.00.039.550 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.550 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.550 I print_info: f_logit_scale    = 0.0e+00
0.00.039.551 I print_info: n_ff             = 8192
0.00.039.551 I print_info: n_expert         = 0
0.00.039.551 I print_info: n_expert_used    = 0
0.00.039.551 I print_info: causal attn      = 1
0.00.039.551 I print_info: pooling type     = 0
0.00.039.551 I print_info: rope type        = 2
0.00.039.552 I print_info: rope scaling     = linear
0.00.039.552 I print_info: freq_base_train  = 10000.0
0.00.039.552 I print_info: freq_scale_train = 1
0.00.039.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.552 I print_info: rope_finetuned   = unknown
0.00.039.553 I print_info: ssm_d_conv       = 0
0.00.039.553 I print_info: ssm_d_inner      = 0
0.00.039.553 I print_info: ssm_d_state      = 0
0.00.039.553 I print_info: ssm_dt_rank      = 0
0.00.039.553 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.553 I print_info: model type       = 1.4B
0.00.039.553 I print_info: model params     = 1.41 B
0.00.039.554 I print_info: general.name     = 1.4B
0.00.039.554 I print_info: vocab type       = BPE
0.00.039.554 I print_info: n_vocab          = 50304
0.00.039.554 I print_info: n_merges         = 50009
0.00.039.555 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.555 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.555 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: LF token         = 187 'Ċ'
0.00.039.556 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.556 I print_info: max token length = 1024
0.00.039.557 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.591.102 I load_tensors: offloading 24 repeating layers to GPU
0.00.591.117 I load_tensors: offloading output layer to GPU
0.00.591.118 I load_tensors: offloaded 25/25 layers to GPU
0.00.591.146 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.591.147 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.592.828 I llama_context_unified: n_seq_max     = 1
0.00.592.831 I llama_context_unified: n_ctx         = 128
0.00.592.831 I llama_context_unified: n_ctx_per_seq = 128
0.00.592.832 I llama_context_unified: n_batch       = 128
0.00.592.832 I llama_context_unified: n_ubatch      = 128
0.00.592.833 I llama_context_unified: flash_attn    = 0
0.00.592.835 I llama_context_unified: freq_base     = 10000.0
0.00.592.836 I llama_context_unified: freq_scale    = 1
0.00.592.836 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.592.842 I ggml_metal_init: allocating
0.00.592.953 I ggml_metal_init: found device: Apple M4
0.00.592.967 I ggml_metal_init: picking default device: Apple M4
0.00.594.851 I ggml_metal_init: using embedded metal library
0.00.601.316 I ggml_metal_init: GPU name:   Apple M4
0.00.601.321 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.321 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.322 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.323 I ggml_metal_init: simdgroup reduction   = true
0.00.601.323 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.323 I ggml_metal_init: has residency sets    = true
0.00.601.323 I ggml_metal_init: has bfloat            = true
0.00.601.324 I ggml_metal_init: use bfloat            = true
0.00.601.324 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.334 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.618.322 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.800 I init:      Metal KV buffer size =    24.00 MiB
0.00.621.804 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.833 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.625.072 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.625.074 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.625.075 I llama_context_unified: graph nodes  = 967
0.00.625.075 I llama_context_unified: graph splits = 2
0.00.625.080 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.625.080 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.114 I 
0.00.661.193 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.213 I perplexity: tokenizing the input ..
0.00.666.469 I perplexity: tokenization took 5.254 ms
0.00.666.481 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.825 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.156 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.182 I llama_perf_context_print:        load time =     651.08 ms
0.00.809.183 I llama_perf_context_print: prompt eval time =     141.11 ms /   128 tokens (    1.10 ms per token,   907.07 tokens per second)
0.00.809.184 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.184 I llama_perf_context_print:       total time =     148.07 ms /   129 tokens
0.00.809.790 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.076s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.750 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.405 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.418 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.418 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.419 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.419 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.420 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.420 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.421 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.421 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.421 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.422 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.423 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.424 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.424 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.279 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.115 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.116 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.117 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.117 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.118 I llama_model_loader: - type  f32:  194 tensors
0.00.024.119 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.119 I print_info: file format = GGUF V3 (latest)
0.00.024.120 I print_info: file type   = Q6_K
0.00.024.121 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.896 I load: special tokens cache size = 25
0.00.037.984 I load: token to piece cache size = 0.2984 MB
0.00.037.999 I print_info: arch             = gptneox
0.00.038.000 I print_info: vocab_only       = 0
0.00.038.000 I print_info: n_ctx_train      = 2048
0.00.038.000 I print_info: n_embd           = 2048
0.00.038.000 I print_info: n_layer          = 24
0.00.038.004 I print_info: n_head           = 16
0.00.038.005 I print_info: n_head_kv        = 16
0.00.038.005 I print_info: n_rot            = 32
0.00.038.005 I print_info: n_swa            = 0
0.00.038.005 I print_info: n_embd_head_k    = 128
0.00.038.006 I print_info: n_embd_head_v    = 128
0.00.038.006 I print_info: n_gqa            = 1
0.00.038.007 I print_info: n_embd_k_gqa     = 2048
0.00.038.008 I print_info: n_embd_v_gqa     = 2048
0.00.038.008 I print_info: f_norm_eps       = 1.0e-05
0.00.038.009 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.009 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.009 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.009 I print_info: f_logit_scale    = 0.0e+00
0.00.038.010 I print_info: n_ff             = 8192
0.00.038.010 I print_info: n_expert         = 0
0.00.038.010 I print_info: n_expert_used    = 0
0.00.038.010 I print_info: causal attn      = 1
0.00.038.010 I print_info: pooling type     = 0
0.00.038.010 I print_info: rope type        = 2
0.00.038.010 I print_info: rope scaling     = linear
0.00.038.011 I print_info: freq_base_train  = 10000.0
0.00.038.011 I print_info: freq_scale_train = 1
0.00.038.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.011 I print_info: rope_finetuned   = unknown
0.00.038.011 I print_info: ssm_d_conv       = 0
0.00.038.012 I print_info: ssm_d_inner      = 0
0.00.038.012 I print_info: ssm_d_state      = 0
0.00.038.012 I print_info: ssm_dt_rank      = 0
0.00.038.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.012 I print_info: model type       = 1.4B
0.00.038.012 I print_info: model params     = 1.41 B
0.00.038.013 I print_info: general.name     = 1.4B
0.00.038.013 I print_info: vocab type       = BPE
0.00.038.013 I print_info: n_vocab          = 50304
0.00.038.013 I print_info: n_merges         = 50009
0.00.038.014 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.014 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.014 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.014 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.014 I print_info: LF token         = 187 'Ċ'
0.00.038.015 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.015 I print_info: max token length = 1024
0.00.038.016 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.362.451 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.459 I load_tensors: offloading output layer to GPU
0.00.362.461 I load_tensors: offloaded 25/25 layers to GPU
0.00.362.485 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.362.488 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.363.908 I llama_context_unified: n_seq_max     = 1
0.00.363.910 I llama_context_unified: n_ctx         = 128
0.00.363.911 I llama_context_unified: n_ctx_per_seq = 128
0.00.363.911 I llama_context_unified: n_batch       = 128
0.00.363.912 I llama_context_unified: n_ubatch      = 128
0.00.363.912 I llama_context_unified: flash_attn    = 0
0.00.363.913 I llama_context_unified: freq_base     = 10000.0
0.00.363.913 I llama_context_unified: freq_scale    = 1
0.00.363.914 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.363.915 I ggml_metal_init: allocating
0.00.363.941 I ggml_metal_init: found device: Apple M4
0.00.363.950 I ggml_metal_init: picking default device: Apple M4
0.00.365.198 I ggml_metal_init: using embedded metal library
0.00.370.779 I ggml_metal_init: GPU name:   Apple M4
0.00.370.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.370.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.370.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.370.785 I ggml_metal_init: simdgroup reduction   = true
0.00.370.785 I ggml_metal_init: simdgroup matrix mul. = true
0.00.370.785 I ggml_metal_init: has residency sets    = true
0.00.370.785 I ggml_metal_init: has bfloat            = true
0.00.370.786 I ggml_metal_init: use bfloat            = true
0.00.370.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.370.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.386.577 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.389.944 I init:      Metal KV buffer size =    24.00 MiB
0.00.389.948 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.389.996 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.00.393.105 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.00.393.106 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.00.393.107 I llama_context_unified: graph nodes  = 967
0.00.393.107 I llama_context_unified: graph splits = 2
0.00.393.110 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.393.111 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.339 I 
0.00.426.417 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.438 I perplexity: tokenizing the input ..
0.00.433.505 I perplexity: tokenization took 7.065 ms
0.00.433.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.573.446 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.574.900 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.574.911 I llama_perf_context_print:        load time =     417.58 ms
0.00.574.912 I llama_perf_context_print: prompt eval time =     139.52 ms /   128 tokens (    1.09 ms per token,   917.42 tokens per second)
0.00.574.913 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.574.913 I llama_perf_context_print:       total time =     148.58 ms /   129 tokens
0.00.575.462 I ggml_metal_free: deallocating

real	0m0.588s
user	0m0.075s
sys	0m0.108s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.341 I build: 4724 (f63aeecc) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.018.759 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.031 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.043 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.048 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.048 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.103 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.701 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.703 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.704 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.710 I llama_model_loader: - type  f32:  194 tensors
0.00.040.710 I llama_model_loader: - type  f16:   98 tensors
0.00.040.711 I print_info: file format = GGUF V3 (latest)
0.00.040.712 I print_info: file type   = all F32 (guessed)
0.00.040.713 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.051.776 I load: special tokens cache size = 25
0.00.059.267 I load: token to piece cache size = 0.2984 MB
0.00.059.289 I print_info: arch             = gptneox
0.00.059.290 I print_info: vocab_only       = 0
0.00.059.291 I print_info: n_ctx_train      = 2048
0.00.059.291 I print_info: n_embd           = 2048
0.00.059.291 I print_info: n_layer          = 24
0.00.059.296 I print_info: n_head           = 16
0.00.059.297 I print_info: n_head_kv        = 16
0.00.059.297 I print_info: n_rot            = 32
0.00.059.297 I print_info: n_swa            = 0
0.00.059.297 I print_info: n_embd_head_k    = 128
0.00.059.298 I print_info: n_embd_head_v    = 128
0.00.059.298 I print_info: n_gqa            = 1
0.00.059.299 I print_info: n_embd_k_gqa     = 2048
0.00.059.300 I print_info: n_embd_v_gqa     = 2048
0.00.059.300 I print_info: f_norm_eps       = 1.0e-05
0.00.059.301 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.301 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.301 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.301 I print_info: f_logit_scale    = 0.0e+00
0.00.059.302 I print_info: n_ff             = 8192
0.00.059.305 I print_info: n_expert         = 0
0.00.059.305 I print_info: n_expert_used    = 0
0.00.059.305 I print_info: causal attn      = 1
0.00.059.305 I print_info: pooling type     = 0
0.00.059.305 I print_info: rope type        = 2
0.00.059.306 I print_info: rope scaling     = linear
0.00.059.306 I print_info: freq_base_train  = 10000.0
0.00.059.306 I print_info: freq_scale_train = 1
0.00.059.306 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.307 I print_info: rope_finetuned   = unknown
0.00.059.309 I print_info: ssm_d_conv       = 0
0.00.059.309 I print_info: ssm_d_inner      = 0
0.00.059.309 I print_info: ssm_d_state      = 0
0.00.059.309 I print_info: ssm_dt_rank      = 0
0.00.059.309 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.309 I print_info: model type       = 1.4B
0.00.059.310 I print_info: model params     = 1.41 B
0.00.059.310 I print_info: general.name     = 1.4B
0.00.059.310 I print_info: vocab type       = BPE
0.00.059.314 I print_info: n_vocab          = 50304
0.00.059.314 I print_info: n_merges         = 50009
0.00.059.314 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.315 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.315 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.315 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.317 I print_info: LF token         = 187 'Ċ'
0.00.059.317 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.317 I print_info: max token length = 1024
0.00.059.318 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.321.118 I load_tensors: offloading 24 repeating layers to GPU
0.01.321.123 I load_tensors: offloading output layer to GPU
0.01.321.124 I load_tensors: offloaded 25/25 layers to GPU
0.01.321.143 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.321.146 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.321.967 I llama_context_unified: n_seq_max     = 1
0.01.321.969 I llama_context_unified: n_ctx         = 128
0.01.321.969 I llama_context_unified: n_ctx_per_seq = 128
0.01.321.969 I llama_context_unified: n_batch       = 128
0.01.321.970 I llama_context_unified: n_ubatch      = 128
0.01.321.970 I llama_context_unified: flash_attn    = 0
0.01.321.971 I llama_context_unified: freq_base     = 10000.0
0.01.321.972 I llama_context_unified: freq_scale    = 1
0.01.321.972 W llama_context_unified: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.321.974 I ggml_metal_init: allocating
0.01.322.008 I ggml_metal_init: found device: Apple M4
0.01.322.018 I ggml_metal_init: picking default device: Apple M4
0.01.323.029 I ggml_metal_init: using embedded metal library
0.01.327.368 I ggml_metal_init: GPU name:   Apple M4
0.01.327.370 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.327.371 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.327.371 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.327.372 I ggml_metal_init: simdgroup reduction   = true
0.01.327.372 I ggml_metal_init: simdgroup matrix mul. = true
0.01.327.373 I ggml_metal_init: has residency sets    = true
0.01.327.373 I ggml_metal_init: has bfloat            = true
0.01.327.373 I ggml_metal_init: use bfloat            = true
0.01.327.374 I ggml_metal_init: hasUnifiedMemory      = true
0.01.327.375 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.341.037 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.343.295 I init:      Metal KV buffer size =    24.00 MiB
0.01.343.298 I llama_context_unified: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.343.348 I llama_context_unified:        CPU  output buffer size =     0.19 MiB
0.01.345.249 I llama_context_unified:      Metal compute buffer size =    25.56 MiB
0.01.345.251 I llama_context_unified:        CPU compute buffer size =     1.06 MiB
0.01.345.251 I llama_context_unified: graph nodes  = 967
0.01.345.251 I llama_context_unified: graph splits = 2
0.01.345.253 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.345.254 I 
0.01.345.295 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.345.297 I compute_imatrix: tokenizing the input ..
0.01.350.308 I compute_imatrix: tokenization took 5.01 ms
0.01.350.311 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.626.815 I compute_imatrix: 0.28 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.629.347 I llama_perf_context_print:        load time =    1608.06 ms
0.01.629.348 I llama_perf_context_print: prompt eval time =     274.44 ms /   128 tokens (    2.14 ms per token,   466.41 tokens per second)
0.01.629.349 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.629.349 I llama_perf_context_print:       total time =    1610.58 ms /   129 tokens
0.01.630.111 I ggml_metal_free: deallocating

real	0m1.810s
user	0m0.124s
sys	0m0.208s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4724 (f63aeecc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da080c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da087d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da08d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da09330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da098e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da09e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da0a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da0b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da0b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da0c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da0d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da0e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da0eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da10c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da11330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da12bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da13830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da13d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da14030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da144d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da15020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da15cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da16160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da16600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da16f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da17880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da17d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da18480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da18a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da19fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da1abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da1b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da1b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da1be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da1cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da1cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da1d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da1d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da1e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da1ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da1f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da1f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da1fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da20330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12da21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12da224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12da22a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12da22f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12da234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12da23a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12da23f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12da244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12da24a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12da24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12da254c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12da25a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12da25f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12da264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12da26a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12da26f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12da274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12da279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12da27f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12da28490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12da289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12da28f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12da29480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12da299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12da196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12da29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12da2a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12da2ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12da2b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12da2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12da2bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12da2c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12da2c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12da2cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12da2d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12da2d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12da2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12da2e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12da2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12da2eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da2f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da2f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da31000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da31940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da31de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da32280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da32bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da33500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da33e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da35a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da36340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da36c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da37120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da37a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da37f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da38840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da38ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da39180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da39620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da39ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c604230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c6046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c604b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c604f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c6053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c605860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c605cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c606140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c6065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c606a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c606e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c607300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c607770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c607be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c608050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c6084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c608930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c608da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c609210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c609680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c609af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c609f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c60a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c60a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c60acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c60b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c60b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c60ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c60be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c60c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c60c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c60cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c60d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c60d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c60d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c60dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c60e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c60e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c60ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c60ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c60f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c60f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c60fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c610100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c610570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c6109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c610e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c6112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c611730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c611ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c612010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c612480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c6128f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c612d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c6131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c613640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c613ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c613f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c614390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c614f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c615210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c6158c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c615e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c616430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c6169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c616f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c617540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c617af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c6180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c618650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c618c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c6191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c619760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c619d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c61a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c61a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c61ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c61b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c61b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c61bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c61c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c61ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c61d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c61d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c61dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c61e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c61e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c61ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c61f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c61f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c61fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c620370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c620920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c620ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c621480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c621a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c621fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c622590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c622b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c6230f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c6236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c623c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c624200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c6247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c624d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c625310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c6258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c625e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c626420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c6269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c626f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c627530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c627ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c628090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c628640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c628bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c6291a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c6296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c629ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c62a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c62a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c62aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c62afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c62b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c62b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c62bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c62c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c62c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c62cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c62d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c62d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c62dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c62e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c62edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c62f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c62fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c62fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c6306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c630980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c630f90 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
0.00.741.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11c704b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11c704f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11c705400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11c705870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11c705ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11c706150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11c7065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11c706a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11c706ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11c707310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11c707780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11c707e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11c708990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11c709140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11c709950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11c70a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11c70a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11c70aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11c70b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11c70bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11c70c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11c70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11c70d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11c70d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11c70e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11c70e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11c70e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11c70ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11c70ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11c70f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11c70f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11c70fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11c710180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11c710440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11c7108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11c710d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11c711190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11c711600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11c711a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11c711ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11c712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11c7127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11c712c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11c7130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11c713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11c713980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11c713df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11c714260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11c7146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11c714b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11c714fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11c715420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11c715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11c715d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11c716170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11c7165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11c716b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11c717050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11c7174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11c717930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11c717da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11c718210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11c718680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11c718af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11c718f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11c7193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11c719840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11c719cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11c71a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11c71a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11c71aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11c71ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11c71b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11c71b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11c71bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11c71c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11c71c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11c71c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11c71cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11c71d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11c71d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11c71dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11c71df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11c71e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11c71e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11c71ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11c71f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11c71f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11c71f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11c71fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11c7202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11c720730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11c720ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11c721010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11c721480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11c7218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11c721d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11c7221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11c722640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11c722ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11c722f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11c723390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11c723800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11c723c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11c7240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11c724550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11c7249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11c724e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11c7252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11c725710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11c725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11c725ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11c726460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11c7268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11c726d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11c7271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11c727620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11c727a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11c727f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11c728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11c7287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11c728c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11c7290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11c729530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11c7299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11c729e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11c72a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11c72a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11c72ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11c72afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11c72b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11c72b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11c72bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11c72c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11c72c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11c72ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11c72cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11c72d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11c72d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11c72dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11c72e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11c72e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11c72e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11c72edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11c72f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11c72f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11c72fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11c72ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11c730420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11c730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11c730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11c731170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11c7315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11c731a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11c731ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11c732330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11c7327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11c732c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11c733080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11c7334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11c733960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11c733dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11c734240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11c7346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11c734b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11c734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11c735bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11c735e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11c736140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11c7365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11c736a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11c736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11c737300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11c737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11c737be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11c738050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11c7384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11c738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11c738da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11c739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11c739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11c739af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11c739f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11c73a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11c73a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11c73acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11c73b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11c73b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11c73ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11c73be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11c73c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11c73c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11c73cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11c73d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11c73d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11c73d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11c73dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11c73e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11c104280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11c1046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11c104b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11c104fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11c105440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11c1058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11c105d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11c106190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11c106600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11c106a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11c106ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11c107350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11c107f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11c1081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11c108480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11c1088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11c108d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11c1091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11c109640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11c109ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11c109f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11c10a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11c10a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11c10ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11c10b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11c10b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11c10b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11c10be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11c10c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11c10c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11c10cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11c10cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11c10d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11c10d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11c10dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11c10e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11c10e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11c10ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11c10ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11c10f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11c10f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11c10fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11c1100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11c110530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11c1109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11c110e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11c111280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11c1116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11c111b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11c111fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11c112440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11c1128b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11c112d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11c113190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11c113600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11c113a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11c113ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11c114350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11c1147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11c114c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11c1150a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11c115510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11c115980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11c115df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11c116260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11c1166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11c116b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11c116fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11c117420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11c117890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11c117d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11c118170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11c1185e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11c118a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11c118ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11c119330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11c1197a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11c119c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11c11a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11c11a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11c11a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11c11add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11c11b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11c11b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11c11bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11c11c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11c11ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11c11d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11c11daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11c11ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11c11e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11c11e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11c11ee30 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 0
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12da1aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12da1b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12da1d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12da12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12da19360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12da19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12da1a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12da18d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12da18740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12da1bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12da1a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12da11870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12da076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12da1dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12da14a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12da14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12da12e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12da13140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12da13400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12da39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12da3a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12da3a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12da3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12da3a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12da3ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12da3ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12da3b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12da3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12da3b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12da3b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12da3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12da3be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12da3c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12da3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12da3c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12da3c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12da3cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12da3cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12da3d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12da3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12da3d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12da3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12da3dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12da3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12da3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12da3e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12da3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12da3ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12da3ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12da3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12da3f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12da3f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12da3f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12da3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12da3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12da40080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12da40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12da40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12da408c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12da40b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12da40e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12da41100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12da413c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12da41680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12da41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12da41c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12da41ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12da42180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12da42440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12da42700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12da429c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12da42c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12da42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12da43200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12da434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12da43780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12da43a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12da43d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12da43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12da44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12da44540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12da44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12da44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12da44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12da45040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12da45300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12da455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12da45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12da45b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12da45e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12da460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12da46380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12da46640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12da46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12da46bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12da46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12da47140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12da47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12da476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12da47980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12da47c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12da47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12da481c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12da48480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12da48740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12da48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12da48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12da48f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12da49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12da49500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12da497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12da49a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12da49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12da4a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12da4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12da4a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12da4ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12da4b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12da4b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12da4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12da4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12da4c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12da4c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12da4cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12da4d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12da4d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12da4da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12da4dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12da4e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12da4e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12da4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12da4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12da4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12da4faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12da4ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12da503e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12da50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12da50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12da511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12da51660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12da51b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12da51fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12da52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12da528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12da52d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12da53220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12da536c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12da53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12da54000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12da544a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12da54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12da54de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12da55280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12da55720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12da55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12da56060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12da56500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12da569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12da56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12da572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12da57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12da57c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12da580c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12da58560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12da58a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12da58ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12da59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12da597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12da59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12da5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12da5a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12da5aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12da5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12da5b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12da5b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12da5bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12da5c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12da5c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12da5cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12da5cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12da5d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12da5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12da5dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12da5e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12da5e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12da5eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12da5efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12da5f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12da5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12da5fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12da60240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12da606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12da60b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12da610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12da61620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12da61b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12da620c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12da62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12da62990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12da62fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12da635b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12da63da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12da64240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12da64500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12da64b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12da65120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12da65910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12da65db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12da66250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12da666f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12da66ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12da673f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12da67940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12da67e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12da683e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12da68930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12da68e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12da693d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12da69920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12da69e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12da6a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12da6a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12da6ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12da6b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12da6b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12da6be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12da6c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12da6c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12da6ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12da6d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12da6d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12da6de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12da6e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12da6e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12da6ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12da6f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12da6f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12da6fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12da70360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12da708b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12da70e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12da71350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12da718a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12da71df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12da72340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12da72890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12da72de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12da73330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12da73880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12da73dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12da74320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12da74870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12da74dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12da75310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12da75860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12da75db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12da76300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12da76850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12da76da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12da772f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12da77840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12da77d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12da782e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12da78830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12da78d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12da792d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12da79820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12da79cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12da7a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12da7a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12da7aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12da7af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12da7b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12da7b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12da7bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12da7c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12da7c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12da7cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12da7cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12da7d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12da7d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12da7dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12da7e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12da7e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12da7f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12da7f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12da7ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12da80210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12da80a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12da80cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12da812d0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 967
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.784s
user	0m0.280s
sys	0m0.334s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4724 (f63aeecc)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x138f0d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x138f0d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x138f0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x138f0e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x138f0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x138f0f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x138f0f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x138f0fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x138f10120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x138f10620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x138f10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x138f11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x138f11b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x138f122f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x138f12b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x138f13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x138f13940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x138f14060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x138f14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x138f14f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x138f15670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x138f15d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x138f164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a104980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a1050a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a105360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a1057d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a1060a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a106360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a1067d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a106cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a107200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a107670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a107ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a107f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a108700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a108bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a1090a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a109570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a109a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a109f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a10a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a10a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a10ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a10b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a10b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a10bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a10bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a10c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a10cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a10d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a10d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a10d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a10dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a10e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a10e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a10ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a10f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a10f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a10fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a1100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a110580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a110a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a110ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a111360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a111800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a111ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a112140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a1125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a112a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a112f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a1133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a113860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a113db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a114300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a114850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a114da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a1152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a115840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a115d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a1162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a116830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a116d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a1172d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a117820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a117d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a1182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a118810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a118d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a1192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a119800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a119d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a11a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a11a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a11ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a11b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a11b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a10c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a11bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a11c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a11c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a11cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a11d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a11d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a11de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a11e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a11e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a11ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a11f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a11f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a11fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a1203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a120910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a120db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a121250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a1216f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a121b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a122030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a1224d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a122970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a122e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a1232b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a123750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a123e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a124120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a124620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a124b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a125020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a125520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a125a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a125f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a126420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a126920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a126e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a127320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a127820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a127d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a128220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a128720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a128c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a129120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a129620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a129b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a12a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a12a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a12aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a12af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a12b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a12b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a12be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a12c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a12c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a12cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a12d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a12d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a12dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a12e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a12e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a12eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a12f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a12f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a12fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a12ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a130420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a130920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a130e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a131320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a131820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a131d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a132220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a132720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a132c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a133120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a133620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a133b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a134020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a134520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a134a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a134f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a135420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a135920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a135e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a136320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a136820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a136d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a137220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a137720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a137c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a138120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a138620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a138b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a139020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a139520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a139a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a139fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a13a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a13ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a13b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a13b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a13bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a13c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a13cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a13cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a13d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a13d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a13de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a13e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a13eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a13efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a13f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a13fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a140150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a1406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a140bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a141140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a141690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a141be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a142130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a142680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a142bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a143120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a143670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a143bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a144110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a144660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a144bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a145100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a145650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a145ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a1460f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a146640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a146b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a1470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a147630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a147b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a1480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a148620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a148b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a1490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a149610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a149b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a14a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a14a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a14ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a14b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a14b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a14bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a14c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a14c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a14cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a14d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a14d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a14db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a14e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a14e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a14eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a14f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a14f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a14fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a150050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a1505a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a150af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a151040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a151590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a151ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a152030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a152580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a152a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a152ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a153360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a153800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a153ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a154140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a1545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a154a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a154f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a1553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a155860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a155d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a1561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a156640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a156ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a157030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a157750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a157e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a158590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a158cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a158f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a159760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a159a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a15a030 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
0.00.095.236 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a139ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a13b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a13d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a159ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a13b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a13bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a13db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a104620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a11bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a159230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a13e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a13c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a105da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a15a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a15add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a15b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a15b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a15b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a15b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a15bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a15be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a15c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a15c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a15c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a15c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a15cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a15ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a15d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a15d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a15d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a15d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a15dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a15df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a15e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a15e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a15e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a15ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a15ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a15efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a15f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a15f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a15f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a15fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a15fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a160050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a160310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a1605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a160890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a160b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a160e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a1610d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a161390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a161650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a161910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a161bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a161e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a162150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a162410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a1626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a162990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a162c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a162f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a1631d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a163490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a163750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a163a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a163cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a163f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a164250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a164510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a1647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a164a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a164d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a165010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a1652d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a165590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a165850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a165b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a165dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a166090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a166350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a166610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a1668d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a166b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a166e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a167110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a1673d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a167690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a167950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a167c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a167ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a168190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a168450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a168710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a1689d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a168c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a168f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a169210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a1694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a169790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a169a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a169d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a169fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a16a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a16a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a16a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a16aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a16ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a16b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a16b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a16b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a16b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a16bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a16be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a16c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a16c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a16c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a16c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a16cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a16ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a16d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a16d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a16d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a16d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a16dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a16df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a16e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a16e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a16e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a16ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a16ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a16ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a16f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a16f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a16f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a16fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a16fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a170010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a1702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a170590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a170850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a170b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a170dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a171090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a171350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a171610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a1718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a171b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a171e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a172110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a1723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a172690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a172950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a172c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a172ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a173190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a173450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a173710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a1739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a173c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a173f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a174210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a1744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a174790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a174a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a174d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a174fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a175290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a175550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a175810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a175ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a175d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a176050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a176310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a1765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a176890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a176b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a176e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a1770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a177390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a177650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a177910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a177bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a177e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a178150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a178410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a1786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a178990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a178c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a178f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a1791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a179490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a179750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a179a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a179cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a179f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a17a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a17a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a17aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a17ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a17b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a17b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a17ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a17bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a17c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a17c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a17cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a17d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a17d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a17daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a17e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a17e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a17eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a17efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a17f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a17f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a17fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a1801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a180610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a180a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a180ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a181360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a1817d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a181c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a1820b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a182520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a182990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a182e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a183270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a1836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a183b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a183fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a184430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a1848a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a184d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a185180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a1855f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a185a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a185ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a186340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a1867b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a186c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a187090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a187500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a187970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a187de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a188250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a1886c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a188b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a188fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a189410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a189880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a189cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a18a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a18a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a18aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a18aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a18b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a18b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a18bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a18c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a18c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a18c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a18cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a18d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a18d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a18db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a18df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a18e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a18e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a18ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a18f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a18f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a18fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a18fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a190300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a190770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a190be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a191050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a1914c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a191930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a191da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a192210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a192c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a1933a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a193ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a1941e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a1944a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a194910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a194f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a195520 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context_unified: n_seq_max     = 1
llama_context_unified: n_ctx         = 2048
llama_context_unified: n_ctx_per_seq = 2048
llama_context_unified: n_batch       = 2048
llama_context_unified: n_ubatch      = 512
llama_context_unified: flash_attn    = 1
llama_context_unified: freq_base     = 10000.0
llama_context_unified: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13a0044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13a004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13a004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13a005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13a0056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13a005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13a005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13a0063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13a006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13a006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13a007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13a007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13a008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13a008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13a0092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13a009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13a00a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13a00a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13a00af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13a00b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13a00be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13a00c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13a00cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13a00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13a00dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13a00dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13a00e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13a00e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13a00e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13a00edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13a00f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13a00f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13a00fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13a00fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13a0102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13a010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13a010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13a011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13a0114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13a011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13a011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13a012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13a012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13a012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13a012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13a0133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13a013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13a013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13a014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13a0149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13a014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13a0152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13a015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13a015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13a016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13a016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13a016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13a016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13a017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13a0177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13a017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13a0180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13a018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13a0189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13a018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13a019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13a0196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13a019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13a019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a01a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13a01a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13a01ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a01b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a01b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13a01ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13a01bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13a01c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13a01c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13a01cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a01d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13a01d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13a01d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13a01ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13a01e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13a01e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13a01eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13a01efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13a01f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13a01f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13a01fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13a020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13a0205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13a020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13a020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13a021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13a0217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13a021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13a022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13a0224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13a022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13a022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13a023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13a023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13a023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13a024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13a024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13a024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13a024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13a0253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13a025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13a025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13a026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13a026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a0269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13a0272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13a027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13a027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13a028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13a028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13a0291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13a029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13a029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13a02a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13a02a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13a02ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13a02b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13a02b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13a02b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13a02be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a02c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13a02c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13a02cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a02d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13a02d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13a02d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a02dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13a02e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13a02e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13a02eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13a02ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a02f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13a02f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a02fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13a0300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13a030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13a0309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13a030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13a031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13a031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13a032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13a0328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13a032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13a0331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13a033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13a033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13a033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13a034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a0347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13a034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13a0350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13a035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13a035990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13a035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13a036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13a0366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13a036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13a036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13a037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13a0378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13a037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13a038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13a0385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13a038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13a038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13a039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13a0397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13a039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13a03a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13a03a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13a03a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13a03ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13a03b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13a03b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13a03bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13a03bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13a03c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13a03c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13a03ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13a03d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13a03d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13a03da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13a03deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13a03e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13a03e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13a03ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13a03f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13a03f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13a03f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13a03fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13a040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13a0406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13a040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13a040f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13a041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13a041dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13a042080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13a0424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13a043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13a0436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13a043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13a043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13a044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13a044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13a044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13a0455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13a045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13a045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13a046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13a046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13a046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13a047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13a0474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13a047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13a047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13a048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13a048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13a048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13a048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13a0493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13a049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13a049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13a04a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13a04a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13a04aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a04ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13a04b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13a04b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13a04bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13a04c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13a04c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13a04c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13a04cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13a04d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13a04d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13a04dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13a04df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13a04e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13a04e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13a04eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13a04f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13a04f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13a04f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13a04fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13a0502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13a050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13a050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13a051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13a051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13a051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13a0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13a052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13a052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13a0533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13a053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13a053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a0540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13a054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13a0549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a0552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13a055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13a056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13a0568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13a056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13a0576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13a0579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13a057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a058420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13a058a30 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context_unified: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context_unified:        CPU  output buffer size =     0.19 MiB
llama_context_unified:      Metal compute buffer size =   102.25 MiB
llama_context_unified:        CPU compute buffer size =     8.01 MiB
llama_context_unified: graph nodes  = 872
llama_context_unified: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.946s
user	0m0.229s
sys	0m0.185s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
