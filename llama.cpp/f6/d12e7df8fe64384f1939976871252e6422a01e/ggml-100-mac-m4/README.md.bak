### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.38 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.85 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.70 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.44 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.50 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.02 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.35 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.28 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   25.91 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.33 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    2.25 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.19 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.18 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.24 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.18 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.79 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed  178.70 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.32 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.28 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 221.57 sec*proc (27 tests)

Total Test time (real) = 221.58 sec

real	3m41.645s
user	7m37.696s
sys	0m5.832s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.36 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.32 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.19 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.17 sec
      Start 17: test-quantize-fns
17/27 Test #17: test-quantize-fns .................   Passed   14.57 sec
      Start 18: test-quantize-perf
18/27 Test #18: test-quantize-perf ................   Passed    0.23 sec
      Start 19: test-sampling
19/27 Test #19: test-sampling .....................   Passed    1.00 sec
      Start 20: test-chat-template
20/27 Test #20: test-chat-template ................   Passed    0.19 sec
      Start 21: test-grammar-parser
21/27 Test #21: test-grammar-parser ...............   Passed    0.17 sec
      Start 22: test-grammar-integration
22/27 Test #22: test-grammar-integration ..........   Passed    0.17 sec
      Start 23: test-llama-grammar
23/27 Test #23: test-llama-grammar ................   Passed    0.20 sec
      Start 24: test-barrier
24/27 Test #24: test-barrier ......................   Passed    0.28 sec
      Start 25: test-backend-ops
25/27 Test #25: test-backend-ops ..................   Passed   28.75 sec
      Start 26: test-rope
26/27 Test #26: test-rope .........................   Passed    0.20 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.14 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  51.16 sec*proc (27 tests)

Total Test time (real) =  51.17 sec

real	0m51.182s
user	1m10.955s
sys	0m5.120s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.129 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.793 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.021 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.031 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.032 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.033 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.034 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.035 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.036 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.037 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.039 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.042 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.045 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.046 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.046 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.048 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.049 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.050 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.050 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.013 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.152 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.154 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.154 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.155 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.155 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.156 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.156 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.157 I llama_model_loader: - type  f32:  124 tensors
0.00.028.157 I llama_model_loader: - type  f16:   73 tensors
0.00.032.612 I llm_load_vocab: special tokens cache size = 5
0.00.034.820 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.034.824 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.034.824 I llm_load_print_meta: arch             = bert
0.00.034.825 I llm_load_print_meta: vocab type       = WPM
0.00.034.825 I llm_load_print_meta: n_vocab          = 30522
0.00.034.825 I llm_load_print_meta: n_merges         = 0
0.00.034.826 I llm_load_print_meta: vocab_only       = 0
0.00.034.826 I llm_load_print_meta: n_ctx_train      = 512
0.00.034.826 I llm_load_print_meta: n_embd           = 384
0.00.034.826 I llm_load_print_meta: n_layer          = 12
0.00.034.830 I llm_load_print_meta: n_head           = 12
0.00.034.831 I llm_load_print_meta: n_head_kv        = 12
0.00.034.831 I llm_load_print_meta: n_rot            = 32
0.00.034.831 I llm_load_print_meta: n_swa            = 0
0.00.034.832 I llm_load_print_meta: n_embd_head_k    = 32
0.00.034.832 I llm_load_print_meta: n_embd_head_v    = 32
0.00.034.833 I llm_load_print_meta: n_gqa            = 1
0.00.034.833 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.034.834 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.034.835 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.034.835 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.034.835 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.034.836 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.034.836 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.034.837 I llm_load_print_meta: n_ff             = 1536
0.00.034.837 I llm_load_print_meta: n_expert         = 0
0.00.034.837 I llm_load_print_meta: n_expert_used    = 0
0.00.034.838 I llm_load_print_meta: causal attn      = 0
0.00.034.838 I llm_load_print_meta: pooling type     = 2
0.00.034.839 I llm_load_print_meta: rope type        = 2
0.00.034.840 I llm_load_print_meta: rope scaling     = linear
0.00.034.840 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.034.841 I llm_load_print_meta: freq_scale_train = 1
0.00.034.843 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.034.843 I llm_load_print_meta: rope_finetuned   = unknown
0.00.034.843 I llm_load_print_meta: ssm_d_conv       = 0
0.00.034.844 I llm_load_print_meta: ssm_d_inner      = 0
0.00.034.844 I llm_load_print_meta: ssm_d_state      = 0
0.00.034.844 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.034.844 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.034.858 I llm_load_print_meta: model type       = 33M
0.00.034.859 I llm_load_print_meta: model ftype      = F16
0.00.034.859 I llm_load_print_meta: model params     = 33.21 M
0.00.034.860 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.034.860 I llm_load_print_meta: general.name     = Bge Small
0.00.034.861 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.034.861 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.034.862 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.034.862 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.034.862 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.034.862 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.034.863 I llm_load_print_meta: max token length = 21
0.00.036.929 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.036.931 I llm_load_tensors: offloading output layer to GPU
0.00.036.931 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.036.956 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.036.958 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.557 I llama_new_context_with_model: n_seq_max     = 1
0.00.037.558 I llama_new_context_with_model: n_ctx         = 512
0.00.037.559 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.037.559 I llama_new_context_with_model: n_batch       = 2048
0.00.037.559 I llama_new_context_with_model: n_ubatch      = 2048
0.00.037.560 I llama_new_context_with_model: flash_attn    = 0
0.00.037.560 I llama_new_context_with_model: freq_base     = 10000.0
0.00.037.561 I llama_new_context_with_model: freq_scale    = 1
0.00.037.561 I ggml_metal_init: allocating
0.00.037.566 I ggml_metal_init: found device: Apple M4
0.00.037.568 I ggml_metal_init: picking default device: Apple M4
0.00.038.421 I ggml_metal_init: using embedded metal library
0.00.042.032 I ggml_metal_init: GPU name:   Apple M4
0.00.042.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.036 I ggml_metal_init: simdgroup reduction   = true
0.00.042.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.036 I ggml_metal_init: has bfloat            = true
0.00.042.036 I ggml_metal_init: use bfloat            = true
0.00.042.037 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.254 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.257 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.258 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.290 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.054.291 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.054.292 I llama_new_context_with_model: graph nodes  = 429
0.00.054.292 I llama_new_context_with_model: graph splits = 2
0.00.054.316 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.061.786 I 
0.00.061.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.062.530 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.067.721 I llama_perf_context_print:        load time =      43.99 ms
0.00.067.722 I llama_perf_context_print: prompt eval time =       5.04 ms /     9 tokens (    0.56 ms per token,  1786.78 tokens per second)
0.00.067.723 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.067.724 I llama_perf_context_print:       total time =       5.94 ms /    10 tokens
0.00.067.857 I ggml_metal_free: deallocating

real	0m0.247s
user	0m0.047s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.147 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.251 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.254 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.256 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.256 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.257 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.257 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.257 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.258 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.259 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.259 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.260 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.260 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.262 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.262 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.263 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.263 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.263 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.264 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.264 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.475 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.476 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.476 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.477 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.477 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.477 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.478 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.478 I llama_model_loader: - type  f32:  124 tensors
0.00.014.478 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.002 I llm_load_vocab: special tokens cache size = 5
0.00.018.362 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.364 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.365 I llm_load_print_meta: arch             = bert
0.00.018.365 I llm_load_print_meta: vocab type       = WPM
0.00.018.365 I llm_load_print_meta: n_vocab          = 30522
0.00.018.365 I llm_load_print_meta: n_merges         = 0
0.00.018.366 I llm_load_print_meta: vocab_only       = 0
0.00.018.366 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.366 I llm_load_print_meta: n_embd           = 384
0.00.018.366 I llm_load_print_meta: n_layer          = 12
0.00.018.367 I llm_load_print_meta: n_head           = 12
0.00.018.368 I llm_load_print_meta: n_head_kv        = 12
0.00.018.368 I llm_load_print_meta: n_rot            = 32
0.00.018.368 I llm_load_print_meta: n_swa            = 0
0.00.018.369 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.369 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.369 I llm_load_print_meta: n_gqa            = 1
0.00.018.370 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.372 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.372 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.373 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.373 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.373 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.373 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.374 I llm_load_print_meta: n_ff             = 1536
0.00.018.374 I llm_load_print_meta: n_expert         = 0
0.00.018.377 I llm_load_print_meta: n_expert_used    = 0
0.00.018.377 I llm_load_print_meta: causal attn      = 0
0.00.018.377 I llm_load_print_meta: pooling type     = 2
0.00.018.378 I llm_load_print_meta: rope type        = 2
0.00.018.378 I llm_load_print_meta: rope scaling     = linear
0.00.018.378 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.378 I llm_load_print_meta: freq_scale_train = 1
0.00.018.379 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.379 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.379 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.379 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.379 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.379 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.380 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.386 I llm_load_print_meta: model type       = 33M
0.00.018.387 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.387 I llm_load_print_meta: model params     = 33.21 M
0.00.018.387 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.388 I llm_load_print_meta: general.name     = Bge Small
0.00.018.388 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.388 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.388 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.389 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.390 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.390 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.390 I llm_load_print_meta: max token length = 21
0.00.019.698 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.699 I llm_load_tensors: offloading output layer to GPU
0.00.019.700 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.706 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.707 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.083 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.084 I llama_new_context_with_model: n_ctx         = 512
0.00.020.084 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.084 I llama_new_context_with_model: n_batch       = 2048
0.00.020.084 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.084 I llama_new_context_with_model: flash_attn    = 0
0.00.020.085 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.085 I llama_new_context_with_model: freq_scale    = 1
0.00.020.085 I ggml_metal_init: allocating
0.00.020.088 I ggml_metal_init: found device: Apple M4
0.00.020.090 I ggml_metal_init: picking default device: Apple M4
0.00.020.608 I ggml_metal_init: using embedded metal library
0.00.022.696 I ggml_metal_init: GPU name:   Apple M4
0.00.022.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.698 I ggml_metal_init: simdgroup reduction   = true
0.00.022.698 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.699 I ggml_metal_init: has bfloat            = true
0.00.022.699 I ggml_metal_init: use bfloat            = true
0.00.022.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.700 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.030.537 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.030.542 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.030.544 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.031.130 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.031.131 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.031.131 I llama_new_context_with_model: graph nodes  = 429
0.00.031.132 I llama_new_context_with_model: graph splits = 2
0.00.031.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.036.282 I 
0.00.036.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.036.863 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.041.315 I llama_perf_context_print:        load time =      27.13 ms
0.00.041.316 I llama_perf_context_print: prompt eval time =       4.31 ms /     9 tokens (    0.48 ms per token,  2090.11 tokens per second)
0.00.041.317 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.041.317 I llama_perf_context_print:       total time =       5.03 ms /    10 tokens
0.00.041.465 I ggml_metal_free: deallocating

real	0m0.053s
user	0m0.027s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.157 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.944 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.386 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.394 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.395 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.396 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.396 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.397 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.399 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.400 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.400 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.401 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.401 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.405 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.409 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.410 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.411 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.748 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.931 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.674 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.674 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.675 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.050.675 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.675 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.676 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.676 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.676 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.050.677 I llama_model_loader: - type  f32:   41 tensors
0.00.050.677 I llama_model_loader: - type  f16:   29 tensors
0.00.069.149 W llm_load_vocab: empty token at index 5
0.00.073.591 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.837 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.882 I llm_load_vocab: special tokens cache size = 5
0.00.319.616 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.319.623 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.319.623 I llm_load_print_meta: arch             = jina-bert-v2
0.00.319.624 I llm_load_print_meta: vocab type       = BPE
0.00.319.626 I llm_load_print_meta: n_vocab          = 61056
0.00.319.626 I llm_load_print_meta: n_merges         = 39382
0.00.319.626 I llm_load_print_meta: vocab_only       = 0
0.00.319.626 I llm_load_print_meta: n_ctx_train      = 8192
0.00.319.626 I llm_load_print_meta: n_embd           = 384
0.00.319.627 I llm_load_print_meta: n_layer          = 4
0.00.319.634 I llm_load_print_meta: n_head           = 12
0.00.319.635 I llm_load_print_meta: n_head_kv        = 12
0.00.319.635 I llm_load_print_meta: n_rot            = 32
0.00.319.635 I llm_load_print_meta: n_swa            = 0
0.00.319.635 I llm_load_print_meta: n_embd_head_k    = 32
0.00.319.636 I llm_load_print_meta: n_embd_head_v    = 32
0.00.319.636 I llm_load_print_meta: n_gqa            = 1
0.00.319.637 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.319.637 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.319.638 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.319.639 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.319.640 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.319.642 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.319.642 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.319.643 I llm_load_print_meta: n_ff             = 1536
0.00.319.643 I llm_load_print_meta: n_expert         = 0
0.00.319.643 I llm_load_print_meta: n_expert_used    = 0
0.00.319.643 I llm_load_print_meta: causal attn      = 0
0.00.319.644 I llm_load_print_meta: pooling type     = -1
0.00.319.644 I llm_load_print_meta: rope type        = -1
0.00.319.644 I llm_load_print_meta: rope scaling     = linear
0.00.319.644 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.319.644 I llm_load_print_meta: freq_scale_train = 1
0.00.319.646 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.319.646 I llm_load_print_meta: rope_finetuned   = unknown
0.00.319.646 I llm_load_print_meta: ssm_d_conv       = 0
0.00.319.646 I llm_load_print_meta: ssm_d_inner      = 0
0.00.319.647 I llm_load_print_meta: ssm_d_state      = 0
0.00.319.647 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.319.647 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.319.670 I llm_load_print_meta: model type       = 33M
0.00.319.670 I llm_load_print_meta: model ftype      = F16
0.00.319.671 I llm_load_print_meta: model params     = 32.90 M
0.00.319.672 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.319.673 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.319.673 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.319.673 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.319.675 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.319.675 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.319.675 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.319.675 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.319.675 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.319.675 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.319.676 I llm_load_print_meta: max token length = 45
0.00.320.928 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.320.929 I llm_load_tensors: offloading output layer to GPU
0.00.320.929 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.320.947 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.320.948 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.322.079 I llama_new_context_with_model: n_seq_max     = 1
0.00.322.080 I llama_new_context_with_model: n_ctx         = 8192
0.00.322.080 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.322.081 I llama_new_context_with_model: n_batch       = 2048
0.00.322.081 I llama_new_context_with_model: n_ubatch      = 2048
0.00.322.081 I llama_new_context_with_model: flash_attn    = 0
0.00.322.082 I llama_new_context_with_model: freq_base     = 10000.0
0.00.322.082 I llama_new_context_with_model: freq_scale    = 1
0.00.322.083 I ggml_metal_init: allocating
0.00.322.087 I ggml_metal_init: found device: Apple M4
0.00.322.089 I ggml_metal_init: picking default device: Apple M4
0.00.323.130 I ggml_metal_init: using embedded metal library
0.00.325.256 I ggml_metal_init: GPU name:   Apple M4
0.00.325.258 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.325.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.325.259 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.325.259 I ggml_metal_init: simdgroup reduction   = true
0.00.325.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.325.259 I ggml_metal_init: has bfloat            = true
0.00.325.259 I ggml_metal_init: use bfloat            = true
0.00.325.260 I ggml_metal_init: hasUnifiedMemory      = true
0.00.325.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.336.062 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.336.064 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.336.066 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.336.700 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.336.701 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.336.702 I llama_new_context_with_model: graph nodes  = 154
0.00.336.702 I llama_new_context_with_model: graph splits = 2
0.00.336.720 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.350.574 I 
0.00.350.605 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.350.767 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.350.768 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.350.771 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.350.771 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.350.775 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.350.776 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.351.317 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.355.106 I llama_perf_context_print:        load time =     326.62 ms
0.00.355.110 I llama_perf_context_print: prompt eval time =       3.78 ms /    62 tokens (    0.06 ms per token, 16402.12 tokens per second)
0.00.355.112 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.355.113 I llama_perf_context_print:       total time =       4.53 ms /    63 tokens
0.00.355.338 I ggml_metal_free: deallocating

real	0m1.050s
user	0m0.323s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.108 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.258 I main: llama backend init
0.00.000.265 I main: load the model and apply lora adapter, if any
0.00.031.571 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.044.151 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.044.162 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.044.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.044.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.044.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.044.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.044.172 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.044.174 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.044.174 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.044.175 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.044.175 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.044.176 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.044.177 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.044.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.044.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.044.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.044.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.200 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.155 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.692 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.060.694 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.694 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.695 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.695 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.696 I llama_model_loader: - type  f32:  194 tensors
0.00.060.696 I llama_model_loader: - type  f16:   98 tensors
0.00.089.518 I llm_load_vocab: special tokens cache size = 25
0.00.096.096 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.096.099 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.096.099 I llm_load_print_meta: arch             = gptneox
0.00.096.099 I llm_load_print_meta: vocab type       = BPE
0.00.096.100 I llm_load_print_meta: n_vocab          = 50304
0.00.096.100 I llm_load_print_meta: n_merges         = 50009
0.00.096.100 I llm_load_print_meta: vocab_only       = 0
0.00.096.100 I llm_load_print_meta: n_ctx_train      = 2048
0.00.096.100 I llm_load_print_meta: n_embd           = 2048
0.00.096.100 I llm_load_print_meta: n_layer          = 24
0.00.096.103 I llm_load_print_meta: n_head           = 16
0.00.096.104 I llm_load_print_meta: n_head_kv        = 16
0.00.096.104 I llm_load_print_meta: n_rot            = 32
0.00.096.104 I llm_load_print_meta: n_swa            = 0
0.00.096.105 I llm_load_print_meta: n_embd_head_k    = 128
0.00.096.105 I llm_load_print_meta: n_embd_head_v    = 128
0.00.096.105 I llm_load_print_meta: n_gqa            = 1
0.00.096.106 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.096.107 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.096.107 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.096.107 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.096.107 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.096.108 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.096.108 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.096.108 I llm_load_print_meta: n_ff             = 8192
0.00.096.108 I llm_load_print_meta: n_expert         = 0
0.00.096.109 I llm_load_print_meta: n_expert_used    = 0
0.00.096.109 I llm_load_print_meta: causal attn      = 1
0.00.096.109 I llm_load_print_meta: pooling type     = 0
0.00.096.109 I llm_load_print_meta: rope type        = 2
0.00.096.109 I llm_load_print_meta: rope scaling     = linear
0.00.096.110 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.096.110 I llm_load_print_meta: freq_scale_train = 1
0.00.096.110 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.096.110 I llm_load_print_meta: rope_finetuned   = unknown
0.00.096.110 I llm_load_print_meta: ssm_d_conv       = 0
0.00.096.110 I llm_load_print_meta: ssm_d_inner      = 0
0.00.096.111 I llm_load_print_meta: ssm_d_state      = 0
0.00.096.111 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.096.111 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.096.116 I llm_load_print_meta: model type       = 1.4B
0.00.096.117 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.096.117 I llm_load_print_meta: model params     = 1.41 B
0.00.096.118 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.096.118 I llm_load_print_meta: general.name     = 1.4B
0.00.096.119 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.096.119 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.096.119 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.096.119 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.096.119 I llm_load_print_meta: LF token         = 128 ''
0.00.096.120 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.096.120 I llm_load_print_meta: max token length = 1024
0.00.098.113 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.114 I llm_load_tensors: offloading output layer to GPU
0.00.098.114 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.126 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.128 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.085 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.086 I llama_new_context_with_model: n_ctx         = 2048
0.00.099.086 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.099.086 I llama_new_context_with_model: n_batch       = 2048
0.00.099.086 I llama_new_context_with_model: n_ubatch      = 512
0.00.099.086 I llama_new_context_with_model: flash_attn    = 0
0.00.099.087 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.087 I llama_new_context_with_model: freq_scale    = 1
0.00.099.088 I ggml_metal_init: allocating
0.00.099.090 I ggml_metal_init: found device: Apple M4
0.00.099.092 I ggml_metal_init: picking default device: Apple M4
0.00.099.673 I ggml_metal_init: using embedded metal library
0.00.108.281 I ggml_metal_init: GPU name:   Apple M4
0.00.108.283 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.108.283 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.108.283 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.108.283 I ggml_metal_init: simdgroup reduction   = true
0.00.108.284 I ggml_metal_init: simdgroup matrix mul. = true
0.00.108.284 I ggml_metal_init: has bfloat            = true
0.00.108.284 I ggml_metal_init: use bfloat            = true
0.00.108.284 I ggml_metal_init: hasUnifiedMemory      = true
0.00.108.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.143.683 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.143.690 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.143.708 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.144.661 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.144.662 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.144.663 I llama_new_context_with_model: graph nodes  = 967
0.00.144.663 I llama_new_context_with_model: graph splits = 2
0.00.144.685 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.232.445 I main: llama threadpool init, n_threads = 4
0.00.232.478 I 
0.00.232.512 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.232.513 I 
0.00.232.596 I sampler seed: 1234
0.00.232.600 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.232.626 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.232.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.232.628 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.098.789 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54447.85 tokens per second)
0.02.098.790 I llama_perf_context_print:        load time =     200.87 ms
0.02.098.791 I llama_perf_context_print: prompt eval time =      38.13 ms /     7 tokens (    5.45 ms per token,   183.56 tokens per second)
0.02.098.792 I llama_perf_context_print:        eval time =    1825.03 ms /    63 runs   (   28.97 ms per token,    34.52 tokens per second)
0.02.098.792 I llama_perf_context_print:       total time =    1866.34 ms /    70 tokens
0.02.098.978 I ggml_metal_free: deallocating

real	0m2.390s
user	0m0.139s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.579 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.388 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.153 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.172 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.173 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.173 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.174 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.174 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.176 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.177 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.177 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.178 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.180 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.185 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.185 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.186 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.410 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.414 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.414 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.416 I llama_model_loader: - type  f32:  194 tensors
0.00.055.416 I llama_model_loader: - type  f16:   98 tensors
0.00.087.383 I llm_load_vocab: special tokens cache size = 25
0.00.094.485 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.489 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.489 I llm_load_print_meta: arch             = gptneox
0.00.094.489 I llm_load_print_meta: vocab type       = BPE
0.00.094.489 I llm_load_print_meta: n_vocab          = 50304
0.00.094.490 I llm_load_print_meta: n_merges         = 50009
0.00.094.490 I llm_load_print_meta: vocab_only       = 0
0.00.094.490 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.490 I llm_load_print_meta: n_embd           = 2048
0.00.094.490 I llm_load_print_meta: n_layer          = 24
0.00.094.493 I llm_load_print_meta: n_head           = 16
0.00.094.494 I llm_load_print_meta: n_head_kv        = 16
0.00.094.495 I llm_load_print_meta: n_rot            = 32
0.00.094.495 I llm_load_print_meta: n_swa            = 0
0.00.094.495 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.495 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.496 I llm_load_print_meta: n_gqa            = 1
0.00.094.496 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.497 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.497 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.498 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.498 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.498 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.498 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.499 I llm_load_print_meta: n_ff             = 8192
0.00.094.499 I llm_load_print_meta: n_expert         = 0
0.00.094.499 I llm_load_print_meta: n_expert_used    = 0
0.00.094.499 I llm_load_print_meta: causal attn      = 1
0.00.094.499 I llm_load_print_meta: pooling type     = 0
0.00.094.500 I llm_load_print_meta: rope type        = 2
0.00.094.500 I llm_load_print_meta: rope scaling     = linear
0.00.094.500 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.500 I llm_load_print_meta: freq_scale_train = 1
0.00.094.500 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.501 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.503 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.503 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.503 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.503 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.503 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.515 I llm_load_print_meta: model type       = 1.4B
0.00.094.516 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.516 I llm_load_print_meta: model params     = 1.41 B
0.00.094.517 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.519 I llm_load_print_meta: general.name     = 1.4B
0.00.094.519 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.519 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.519 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.519 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.519 I llm_load_print_meta: LF token         = 128 ''
0.00.094.520 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.520 I llm_load_print_meta: max token length = 1024
0.00.097.133 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.133 I llm_load_tensors: offloading output layer to GPU
0.00.097.134 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.144 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.097.145 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.098.208 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.210 I llama_new_context_with_model: n_ctx         = 128
0.00.098.210 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.098.210 I llama_new_context_with_model: n_batch       = 128
0.00.098.210 I llama_new_context_with_model: n_ubatch      = 128
0.00.098.210 I llama_new_context_with_model: flash_attn    = 0
0.00.098.211 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.211 I llama_new_context_with_model: freq_scale    = 1
0.00.098.211 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.098.212 I ggml_metal_init: allocating
0.00.098.222 I ggml_metal_init: found device: Apple M4
0.00.098.224 I ggml_metal_init: picking default device: Apple M4
0.00.098.835 I ggml_metal_init: using embedded metal library
0.00.100.991 I ggml_metal_init: GPU name:   Apple M4
0.00.100.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.100.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.100.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.100.993 I ggml_metal_init: simdgroup reduction   = true
0.00.100.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.100.994 I ggml_metal_init: has bfloat            = true
0.00.100.994 I ggml_metal_init: use bfloat            = true
0.00.100.994 I ggml_metal_init: hasUnifiedMemory      = true
0.00.100.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.110.032 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.036 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.053 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.914 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.915 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.915 I llama_new_context_with_model: graph nodes  = 967
0.00.110.916 I llama_new_context_with_model: graph splits = 2
0.00.110.928 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.100.918 I 
0.01.100.961 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.101.030 I perplexity: tokenizing the input ..
0.01.114.226 I perplexity: tokenization took 13.194 ms
0.01.114.231 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.236.326 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.238.022 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.238.044 I llama_perf_context_print:        load time =    1076.51 ms
0.01.238.048 I llama_perf_context_print: prompt eval time =     121.12 ms /   128 tokens (    0.95 ms per token,  1056.81 tokens per second)
0.01.238.049 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.238.050 I llama_perf_context_print:       total time =     137.13 ms /   129 tokens
0.01.238.825 I ggml_metal_free: deallocating

real	0m1.422s
user	0m0.123s
sys	0m0.233s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.597 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.173 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.177 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.180 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.181 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.181 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.182 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.183 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.186 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.189 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.189 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.189 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.671 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.910 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.910 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.911 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.911 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.912 I llama_model_loader: - type  f32:  194 tensors
0.00.028.913 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.945 I llm_load_vocab: special tokens cache size = 25
0.00.056.750 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.754 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.755 I llm_load_print_meta: arch             = gptneox
0.00.056.755 I llm_load_print_meta: vocab type       = BPE
0.00.056.755 I llm_load_print_meta: n_vocab          = 50304
0.00.056.756 I llm_load_print_meta: n_merges         = 50009
0.00.056.758 I llm_load_print_meta: vocab_only       = 0
0.00.056.758 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.759 I llm_load_print_meta: n_embd           = 2048
0.00.056.759 I llm_load_print_meta: n_layer          = 24
0.00.056.764 I llm_load_print_meta: n_head           = 16
0.00.056.765 I llm_load_print_meta: n_head_kv        = 16
0.00.056.765 I llm_load_print_meta: n_rot            = 32
0.00.056.765 I llm_load_print_meta: n_swa            = 0
0.00.056.766 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.768 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.769 I llm_load_print_meta: n_gqa            = 1
0.00.056.770 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.770 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.771 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.771 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.772 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.772 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.772 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.773 I llm_load_print_meta: n_ff             = 8192
0.00.056.773 I llm_load_print_meta: n_expert         = 0
0.00.056.773 I llm_load_print_meta: n_expert_used    = 0
0.00.056.773 I llm_load_print_meta: causal attn      = 1
0.00.056.773 I llm_load_print_meta: pooling type     = 0
0.00.056.773 I llm_load_print_meta: rope type        = 2
0.00.056.774 I llm_load_print_meta: rope scaling     = linear
0.00.056.774 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.775 I llm_load_print_meta: freq_scale_train = 1
0.00.056.775 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.775 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.775 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.775 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.775 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.775 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.775 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.789 I llm_load_print_meta: model type       = 1.4B
0.00.056.789 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.789 I llm_load_print_meta: model params     = 1.41 B
0.00.056.790 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.791 I llm_load_print_meta: general.name     = 1.4B
0.00.056.792 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.792 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.792 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.792 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.793 I llm_load_print_meta: LF token         = 128 ''
0.00.056.793 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.793 I llm_load_print_meta: max token length = 1024
0.00.059.246 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.059.246 I llm_load_tensors: offloading output layer to GPU
0.00.059.246 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.059.257 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.059.258 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.060.294 I llama_new_context_with_model: n_seq_max     = 1
0.00.060.295 I llama_new_context_with_model: n_ctx         = 2048
0.00.060.296 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.060.296 I llama_new_context_with_model: n_batch       = 2048
0.00.060.296 I llama_new_context_with_model: n_ubatch      = 512
0.00.060.296 I llama_new_context_with_model: flash_attn    = 0
0.00.060.297 I llama_new_context_with_model: freq_base     = 10000.0
0.00.060.297 I llama_new_context_with_model: freq_scale    = 1
0.00.060.298 I ggml_metal_init: allocating
0.00.060.301 I ggml_metal_init: found device: Apple M4
0.00.060.303 I ggml_metal_init: picking default device: Apple M4
0.00.060.976 I ggml_metal_init: using embedded metal library
0.00.063.094 I ggml_metal_init: GPU name:   Apple M4
0.00.063.096 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.063.096 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.063.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.063.097 I ggml_metal_init: simdgroup reduction   = true
0.00.063.097 I ggml_metal_init: simdgroup matrix mul. = true
0.00.063.097 I ggml_metal_init: has bfloat            = true
0.00.063.097 I ggml_metal_init: use bfloat            = true
0.00.063.098 I ggml_metal_init: hasUnifiedMemory      = true
0.00.063.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.095.948 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.962 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.998 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.156 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.158 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.158 I llama_new_context_with_model: graph nodes  = 967
0.00.097.158 I llama_new_context_with_model: graph splits = 2
0.00.097.173 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.416.739 I main: llama threadpool init, n_threads = 4
0.01.416.778 I 
0.01.416.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.01.416.798 I 
0.01.417.096 I sampler seed: 1234
0.01.417.102 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.417.123 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.417.125 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.417.125 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.502.554 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.02.502.555 I llama_perf_context_print:        load time =    1407.14 ms
0.02.502.556 I llama_perf_context_print: prompt eval time =      33.17 ms /     7 tokens (    4.74 ms per token,   211.03 tokens per second)
0.02.502.556 I llama_perf_context_print:        eval time =    1049.42 ms /    63 runs   (   16.66 ms per token,    60.03 tokens per second)
0.02.502.558 I llama_perf_context_print:       total time =    1085.82 ms /    70 tokens
0.02.502.735 I ggml_metal_free: deallocating

real	0m2.520s
user	0m0.116s
sys	0m0.295s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.302 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.192 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.827 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.834 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.835 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.835 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.836 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.836 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.837 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.837 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.838 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.838 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.838 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.839 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.839 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.841 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.841 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.841 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.866 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.167 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.136 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.137 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.137 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.138 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.138 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.139 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.139 I llama_model_loader: - type  f32:  194 tensors
0.00.030.140 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.673 I llm_load_vocab: special tokens cache size = 25
0.00.059.537 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.540 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.540 I llm_load_print_meta: arch             = gptneox
0.00.059.541 I llm_load_print_meta: vocab type       = BPE
0.00.059.541 I llm_load_print_meta: n_vocab          = 50304
0.00.059.541 I llm_load_print_meta: n_merges         = 50009
0.00.059.541 I llm_load_print_meta: vocab_only       = 0
0.00.059.541 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.542 I llm_load_print_meta: n_embd           = 2048
0.00.059.542 I llm_load_print_meta: n_layer          = 24
0.00.059.544 I llm_load_print_meta: n_head           = 16
0.00.059.545 I llm_load_print_meta: n_head_kv        = 16
0.00.059.545 I llm_load_print_meta: n_rot            = 32
0.00.059.546 I llm_load_print_meta: n_swa            = 0
0.00.059.546 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.547 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.548 I llm_load_print_meta: n_gqa            = 1
0.00.059.548 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.549 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.551 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.552 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.552 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.553 I llm_load_print_meta: n_ff             = 8192
0.00.059.553 I llm_load_print_meta: n_expert         = 0
0.00.059.553 I llm_load_print_meta: n_expert_used    = 0
0.00.059.553 I llm_load_print_meta: causal attn      = 1
0.00.059.554 I llm_load_print_meta: pooling type     = 0
0.00.059.554 I llm_load_print_meta: rope type        = 2
0.00.059.560 I llm_load_print_meta: rope scaling     = linear
0.00.059.561 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.562 I llm_load_print_meta: freq_scale_train = 1
0.00.059.562 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.562 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.562 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.562 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.562 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.563 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.576 I llm_load_print_meta: model type       = 1.4B
0.00.059.576 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.576 I llm_load_print_meta: model params     = 1.41 B
0.00.059.577 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.577 I llm_load_print_meta: general.name     = 1.4B
0.00.059.577 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.578 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.578 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.578 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.578 I llm_load_print_meta: LF token         = 128 ''
0.00.059.578 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.578 I llm_load_print_meta: max token length = 1024
0.00.061.752 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.752 I llm_load_tensors: offloading output layer to GPU
0.00.061.753 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.762 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.763 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.114 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.115 I llama_new_context_with_model: n_ctx         = 128
0.00.063.115 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.116 I llama_new_context_with_model: n_batch       = 128
0.00.063.116 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.116 I llama_new_context_with_model: flash_attn    = 0
0.00.063.116 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.116 I llama_new_context_with_model: freq_scale    = 1
0.00.063.117 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.117 I ggml_metal_init: allocating
0.00.063.120 I ggml_metal_init: found device: Apple M4
0.00.063.122 I ggml_metal_init: picking default device: Apple M4
0.00.063.641 I ggml_metal_init: using embedded metal library
0.00.065.568 I ggml_metal_init: GPU name:   Apple M4
0.00.065.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.570 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.570 I ggml_metal_init: simdgroup reduction   = true
0.00.065.571 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.571 I ggml_metal_init: has bfloat            = true
0.00.065.571 I ggml_metal_init: use bfloat            = true
0.00.065.571 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.573 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.773 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.074.776 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.074.792 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.075.700 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.075.701 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.075.702 I llama_new_context_with_model: graph nodes  = 967
0.00.075.702 I llama_new_context_with_model: graph splits = 2
0.00.075.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.255 I 
0.00.942.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.942.285 I perplexity: tokenizing the input ..
0.00.949.897 I perplexity: tokenization took 7.611 ms
0.00.949.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.072.099 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.073.363 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.073.379 I llama_perf_context_print:        load time =     931.06 ms
0.01.073.380 I llama_perf_context_print: prompt eval time =     121.98 ms /   128 tokens (    0.95 ms per token,  1049.38 tokens per second)
0.01.073.381 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.073.382 I llama_perf_context_print:       total time =     131.12 ms /   129 tokens
0.01.073.806 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.085s
sys	0m0.169s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.011.127 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.186 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.191 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.193 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.193 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.194 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.194 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.194 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.195 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.196 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.196 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.196 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.197 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.197 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.198 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.202 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.094 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.985 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.986 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.986 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.987 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.987 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.987 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.988 I llama_model_loader: - type  f32:  194 tensors
0.00.026.988 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.989 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.247 I llm_load_vocab: special tokens cache size = 25
0.00.054.268 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.272 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.272 I llm_load_print_meta: arch             = gptneox
0.00.054.273 I llm_load_print_meta: vocab type       = BPE
0.00.054.273 I llm_load_print_meta: n_vocab          = 50304
0.00.054.273 I llm_load_print_meta: n_merges         = 50009
0.00.054.273 I llm_load_print_meta: vocab_only       = 0
0.00.054.273 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.274 I llm_load_print_meta: n_embd           = 2048
0.00.054.274 I llm_load_print_meta: n_layer          = 24
0.00.054.281 I llm_load_print_meta: n_head           = 16
0.00.054.281 I llm_load_print_meta: n_head_kv        = 16
0.00.054.282 I llm_load_print_meta: n_rot            = 32
0.00.054.283 I llm_load_print_meta: n_swa            = 0
0.00.054.286 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.286 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.287 I llm_load_print_meta: n_gqa            = 1
0.00.054.288 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.290 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.291 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.291 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.291 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.291 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.292 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.292 I llm_load_print_meta: n_ff             = 8192
0.00.054.292 I llm_load_print_meta: n_expert         = 0
0.00.054.294 I llm_load_print_meta: n_expert_used    = 0
0.00.054.294 I llm_load_print_meta: causal attn      = 1
0.00.054.294 I llm_load_print_meta: pooling type     = 0
0.00.054.294 I llm_load_print_meta: rope type        = 2
0.00.054.295 I llm_load_print_meta: rope scaling     = linear
0.00.054.295 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.296 I llm_load_print_meta: freq_scale_train = 1
0.00.054.296 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.296 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.296 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.296 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.296 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.296 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.297 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.310 I llm_load_print_meta: model type       = 1.4B
0.00.054.310 I llm_load_print_meta: model ftype      = Q4_0
0.00.054.310 I llm_load_print_meta: model params     = 1.41 B
0.00.054.312 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.054.312 I llm_load_print_meta: general.name     = 1.4B
0.00.054.313 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.313 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.313 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.313 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.313 I llm_load_print_meta: LF token         = 128 ''
0.00.054.314 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.314 I llm_load_print_meta: max token length = 1024
0.00.056.581 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.582 I llm_load_tensors: offloading output layer to GPU
0.00.056.582 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.593 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.056.594 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.057.645 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.646 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.646 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.646 I llama_new_context_with_model: n_batch       = 2048
0.00.057.646 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.646 I llama_new_context_with_model: flash_attn    = 0
0.00.057.647 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.647 I llama_new_context_with_model: freq_scale    = 1
0.00.057.648 I ggml_metal_init: allocating
0.00.057.651 I ggml_metal_init: found device: Apple M4
0.00.057.653 I ggml_metal_init: picking default device: Apple M4
0.00.058.351 I ggml_metal_init: using embedded metal library
0.00.060.554 I ggml_metal_init: GPU name:   Apple M4
0.00.060.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.556 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.557 I ggml_metal_init: simdgroup reduction   = true
0.00.060.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.557 I ggml_metal_init: has bfloat            = true
0.00.060.557 I ggml_metal_init: use bfloat            = true
0.00.060.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.669 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.677 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.699 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.836 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.838 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.839 I llama_new_context_with_model: graph nodes  = 967
0.00.095.839 I llama_new_context_with_model: graph splits = 2
0.00.095.848 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.742.093 I main: llama threadpool init, n_threads = 4
0.00.742.121 I 
0.00.742.142 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.742.142 I 
0.00.742.283 I sampler seed: 1234
0.00.742.289 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.742.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.742.305 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.742.305 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.419.196 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.419.197 I llama_perf_context_print:        load time =     730.96 ms
0.01.419.197 I llama_perf_context_print: prompt eval time =      32.66 ms /     7 tokens (    4.67 ms per token,   214.31 tokens per second)
0.01.419.198 I llama_perf_context_print:        eval time =     641.13 ms /    63 runs   (   10.18 ms per token,    98.26 tokens per second)
0.01.419.198 I llama_perf_context_print:       total time =     677.10 ms /    70 tokens
0.01.419.391 I ggml_metal_free: deallocating

real	0m1.436s
user	0m0.110s
sys	0m0.177s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.263 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.129 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.169 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.174 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.177 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.177 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.178 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.179 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.179 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.180 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.180 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.180 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.181 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.181 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.801 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.802 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.802 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.803 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.803 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.804 I llama_model_loader: - type  f32:  194 tensors
0.00.024.804 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.804 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.108 I llm_load_vocab: special tokens cache size = 25
0.00.052.251 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.254 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.254 I llm_load_print_meta: arch             = gptneox
0.00.052.254 I llm_load_print_meta: vocab type       = BPE
0.00.052.255 I llm_load_print_meta: n_vocab          = 50304
0.00.052.255 I llm_load_print_meta: n_merges         = 50009
0.00.052.255 I llm_load_print_meta: vocab_only       = 0
0.00.052.255 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.255 I llm_load_print_meta: n_embd           = 2048
0.00.052.255 I llm_load_print_meta: n_layer          = 24
0.00.052.258 I llm_load_print_meta: n_head           = 16
0.00.052.259 I llm_load_print_meta: n_head_kv        = 16
0.00.052.259 I llm_load_print_meta: n_rot            = 32
0.00.052.260 I llm_load_print_meta: n_swa            = 0
0.00.052.260 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.260 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.262 I llm_load_print_meta: n_gqa            = 1
0.00.052.262 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.263 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.264 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.264 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.264 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.265 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.265 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.265 I llm_load_print_meta: n_ff             = 8192
0.00.052.266 I llm_load_print_meta: n_expert         = 0
0.00.052.266 I llm_load_print_meta: n_expert_used    = 0
0.00.052.266 I llm_load_print_meta: causal attn      = 1
0.00.052.266 I llm_load_print_meta: pooling type     = 0
0.00.052.266 I llm_load_print_meta: rope type        = 2
0.00.052.266 I llm_load_print_meta: rope scaling     = linear
0.00.052.267 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.267 I llm_load_print_meta: freq_scale_train = 1
0.00.052.267 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.268 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.268 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.268 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.269 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.269 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.270 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.281 I llm_load_print_meta: model type       = 1.4B
0.00.052.281 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.282 I llm_load_print_meta: model params     = 1.41 B
0.00.052.282 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.283 I llm_load_print_meta: general.name     = 1.4B
0.00.052.283 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.283 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.283 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.283 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.284 I llm_load_print_meta: LF token         = 128 ''
0.00.052.284 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.284 I llm_load_print_meta: max token length = 1024
0.00.054.209 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.209 I llm_load_tensors: offloading output layer to GPU
0.00.054.209 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.219 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.220 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.225 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.226 I llama_new_context_with_model: n_ctx         = 128
0.00.055.226 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.226 I llama_new_context_with_model: n_batch       = 128
0.00.055.226 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.226 I llama_new_context_with_model: flash_attn    = 0
0.00.055.227 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.227 I llama_new_context_with_model: freq_scale    = 1
0.00.055.227 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.228 I ggml_metal_init: allocating
0.00.055.231 I ggml_metal_init: found device: Apple M4
0.00.055.232 I ggml_metal_init: picking default device: Apple M4
0.00.055.765 I ggml_metal_init: using embedded metal library
0.00.057.702 I ggml_metal_init: GPU name:   Apple M4
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.703 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.704 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.704 I ggml_metal_init: simdgroup reduction   = true
0.00.057.704 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.704 I ggml_metal_init: has bfloat            = true
0.00.057.704 I ggml_metal_init: use bfloat            = true
0.00.057.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.705 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.114 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.117 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.132 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.041 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.042 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.042 I llama_new_context_with_model: graph nodes  = 967
0.00.068.042 I llama_new_context_with_model: graph splits = 2
0.00.068.055 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.655.892 I 
0.00.655.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.655.983 I perplexity: tokenizing the input ..
0.00.663.998 I perplexity: tokenization took 8.012 ms
0.00.664.002 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.500 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.787.719 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.787.739 I llama_perf_context_print:        load time =     645.75 ms
0.00.787.740 I llama_perf_context_print: prompt eval time =     122.27 ms /   128 tokens (    0.96 ms per token,  1046.84 tokens per second)
0.00.787.743 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.743 I llama_perf_context_print:       total time =     131.86 ms /   129 tokens
0.00.788.218 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.077s
sys	0m0.112s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.691 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.696 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.697 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.698 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.698 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.699 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.701 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.627 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.411 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.413 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.413 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.413 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.414 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.414 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.415 I llama_model_loader: - type  f32:  194 tensors
0.00.024.415 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.415 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.034 I llm_load_vocab: special tokens cache size = 25
0.00.051.061 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.064 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.064 I llm_load_print_meta: arch             = gptneox
0.00.051.064 I llm_load_print_meta: vocab type       = BPE
0.00.051.064 I llm_load_print_meta: n_vocab          = 50304
0.00.051.065 I llm_load_print_meta: n_merges         = 50009
0.00.051.065 I llm_load_print_meta: vocab_only       = 0
0.00.051.065 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.065 I llm_load_print_meta: n_embd           = 2048
0.00.051.065 I llm_load_print_meta: n_layer          = 24
0.00.051.068 I llm_load_print_meta: n_head           = 16
0.00.051.069 I llm_load_print_meta: n_head_kv        = 16
0.00.051.069 I llm_load_print_meta: n_rot            = 32
0.00.051.069 I llm_load_print_meta: n_swa            = 0
0.00.051.069 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.069 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.070 I llm_load_print_meta: n_gqa            = 1
0.00.051.071 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.071 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.072 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.072 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.072 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.073 I llm_load_print_meta: n_ff             = 8192
0.00.051.074 I llm_load_print_meta: n_expert         = 0
0.00.051.074 I llm_load_print_meta: n_expert_used    = 0
0.00.051.074 I llm_load_print_meta: causal attn      = 1
0.00.051.074 I llm_load_print_meta: pooling type     = 0
0.00.051.074 I llm_load_print_meta: rope type        = 2
0.00.051.074 I llm_load_print_meta: rope scaling     = linear
0.00.051.077 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.078 I llm_load_print_meta: freq_scale_train = 1
0.00.051.078 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.078 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.079 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.079 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.085 I llm_load_print_meta: model type       = 1.4B
0.00.051.085 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.086 I llm_load_print_meta: model params     = 1.41 B
0.00.051.086 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.086 I llm_load_print_meta: general.name     = 1.4B
0.00.051.087 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.088 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.088 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.088 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.088 I llm_load_print_meta: LF token         = 128 ''
0.00.051.089 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.089 I llm_load_print_meta: max token length = 1024
0.00.052.607 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.607 I llm_load_tensors: offloading output layer to GPU
0.00.052.607 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.611 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.612 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.498 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.499 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.499 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.499 I llama_new_context_with_model: n_batch       = 2048
0.00.053.499 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.500 I llama_new_context_with_model: flash_attn    = 0
0.00.053.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.500 I llama_new_context_with_model: freq_scale    = 1
0.00.053.501 I ggml_metal_init: allocating
0.00.053.507 I ggml_metal_init: found device: Apple M4
0.00.053.509 I ggml_metal_init: picking default device: Apple M4
0.00.054.036 I ggml_metal_init: using embedded metal library
0.00.055.952 I ggml_metal_init: GPU name:   Apple M4
0.00.055.953 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.954 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.954 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.954 I ggml_metal_init: simdgroup reduction   = true
0.00.055.954 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.954 I ggml_metal_init: has bfloat            = true
0.00.055.955 I ggml_metal_init: use bfloat            = true
0.00.055.955 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.956 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.743 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.750 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.767 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.662 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.664 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.664 I llama_new_context_with_model: graph nodes  = 967
0.00.084.664 I llama_new_context_with_model: graph splits = 2
0.00.084.672 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.752 I main: llama threadpool init, n_threads = 4
0.00.732.781 I 
0.00.732.806 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.732.807 I 
0.00.732.946 I sampler seed: 1234
0.00.732.951 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.732.961 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.732.961 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.732.961 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.463.284 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64195.30 tokens per second)
0.01.463.284 I llama_perf_context_print:        load time =     723.95 ms
0.01.463.285 I llama_perf_context_print: prompt eval time =      32.79 ms /     7 tokens (    4.68 ms per token,   213.49 tokens per second)
0.01.463.286 I llama_perf_context_print:        eval time =     694.64 ms /    63 runs   (   11.03 ms per token,    90.69 tokens per second)
0.01.463.286 I llama_perf_context_print:       total time =     730.53 ms /    70 tokens
0.01.463.453 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.108s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.769 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.756 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.760 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.763 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.763 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.766 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.767 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.769 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.769 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.535 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.619 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.491 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.492 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.492 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.492 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.493 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.493 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.494 I llama_model_loader: - type  f32:  194 tensors
0.00.023.494 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.494 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.097 I llm_load_vocab: special tokens cache size = 25
0.00.050.009 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.012 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.012 I llm_load_print_meta: arch             = gptneox
0.00.050.013 I llm_load_print_meta: vocab type       = BPE
0.00.050.013 I llm_load_print_meta: n_vocab          = 50304
0.00.050.013 I llm_load_print_meta: n_merges         = 50009
0.00.050.013 I llm_load_print_meta: vocab_only       = 0
0.00.050.014 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.014 I llm_load_print_meta: n_embd           = 2048
0.00.050.014 I llm_load_print_meta: n_layer          = 24
0.00.050.017 I llm_load_print_meta: n_head           = 16
0.00.050.017 I llm_load_print_meta: n_head_kv        = 16
0.00.050.018 I llm_load_print_meta: n_rot            = 32
0.00.050.018 I llm_load_print_meta: n_swa            = 0
0.00.050.018 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.018 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.019 I llm_load_print_meta: n_gqa            = 1
0.00.050.020 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.020 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.021 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.021 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.022 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.022 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.022 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.023 I llm_load_print_meta: n_ff             = 8192
0.00.050.023 I llm_load_print_meta: n_expert         = 0
0.00.050.023 I llm_load_print_meta: n_expert_used    = 0
0.00.050.023 I llm_load_print_meta: causal attn      = 1
0.00.050.023 I llm_load_print_meta: pooling type     = 0
0.00.050.023 I llm_load_print_meta: rope type        = 2
0.00.050.024 I llm_load_print_meta: rope scaling     = linear
0.00.050.024 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.024 I llm_load_print_meta: freq_scale_train = 1
0.00.050.025 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.025 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.026 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.038 I llm_load_print_meta: model type       = 1.4B
0.00.050.038 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.038 I llm_load_print_meta: model params     = 1.41 B
0.00.050.039 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.039 I llm_load_print_meta: general.name     = 1.4B
0.00.050.039 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.039 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.040 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.040 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.040 I llm_load_print_meta: LF token         = 128 ''
0.00.050.040 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.040 I llm_load_print_meta: max token length = 1024
0.00.052.082 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.082 I llm_load_tensors: offloading output layer to GPU
0.00.052.082 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.092 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.093 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.104 I llama_new_context_with_model: n_ctx         = 128
0.00.053.104 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.104 I llama_new_context_with_model: n_batch       = 128
0.00.053.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.104 I llama_new_context_with_model: flash_attn    = 0
0.00.053.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.105 I llama_new_context_with_model: freq_scale    = 1
0.00.053.105 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.106 I ggml_metal_init: allocating
0.00.053.109 I ggml_metal_init: found device: Apple M4
0.00.053.111 I ggml_metal_init: picking default device: Apple M4
0.00.053.662 I ggml_metal_init: using embedded metal library
0.00.055.598 I ggml_metal_init: GPU name:   Apple M4
0.00.055.600 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.600 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.601 I ggml_metal_init: simdgroup reduction   = true
0.00.055.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.601 I ggml_metal_init: has bfloat            = true
0.00.055.601 I ggml_metal_init: use bfloat            = true
0.00.055.602 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.602 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.293 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.296 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.312 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.228 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.229 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.229 I llama_new_context_with_model: graph nodes  = 967
0.00.066.230 I llama_new_context_with_model: graph splits = 2
0.00.066.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.953 I 
0.00.670.984 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.671.015 I perplexity: tokenizing the input ..
0.00.678.864 I perplexity: tokenization took 7.848 ms
0.00.678.867 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.096 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.802.254 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.802.269 I llama_perf_context_print:        load time =     662.18 ms
0.00.802.270 I llama_perf_context_print: prompt eval time =     122.00 ms /   128 tokens (    0.95 ms per token,  1049.21 tokens per second)
0.00.802.271 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.271 I llama_perf_context_print:       total time =     131.32 ms /   129 tokens
0.00.802.642 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.077s
sys	0m0.114s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.911 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.822 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.832 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.832 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.833 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.833 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.833 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.834 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.835 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.835 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.836 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.836 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.838 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.838 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.641 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.641 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.384 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.386 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.386 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.387 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.387 I llama_model_loader: - type  f32:  194 tensors
0.00.025.388 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.388 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.570 I llm_load_vocab: special tokens cache size = 25
0.00.052.532 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.534 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.535 I llm_load_print_meta: arch             = gptneox
0.00.052.535 I llm_load_print_meta: vocab type       = BPE
0.00.052.535 I llm_load_print_meta: n_vocab          = 50304
0.00.052.535 I llm_load_print_meta: n_merges         = 50009
0.00.052.536 I llm_load_print_meta: vocab_only       = 0
0.00.052.536 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.536 I llm_load_print_meta: n_embd           = 2048
0.00.052.536 I llm_load_print_meta: n_layer          = 24
0.00.052.539 I llm_load_print_meta: n_head           = 16
0.00.052.540 I llm_load_print_meta: n_head_kv        = 16
0.00.052.540 I llm_load_print_meta: n_rot            = 32
0.00.052.540 I llm_load_print_meta: n_swa            = 0
0.00.052.540 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.540 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.541 I llm_load_print_meta: n_gqa            = 1
0.00.052.542 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.543 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.543 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.544 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.544 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.544 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.544 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.545 I llm_load_print_meta: n_ff             = 8192
0.00.052.545 I llm_load_print_meta: n_expert         = 0
0.00.052.545 I llm_load_print_meta: n_expert_used    = 0
0.00.052.545 I llm_load_print_meta: causal attn      = 1
0.00.052.545 I llm_load_print_meta: pooling type     = 0
0.00.052.546 I llm_load_print_meta: rope type        = 2
0.00.052.546 I llm_load_print_meta: rope scaling     = linear
0.00.052.549 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.549 I llm_load_print_meta: freq_scale_train = 1
0.00.052.549 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.549 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.550 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.550 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.550 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.550 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.550 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.562 I llm_load_print_meta: model type       = 1.4B
0.00.052.562 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.562 I llm_load_print_meta: model params     = 1.41 B
0.00.052.563 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.563 I llm_load_print_meta: general.name     = 1.4B
0.00.052.563 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.563 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: LF token         = 128 ''
0.00.052.564 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.564 I llm_load_print_meta: max token length = 1024
0.00.054.659 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.660 I llm_load_tensors: offloading output layer to GPU
0.00.054.660 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.670 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.671 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.628 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.628 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.628 I llama_new_context_with_model: n_batch       = 2048
0.00.055.628 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.628 I llama_new_context_with_model: flash_attn    = 0
0.00.055.629 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.629 I llama_new_context_with_model: freq_scale    = 1
0.00.055.629 I ggml_metal_init: allocating
0.00.055.633 I ggml_metal_init: found device: Apple M4
0.00.055.635 I ggml_metal_init: picking default device: Apple M4
0.00.056.212 I ggml_metal_init: using embedded metal library
0.00.058.250 I ggml_metal_init: GPU name:   Apple M4
0.00.058.252 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.252 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.252 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.253 I ggml_metal_init: simdgroup reduction   = true
0.00.058.253 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.253 I ggml_metal_init: has bfloat            = true
0.00.058.253 I ggml_metal_init: use bfloat            = true
0.00.058.254 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.254 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.162 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.168 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.186 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.254 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.255 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.255 I llama_new_context_with_model: graph nodes  = 967
0.00.088.256 I llama_new_context_with_model: graph splits = 2
0.00.088.269 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.837.421 I main: llama threadpool init, n_threads = 4
0.00.837.454 I 
0.00.837.475 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.837.475 I 
0.00.837.695 I sampler seed: 1234
0.00.837.700 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.837.711 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.837.712 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.837.712 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.631.704 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.631.705 I llama_perf_context_print:        load time =     827.51 ms
0.01.631.706 I llama_perf_context_print: prompt eval time =      36.66 ms /     7 tokens (    5.24 ms per token,   190.93 tokens per second)
0.01.631.706 I llama_perf_context_print:        eval time =     754.38 ms /    63 runs   (   11.97 ms per token,    83.51 tokens per second)
0.01.631.707 I llama_perf_context_print:       total time =     794.28 ms /    70 tokens
0.01.631.902 I ggml_metal_free: deallocating

real	0m1.648s
user	0m0.110s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.582 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.454 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.460 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.461 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.463 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.463 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.464 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.464 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.465 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.466 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.467 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.320 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.197 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.198 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.199 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.199 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.200 I llama_model_loader: - type  f32:  194 tensors
0.00.024.200 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.201 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.431 I llm_load_vocab: special tokens cache size = 25
0.00.051.568 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.570 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.571 I llm_load_print_meta: arch             = gptneox
0.00.051.571 I llm_load_print_meta: vocab type       = BPE
0.00.051.571 I llm_load_print_meta: n_vocab          = 50304
0.00.051.571 I llm_load_print_meta: n_merges         = 50009
0.00.051.572 I llm_load_print_meta: vocab_only       = 0
0.00.051.572 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.572 I llm_load_print_meta: n_embd           = 2048
0.00.051.572 I llm_load_print_meta: n_layer          = 24
0.00.051.574 I llm_load_print_meta: n_head           = 16
0.00.051.575 I llm_load_print_meta: n_head_kv        = 16
0.00.051.575 I llm_load_print_meta: n_rot            = 32
0.00.051.575 I llm_load_print_meta: n_swa            = 0
0.00.051.575 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.578 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.579 I llm_load_print_meta: n_gqa            = 1
0.00.051.580 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.581 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.581 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.581 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.582 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.582 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.582 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.583 I llm_load_print_meta: n_ff             = 8192
0.00.051.583 I llm_load_print_meta: n_expert         = 0
0.00.051.583 I llm_load_print_meta: n_expert_used    = 0
0.00.051.583 I llm_load_print_meta: causal attn      = 1
0.00.051.583 I llm_load_print_meta: pooling type     = 0
0.00.051.583 I llm_load_print_meta: rope type        = 2
0.00.051.584 I llm_load_print_meta: rope scaling     = linear
0.00.051.584 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.584 I llm_load_print_meta: freq_scale_train = 1
0.00.051.584 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.585 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.585 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.585 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.585 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.585 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.585 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.597 I llm_load_print_meta: model type       = 1.4B
0.00.051.597 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.598 I llm_load_print_meta: model params     = 1.41 B
0.00.051.598 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.599 I llm_load_print_meta: general.name     = 1.4B
0.00.051.599 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.600 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.601 I llm_load_print_meta: LF token         = 128 ''
0.00.051.601 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.601 I llm_load_print_meta: max token length = 1024
0.00.053.642 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.642 I llm_load_tensors: offloading output layer to GPU
0.00.053.642 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.653 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.654 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.049 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.049 I llama_new_context_with_model: n_ctx         = 128
0.00.055.049 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.050 I llama_new_context_with_model: n_batch       = 128
0.00.055.050 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.050 I llama_new_context_with_model: flash_attn    = 0
0.00.055.050 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.051 I llama_new_context_with_model: freq_scale    = 1
0.00.055.051 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.052 I ggml_metal_init: allocating
0.00.055.057 I ggml_metal_init: found device: Apple M4
0.00.055.059 I ggml_metal_init: picking default device: Apple M4
0.00.055.575 I ggml_metal_init: using embedded metal library
0.00.057.479 I ggml_metal_init: GPU name:   Apple M4
0.00.057.480 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.480 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.481 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.481 I ggml_metal_init: simdgroup reduction   = true
0.00.057.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.481 I ggml_metal_init: has bfloat            = true
0.00.057.481 I ggml_metal_init: use bfloat            = true
0.00.057.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.482 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.823 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.827 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.852 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.731 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.732 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.732 I llama_new_context_with_model: graph nodes  = 967
0.00.067.732 I llama_new_context_with_model: graph splits = 2
0.00.067.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.209 I 
0.00.759.312 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.759.373 I perplexity: tokenizing the input ..
0.00.776.693 I perplexity: tokenization took 17.316 ms
0.00.776.712 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.915.260 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.916.724 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.916.745 I llama_perf_context_print:        load time =     749.61 ms
0.00.916.746 I llama_perf_context_print: prompt eval time =     137.63 ms /   128 tokens (    1.08 ms per token,   930.03 tokens per second)
0.00.916.747 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.916.747 I llama_perf_context_print:       total time =     157.55 ms /   129 tokens
0.00.917.518 I ggml_metal_free: deallocating

real	0m0.933s
user	0m0.090s
sys	0m0.136s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.674 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.098 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.102 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.103 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.104 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.104 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.105 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.106 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.106 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.107 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.107 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.107 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.108 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.109 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.109 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.110 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.016 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.101 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.936 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.937 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.937 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.938 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.938 I llama_model_loader: - type  f32:  194 tensors
0.00.024.939 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.939 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.775 I llm_load_vocab: special tokens cache size = 25
0.00.051.734 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.738 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.738 I llm_load_print_meta: arch             = gptneox
0.00.051.739 I llm_load_print_meta: vocab type       = BPE
0.00.051.739 I llm_load_print_meta: n_vocab          = 50304
0.00.051.739 I llm_load_print_meta: n_merges         = 50009
0.00.051.739 I llm_load_print_meta: vocab_only       = 0
0.00.051.741 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.741 I llm_load_print_meta: n_embd           = 2048
0.00.051.741 I llm_load_print_meta: n_layer          = 24
0.00.051.744 I llm_load_print_meta: n_head           = 16
0.00.051.745 I llm_load_print_meta: n_head_kv        = 16
0.00.051.745 I llm_load_print_meta: n_rot            = 32
0.00.051.745 I llm_load_print_meta: n_swa            = 0
0.00.051.745 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.746 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.746 I llm_load_print_meta: n_gqa            = 1
0.00.051.747 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.748 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.748 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.749 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.749 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.749 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.749 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.750 I llm_load_print_meta: n_ff             = 8192
0.00.051.750 I llm_load_print_meta: n_expert         = 0
0.00.051.750 I llm_load_print_meta: n_expert_used    = 0
0.00.051.750 I llm_load_print_meta: causal attn      = 1
0.00.051.750 I llm_load_print_meta: pooling type     = 0
0.00.051.750 I llm_load_print_meta: rope type        = 2
0.00.051.753 I llm_load_print_meta: rope scaling     = linear
0.00.051.753 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.754 I llm_load_print_meta: freq_scale_train = 1
0.00.051.754 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.754 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.754 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.754 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.754 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.755 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.755 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.766 I llm_load_print_meta: model type       = 1.4B
0.00.051.767 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.767 I llm_load_print_meta: model params     = 1.41 B
0.00.051.767 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.768 I llm_load_print_meta: general.name     = 1.4B
0.00.051.768 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.768 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.768 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.768 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.769 I llm_load_print_meta: LF token         = 128 ''
0.00.051.769 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.769 I llm_load_print_meta: max token length = 1024
0.00.053.901 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.901 I llm_load_tensors: offloading output layer to GPU
0.00.053.901 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.912 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.913 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.921 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.922 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.922 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.922 I llama_new_context_with_model: n_batch       = 2048
0.00.054.922 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.923 I llama_new_context_with_model: flash_attn    = 0
0.00.054.923 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.923 I llama_new_context_with_model: freq_scale    = 1
0.00.054.924 I ggml_metal_init: allocating
0.00.054.931 I ggml_metal_init: found device: Apple M4
0.00.054.934 I ggml_metal_init: picking default device: Apple M4
0.00.055.495 I ggml_metal_init: using embedded metal library
0.00.057.475 I ggml_metal_init: GPU name:   Apple M4
0.00.057.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.477 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.477 I ggml_metal_init: simdgroup reduction   = true
0.00.057.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.478 I ggml_metal_init: has bfloat            = true
0.00.057.478 I ggml_metal_init: use bfloat            = true
0.00.057.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.908 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.915 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.943 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.883 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.885 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.885 I llama_new_context_with_model: graph nodes  = 967
0.00.086.885 I llama_new_context_with_model: graph splits = 2
0.00.086.897 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.337 I main: llama threadpool init, n_threads = 4
0.00.831.370 I 
0.00.831.388 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.831.389 I 
0.00.831.545 I sampler seed: 1234
0.00.831.550 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.565 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.566 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.566 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.698.692 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57959.18 tokens per second)
0.01.698.692 I llama_perf_context_print:        load time =     822.66 ms
0.01.698.693 I llama_perf_context_print: prompt eval time =      36.50 ms /     7 tokens (    5.21 ms per token,   191.76 tokens per second)
0.01.698.694 I llama_perf_context_print:        eval time =     827.59 ms /    63 runs   (   13.14 ms per token,    76.12 tokens per second)
0.01.698.694 I llama_perf_context_print:       total time =     867.36 ms /    70 tokens
0.01.698.875 I ggml_metal_free: deallocating

real	0m1.714s
user	0m0.109s
sys	0m0.183s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.917 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.918 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.919 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.919 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.922 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.923 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.923 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.757 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.810 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.627 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.628 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.628 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.629 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.629 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.629 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.630 I llama_model_loader: - type  f32:  194 tensors
0.00.023.630 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.631 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.997 I llm_load_vocab: special tokens cache size = 25
0.00.051.031 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.033 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.034 I llm_load_print_meta: arch             = gptneox
0.00.051.034 I llm_load_print_meta: vocab type       = BPE
0.00.051.034 I llm_load_print_meta: n_vocab          = 50304
0.00.051.035 I llm_load_print_meta: n_merges         = 50009
0.00.051.035 I llm_load_print_meta: vocab_only       = 0
0.00.051.035 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.035 I llm_load_print_meta: n_embd           = 2048
0.00.051.035 I llm_load_print_meta: n_layer          = 24
0.00.051.038 I llm_load_print_meta: n_head           = 16
0.00.051.039 I llm_load_print_meta: n_head_kv        = 16
0.00.051.039 I llm_load_print_meta: n_rot            = 32
0.00.051.039 I llm_load_print_meta: n_swa            = 0
0.00.051.039 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.039 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.040 I llm_load_print_meta: n_gqa            = 1
0.00.051.041 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.042 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.042 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.043 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.043 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.043 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.043 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.044 I llm_load_print_meta: n_ff             = 8192
0.00.051.044 I llm_load_print_meta: n_expert         = 0
0.00.051.044 I llm_load_print_meta: n_expert_used    = 0
0.00.051.044 I llm_load_print_meta: causal attn      = 1
0.00.051.044 I llm_load_print_meta: pooling type     = 0
0.00.051.045 I llm_load_print_meta: rope type        = 2
0.00.051.045 I llm_load_print_meta: rope scaling     = linear
0.00.051.045 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.046 I llm_load_print_meta: freq_scale_train = 1
0.00.051.046 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.046 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.046 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.046 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.047 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.047 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.047 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.058 I llm_load_print_meta: model type       = 1.4B
0.00.051.059 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.059 I llm_load_print_meta: model params     = 1.41 B
0.00.051.060 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.060 I llm_load_print_meta: general.name     = 1.4B
0.00.051.060 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.060 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: LF token         = 128 ''
0.00.051.061 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.061 I llm_load_print_meta: max token length = 1024
0.00.053.108 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.108 I llm_load_tensors: offloading output layer to GPU
0.00.053.108 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.118 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.119 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.149 I llama_new_context_with_model: n_ctx         = 128
0.00.054.149 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.149 I llama_new_context_with_model: n_batch       = 128
0.00.054.149 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.149 I llama_new_context_with_model: flash_attn    = 0
0.00.054.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.150 I llama_new_context_with_model: freq_scale    = 1
0.00.054.150 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.151 I ggml_metal_init: allocating
0.00.054.154 I ggml_metal_init: found device: Apple M4
0.00.054.156 I ggml_metal_init: picking default device: Apple M4
0.00.054.718 I ggml_metal_init: using embedded metal library
0.00.056.711 I ggml_metal_init: GPU name:   Apple M4
0.00.056.713 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.713 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.714 I ggml_metal_init: simdgroup reduction   = true
0.00.056.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.714 I ggml_metal_init: has bfloat            = true
0.00.056.714 I ggml_metal_init: use bfloat            = true
0.00.056.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.715 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.344 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.346 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.360 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.279 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.280 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.280 I llama_new_context_with_model: graph nodes  = 967
0.00.067.281 I llama_new_context_with_model: graph splits = 2
0.00.067.293 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.858 I 
0.00.761.894 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.761.928 I perplexity: tokenizing the input ..
0.00.769.573 I perplexity: tokenization took 7.653 ms
0.00.769.578 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.904.918 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.906.191 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.906.227 I llama_perf_context_print:        load time =     753.08 ms
0.00.906.229 I llama_perf_context_print: prompt eval time =     135.09 ms /   128 tokens (    1.06 ms per token,   947.48 tokens per second)
0.00.906.230 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.906.230 I llama_perf_context_print:       total time =     144.37 ms /   129 tokens
0.00.906.708 I ggml_metal_free: deallocating

real	0m0.921s
user	0m0.078s
sys	0m0.124s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.010.788 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.307 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.308 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.308 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.308 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.311 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.313 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.196 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.204 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.069 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.072 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.072 I llama_model_loader: - type  f32:  194 tensors
0.00.025.073 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.073 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.073 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.741 I llm_load_vocab: special tokens cache size = 25
0.00.051.702 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.705 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.705 I llm_load_print_meta: arch             = gptneox
0.00.051.706 I llm_load_print_meta: vocab type       = BPE
0.00.051.706 I llm_load_print_meta: n_vocab          = 50304
0.00.051.706 I llm_load_print_meta: n_merges         = 50009
0.00.051.706 I llm_load_print_meta: vocab_only       = 0
0.00.051.706 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.706 I llm_load_print_meta: n_embd           = 2048
0.00.051.707 I llm_load_print_meta: n_layer          = 24
0.00.051.710 I llm_load_print_meta: n_head           = 16
0.00.051.711 I llm_load_print_meta: n_head_kv        = 16
0.00.051.711 I llm_load_print_meta: n_rot            = 32
0.00.051.711 I llm_load_print_meta: n_swa            = 0
0.00.051.711 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.711 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.712 I llm_load_print_meta: n_gqa            = 1
0.00.051.713 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.713 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.714 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.714 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.715 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.715 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.715 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.716 I llm_load_print_meta: n_ff             = 8192
0.00.051.716 I llm_load_print_meta: n_expert         = 0
0.00.051.716 I llm_load_print_meta: n_expert_used    = 0
0.00.051.716 I llm_load_print_meta: causal attn      = 1
0.00.051.716 I llm_load_print_meta: pooling type     = 0
0.00.051.716 I llm_load_print_meta: rope type        = 2
0.00.051.717 I llm_load_print_meta: rope scaling     = linear
0.00.051.717 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.720 I llm_load_print_meta: freq_scale_train = 1
0.00.051.721 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.721 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.721 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.721 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.721 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.721 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.722 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.733 I llm_load_print_meta: model type       = 1.4B
0.00.051.733 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.734 I llm_load_print_meta: model params     = 1.41 B
0.00.051.734 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.734 I llm_load_print_meta: general.name     = 1.4B
0.00.051.735 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.735 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.735 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.735 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.735 I llm_load_print_meta: LF token         = 128 ''
0.00.051.737 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.737 I llm_load_print_meta: max token length = 1024
0.00.053.353 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.353 I llm_load_tensors: offloading output layer to GPU
0.00.053.354 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.363 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.364 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.265 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.265 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.265 I llama_new_context_with_model: n_batch       = 2048
0.00.054.266 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.266 I llama_new_context_with_model: flash_attn    = 0
0.00.054.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.266 I llama_new_context_with_model: freq_scale    = 1
0.00.054.267 I ggml_metal_init: allocating
0.00.054.270 I ggml_metal_init: found device: Apple M4
0.00.054.272 I ggml_metal_init: picking default device: Apple M4
0.00.054.853 I ggml_metal_init: using embedded metal library
0.00.056.811 I ggml_metal_init: GPU name:   Apple M4
0.00.056.813 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.813 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.814 I ggml_metal_init: simdgroup reduction   = true
0.00.056.814 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.814 I ggml_metal_init: has bfloat            = true
0.00.056.814 I ggml_metal_init: use bfloat            = true
0.00.056.815 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.582 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.591 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.610 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.553 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.554 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.555 I llama_new_context_with_model: graph nodes  = 967
0.00.085.555 I llama_new_context_with_model: graph splits = 2
0.00.085.570 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.570.313 I main: llama threadpool init, n_threads = 4
0.00.570.342 I 
0.00.570.370 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.570.371 I 
0.00.570.627 I sampler seed: 1234
0.00.570.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.570.674 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.570.691 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.570.691 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.252.520 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 58970.10 tokens per second)
0.01.252.521 I llama_perf_context_print:        load time =     559.52 ms
0.01.252.522 I llama_perf_context_print: prompt eval time =      36.11 ms /     7 tokens (    5.16 ms per token,   193.84 tokens per second)
0.01.252.522 I llama_perf_context_print:        eval time =     642.64 ms /    63 runs   (   10.20 ms per token,    98.03 tokens per second)
0.01.252.523 I llama_perf_context_print:       total time =     682.21 ms /    70 tokens
0.01.252.702 I ggml_metal_free: deallocating

real	0m1.269s
user	0m0.109s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.707 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.414 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.419 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.420 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.421 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.421 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.422 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.424 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.426 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.239 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.264 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.038 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.039 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.039 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.040 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.040 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.041 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.041 I llama_model_loader: - type  f32:  194 tensors
0.00.024.041 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.042 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.042 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.614 I llm_load_vocab: special tokens cache size = 25
0.00.050.570 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.573 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.573 I llm_load_print_meta: arch             = gptneox
0.00.050.573 I llm_load_print_meta: vocab type       = BPE
0.00.050.574 I llm_load_print_meta: n_vocab          = 50304
0.00.050.574 I llm_load_print_meta: n_merges         = 50009
0.00.050.574 I llm_load_print_meta: vocab_only       = 0
0.00.050.574 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.574 I llm_load_print_meta: n_embd           = 2048
0.00.050.574 I llm_load_print_meta: n_layer          = 24
0.00.050.577 I llm_load_print_meta: n_head           = 16
0.00.050.578 I llm_load_print_meta: n_head_kv        = 16
0.00.050.578 I llm_load_print_meta: n_rot            = 32
0.00.050.578 I llm_load_print_meta: n_swa            = 0
0.00.050.578 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.579 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.580 I llm_load_print_meta: n_gqa            = 1
0.00.050.581 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.582 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.582 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.583 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.583 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.583 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.583 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.584 I llm_load_print_meta: n_ff             = 8192
0.00.050.584 I llm_load_print_meta: n_expert         = 0
0.00.050.584 I llm_load_print_meta: n_expert_used    = 0
0.00.050.585 I llm_load_print_meta: causal attn      = 1
0.00.050.585 I llm_load_print_meta: pooling type     = 0
0.00.050.585 I llm_load_print_meta: rope type        = 2
0.00.050.585 I llm_load_print_meta: rope scaling     = linear
0.00.050.587 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.587 I llm_load_print_meta: freq_scale_train = 1
0.00.050.588 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.588 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.588 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.588 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.588 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.588 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.588 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.600 I llm_load_print_meta: model type       = 1.4B
0.00.050.600 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.601 I llm_load_print_meta: model params     = 1.41 B
0.00.050.601 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.602 I llm_load_print_meta: general.name     = 1.4B
0.00.050.602 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.602 I llm_load_print_meta: LF token         = 128 ''
0.00.050.603 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.603 I llm_load_print_meta: max token length = 1024
0.00.052.460 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.461 I llm_load_tensors: offloading output layer to GPU
0.00.052.461 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.471 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.472 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.392 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.392 I llama_new_context_with_model: n_ctx         = 128
0.00.053.393 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.393 I llama_new_context_with_model: n_batch       = 128
0.00.053.393 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.393 I llama_new_context_with_model: flash_attn    = 0
0.00.053.393 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.394 I llama_new_context_with_model: freq_scale    = 1
0.00.053.394 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.394 I ggml_metal_init: allocating
0.00.053.398 I ggml_metal_init: found device: Apple M4
0.00.053.399 I ggml_metal_init: picking default device: Apple M4
0.00.053.939 I ggml_metal_init: using embedded metal library
0.00.055.887 I ggml_metal_init: GPU name:   Apple M4
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.889 I ggml_metal_init: simdgroup reduction   = true
0.00.055.889 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.889 I ggml_metal_init: has bfloat            = true
0.00.055.889 I ggml_metal_init: use bfloat            = true
0.00.055.890 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.251 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.257 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.270 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.201 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.202 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.202 I llama_new_context_with_model: graph nodes  = 967
0.00.066.203 I llama_new_context_with_model: graph splits = 2
0.00.066.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.046 I 
0.00.497.080 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.497.108 I perplexity: tokenizing the input ..
0.00.504.826 I perplexity: tokenization took 7.719 ms
0.00.504.829 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.637.379 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.638.641 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.638.661 I llama_perf_context_print:        load time =     487.33 ms
0.00.638.661 I llama_perf_context_print: prompt eval time =     132.33 ms /   128 tokens (    1.03 ms per token,   967.31 tokens per second)
0.00.638.662 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.638.662 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.639.144 I ggml_metal_free: deallocating

real	0m0.654s
user	0m0.076s
sys	0m0.090s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.012.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.371 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.018.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.379 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.380 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.381 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.382 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.382 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.382 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.383 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.383 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.383 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.385 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.385 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.385 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.168 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.240 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.070 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.071 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.072 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.072 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.027.073 I llama_model_loader: - type  f32:  194 tensors
0.00.027.073 I llama_model_loader: - type q3_K:   25 tensors
0.00.027.073 I llama_model_loader: - type q4_K:   71 tensors
0.00.027.073 I llama_model_loader: - type q5_K:    1 tensors
0.00.027.074 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.929 I llm_load_vocab: special tokens cache size = 25
0.00.053.978 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.981 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.981 I llm_load_print_meta: arch             = gptneox
0.00.053.981 I llm_load_print_meta: vocab type       = BPE
0.00.053.982 I llm_load_print_meta: n_vocab          = 50304
0.00.053.982 I llm_load_print_meta: n_merges         = 50009
0.00.053.982 I llm_load_print_meta: vocab_only       = 0
0.00.053.982 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.982 I llm_load_print_meta: n_embd           = 2048
0.00.053.982 I llm_load_print_meta: n_layer          = 24
0.00.053.985 I llm_load_print_meta: n_head           = 16
0.00.053.986 I llm_load_print_meta: n_head_kv        = 16
0.00.053.988 I llm_load_print_meta: n_rot            = 32
0.00.053.988 I llm_load_print_meta: n_swa            = 0
0.00.053.989 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.989 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.989 I llm_load_print_meta: n_gqa            = 1
0.00.053.990 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.991 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.992 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.992 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.992 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.992 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.992 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.993 I llm_load_print_meta: n_ff             = 8192
0.00.053.993 I llm_load_print_meta: n_expert         = 0
0.00.053.993 I llm_load_print_meta: n_expert_used    = 0
0.00.053.993 I llm_load_print_meta: causal attn      = 1
0.00.053.994 I llm_load_print_meta: pooling type     = 0
0.00.053.994 I llm_load_print_meta: rope type        = 2
0.00.053.994 I llm_load_print_meta: rope scaling     = linear
0.00.053.994 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.995 I llm_load_print_meta: freq_scale_train = 1
0.00.053.995 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.995 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.995 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.995 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.995 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.996 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.996 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.007 I llm_load_print_meta: model type       = 1.4B
0.00.054.008 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.054.008 I llm_load_print_meta: model params     = 1.41 B
0.00.054.009 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.054.009 I llm_load_print_meta: general.name     = 1.4B
0.00.054.009 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.009 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.009 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.009 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.010 I llm_load_print_meta: LF token         = 128 ''
0.00.054.010 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.010 I llm_load_print_meta: max token length = 1024
0.00.055.978 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.979 I llm_load_tensors: offloading output layer to GPU
0.00.055.979 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.989 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.055.990 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.056.993 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.994 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.994 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.994 I llama_new_context_with_model: n_batch       = 2048
0.00.056.995 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.995 I llama_new_context_with_model: flash_attn    = 0
0.00.056.995 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.995 I llama_new_context_with_model: freq_scale    = 1
0.00.056.996 I ggml_metal_init: allocating
0.00.056.999 I ggml_metal_init: found device: Apple M4
0.00.057.001 I ggml_metal_init: picking default device: Apple M4
0.00.057.549 I ggml_metal_init: using embedded metal library
0.00.059.514 I ggml_metal_init: GPU name:   Apple M4
0.00.059.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.516 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.517 I ggml_metal_init: simdgroup reduction   = true
0.00.059.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.517 I ggml_metal_init: has bfloat            = true
0.00.059.517 I ggml_metal_init: use bfloat            = true
0.00.059.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.365 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.376 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.398 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.402 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.403 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.403 I llama_new_context_with_model: graph nodes  = 967
0.00.089.404 I llama_new_context_with_model: graph splits = 2
0.00.089.418 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.611.419 I main: llama threadpool init, n_threads = 4
0.00.611.450 I 
0.00.611.470 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.611.470 I 
0.00.611.706 I sampler seed: 1234
0.00.611.712 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.611.723 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.611.725 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.611.725 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.356.910 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.356.911 I llama_perf_context_print:        load time =     598.64 ms
0.01.356.912 I llama_perf_context_print: prompt eval time =      35.55 ms /     7 tokens (    5.08 ms per token,   196.88 tokens per second)
0.01.356.912 I llama_perf_context_print:        eval time =     706.57 ms /    63 runs   (   11.22 ms per token,    89.16 tokens per second)
0.01.356.913 I llama_perf_context_print:       total time =     745.50 ms /    70 tokens
0.01.357.085 I ggml_metal_free: deallocating

real	0m1.371s
user	0m0.108s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.204 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.001 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.005 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.007 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.008 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.008 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.008 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.009 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.010 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.011 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.833 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.877 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.878 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.878 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.879 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.879 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.879 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.880 I llama_model_loader: - type  f32:  194 tensors
0.00.023.880 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.880 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.881 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.881 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.258 I llm_load_vocab: special tokens cache size = 25
0.00.051.208 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.212 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.213 I llm_load_print_meta: arch             = gptneox
0.00.051.213 I llm_load_print_meta: vocab type       = BPE
0.00.051.213 I llm_load_print_meta: n_vocab          = 50304
0.00.051.219 I llm_load_print_meta: n_merges         = 50009
0.00.051.219 I llm_load_print_meta: vocab_only       = 0
0.00.051.219 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.219 I llm_load_print_meta: n_embd           = 2048
0.00.051.220 I llm_load_print_meta: n_layer          = 24
0.00.051.223 I llm_load_print_meta: n_head           = 16
0.00.051.223 I llm_load_print_meta: n_head_kv        = 16
0.00.051.224 I llm_load_print_meta: n_rot            = 32
0.00.051.224 I llm_load_print_meta: n_swa            = 0
0.00.051.224 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.224 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.225 I llm_load_print_meta: n_gqa            = 1
0.00.051.226 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.226 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.227 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.227 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.228 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.228 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.228 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.229 I llm_load_print_meta: n_ff             = 8192
0.00.051.229 I llm_load_print_meta: n_expert         = 0
0.00.051.229 I llm_load_print_meta: n_expert_used    = 0
0.00.051.229 I llm_load_print_meta: causal attn      = 1
0.00.051.229 I llm_load_print_meta: pooling type     = 0
0.00.051.230 I llm_load_print_meta: rope type        = 2
0.00.051.230 I llm_load_print_meta: rope scaling     = linear
0.00.051.230 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.231 I llm_load_print_meta: freq_scale_train = 1
0.00.051.231 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.231 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.231 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.231 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.232 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.232 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.232 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.243 I llm_load_print_meta: model type       = 1.4B
0.00.051.244 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.244 I llm_load_print_meta: model params     = 1.41 B
0.00.051.245 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.245 I llm_load_print_meta: general.name     = 1.4B
0.00.051.245 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.245 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: LF token         = 128 ''
0.00.051.247 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.247 I llm_load_print_meta: max token length = 1024
0.00.053.253 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.254 I llm_load_tensors: offloading output layer to GPU
0.00.053.254 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.264 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.265 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.273 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.273 I llama_new_context_with_model: n_ctx         = 128
0.00.054.274 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.274 I llama_new_context_with_model: n_batch       = 128
0.00.054.274 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.274 I llama_new_context_with_model: flash_attn    = 0
0.00.054.274 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.275 I llama_new_context_with_model: freq_scale    = 1
0.00.054.275 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.276 I ggml_metal_init: allocating
0.00.054.279 I ggml_metal_init: found device: Apple M4
0.00.054.281 I ggml_metal_init: picking default device: Apple M4
0.00.054.822 I ggml_metal_init: using embedded metal library
0.00.056.790 I ggml_metal_init: GPU name:   Apple M4
0.00.056.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.792 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.793 I ggml_metal_init: simdgroup reduction   = true
0.00.056.793 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.793 I ggml_metal_init: has bfloat            = true
0.00.056.793 I ggml_metal_init: use bfloat            = true
0.00.056.794 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.794 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.378 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.380 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.394 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.339 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.340 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.340 I llama_new_context_with_model: graph nodes  = 967
0.00.067.340 I llama_new_context_with_model: graph splits = 2
0.00.067.353 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.033 I 
0.00.532.079 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.532.104 I perplexity: tokenizing the input ..
0.00.540.445 I perplexity: tokenization took 8.34 ms
0.00.540.449 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.671.755 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.672.987 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.673.001 I llama_perf_context_print:        load time =     522.83 ms
0.00.673.002 I llama_perf_context_print: prompt eval time =     131.08 ms /   128 tokens (    1.02 ms per token,   976.51 tokens per second)
0.00.673.003 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.673.003 I llama_perf_context_print:       total time =     140.97 ms /   129 tokens
0.00.673.293 I ggml_metal_free: deallocating

real	0m0.684s
user	0m0.078s
sys	0m0.097s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.322 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.769 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.773 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.775 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.775 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.776 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.776 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.777 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.777 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.778 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.778 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.778 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.779 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.779 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.781 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.782 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.782 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.357 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.358 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.358 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.359 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.359 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.360 I llama_model_loader: - type  f32:  194 tensors
0.00.025.360 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.360 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.361 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.956 I llm_load_vocab: special tokens cache size = 25
0.00.051.866 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.869 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.869 I llm_load_print_meta: arch             = gptneox
0.00.051.870 I llm_load_print_meta: vocab type       = BPE
0.00.051.870 I llm_load_print_meta: n_vocab          = 50304
0.00.051.870 I llm_load_print_meta: n_merges         = 50009
0.00.051.870 I llm_load_print_meta: vocab_only       = 0
0.00.051.870 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.871 I llm_load_print_meta: n_embd           = 2048
0.00.051.871 I llm_load_print_meta: n_layer          = 24
0.00.051.873 I llm_load_print_meta: n_head           = 16
0.00.051.874 I llm_load_print_meta: n_head_kv        = 16
0.00.051.874 I llm_load_print_meta: n_rot            = 32
0.00.051.874 I llm_load_print_meta: n_swa            = 0
0.00.051.874 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.875 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.875 I llm_load_print_meta: n_gqa            = 1
0.00.051.876 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.877 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.877 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.878 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.878 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.878 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.878 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.882 I llm_load_print_meta: n_ff             = 8192
0.00.051.882 I llm_load_print_meta: n_expert         = 0
0.00.051.883 I llm_load_print_meta: n_expert_used    = 0
0.00.051.884 I llm_load_print_meta: causal attn      = 1
0.00.051.884 I llm_load_print_meta: pooling type     = 0
0.00.051.884 I llm_load_print_meta: rope type        = 2
0.00.051.884 I llm_load_print_meta: rope scaling     = linear
0.00.051.885 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.885 I llm_load_print_meta: freq_scale_train = 1
0.00.051.885 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.886 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.886 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.886 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.886 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.892 I llm_load_print_meta: model type       = 1.4B
0.00.051.892 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.893 I llm_load_print_meta: model params     = 1.41 B
0.00.051.893 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.893 I llm_load_print_meta: general.name     = 1.4B
0.00.051.893 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.894 I llm_load_print_meta: LF token         = 128 ''
0.00.051.894 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.895 I llm_load_print_meta: max token length = 1024
0.00.053.412 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.412 I llm_load_tensors: offloading output layer to GPU
0.00.053.412 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.416 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.418 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.263 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.264 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.264 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.264 I llama_new_context_with_model: n_batch       = 2048
0.00.054.264 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.265 I llama_new_context_with_model: flash_attn    = 0
0.00.054.265 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.265 I llama_new_context_with_model: freq_scale    = 1
0.00.054.266 I ggml_metal_init: allocating
0.00.054.269 I ggml_metal_init: found device: Apple M4
0.00.054.271 I ggml_metal_init: picking default device: Apple M4
0.00.054.830 I ggml_metal_init: using embedded metal library
0.00.056.813 I ggml_metal_init: GPU name:   Apple M4
0.00.056.814 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.815 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.815 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.816 I ggml_metal_init: simdgroup reduction   = true
0.00.056.816 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.817 I ggml_metal_init: has bfloat            = true
0.00.056.817 I ggml_metal_init: use bfloat            = true
0.00.056.817 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.818 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.300 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.304 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.322 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.226 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.227 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.227 I llama_new_context_with_model: graph nodes  = 967
0.00.085.228 I llama_new_context_with_model: graph splits = 2
0.00.085.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.568 I main: llama threadpool init, n_threads = 4
0.00.667.598 I 
0.00.667.619 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.667.619 I 
0.00.667.755 I sampler seed: 1234
0.00.667.760 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.667.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.667.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.667.770 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.423.083 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53665.91 tokens per second)
0.01.423.083 I llama_perf_context_print:        load time =     657.24 ms
0.01.423.084 I llama_perf_context_print: prompt eval time =      36.41 ms /     7 tokens (    5.20 ms per token,   192.24 tokens per second)
0.01.423.085 I llama_perf_context_print:        eval time =     715.70 ms /    63 runs   (   11.36 ms per token,    88.03 tokens per second)
0.01.423.085 I llama_perf_context_print:       total time =     755.52 ms /    70 tokens
0.01.423.248 I ggml_metal_free: deallocating

real	0m1.438s
user	0m0.109s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.258 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.145 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.146 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.146 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.146 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.147 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.147 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.147 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.148 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.149 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.150 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.978 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.979 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.979 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.979 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.980 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.980 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.981 I llama_model_loader: - type  f32:  194 tensors
0.00.022.981 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.981 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.981 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.524 I llm_load_vocab: special tokens cache size = 25
0.00.049.518 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.521 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.521 I llm_load_print_meta: arch             = gptneox
0.00.049.522 I llm_load_print_meta: vocab type       = BPE
0.00.049.522 I llm_load_print_meta: n_vocab          = 50304
0.00.049.522 I llm_load_print_meta: n_merges         = 50009
0.00.049.522 I llm_load_print_meta: vocab_only       = 0
0.00.049.522 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.522 I llm_load_print_meta: n_embd           = 2048
0.00.049.523 I llm_load_print_meta: n_layer          = 24
0.00.049.525 I llm_load_print_meta: n_head           = 16
0.00.049.526 I llm_load_print_meta: n_head_kv        = 16
0.00.049.526 I llm_load_print_meta: n_rot            = 32
0.00.049.526 I llm_load_print_meta: n_swa            = 0
0.00.049.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.530 I llm_load_print_meta: n_gqa            = 1
0.00.049.530 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.532 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.533 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.533 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.533 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.533 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.534 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.534 I llm_load_print_meta: n_ff             = 8192
0.00.049.536 I llm_load_print_meta: n_expert         = 0
0.00.049.536 I llm_load_print_meta: n_expert_used    = 0
0.00.049.536 I llm_load_print_meta: causal attn      = 1
0.00.049.536 I llm_load_print_meta: pooling type     = 0
0.00.049.536 I llm_load_print_meta: rope type        = 2
0.00.049.537 I llm_load_print_meta: rope scaling     = linear
0.00.049.537 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.537 I llm_load_print_meta: freq_scale_train = 1
0.00.049.538 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.538 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.538 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.538 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.538 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.538 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.550 I llm_load_print_meta: model type       = 1.4B
0.00.049.550 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.551 I llm_load_print_meta: model params     = 1.41 B
0.00.049.552 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.552 I llm_load_print_meta: general.name     = 1.4B
0.00.049.552 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.552 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.552 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.552 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.553 I llm_load_print_meta: LF token         = 128 ''
0.00.049.553 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.553 I llm_load_print_meta: max token length = 1024
0.00.051.576 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.576 I llm_load_tensors: offloading output layer to GPU
0.00.051.577 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.587 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.588 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.553 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.553 I llama_new_context_with_model: n_ctx         = 128
0.00.052.554 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.554 I llama_new_context_with_model: n_batch       = 128
0.00.052.554 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.554 I llama_new_context_with_model: flash_attn    = 0
0.00.052.555 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.555 I llama_new_context_with_model: freq_scale    = 1
0.00.052.555 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.556 I ggml_metal_init: allocating
0.00.052.561 I ggml_metal_init: found device: Apple M4
0.00.052.563 I ggml_metal_init: picking default device: Apple M4
0.00.053.091 I ggml_metal_init: using embedded metal library
0.00.055.001 I ggml_metal_init: GPU name:   Apple M4
0.00.055.002 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.003 I ggml_metal_init: simdgroup reduction   = true
0.00.055.003 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.003 I ggml_metal_init: has bfloat            = true
0.00.055.003 I ggml_metal_init: use bfloat            = true
0.00.055.004 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.004 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.296 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.299 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.314 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.146 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.147 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.147 I llama_new_context_with_model: graph nodes  = 967
0.00.065.147 I llama_new_context_with_model: graph splits = 2
0.00.065.155 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.601.322 I 
0.00.601.369 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.601.408 I perplexity: tokenizing the input ..
0.00.609.472 I perplexity: tokenization took 8.063 ms
0.00.609.474 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.744.045 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.745.312 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.745.329 I llama_perf_context_print:        load time =     593.05 ms
0.00.745.330 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.71 tokens per second)
0.00.745.331 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.745.331 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.745.788 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.076s
sys	0m0.119s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.047 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.675 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.676 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.676 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.677 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.678 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.679 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.679 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.680 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.680 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.682 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.682 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.682 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.633 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.532 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.534 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.534 I llama_model_loader: - type  f32:  194 tensors
0.00.024.535 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.535 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.986 I llm_load_vocab: special tokens cache size = 25
0.00.052.023 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.025 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.026 I llm_load_print_meta: arch             = gptneox
0.00.052.026 I llm_load_print_meta: vocab type       = BPE
0.00.052.026 I llm_load_print_meta: n_vocab          = 50304
0.00.052.026 I llm_load_print_meta: n_merges         = 50009
0.00.052.027 I llm_load_print_meta: vocab_only       = 0
0.00.052.027 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.027 I llm_load_print_meta: n_embd           = 2048
0.00.052.027 I llm_load_print_meta: n_layer          = 24
0.00.052.029 I llm_load_print_meta: n_head           = 16
0.00.052.030 I llm_load_print_meta: n_head_kv        = 16
0.00.052.030 I llm_load_print_meta: n_rot            = 32
0.00.052.030 I llm_load_print_meta: n_swa            = 0
0.00.052.033 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.033 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.034 I llm_load_print_meta: n_gqa            = 1
0.00.052.035 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.035 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.036 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.036 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.036 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.036 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.037 I llm_load_print_meta: n_ff             = 8192
0.00.052.037 I llm_load_print_meta: n_expert         = 0
0.00.052.038 I llm_load_print_meta: n_expert_used    = 0
0.00.052.038 I llm_load_print_meta: causal attn      = 1
0.00.052.038 I llm_load_print_meta: pooling type     = 0
0.00.052.038 I llm_load_print_meta: rope type        = 2
0.00.052.038 I llm_load_print_meta: rope scaling     = linear
0.00.052.039 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.039 I llm_load_print_meta: freq_scale_train = 1
0.00.052.039 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.039 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.039 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.040 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.040 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.040 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.040 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.053 I llm_load_print_meta: model type       = 1.4B
0.00.052.053 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.054 I llm_load_print_meta: model params     = 1.41 B
0.00.052.054 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.054 I llm_load_print_meta: general.name     = 1.4B
0.00.052.054 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.055 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.055 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.055 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.056 I llm_load_print_meta: LF token         = 128 ''
0.00.052.056 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.057 I llm_load_print_meta: max token length = 1024
0.00.054.102 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.102 I llm_load_tensors: offloading output layer to GPU
0.00.054.102 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.112 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.113 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.055.043 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.044 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.044 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.044 I llama_new_context_with_model: n_batch       = 2048
0.00.055.044 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.044 I llama_new_context_with_model: flash_attn    = 0
0.00.055.045 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.045 I llama_new_context_with_model: freq_scale    = 1
0.00.055.046 I ggml_metal_init: allocating
0.00.055.054 I ggml_metal_init: found device: Apple M4
0.00.055.056 I ggml_metal_init: picking default device: Apple M4
0.00.055.624 I ggml_metal_init: using embedded metal library
0.00.057.578 I ggml_metal_init: GPU name:   Apple M4
0.00.057.580 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.580 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.581 I ggml_metal_init: simdgroup reduction   = true
0.00.057.581 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.581 I ggml_metal_init: has bfloat            = true
0.00.057.581 I ggml_metal_init: use bfloat            = true
0.00.057.581 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.582 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.305 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.326 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.318 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.319 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.320 I llama_new_context_with_model: graph nodes  = 967
0.00.089.320 I llama_new_context_with_model: graph splits = 2
0.00.089.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.077 I main: llama threadpool init, n_threads = 4
0.00.755.110 I 
0.00.755.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.755.159 I 
0.00.755.384 I sampler seed: 1234
0.00.755.390 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.419 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.420 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.420 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.598.643 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.01.598.644 I llama_perf_context_print:        load time =     746.03 ms
0.01.598.645 I llama_perf_context_print: prompt eval time =      38.76 ms /     7 tokens (    5.54 ms per token,   180.61 tokens per second)
0.01.598.645 I llama_perf_context_print:        eval time =     801.46 ms /    63 runs   (   12.72 ms per token,    78.61 tokens per second)
0.01.598.646 I llama_perf_context_print:       total time =     843.57 ms /    70 tokens
0.01.598.810 I ggml_metal_free: deallocating

real	0m1.614s
user	0m0.109s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.418 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.410 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.415 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.417 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.417 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.419 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.420 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.420 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.422 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.272 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.239 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.240 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.240 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.241 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.242 I llama_model_loader: - type  f32:  194 tensors
0.00.024.242 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.242 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.555 I llm_load_vocab: special tokens cache size = 25
0.00.051.304 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.307 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.307 I llm_load_print_meta: arch             = gptneox
0.00.051.308 I llm_load_print_meta: vocab type       = BPE
0.00.051.308 I llm_load_print_meta: n_vocab          = 50304
0.00.051.308 I llm_load_print_meta: n_merges         = 50009
0.00.051.308 I llm_load_print_meta: vocab_only       = 0
0.00.051.309 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.309 I llm_load_print_meta: n_embd           = 2048
0.00.051.309 I llm_load_print_meta: n_layer          = 24
0.00.051.311 I llm_load_print_meta: n_head           = 16
0.00.051.312 I llm_load_print_meta: n_head_kv        = 16
0.00.051.312 I llm_load_print_meta: n_rot            = 32
0.00.051.313 I llm_load_print_meta: n_swa            = 0
0.00.051.313 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.313 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.314 I llm_load_print_meta: n_gqa            = 1
0.00.051.315 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.315 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.316 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.316 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.316 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.316 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.317 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.317 I llm_load_print_meta: n_ff             = 8192
0.00.051.318 I llm_load_print_meta: n_expert         = 0
0.00.051.318 I llm_load_print_meta: n_expert_used    = 0
0.00.051.318 I llm_load_print_meta: causal attn      = 1
0.00.051.318 I llm_load_print_meta: pooling type     = 0
0.00.051.318 I llm_load_print_meta: rope type        = 2
0.00.051.318 I llm_load_print_meta: rope scaling     = linear
0.00.051.319 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.319 I llm_load_print_meta: freq_scale_train = 1
0.00.051.319 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.320 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.320 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.320 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.320 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.320 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.320 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.331 I llm_load_print_meta: model type       = 1.4B
0.00.051.331 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.332 I llm_load_print_meta: model params     = 1.41 B
0.00.051.332 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.333 I llm_load_print_meta: general.name     = 1.4B
0.00.051.333 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.333 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.333 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.333 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.333 I llm_load_print_meta: LF token         = 128 ''
0.00.051.334 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.334 I llm_load_print_meta: max token length = 1024
0.00.052.985 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.985 I llm_load_tensors: offloading output layer to GPU
0.00.052.985 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.995 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.996 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.900 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.901 I llama_new_context_with_model: n_ctx         = 128
0.00.053.901 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.902 I llama_new_context_with_model: n_batch       = 128
0.00.053.902 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.902 I llama_new_context_with_model: flash_attn    = 0
0.00.053.903 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.903 I llama_new_context_with_model: freq_scale    = 1
0.00.053.903 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.904 I ggml_metal_init: allocating
0.00.053.910 I ggml_metal_init: found device: Apple M4
0.00.053.913 I ggml_metal_init: picking default device: Apple M4
0.00.054.449 I ggml_metal_init: using embedded metal library
0.00.056.389 I ggml_metal_init: GPU name:   Apple M4
0.00.056.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.391 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.391 I ggml_metal_init: simdgroup reduction   = true
0.00.056.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.392 I ggml_metal_init: has bfloat            = true
0.00.056.392 I ggml_metal_init: use bfloat            = true
0.00.056.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.393 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.908 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.914 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.945 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.811 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.812 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.813 I llama_new_context_with_model: graph nodes  = 967
0.00.066.813 I llama_new_context_with_model: graph splits = 2
0.00.066.824 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.687.127 I 
0.00.687.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.687.193 I perplexity: tokenizing the input ..
0.00.695.365 I perplexity: tokenization took 8.172 ms
0.00.695.368 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.245 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.837.475 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.837.495 I llama_perf_context_print:        load time =     677.70 ms
0.00.837.496 I llama_perf_context_print: prompt eval time =     140.65 ms /   128 tokens (    1.10 ms per token,   910.04 tokens per second)
0.00.837.500 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.503 I llama_perf_context_print:       total time =     150.38 ms /   129 tokens
0.00.837.916 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.077s
sys	0m0.133s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.563 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.240 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.245 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.246 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.247 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.247 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.248 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.249 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.249 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.250 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.250 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.250 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.251 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.254 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.254 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.254 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.874 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.876 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.877 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.877 I llama_model_loader: - type  f32:  194 tensors
0.00.024.878 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.599 I llm_load_vocab: special tokens cache size = 25
0.00.051.617 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.620 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.620 I llm_load_print_meta: arch             = gptneox
0.00.051.620 I llm_load_print_meta: vocab type       = BPE
0.00.051.621 I llm_load_print_meta: n_vocab          = 50304
0.00.051.621 I llm_load_print_meta: n_merges         = 50009
0.00.051.621 I llm_load_print_meta: vocab_only       = 0
0.00.051.621 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.621 I llm_load_print_meta: n_embd           = 2048
0.00.051.622 I llm_load_print_meta: n_layer          = 24
0.00.051.625 I llm_load_print_meta: n_head           = 16
0.00.051.625 I llm_load_print_meta: n_head_kv        = 16
0.00.051.626 I llm_load_print_meta: n_rot            = 32
0.00.051.626 I llm_load_print_meta: n_swa            = 0
0.00.051.626 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.626 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.627 I llm_load_print_meta: n_gqa            = 1
0.00.051.628 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.628 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.629 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.629 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.629 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.629 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.631 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.631 I llm_load_print_meta: n_ff             = 8192
0.00.051.632 I llm_load_print_meta: n_expert         = 0
0.00.051.632 I llm_load_print_meta: n_expert_used    = 0
0.00.051.632 I llm_load_print_meta: causal attn      = 1
0.00.051.632 I llm_load_print_meta: pooling type     = 0
0.00.051.634 I llm_load_print_meta: rope type        = 2
0.00.051.634 I llm_load_print_meta: rope scaling     = linear
0.00.051.634 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.635 I llm_load_print_meta: freq_scale_train = 1
0.00.051.635 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.635 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.635 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.635 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.635 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.636 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.636 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.647 I llm_load_print_meta: model type       = 1.4B
0.00.051.647 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.648 I llm_load_print_meta: model params     = 1.41 B
0.00.051.648 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.648 I llm_load_print_meta: general.name     = 1.4B
0.00.051.649 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.649 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.649 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.649 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.649 I llm_load_print_meta: LF token         = 128 ''
0.00.051.650 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.650 I llm_load_print_meta: max token length = 1024
0.00.053.239 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.239 I llm_load_tensors: offloading output layer to GPU
0.00.053.239 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.249 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.250 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.136 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.137 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.137 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.137 I llama_new_context_with_model: n_batch       = 2048
0.00.054.138 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.138 I llama_new_context_with_model: flash_attn    = 0
0.00.054.138 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.138 I llama_new_context_with_model: freq_scale    = 1
0.00.054.139 I ggml_metal_init: allocating
0.00.054.144 I ggml_metal_init: found device: Apple M4
0.00.054.147 I ggml_metal_init: picking default device: Apple M4
0.00.054.719 I ggml_metal_init: using embedded metal library
0.00.056.698 I ggml_metal_init: GPU name:   Apple M4
0.00.056.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.700 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.700 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.700 I ggml_metal_init: simdgroup reduction   = true
0.00.056.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.701 I ggml_metal_init: has bfloat            = true
0.00.056.701 I ggml_metal_init: use bfloat            = true
0.00.056.701 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.702 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.671 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.678 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.696 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.654 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.655 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.655 I llama_new_context_with_model: graph nodes  = 967
0.00.085.656 I llama_new_context_with_model: graph splits = 2
0.00.085.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.835.576 I main: llama threadpool init, n_threads = 4
0.00.835.612 I 
0.00.835.638 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.835.638 I 
0.00.835.897 I sampler seed: 1234
0.00.835.903 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.835.914 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.835.914 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.835.914 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.709.982 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.709.982 I llama_perf_context_print:        load time =     826.01 ms
0.01.709.983 I llama_perf_context_print: prompt eval time =      38.69 ms /     7 tokens (    5.53 ms per token,   180.92 tokens per second)
0.01.709.985 I llama_perf_context_print:        eval time =     832.47 ms /    63 runs   (   13.21 ms per token,    75.68 tokens per second)
0.01.709.985 I llama_perf_context_print:       total time =     874.41 ms /    70 tokens
0.01.710.173 I ggml_metal_free: deallocating

real	0m1.727s
user	0m0.109s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4162 (f6d12e7d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.680 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.440 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.445 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.447 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.448 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.448 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.451 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.452 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.243 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.331 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.248 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.249 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.250 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.250 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.250 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.251 I llama_model_loader: - type  f32:  194 tensors
0.00.023.251 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.807 I llm_load_vocab: special tokens cache size = 25
0.00.049.815 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.817 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.817 I llm_load_print_meta: arch             = gptneox
0.00.049.818 I llm_load_print_meta: vocab type       = BPE
0.00.049.818 I llm_load_print_meta: n_vocab          = 50304
0.00.049.818 I llm_load_print_meta: n_merges         = 50009
0.00.049.818 I llm_load_print_meta: vocab_only       = 0
0.00.049.818 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.819 I llm_load_print_meta: n_embd           = 2048
0.00.049.819 I llm_load_print_meta: n_layer          = 24
0.00.049.821 I llm_load_print_meta: n_head           = 16
0.00.049.822 I llm_load_print_meta: n_head_kv        = 16
0.00.049.822 I llm_load_print_meta: n_rot            = 32
0.00.049.825 I llm_load_print_meta: n_swa            = 0
0.00.049.825 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.825 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.826 I llm_load_print_meta: n_gqa            = 1
0.00.049.827 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.832 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.832 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.833 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.833 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.833 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.833 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.834 I llm_load_print_meta: n_ff             = 8192
0.00.049.834 I llm_load_print_meta: n_expert         = 0
0.00.049.835 I llm_load_print_meta: n_expert_used    = 0
0.00.049.835 I llm_load_print_meta: causal attn      = 1
0.00.049.835 I llm_load_print_meta: pooling type     = 0
0.00.049.835 I llm_load_print_meta: rope type        = 2
0.00.049.835 I llm_load_print_meta: rope scaling     = linear
0.00.049.836 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.837 I llm_load_print_meta: freq_scale_train = 1
0.00.049.837 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.838 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.838 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.838 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.838 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.840 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.840 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.851 I llm_load_print_meta: model type       = 1.4B
0.00.049.852 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.852 I llm_load_print_meta: model params     = 1.41 B
0.00.049.852 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.852 I llm_load_print_meta: general.name     = 1.4B
0.00.049.859 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.860 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.860 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.861 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.861 I llm_load_print_meta: LF token         = 128 ''
0.00.049.863 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.863 I llm_load_print_meta: max token length = 1024
0.00.051.895 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.895 I llm_load_tensors: offloading output layer to GPU
0.00.051.895 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.905 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.906 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.836 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.837 I llama_new_context_with_model: n_ctx         = 128
0.00.052.837 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.837 I llama_new_context_with_model: n_batch       = 128
0.00.052.837 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.838 I llama_new_context_with_model: flash_attn    = 0
0.00.052.838 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.838 I llama_new_context_with_model: freq_scale    = 1
0.00.052.839 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.839 I ggml_metal_init: allocating
0.00.052.843 I ggml_metal_init: found device: Apple M4
0.00.052.845 I ggml_metal_init: picking default device: Apple M4
0.00.053.379 I ggml_metal_init: using embedded metal library
0.00.055.313 I ggml_metal_init: GPU name:   Apple M4
0.00.055.314 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.315 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.315 I ggml_metal_init: simdgroup reduction   = true
0.00.055.315 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.316 I ggml_metal_init: has bfloat            = true
0.00.055.316 I ggml_metal_init: use bfloat            = true
0.00.055.318 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.318 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.548 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.551 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.565 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.444 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.445 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.446 I llama_new_context_with_model: graph nodes  = 967
0.00.065.446 I llama_new_context_with_model: graph splits = 2
0.00.065.458 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.451.923 I 
0.00.451.988 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | 
0.00.452.027 I perplexity: tokenizing the input ..
0.00.459.783 I perplexity: tokenization took 7.755 ms
0.00.459.786 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.600.328 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.601.666 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.601.688 I llama_perf_context_print:        load time =     443.24 ms
0.00.601.689 I llama_perf_context_print: prompt eval time =     140.31 ms /   128 tokens (    1.10 ms per token,   912.26 tokens per second)
0.00.601.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.601.690 I llama_perf_context_print:       total time =     149.77 ms /   129 tokens
0.00.602.117 I ggml_metal_free: deallocating

real	0m0.614s
user	0m0.075s
sys	0m0.095s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4162 (f6d12e7d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12460a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12460a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12460adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12460b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12460b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12460bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12460c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12460ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12460cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12460d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12460d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12460dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12460ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12460f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12460f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1246100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124610800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124610f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124611640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124611e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124612530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124612c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124613370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124613c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124614330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1246145f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124614c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124615870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124615db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124616070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124616510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1246167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124617060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1246175a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124617860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124617d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1246181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124618640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124618ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124618f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124619420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1246198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124619d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12461a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12461a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12461aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12461b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12461ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12461c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12461c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12461cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12461d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12461d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12461de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12461e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12461eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12461ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12461f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12461f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124620050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124620310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1246207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124620c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1246210f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124621590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124621a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124621ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124622370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124622810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124622cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124623150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1246235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124623a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124623f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1246243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124624870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124624d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1246251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124625650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124625af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124625f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124626430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1246268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124626d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124627210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1246276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124627b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124627ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124628490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124628930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124628dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124629270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124629710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124629bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12462a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12462a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12462a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12461b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12462afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12462b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12462b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12462bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12462c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12462c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12462cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12462d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12462d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12462d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12462de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12462e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12462e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12462ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12462f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12462f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12462f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12462fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1246307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124630c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124631100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1246315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124631a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124631ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124632380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124632820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124632cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124633160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124633600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124633aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124633f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1246343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124634880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124634d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1246351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124635660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124635b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124635fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124636440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1246368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124636d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124637220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1246376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124637b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124638000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1246384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124638940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124638de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124639280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124639720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124639bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12463a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12463a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12463a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12463aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12463b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12463b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12463bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12463c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12463c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12463cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12463d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12463d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12463dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12463e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12463ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12463f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12463f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12463fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1246402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124640810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124640d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1246412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124641800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1246422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1246427f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124642d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124643290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1246437e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124643d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124644280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1246447d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124644d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124645270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1246457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124645d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124646260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1246467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124646d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124647250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1246477a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124647cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124648240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124648790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124648ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124649230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124649780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124649cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12464a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12464a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12464acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12464b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12464b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12464bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12464c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12464c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12464cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12464d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12464d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12464dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12464e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12464e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12464ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12464f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12464f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12464fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1246501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124650710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124650c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1246511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124651700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124651c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1246521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1246526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124652b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124653030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1246534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124653970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124653e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1246542b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124654750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124654bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124655090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124655530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1246559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124655e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124656310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124656860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124656f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1246576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124657dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1246584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1246587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124658db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1246593c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.157.395 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12460dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12460e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12460e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12460ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12460eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12460f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12460f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12460fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124610080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1246104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124610960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124610f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124611830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124611fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124612790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124613570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124613c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124614350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124614cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1246153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124615ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1246161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1246173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124617860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124617cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124618140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1246185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124618a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124618e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124619300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1246195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124619a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124619ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12461a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12461a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12461abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12461b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12461b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12461b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12461bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12461c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12461c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12461cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12461cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12461d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12461d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12461dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12461e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12461e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12461ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12461ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12461f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12461f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12461fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124620040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1246204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124620920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124620d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124621200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124621670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124621ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124621f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1246223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124622830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124622ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124623110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124623580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1246239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124623e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1246242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124624740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124624bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124625020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124625490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124625900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124625d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1246261e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124626650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124626ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124626f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1246273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124627810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124627c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1246280f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124628560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1246289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124628e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1246292b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124629720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124629b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12462a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12462a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12462a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12462ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12462b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12462b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12462baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12462bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12462c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12462c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12462cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12462d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12462d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12462d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12462de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12462e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12462e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12462eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12462efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12462f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12462f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12462fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1246301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124630610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124630a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124630ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124631360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1246317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124631c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1246320b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124632520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124632990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124632e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124633270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1246336e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124633b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124633fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124634430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1246348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124634d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124635180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1246355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124635a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124635ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124636340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1246367b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124636c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124637090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124637500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124637970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124637de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124638250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1246386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124638b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124638fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124639410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124639880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124639cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12463a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12463a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12463aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12463aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12463b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12463b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12463bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12463c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12463c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12463c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12463cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12463d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12463d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12463db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12463df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12463e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12463eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12463efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12463f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12463f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12463fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1246401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124640610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124640a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124640ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124641360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1246417d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124641c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1246420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124642520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124642990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124642e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124643270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1246436e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124643b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124643fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124644430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1246448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124644d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124645180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1246455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124645a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124645ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124646340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1246467b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124646c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124647500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124647970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124647de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124648250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1246486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124648b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124648fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124649410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124649880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124649cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12464a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12464a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12464aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12464aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12464b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12464b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12464bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12464c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12464c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12464c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12464cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12464d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12464d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12464db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12464df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12464e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12464e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12464ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12464f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12464f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12464fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12464fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124650300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124650770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124650be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124651050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1246514c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124651930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124651da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124652210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124652900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124652ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1246536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124653dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124654240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1246546b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124654b20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x106405810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x106406100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1064063c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x106406830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x106406ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x106407110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x106407580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1064079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x106407e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1064082d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x106404230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x106404810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x106408b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1064092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x106409ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10640a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10640a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10640b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10640b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10640bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10640c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10640cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10640d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10640dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10640e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10640e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10640ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10640ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10640f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10640f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10640fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x106410190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x106410600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x106410a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x106410d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x106411240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x106411710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x106411be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1064120b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x106412580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x106412a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x106412f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1064133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1064138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x106413d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x106414200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x106414670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x106414ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x106414f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1064153c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x106415830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x106415ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x106416110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x106416580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x106416bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x106417090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x106417530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1064177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x106417c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1064180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x106418620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x106418b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x106419040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x106419550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x106419a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x106419f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10641a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10641a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10641ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10641b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10641b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10641bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10641c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10641c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10641cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10641d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10641d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10641dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10641e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10641e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10641eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10641f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10641f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10641fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10641ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x106420490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1064209a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x106420eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1064213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1064218d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x106421de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1064222f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x106422800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x106422d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x106423220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x106423730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x106423c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x106424150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x106424660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x106424b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x106425080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x106425590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x106425aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x106425fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1064264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1064269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x106426ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1064273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x106427900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x106427e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x106428310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x106428810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x106428d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x106429230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x106429740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x106429c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10642a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10642a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10642ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10642b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10642b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10642bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10642bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10642c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10642c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10642cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10642d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10642d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10642de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10642e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10642e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10642ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10642f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10642f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10642fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x106430190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1064306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x106430bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1064310c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1064315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x106431ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x106431ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x106432500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x106432a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x106432f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x106433430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x106433940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x106433e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x106434360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x106434870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x106434d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x106435290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1064357a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x106435cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x106436260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x106436810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x106436dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x106437370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x106437980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x106437f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1064385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x106438bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1064391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1064399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x106439e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10643a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10643a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10643af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10643b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10643b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10643bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10643c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10643c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10643cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10643d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10643d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10643df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10643e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10643e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10643ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10643f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10643f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10643fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x106440440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x106440990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x106440ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x106441430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x106441980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x106441ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x106442420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x106442970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x106442ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x106443410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x106443960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x106443eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x106444400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x106444950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x106444ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1064453f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x106445940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x106445e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1064463e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x106446930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x106446e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1064473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x106447920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x106447e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1064483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x106448910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x106448e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1064493b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x106449900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x106449e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10644a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10644a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10644ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10644b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10644b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10644be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10644c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10644c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10644ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10644d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10644d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10644dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10644e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10644e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10644eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10644efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10644f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10644f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10644fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x106450260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x106450700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x106450ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x106451040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1064514e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x106451a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x106452150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x106452870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x106452f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1064536b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x106453970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x106453f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x106454590 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.849s
user	0m0.290s
sys	0m0.297s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4162 (f6d12e7d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14870edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14870f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14870fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148710090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148710640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148710bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1487111a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148711d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148712200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148712c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148713720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148713ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1487146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148714e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148715520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148716360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148716b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148717250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148718090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148718930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148719050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148719920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14871a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14871aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14871ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14871b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14871b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14871bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14871c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14871c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14871ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14871cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14871d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14871d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14871dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14871e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14871e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14871ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14871ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14871f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14871f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14871fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148720720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148720d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148721340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148721950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148721f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148722570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148722b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148723810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148723cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148723f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148724580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148724d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148725030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1487254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148725970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148725e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1487262b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148726750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148726bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148727090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148727530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1487279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148727e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148728310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1487287b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148728c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1487290f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148729590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148729a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148729ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14872a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14872a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14872acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14872b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14872b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14872ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14872bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14872c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14872c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14872cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14872d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14872d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14872daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14872df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14872e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14872e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14872ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14872f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14872f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148720410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14872fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1487301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148730640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148730ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148730f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148731420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1487318c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148731d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148732200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1487326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148732b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148732fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148733480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148733920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148733dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148734260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148734700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148734ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148735040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1487354e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148735980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148735e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1487362c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148736760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148736c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1487370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148737540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1487379e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148737e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148738320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1487387c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148738c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148739100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1487395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148739a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148739ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14873a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14873a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14873acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14873b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14873b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14873baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14873bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14873c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14873c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14873cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14873d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14873d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14873db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14873dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14873e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14873e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14873ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14873f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14873f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14873fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148740160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1487406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148740c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148740ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1487414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1487420f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148742700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148742d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148743500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1487439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148743e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1487442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148744a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148744fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148745530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148745a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148745fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148746520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148746a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148746fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148747a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148747fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148748500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148748a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148748fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1487494f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148749a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148749f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14874a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14874aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14874af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14874b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14874ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14874bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14874c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14874ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14874cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14874d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14874da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14874df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14874e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14874e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14874ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14874f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14874f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14874ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148750480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1487509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148750f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148751470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1487519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148751f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148752460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1487529b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148752f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148753450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1487539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148753ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148754440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148754990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148754ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148755430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148755980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148755ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148756420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148756970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148756ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148757410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1487578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148757d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1487581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148758690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148758b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148758fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148759470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148759910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148759db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14875a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14875a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14875ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14875b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14875b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14875bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14875c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14875cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14875d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14875d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14875dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14875e0e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.085.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a006050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a0064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a006930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a006da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a0083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a009390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a009eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a00a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a00ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a00b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a00bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a00c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a00caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a00d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a00d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a00e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a00e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a00ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a00f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a00f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a00fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a010050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a0104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a010930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a010da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a0112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a011740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a011a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a011e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a0122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a012750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a012bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a013030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a0134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a013910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a013d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a0141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a014660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a014ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a014f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a0153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a015820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a015c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a016100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a016570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a0169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a0172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a017730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a017ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a018110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a018610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a018a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a018ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a019360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a0197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a019c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a01a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a01a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a01a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a01ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a01b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a01b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a01bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a01bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a01c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a01c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a01cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a01d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a01d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a01da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a01ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a01e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a01e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a01ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a01f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a01f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a01f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a01fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a020250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a0206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a020b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a020fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a021410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a021880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a021cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a0225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a022a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a022eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a023320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a023790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a023c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a024070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a0244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a024950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a024dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a025230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a0256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a025b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a025f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a0263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a026860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a026cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a027140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a0275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a027a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a027e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a028770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a028be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a029050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a0294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a029930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a029da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a02a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a02a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a02aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a02af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a02b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a02b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a02bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a02c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a02c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a02ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a02ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a02d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a02d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a02dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a02e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a02e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a02e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a02ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a02f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a02f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a02fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a02ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a0303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a030820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a030c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a031100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a031570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a0319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a031e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a0322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a032730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a032ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a033010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a033480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a0338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a033d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a0341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a034640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a034ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a034f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a035390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a035800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a035c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a0360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a036550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a0369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a037550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a037810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a037ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a037f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a0383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a038820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a038c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a039100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a039570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a0399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a039e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a03a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a03a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a03aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a03b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a03b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a03b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a03bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a03c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a03c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a03cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a03cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a03d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a03d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a03dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a03e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a03e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a03e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a03ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a03f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a03f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a03fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a03fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a040460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a0408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a040d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a0411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a041620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a041a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a041f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a042370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a0427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a042c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a0430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a043530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a0439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a043e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a044280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a0446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a044b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a045440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a0458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a045d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a046190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a046600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a046a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a046ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a0477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a047c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a0480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a048510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a048980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a049260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a0496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a049b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a049fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a04a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a04a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a04b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a04baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a04c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a04c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a04cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a04ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a04d320 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a006050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a0064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a006930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a006da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a0083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a009290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a009b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a00a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a00aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a00b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a00b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a00bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a00c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a00d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a00d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a00de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a00e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a00ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a00f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a00f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a00fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a010020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a011910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a011d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a0121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a012660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a012ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a012f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a0133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a013820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a013c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a014100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a014570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a0149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a014e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a0152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a015730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a015ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a016010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a016480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a0168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a016d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a0171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a017640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a017ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a017f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a018390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a018800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a018c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a0190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a019550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a0199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a019e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a01a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a01a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a01ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a01aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a01b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a01b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a01bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a01c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a01c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a01ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a01cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a01d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a01d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a01dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a01e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a01e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a01e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a01ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a01f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a01f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a01fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a01ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a020440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a0208b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a020d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a021190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a021600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a021a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a021ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a022350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a0227c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a022c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a0230a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a023510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a023980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a023df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a024260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a0246d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a024b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a024fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a025420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a025890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a025d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a026170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a0265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a026a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a026ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a027330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a0277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a027c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a028080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a0284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a028960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a028dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a029240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a0296b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a029b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a029f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a02a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a02a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a02ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a02b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a02b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a02ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a02bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a02c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a02c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a02cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a02d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a02d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a02d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a02ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a02e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a02e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a02eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a02ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a02f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a02f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a02fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a030130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a0305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a030a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a030e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a0312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a031760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a031bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a032040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a0324b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a032920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a032d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a033200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a033670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a033ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a033f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a0343c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a034830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a034ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a035110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a035580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a0359f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a035e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a0362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a036740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a036ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a037330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a0377a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a037c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a038080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a0384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a038960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a038dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a039240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a0396b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a039b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a039f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a03a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a03a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a03ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a03b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a03b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a03ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a03bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a03c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a03c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a03cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a03d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a03d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a03d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a03ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a03e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a03e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a03eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a03ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a03f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a03f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a03fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a040130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a0405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a040a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a040e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a0412f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a041760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a041bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a042040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a0424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a042920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a042d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a043200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a043670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a043ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a043f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a0443c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a044830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a044ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a045110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a045580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a0459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a045e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a0462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a046740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a046bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a047020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a047490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a047900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a047d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a0481e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a048650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a048ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a048f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a0493a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a049810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a049c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a04a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a04a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a04ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a04b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a04ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a04c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a04c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a04ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a04ce70 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.949s
user	0m0.238s
sys	0m0.129s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.55 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 27: test-model-load-cancel
1/2 Test #27: test-model-load-cancel ...........   Passed    0.25 sec
    Start 28: test-autorelease
2/2 Test #28: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.52 sec
        0.53 real         0.14 user         0.04 sys
```
