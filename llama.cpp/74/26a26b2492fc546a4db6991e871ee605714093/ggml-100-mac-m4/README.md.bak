### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.24 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.65 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.41 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.32 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.32 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.32 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.94 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.31 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.21 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.25 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.17 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.26 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.28 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.92 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.22 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.29 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.41 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 223.87 sec*proc (28 tests)

Total Test time (real) = 223.88 sec

real	3m43.883s
user	7m47.384s
sys	0m5.870s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.96 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.45 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.56 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.38 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.22 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.89 sec*proc (28 tests)

Total Test time (real) =  51.91 sec

real	0m51.917s
user	1m12.513s
sys	0m5.701s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.085 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.529 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.428 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.434 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.437 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.438 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.438 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.439 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.440 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.441 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.442 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.442 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.443 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.443 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.447 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.448 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.448 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.449 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.449 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.450 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.455 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.973 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.219 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.221 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.222 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.222 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.223 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.223 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.224 I llama_model_loader: - type  f32:  124 tensors
0.00.027.224 I llama_model_loader: - type  f16:   73 tensors
0.00.027.225 I print_info: file format = GGUF V3 (latest)
0.00.027.225 I print_info: file type   = F16
0.00.027.227 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.612 I load: special tokens cache size = 5
0.00.033.714 I load: token to piece cache size = 0.2032 MB
0.00.033.719 I print_info: arch             = bert
0.00.033.719 I print_info: vocab_only       = 0
0.00.033.719 I print_info: n_ctx_train      = 512
0.00.033.719 I print_info: n_embd           = 384
0.00.033.720 I print_info: n_layer          = 12
0.00.033.723 I print_info: n_head           = 12
0.00.033.724 I print_info: n_head_kv        = 12
0.00.033.724 I print_info: n_rot            = 32
0.00.033.725 I print_info: n_swa            = 0
0.00.033.725 I print_info: n_embd_head_k    = 32
0.00.033.725 I print_info: n_embd_head_v    = 32
0.00.033.726 I print_info: n_gqa            = 1
0.00.033.727 I print_info: n_embd_k_gqa     = 384
0.00.033.728 I print_info: n_embd_v_gqa     = 384
0.00.033.729 I print_info: f_norm_eps       = 1.0e-12
0.00.033.729 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.033.729 I print_info: f_clamp_kqv      = 0.0e+00
0.00.033.729 I print_info: f_max_alibi_bias = 0.0e+00
0.00.033.732 I print_info: f_logit_scale    = 0.0e+00
0.00.033.733 I print_info: n_ff             = 1536
0.00.033.733 I print_info: n_expert         = 0
0.00.033.733 I print_info: n_expert_used    = 0
0.00.033.733 I print_info: causal attn      = 0
0.00.033.734 I print_info: pooling type     = 2
0.00.033.736 I print_info: rope type        = 2
0.00.033.736 I print_info: rope scaling     = linear
0.00.033.736 I print_info: freq_base_train  = 10000.0
0.00.033.737 I print_info: freq_scale_train = 1
0.00.033.737 I print_info: n_ctx_orig_yarn  = 512
0.00.033.737 I print_info: rope_finetuned   = unknown
0.00.033.738 I print_info: ssm_d_conv       = 0
0.00.033.738 I print_info: ssm_d_inner      = 0
0.00.033.738 I print_info: ssm_d_state      = 0
0.00.033.738 I print_info: ssm_dt_rank      = 0
0.00.033.738 I print_info: ssm_dt_b_c_rms   = 0
0.00.033.739 I print_info: model type       = 33M
0.00.033.739 I print_info: model params     = 33.21 M
0.00.033.739 I print_info: general.name     = Bge Small
0.00.033.740 I print_info: vocab type       = WPM
0.00.033.741 I print_info: n_vocab          = 30522
0.00.033.741 I print_info: n_merges         = 0
0.00.033.741 I print_info: BOS token        = 101 '[CLS]'
0.00.033.742 I print_info: UNK token        = 100 '[UNK]'
0.00.033.742 I print_info: SEP token        = 102 '[SEP]'
0.00.033.742 I print_info: PAD token        = 0 '[PAD]'
0.00.033.742 I print_info: MASK token       = 103 '[MASK]'
0.00.033.748 I print_info: LF token         = 0 '[PAD]'
0.00.033.748 I print_info: max token length = 21
0.00.035.896 I load_tensors: offloading 12 repeating layers to GPU
0.00.035.897 I load_tensors: offloading output layer to GPU
0.00.035.897 I load_tensors: offloaded 13/13 layers to GPU
0.00.035.924 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.035.925 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.036.175 I llama_init_from_model: n_seq_max     = 1
0.00.036.177 I llama_init_from_model: n_ctx         = 512
0.00.036.177 I llama_init_from_model: n_ctx_per_seq = 512
0.00.036.177 I llama_init_from_model: n_batch       = 2048
0.00.036.177 I llama_init_from_model: n_ubatch      = 2048
0.00.036.178 I llama_init_from_model: flash_attn    = 0
0.00.036.178 I llama_init_from_model: freq_base     = 10000.0
0.00.036.178 I llama_init_from_model: freq_scale    = 1
0.00.036.179 I ggml_metal_init: allocating
0.00.036.183 I ggml_metal_init: found device: Apple M4
0.00.036.186 I ggml_metal_init: picking default device: Apple M4
0.00.037.031 I ggml_metal_init: using embedded metal library
0.00.041.088 I ggml_metal_init: GPU name:   Apple M4
0.00.041.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.041.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.041.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.041.092 I ggml_metal_init: simdgroup reduction   = true
0.00.041.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.041.092 I ggml_metal_init: has bfloat            = true
0.00.041.092 I ggml_metal_init: use bfloat            = true
0.00.041.093 I ggml_metal_init: hasUnifiedMemory      = true
0.00.041.094 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.476 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.054.114 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.054.116 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.054.122 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.054.935 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.054.937 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.054.937 I llama_init_from_model: graph nodes  = 429
0.00.054.937 I llama_init_from_model: graph splits = 2
0.00.054.939 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.054.939 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.735 I 
0.00.062.757 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.063.448 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.827 I llama_perf_context_print:        load time =      46.20 ms
0.00.068.828 I llama_perf_context_print: prompt eval time =       5.22 ms /     9 tokens (    0.58 ms per token,  1724.14 tokens per second)
0.00.068.829 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.830 I llama_perf_context_print:       total time =       6.10 ms /    10 tokens
0.00.068.979 I ggml_metal_free: deallocating

real	0m0.257s
user	0m0.049s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.038 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.538 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.222 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.226 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.228 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.228 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.228 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.229 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.229 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.230 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.230 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.230 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.231 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.231 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.233 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.233 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.012.235 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.012.237 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.237 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.012.237 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.609 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.274 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.275 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.276 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.276 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.276 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.277 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.015.277 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.015.278 I llama_model_loader: - type  f32:  124 tensors
0.00.015.278 I llama_model_loader: - type q8_0:   73 tensors
0.00.015.278 I print_info: file format = GGUF V3 (latest)
0.00.015.279 I print_info: file type   = Q8_0
0.00.015.280 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.822 I load: special tokens cache size = 5
0.00.019.114 I load: token to piece cache size = 0.2032 MB
0.00.019.116 I print_info: arch             = bert
0.00.019.117 I print_info: vocab_only       = 0
0.00.019.117 I print_info: n_ctx_train      = 512
0.00.019.117 I print_info: n_embd           = 384
0.00.019.117 I print_info: n_layer          = 12
0.00.019.120 I print_info: n_head           = 12
0.00.019.121 I print_info: n_head_kv        = 12
0.00.019.121 I print_info: n_rot            = 32
0.00.019.122 I print_info: n_swa            = 0
0.00.019.122 I print_info: n_embd_head_k    = 32
0.00.019.122 I print_info: n_embd_head_v    = 32
0.00.019.123 I print_info: n_gqa            = 1
0.00.019.123 I print_info: n_embd_k_gqa     = 384
0.00.019.124 I print_info: n_embd_v_gqa     = 384
0.00.019.125 I print_info: f_norm_eps       = 1.0e-12
0.00.019.125 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.019.125 I print_info: f_clamp_kqv      = 0.0e+00
0.00.019.125 I print_info: f_max_alibi_bias = 0.0e+00
0.00.019.126 I print_info: f_logit_scale    = 0.0e+00
0.00.019.126 I print_info: n_ff             = 1536
0.00.019.127 I print_info: n_expert         = 0
0.00.019.127 I print_info: n_expert_used    = 0
0.00.019.127 I print_info: causal attn      = 0
0.00.019.127 I print_info: pooling type     = 2
0.00.019.127 I print_info: rope type        = 2
0.00.019.127 I print_info: rope scaling     = linear
0.00.019.128 I print_info: freq_base_train  = 10000.0
0.00.019.128 I print_info: freq_scale_train = 1
0.00.019.128 I print_info: n_ctx_orig_yarn  = 512
0.00.019.128 I print_info: rope_finetuned   = unknown
0.00.019.129 I print_info: ssm_d_conv       = 0
0.00.019.129 I print_info: ssm_d_inner      = 0
0.00.019.129 I print_info: ssm_d_state      = 0
0.00.019.129 I print_info: ssm_dt_rank      = 0
0.00.019.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.019.132 I print_info: model type       = 33M
0.00.019.132 I print_info: model params     = 33.21 M
0.00.019.133 I print_info: general.name     = Bge Small
0.00.019.133 I print_info: vocab type       = WPM
0.00.019.133 I print_info: n_vocab          = 30522
0.00.019.133 I print_info: n_merges         = 0
0.00.019.134 I print_info: BOS token        = 101 '[CLS]'
0.00.019.134 I print_info: UNK token        = 100 '[UNK]'
0.00.019.134 I print_info: SEP token        = 102 '[SEP]'
0.00.019.134 I print_info: PAD token        = 0 '[PAD]'
0.00.019.134 I print_info: MASK token       = 103 '[MASK]'
0.00.019.134 I print_info: LF token         = 0 '[PAD]'
0.00.019.135 I print_info: max token length = 21
0.00.020.578 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.579 I load_tensors: offloading output layer to GPU
0.00.020.580 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.588 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.590 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.020.766 I llama_init_from_model: n_seq_max     = 1
0.00.020.767 I llama_init_from_model: n_ctx         = 512
0.00.020.768 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.768 I llama_init_from_model: n_batch       = 2048
0.00.020.768 I llama_init_from_model: n_ubatch      = 2048
0.00.020.768 I llama_init_from_model: flash_attn    = 0
0.00.020.769 I llama_init_from_model: freq_base     = 10000.0
0.00.020.769 I llama_init_from_model: freq_scale    = 1
0.00.020.769 I ggml_metal_init: allocating
0.00.020.773 I ggml_metal_init: found device: Apple M4
0.00.020.776 I ggml_metal_init: picking default device: Apple M4
0.00.021.451 I ggml_metal_init: using embedded metal library
0.00.024.075 I ggml_metal_init: GPU name:   Apple M4
0.00.024.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.079 I ggml_metal_init: simdgroup reduction   = true
0.00.024.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.079 I ggml_metal_init: has bfloat            = true
0.00.024.079 I ggml_metal_init: use bfloat            = true
0.00.024.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.082 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.636 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.035.117 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.124 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.126 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.782 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.783 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.783 I llama_init_from_model: graph nodes  = 429
0.00.035.783 I llama_init_from_model: graph splits = 2
0.00.035.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.785 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.041.215 I 
0.00.041.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.041.764 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.046.326 I llama_perf_context_print:        load time =      31.67 ms
0.00.046.327 I llama_perf_context_print: prompt eval time =       4.43 ms /     9 tokens (    0.49 ms per token,  2032.06 tokens per second)
0.00.046.328 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.046.328 I llama_perf_context_print:       total time =       5.11 ms /    10 tokens
0.00.046.500 I ggml_metal_free: deallocating

real	0m0.059s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.202 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.245 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.983 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.989 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.991 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.033.995 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.996 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.033.997 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.033.998 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.033.999 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.000 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.001 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.001 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.002 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.006 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.006 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.007 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.008 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.008 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.047.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.809 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.047.809 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.047.810 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.047.810 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.047.810 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.047.811 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.047.811 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.047.812 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.047.812 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.047.813 I llama_model_loader: - type  f32:   40 tensors
0.00.047.813 I llama_model_loader: - type  f16:   30 tensors
0.00.047.814 I print_info: file format = GGUF V3 (latest)
0.00.047.814 I print_info: file type   = F16
0.00.047.816 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.064.624 W load: empty token at index 5
0.00.069.419 W load: model vocab missing newline token, using special_pad_id instead
0.00.070.828 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.859 I load: special tokens cache size = 5
0.00.336.758 I load: token to piece cache size = 1.5060 MB
0.00.336.778 I print_info: arch             = jina-bert-v2
0.00.336.779 I print_info: vocab_only       = 0
0.00.336.780 I print_info: n_ctx_train      = 8192
0.00.336.781 I print_info: n_embd           = 384
0.00.336.781 I print_info: n_layer          = 4
0.00.336.790 I print_info: n_head           = 12
0.00.336.790 I print_info: n_head_kv        = 12
0.00.336.791 I print_info: n_rot            = 32
0.00.336.791 I print_info: n_swa            = 0
0.00.336.791 I print_info: n_embd_head_k    = 32
0.00.336.796 I print_info: n_embd_head_v    = 32
0.00.336.797 I print_info: n_gqa            = 1
0.00.336.798 I print_info: n_embd_k_gqa     = 384
0.00.336.798 I print_info: n_embd_v_gqa     = 384
0.00.336.799 I print_info: f_norm_eps       = 1.0e-12
0.00.336.800 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.336.801 I print_info: f_clamp_kqv      = 0.0e+00
0.00.336.801 I print_info: f_max_alibi_bias = 8.0e+00
0.00.336.804 I print_info: f_logit_scale    = 0.0e+00
0.00.336.804 I print_info: n_ff             = 1536
0.00.336.805 I print_info: n_expert         = 0
0.00.336.805 I print_info: n_expert_used    = 0
0.00.336.805 I print_info: causal attn      = 0
0.00.336.805 I print_info: pooling type     = -1
0.00.336.805 I print_info: rope type        = -1
0.00.336.806 I print_info: rope scaling     = linear
0.00.336.806 I print_info: freq_base_train  = 10000.0
0.00.336.806 I print_info: freq_scale_train = 1
0.00.336.807 I print_info: n_ctx_orig_yarn  = 8192
0.00.336.807 I print_info: rope_finetuned   = unknown
0.00.336.807 I print_info: ssm_d_conv       = 0
0.00.336.807 I print_info: ssm_d_inner      = 0
0.00.336.807 I print_info: ssm_d_state      = 0
0.00.336.807 I print_info: ssm_dt_rank      = 0
0.00.336.808 I print_info: ssm_dt_b_c_rms   = 0
0.00.336.808 I print_info: model type       = 33M
0.00.336.808 I print_info: model params     = 32.90 M
0.00.336.809 I print_info: general.name     = Jina Bert Implementation
0.00.336.811 I print_info: vocab type       = BPE
0.00.336.811 I print_info: n_vocab          = 61056
0.00.336.811 I print_info: n_merges         = 39382
0.00.336.811 I print_info: BOS token        = 0 '<s>'
0.00.336.812 I print_info: EOS token        = 2 '</s>'
0.00.336.812 I print_info: UNK token        = 3 '<unk>'
0.00.336.812 I print_info: SEP token        = 2 '</s>'
0.00.336.812 I print_info: PAD token        = 1 '<pad>'
0.00.336.813 I print_info: MASK token       = 4 '<mask>'
0.00.336.813 I print_info: EOG token        = 2 '</s>'
0.00.336.813 I print_info: max token length = 45
0.00.338.026 I load_tensors: offloading 4 repeating layers to GPU
0.00.338.026 I load_tensors: offloading output layer to GPU
0.00.338.027 I load_tensors: offloaded 5/5 layers to GPU
0.00.338.048 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.338.049 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.338.527 I llama_init_from_model: n_seq_max     = 1
0.00.338.527 I llama_init_from_model: n_ctx         = 8192
0.00.338.528 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.338.528 I llama_init_from_model: n_batch       = 2048
0.00.338.528 I llama_init_from_model: n_ubatch      = 2048
0.00.338.528 I llama_init_from_model: flash_attn    = 0
0.00.338.529 I llama_init_from_model: freq_base     = 10000.0
0.00.338.529 I llama_init_from_model: freq_scale    = 1
0.00.338.530 I ggml_metal_init: allocating
0.00.338.533 I ggml_metal_init: found device: Apple M4
0.00.338.535 I ggml_metal_init: picking default device: Apple M4
0.00.339.548 I ggml_metal_init: using embedded metal library
0.00.342.043 I ggml_metal_init: GPU name:   Apple M4
0.00.342.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.342.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.342.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.342.046 I ggml_metal_init: simdgroup reduction   = true
0.00.342.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.342.046 I ggml_metal_init: has bfloat            = true
0.00.342.047 I ggml_metal_init: use bfloat            = true
0.00.342.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.342.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.352.225 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.354.753 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.354.758 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.354.760 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.355.342 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.355.343 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.355.343 I llama_init_from_model: graph nodes  = 154
0.00.355.343 I llama_init_from_model: graph splits = 2
0.00.355.345 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.355.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.367.655 I 
0.00.367.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.367.995 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.367.996 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.367.999 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.367.999 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.368.003 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.368.004 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.368.538 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.372.068 I llama_perf_context_print:        load time =     344.40 ms
0.00.372.069 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17603.63 tokens per second)
0.00.372.070 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.372.071 I llama_perf_context_print:       total time =       4.41 ms /    63 tokens
0.00.372.310 I ggml_metal_free: deallocating

real	0m1.114s
user	0m0.345s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.186 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.298 I main: llama backend init
0.00.000.308 I main: load the model and apply lora adapter, if any
0.00.032.861 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.048.850 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.048.863 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.048.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.048.869 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.048.878 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.048.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.048.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.048.883 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.048.884 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.048.884 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.048.885 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.048.886 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.048.886 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.048.887 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.048.890 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.048.891 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.048.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.699 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.645 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.392 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.065.394 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.395 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.395 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.395 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.396 I llama_model_loader: - type  f32:  194 tensors
0.00.065.396 I llama_model_loader: - type  f16:   98 tensors
0.00.065.397 I print_info: file format = GGUF V3 (latest)
0.00.065.398 I print_info: file type   = all F32 (guessed)
0.00.065.399 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.091.742 I load: special tokens cache size = 25
0.00.098.336 I load: token to piece cache size = 0.2984 MB
0.00.098.339 I print_info: arch             = gptneox
0.00.098.339 I print_info: vocab_only       = 0
0.00.098.339 I print_info: n_ctx_train      = 2048
0.00.098.339 I print_info: n_embd           = 2048
0.00.098.339 I print_info: n_layer          = 24
0.00.098.342 I print_info: n_head           = 16
0.00.098.343 I print_info: n_head_kv        = 16
0.00.098.343 I print_info: n_rot            = 32
0.00.098.343 I print_info: n_swa            = 0
0.00.098.343 I print_info: n_embd_head_k    = 128
0.00.098.344 I print_info: n_embd_head_v    = 128
0.00.098.344 I print_info: n_gqa            = 1
0.00.098.345 I print_info: n_embd_k_gqa     = 2048
0.00.098.345 I print_info: n_embd_v_gqa     = 2048
0.00.098.346 I print_info: f_norm_eps       = 1.0e-05
0.00.098.346 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.098.346 I print_info: f_clamp_kqv      = 0.0e+00
0.00.098.347 I print_info: f_max_alibi_bias = 0.0e+00
0.00.098.347 I print_info: f_logit_scale    = 0.0e+00
0.00.098.347 I print_info: n_ff             = 8192
0.00.098.347 I print_info: n_expert         = 0
0.00.098.348 I print_info: n_expert_used    = 0
0.00.098.348 I print_info: causal attn      = 1
0.00.098.348 I print_info: pooling type     = 0
0.00.098.348 I print_info: rope type        = 2
0.00.098.348 I print_info: rope scaling     = linear
0.00.098.349 I print_info: freq_base_train  = 10000.0
0.00.098.349 I print_info: freq_scale_train = 1
0.00.098.349 I print_info: n_ctx_orig_yarn  = 2048
0.00.098.349 I print_info: rope_finetuned   = unknown
0.00.098.349 I print_info: ssm_d_conv       = 0
0.00.098.350 I print_info: ssm_d_inner      = 0
0.00.098.350 I print_info: ssm_d_state      = 0
0.00.098.350 I print_info: ssm_dt_rank      = 0
0.00.098.350 I print_info: ssm_dt_b_c_rms   = 0
0.00.098.350 I print_info: model type       = 1.4B
0.00.098.350 I print_info: model params     = 1.41 B
0.00.098.351 I print_info: general.name     = 1.4B
0.00.098.351 I print_info: vocab type       = BPE
0.00.098.351 I print_info: n_vocab          = 50304
0.00.098.352 I print_info: n_merges         = 50009
0.00.098.352 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.098.352 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.098.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.098.353 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.098.353 I print_info: LF token         = 128 'Ä'
0.00.098.353 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.098.355 I print_info: max token length = 1024
0.00.100.875 I load_tensors: offloading 24 repeating layers to GPU
0.00.100.876 I load_tensors: offloading output layer to GPU
0.00.100.876 I load_tensors: offloaded 25/25 layers to GPU
0.00.100.894 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.895 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.101.179 I llama_init_from_model: n_seq_max     = 1
0.00.101.179 I llama_init_from_model: n_ctx         = 2048
0.00.101.180 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.101.180 I llama_init_from_model: n_batch       = 2048
0.00.101.180 I llama_init_from_model: n_ubatch      = 512
0.00.101.180 I llama_init_from_model: flash_attn    = 0
0.00.101.181 I llama_init_from_model: freq_base     = 10000.0
0.00.101.181 I llama_init_from_model: freq_scale    = 1
0.00.101.181 I ggml_metal_init: allocating
0.00.101.184 I ggml_metal_init: found device: Apple M4
0.00.101.186 I ggml_metal_init: picking default device: Apple M4
0.00.101.845 I ggml_metal_init: using embedded metal library
0.00.113.420 I ggml_metal_init: GPU name:   Apple M4
0.00.113.421 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.422 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.422 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.422 I ggml_metal_init: simdgroup reduction   = true
0.00.113.423 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.423 I ggml_metal_init: has bfloat            = true
0.00.113.423 I ggml_metal_init: use bfloat            = true
0.00.113.423 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.424 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.137.090 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.157.452 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.157.458 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.157.481 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.158.446 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.158.448 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.158.449 I llama_init_from_model: graph nodes  = 967
0.00.158.449 I llama_init_from_model: graph splits = 2
0.00.158.452 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.158.586 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.158.586 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.234.955 I main: llama threadpool init, n_threads = 4
0.00.234.999 I 
0.00.235.022 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.235.024 I 
0.00.235.097 I sampler seed: 1234
0.00.235.102 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.235.156 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.235.158 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.235.159 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.078.363 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.02.078.364 I llama_perf_context_print:        load time =     202.08 ms
0.02.078.365 I llama_perf_context_print: prompt eval time =      54.40 ms /     7 tokens (    7.77 ms per token,   128.68 tokens per second)
0.02.078.365 I llama_perf_context_print:        eval time =    1785.88 ms /    63 runs   (   28.35 ms per token,    35.28 tokens per second)
0.02.078.366 I llama_perf_context_print:       total time =    1843.41 ms /    70 tokens
0.02.078.580 I ggml_metal_free: deallocating

real	0m2.405s
user	0m0.142s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.697 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.188 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.409 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.038.415 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.417 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.418 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.421 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.421 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.422 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.424 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.425 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.428 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.616 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.618 I llama_model_loader: - type  f32:  194 tensors
0.00.053.619 I llama_model_loader: - type  f16:   98 tensors
0.00.053.619 I print_info: file format = GGUF V3 (latest)
0.00.053.620 I print_info: file type   = all F32 (guessed)
0.00.053.623 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.079.288 I load: special tokens cache size = 25
0.00.085.426 I load: token to piece cache size = 0.2984 MB
0.00.085.430 I print_info: arch             = gptneox
0.00.085.430 I print_info: vocab_only       = 0
0.00.085.430 I print_info: n_ctx_train      = 2048
0.00.085.430 I print_info: n_embd           = 2048
0.00.085.431 I print_info: n_layer          = 24
0.00.085.435 I print_info: n_head           = 16
0.00.085.435 I print_info: n_head_kv        = 16
0.00.085.435 I print_info: n_rot            = 32
0.00.085.437 I print_info: n_swa            = 0
0.00.085.437 I print_info: n_embd_head_k    = 128
0.00.085.437 I print_info: n_embd_head_v    = 128
0.00.085.438 I print_info: n_gqa            = 1
0.00.085.438 I print_info: n_embd_k_gqa     = 2048
0.00.085.439 I print_info: n_embd_v_gqa     = 2048
0.00.085.440 I print_info: f_norm_eps       = 1.0e-05
0.00.085.440 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.085.440 I print_info: f_clamp_kqv      = 0.0e+00
0.00.085.440 I print_info: f_max_alibi_bias = 0.0e+00
0.00.085.440 I print_info: f_logit_scale    = 0.0e+00
0.00.085.441 I print_info: n_ff             = 8192
0.00.085.441 I print_info: n_expert         = 0
0.00.085.442 I print_info: n_expert_used    = 0
0.00.085.442 I print_info: causal attn      = 1
0.00.085.442 I print_info: pooling type     = 0
0.00.085.442 I print_info: rope type        = 2
0.00.085.442 I print_info: rope scaling     = linear
0.00.085.443 I print_info: freq_base_train  = 10000.0
0.00.085.443 I print_info: freq_scale_train = 1
0.00.085.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.085.443 I print_info: rope_finetuned   = unknown
0.00.085.443 I print_info: ssm_d_conv       = 0
0.00.085.443 I print_info: ssm_d_inner      = 0
0.00.085.444 I print_info: ssm_d_state      = 0
0.00.085.448 I print_info: ssm_dt_rank      = 0
0.00.085.448 I print_info: ssm_dt_b_c_rms   = 0
0.00.085.448 I print_info: model type       = 1.4B
0.00.085.449 I print_info: model params     = 1.41 B
0.00.085.449 I print_info: general.name     = 1.4B
0.00.085.449 I print_info: vocab type       = BPE
0.00.085.450 I print_info: n_vocab          = 50304
0.00.085.450 I print_info: n_merges         = 50009
0.00.085.450 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.085.450 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.085.450 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.085.451 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.085.451 I print_info: LF token         = 128 'Ä'
0.00.085.451 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.085.451 I print_info: max token length = 1024
0.00.087.580 I load_tensors: offloading 24 repeating layers to GPU
0.00.087.580 I load_tensors: offloading output layer to GPU
0.00.087.580 I load_tensors: offloaded 25/25 layers to GPU
0.00.087.591 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.592 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.087.946 I llama_init_from_model: n_seq_max     = 1
0.00.087.947 I llama_init_from_model: n_ctx         = 128
0.00.087.947 I llama_init_from_model: n_ctx_per_seq = 128
0.00.087.947 I llama_init_from_model: n_batch       = 128
0.00.087.947 I llama_init_from_model: n_ubatch      = 128
0.00.087.947 I llama_init_from_model: flash_attn    = 0
0.00.087.948 I llama_init_from_model: freq_base     = 10000.0
0.00.087.948 I llama_init_from_model: freq_scale    = 1
0.00.087.948 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.949 I ggml_metal_init: allocating
0.00.087.952 I ggml_metal_init: found device: Apple M4
0.00.087.953 I ggml_metal_init: picking default device: Apple M4
0.00.088.673 I ggml_metal_init: using embedded metal library
0.00.091.394 I ggml_metal_init: GPU name:   Apple M4
0.00.091.396 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.396 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.396 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.397 I ggml_metal_init: simdgroup reduction   = true
0.00.091.397 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.397 I ggml_metal_init: has bfloat            = true
0.00.091.397 I ggml_metal_init: use bfloat            = true
0.00.091.398 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.398 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.556 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.315 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.103.317 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.103.333 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.305 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.104.306 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.104.307 I llama_init_from_model: graph nodes  = 967
0.00.104.307 I llama_init_from_model: graph splits = 2
0.00.104.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.104.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.043.259 I 
0.01.043.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.043.367 I perplexity: tokenizing the input ..
0.01.056.857 I perplexity: tokenization took 13.491 ms
0.01.056.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.178.850 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.180.684 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.180.739 I llama_perf_context_print:        load time =    1020.05 ms
0.01.180.742 I llama_perf_context_print: prompt eval time =     121.44 ms /   128 tokens (    0.95 ms per token,  1054.06 tokens per second)
0.01.180.743 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.180.744 I llama_perf_context_print:       total time =     137.49 ms /   129 tokens
0.01.181.537 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.124s
sys	0m0.225s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.810 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.953 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.959 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.962 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.969 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.969 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.795 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.827 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.756 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.756 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.756 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.757 I llama_model_loader: - type  f32:  194 tensors
0.00.033.757 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.758 I print_info: file format = GGUF V3 (latest)
0.00.033.758 I print_info: file type   = Q8_0
0.00.033.760 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.053.976 I load: special tokens cache size = 25
0.00.060.088 I load: token to piece cache size = 0.2984 MB
0.00.060.093 I print_info: arch             = gptneox
0.00.060.093 I print_info: vocab_only       = 0
0.00.060.093 I print_info: n_ctx_train      = 2048
0.00.060.094 I print_info: n_embd           = 2048
0.00.060.094 I print_info: n_layer          = 24
0.00.060.099 I print_info: n_head           = 16
0.00.060.100 I print_info: n_head_kv        = 16
0.00.060.100 I print_info: n_rot            = 32
0.00.060.100 I print_info: n_swa            = 0
0.00.060.100 I print_info: n_embd_head_k    = 128
0.00.060.105 I print_info: n_embd_head_v    = 128
0.00.060.106 I print_info: n_gqa            = 1
0.00.060.107 I print_info: n_embd_k_gqa     = 2048
0.00.060.107 I print_info: n_embd_v_gqa     = 2048
0.00.060.108 I print_info: f_norm_eps       = 1.0e-05
0.00.060.109 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.060.109 I print_info: f_clamp_kqv      = 0.0e+00
0.00.060.110 I print_info: f_max_alibi_bias = 0.0e+00
0.00.060.110 I print_info: f_logit_scale    = 0.0e+00
0.00.060.111 I print_info: n_ff             = 8192
0.00.060.113 I print_info: n_expert         = 0
0.00.060.113 I print_info: n_expert_used    = 0
0.00.060.113 I print_info: causal attn      = 1
0.00.060.113 I print_info: pooling type     = 0
0.00.060.114 I print_info: rope type        = 2
0.00.060.115 I print_info: rope scaling     = linear
0.00.060.116 I print_info: freq_base_train  = 10000.0
0.00.060.116 I print_info: freq_scale_train = 1
0.00.060.116 I print_info: n_ctx_orig_yarn  = 2048
0.00.060.117 I print_info: rope_finetuned   = unknown
0.00.060.117 I print_info: ssm_d_conv       = 0
0.00.060.117 I print_info: ssm_d_inner      = 0
0.00.060.117 I print_info: ssm_d_state      = 0
0.00.060.117 I print_info: ssm_dt_rank      = 0
0.00.060.117 I print_info: ssm_dt_b_c_rms   = 0
0.00.060.118 I print_info: model type       = 1.4B
0.00.060.118 I print_info: model params     = 1.41 B
0.00.060.118 I print_info: general.name     = 1.4B
0.00.060.122 I print_info: vocab type       = BPE
0.00.060.123 I print_info: n_vocab          = 50304
0.00.060.123 I print_info: n_merges         = 50009
0.00.060.129 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.060.129 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.060.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.060.130 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.060.130 I print_info: LF token         = 128 'Ä'
0.00.060.130 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.060.130 I print_info: max token length = 1024
0.00.062.515 I load_tensors: offloading 24 repeating layers to GPU
0.00.062.515 I load_tensors: offloading output layer to GPU
0.00.062.515 I load_tensors: offloaded 25/25 layers to GPU
0.00.062.527 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.528 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.062.854 I llama_init_from_model: n_seq_max     = 1
0.00.062.855 I llama_init_from_model: n_ctx         = 2048
0.00.062.855 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.062.855 I llama_init_from_model: n_batch       = 2048
0.00.062.855 I llama_init_from_model: n_ubatch      = 512
0.00.062.855 I llama_init_from_model: flash_attn    = 0
0.00.062.856 I llama_init_from_model: freq_base     = 10000.0
0.00.062.856 I llama_init_from_model: freq_scale    = 1
0.00.062.857 I ggml_metal_init: allocating
0.00.062.861 I ggml_metal_init: found device: Apple M4
0.00.062.862 I ggml_metal_init: picking default device: Apple M4
0.00.063.593 I ggml_metal_init: using embedded metal library
0.00.066.113 I ggml_metal_init: GPU name:   Apple M4
0.00.066.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.116 I ggml_metal_init: simdgroup reduction   = true
0.00.066.116 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.116 I ggml_metal_init: has bfloat            = true
0.00.066.116 I ggml_metal_init: use bfloat            = true
0.00.066.117 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.117 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.477 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.100.977 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.989 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.015 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.102.306 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.102.308 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.102.308 I llama_init_from_model: graph nodes  = 967
0.00.102.308 I llama_init_from_model: graph splits = 2
0.00.102.312 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.102.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.102.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.549.894 I main: llama threadpool init, n_threads = 4
0.01.549.950 I 
0.01.549.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.549.988 I 
0.01.550.223 I sampler seed: 1234
0.01.550.228 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.550.278 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.550.280 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.550.280 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.647.762 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.02.647.763 I llama_perf_context_print:        load time =    1540.08 ms
0.02.647.764 I llama_perf_context_print: prompt eval time =      49.16 ms /     7 tokens (    7.02 ms per token,   142.39 tokens per second)
0.02.647.765 I llama_perf_context_print:        eval time =    1045.03 ms /    63 runs   (   16.59 ms per token,    60.29 tokens per second)
0.02.647.765 I llama_perf_context_print:       total time =    1097.87 ms /    70 tokens
0.02.648.015 I ggml_metal_free: deallocating

real	0m2.666s
user	0m0.115s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.156 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.150 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.159 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.160 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.160 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.161 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.161 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.169 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.173 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.276 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.043.457 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.043.460 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.043.460 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.043.461 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.043.462 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.043.462 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.043.463 I llama_model_loader: - type  f32:  194 tensors
0.00.043.463 I llama_model_loader: - type q8_0:   98 tensors
0.00.043.464 I print_info: file format = GGUF V3 (latest)
0.00.043.470 I print_info: file type   = Q8_0
0.00.043.471 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.067.723 I load: special tokens cache size = 25
0.00.073.768 I load: token to piece cache size = 0.2984 MB
0.00.073.772 I print_info: arch             = gptneox
0.00.073.772 I print_info: vocab_only       = 0
0.00.073.772 I print_info: n_ctx_train      = 2048
0.00.073.772 I print_info: n_embd           = 2048
0.00.073.773 I print_info: n_layer          = 24
0.00.073.777 I print_info: n_head           = 16
0.00.073.778 I print_info: n_head_kv        = 16
0.00.073.781 I print_info: n_rot            = 32
0.00.073.781 I print_info: n_swa            = 0
0.00.073.781 I print_info: n_embd_head_k    = 128
0.00.073.782 I print_info: n_embd_head_v    = 128
0.00.073.782 I print_info: n_gqa            = 1
0.00.073.783 I print_info: n_embd_k_gqa     = 2048
0.00.073.784 I print_info: n_embd_v_gqa     = 2048
0.00.073.785 I print_info: f_norm_eps       = 1.0e-05
0.00.073.785 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.786 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.786 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.786 I print_info: f_logit_scale    = 0.0e+00
0.00.073.787 I print_info: n_ff             = 8192
0.00.073.787 I print_info: n_expert         = 0
0.00.073.788 I print_info: n_expert_used    = 0
0.00.073.789 I print_info: causal attn      = 1
0.00.073.789 I print_info: pooling type     = 0
0.00.073.789 I print_info: rope type        = 2
0.00.073.789 I print_info: rope scaling     = linear
0.00.073.789 I print_info: freq_base_train  = 10000.0
0.00.073.790 I print_info: freq_scale_train = 1
0.00.073.790 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.790 I print_info: rope_finetuned   = unknown
0.00.073.790 I print_info: ssm_d_conv       = 0
0.00.073.791 I print_info: ssm_d_inner      = 0
0.00.073.791 I print_info: ssm_d_state      = 0
0.00.073.791 I print_info: ssm_dt_rank      = 0
0.00.073.791 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.791 I print_info: model type       = 1.4B
0.00.073.792 I print_info: model params     = 1.41 B
0.00.073.792 I print_info: general.name     = 1.4B
0.00.073.793 I print_info: vocab type       = BPE
0.00.073.796 I print_info: n_vocab          = 50304
0.00.073.796 I print_info: n_merges         = 50009
0.00.073.796 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.798 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.798 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.798 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.798 I print_info: LF token         = 128 'Ä'
0.00.073.799 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.799 I print_info: max token length = 1024
0.00.076.119 I load_tensors: offloading 24 repeating layers to GPU
0.00.076.119 I load_tensors: offloading output layer to GPU
0.00.076.119 I load_tensors: offloaded 25/25 layers to GPU
0.00.076.131 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.076.132 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.076.448 I llama_init_from_model: n_seq_max     = 1
0.00.076.449 I llama_init_from_model: n_ctx         = 128
0.00.076.449 I llama_init_from_model: n_ctx_per_seq = 128
0.00.076.449 I llama_init_from_model: n_batch       = 128
0.00.076.450 I llama_init_from_model: n_ubatch      = 128
0.00.076.450 I llama_init_from_model: flash_attn    = 0
0.00.076.450 I llama_init_from_model: freq_base     = 10000.0
0.00.076.450 I llama_init_from_model: freq_scale    = 1
0.00.076.451 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.451 I ggml_metal_init: allocating
0.00.076.455 I ggml_metal_init: found device: Apple M4
0.00.076.457 I ggml_metal_init: picking default device: Apple M4
0.00.077.110 I ggml_metal_init: using embedded metal library
0.00.079.577 I ggml_metal_init: GPU name:   Apple M4
0.00.079.579 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.579 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.580 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.580 I ggml_metal_init: simdgroup reduction   = true
0.00.079.580 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.580 I ggml_metal_init: has bfloat            = true
0.00.079.580 I ggml_metal_init: use bfloat            = true
0.00.079.581 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.582 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.089.905 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.543 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.091.549 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.091.564 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.092.575 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.092.576 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.092.577 I llama_init_from_model: graph nodes  = 967
0.00.092.577 I llama_init_from_model: graph splits = 2
0.00.092.578 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.092.578 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.808.350 I 
0.00.808.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.808.409 I perplexity: tokenizing the input ..
0.00.816.876 I perplexity: tokenization took 8.465 ms
0.00.816.885 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.940.904 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.942.113 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.942.137 I llama_perf_context_print:        load time =     794.19 ms
0.00.942.138 I llama_perf_context_print: prompt eval time =     123.76 ms /   128 tokens (    0.97 ms per token,  1034.23 tokens per second)
0.00.942.139 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.942.140 I llama_perf_context_print:       total time =     133.79 ms /   129 tokens
0.00.942.500 I ggml_metal_free: deallocating

real	0m0.963s
user	0m0.101s
sys	0m0.139s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.021.790 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.315 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.324 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.325 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.325 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.328 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.368 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.338 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.339 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.340 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.049.341 I llama_model_loader: - type  f32:  194 tensors
0.00.049.342 I llama_model_loader: - type q4_0:   97 tensors
0.00.049.342 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.343 I print_info: file format = GGUF V3 (latest)
0.00.049.343 I print_info: file type   = Q4_0
0.00.049.344 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.079.471 I load: special tokens cache size = 25
0.00.088.969 I load: token to piece cache size = 0.2984 MB
0.00.088.974 I print_info: arch             = gptneox
0.00.088.974 I print_info: vocab_only       = 0
0.00.088.974 I print_info: n_ctx_train      = 2048
0.00.088.974 I print_info: n_embd           = 2048
0.00.088.975 I print_info: n_layer          = 24
0.00.088.979 I print_info: n_head           = 16
0.00.088.980 I print_info: n_head_kv        = 16
0.00.088.980 I print_info: n_rot            = 32
0.00.088.981 I print_info: n_swa            = 0
0.00.088.981 I print_info: n_embd_head_k    = 128
0.00.088.981 I print_info: n_embd_head_v    = 128
0.00.088.982 I print_info: n_gqa            = 1
0.00.088.983 I print_info: n_embd_k_gqa     = 2048
0.00.088.984 I print_info: n_embd_v_gqa     = 2048
0.00.088.984 I print_info: f_norm_eps       = 1.0e-05
0.00.088.985 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.985 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.985 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.986 I print_info: f_logit_scale    = 0.0e+00
0.00.088.987 I print_info: n_ff             = 8192
0.00.088.987 I print_info: n_expert         = 0
0.00.088.987 I print_info: n_expert_used    = 0
0.00.088.987 I print_info: causal attn      = 1
0.00.088.987 I print_info: pooling type     = 0
0.00.088.987 I print_info: rope type        = 2
0.00.088.988 I print_info: rope scaling     = linear
0.00.088.988 I print_info: freq_base_train  = 10000.0
0.00.088.989 I print_info: freq_scale_train = 1
0.00.088.989 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.989 I print_info: rope_finetuned   = unknown
0.00.088.990 I print_info: ssm_d_conv       = 0
0.00.088.990 I print_info: ssm_d_inner      = 0
0.00.088.990 I print_info: ssm_d_state      = 0
0.00.088.990 I print_info: ssm_dt_rank      = 0
0.00.088.990 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.991 I print_info: model type       = 1.4B
0.00.088.992 I print_info: model params     = 1.41 B
0.00.088.995 I print_info: general.name     = 1.4B
0.00.088.996 I print_info: vocab type       = BPE
0.00.088.996 I print_info: n_vocab          = 50304
0.00.088.996 I print_info: n_merges         = 50009
0.00.088.997 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.997 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.997 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.997 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.998 I print_info: LF token         = 128 'Ä'
0.00.088.998 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.998 I print_info: max token length = 1024
0.00.091.965 I load_tensors: offloading 24 repeating layers to GPU
0.00.091.965 I load_tensors: offloading output layer to GPU
0.00.091.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.091.979 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.091.980 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.092.463 I llama_init_from_model: n_seq_max     = 1
0.00.092.464 I llama_init_from_model: n_ctx         = 2048
0.00.092.464 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.092.464 I llama_init_from_model: n_batch       = 2048
0.00.092.465 I llama_init_from_model: n_ubatch      = 512
0.00.092.465 I llama_init_from_model: flash_attn    = 0
0.00.092.466 I llama_init_from_model: freq_base     = 10000.0
0.00.092.466 I llama_init_from_model: freq_scale    = 1
0.00.092.466 I ggml_metal_init: allocating
0.00.092.470 I ggml_metal_init: found device: Apple M4
0.00.092.473 I ggml_metal_init: picking default device: Apple M4
0.00.093.437 I ggml_metal_init: using embedded metal library
0.00.097.044 I ggml_metal_init: GPU name:   Apple M4
0.00.097.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.047 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.048 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.048 I ggml_metal_init: simdgroup reduction   = true
0.00.097.048 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.048 I ggml_metal_init: has bfloat            = true
0.00.097.048 I ggml_metal_init: use bfloat            = true
0.00.097.049 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.764 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.132.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.132.305 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.132.328 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.133.363 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.133.364 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.133.365 I llama_init_from_model: graph nodes  = 967
0.00.133.365 I llama_init_from_model: graph splits = 2
0.00.133.375 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.133.504 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.803.443 I main: llama threadpool init, n_threads = 4
0.00.803.501 I 
0.00.803.534 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.803.536 I 
0.00.803.846 I sampler seed: 1234
0.00.803.851 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.803.878 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.803.880 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.803.880 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.107 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.483.107 I llama_perf_context_print:        load time =     781.64 ms
0.01.483.109 I llama_perf_context_print: prompt eval time =      45.66 ms /     7 tokens (    6.52 ms per token,   153.32 tokens per second)
0.01.483.109 I llama_perf_context_print:        eval time =     630.60 ms /    63 runs   (   10.01 ms per token,    99.90 tokens per second)
0.01.483.110 I llama_perf_context_print:       total time =     679.67 ms /    70 tokens
0.01.483.317 I ggml_metal_free: deallocating

real	0m1.509s
user	0m0.134s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.380 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.168 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.174 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.174 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.174 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.928 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.939 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.652 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.653 I llama_model_loader: - type  f32:  194 tensors
0.00.024.653 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.654 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.654 I print_info: file format = GGUF V3 (latest)
0.00.024.654 I print_info: file type   = Q4_0
0.00.024.655 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.043.585 I load: special tokens cache size = 25
0.00.049.589 I load: token to piece cache size = 0.2984 MB
0.00.049.592 I print_info: arch             = gptneox
0.00.049.592 I print_info: vocab_only       = 0
0.00.049.592 I print_info: n_ctx_train      = 2048
0.00.049.592 I print_info: n_embd           = 2048
0.00.049.593 I print_info: n_layer          = 24
0.00.049.595 I print_info: n_head           = 16
0.00.049.596 I print_info: n_head_kv        = 16
0.00.049.596 I print_info: n_rot            = 32
0.00.049.596 I print_info: n_swa            = 0
0.00.049.596 I print_info: n_embd_head_k    = 128
0.00.049.598 I print_info: n_embd_head_v    = 128
0.00.049.599 I print_info: n_gqa            = 1
0.00.049.600 I print_info: n_embd_k_gqa     = 2048
0.00.049.606 I print_info: n_embd_v_gqa     = 2048
0.00.049.608 I print_info: f_norm_eps       = 1.0e-05
0.00.049.608 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.609 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.610 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.610 I print_info: f_logit_scale    = 0.0e+00
0.00.049.617 I print_info: n_ff             = 8192
0.00.049.617 I print_info: n_expert         = 0
0.00.049.618 I print_info: n_expert_used    = 0
0.00.049.619 I print_info: causal attn      = 1
0.00.049.619 I print_info: pooling type     = 0
0.00.049.619 I print_info: rope type        = 2
0.00.049.619 I print_info: rope scaling     = linear
0.00.049.619 I print_info: freq_base_train  = 10000.0
0.00.049.621 I print_info: freq_scale_train = 1
0.00.049.621 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.621 I print_info: rope_finetuned   = unknown
0.00.049.622 I print_info: ssm_d_conv       = 0
0.00.049.622 I print_info: ssm_d_inner      = 0
0.00.049.622 I print_info: ssm_d_state      = 0
0.00.049.622 I print_info: ssm_dt_rank      = 0
0.00.049.622 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.622 I print_info: model type       = 1.4B
0.00.049.623 I print_info: model params     = 1.41 B
0.00.049.623 I print_info: general.name     = 1.4B
0.00.049.624 I print_info: vocab type       = BPE
0.00.049.624 I print_info: n_vocab          = 50304
0.00.049.625 I print_info: n_merges         = 50009
0.00.049.625 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.625 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.626 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.626 I print_info: LF token         = 128 'Ä'
0.00.049.626 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.626 I print_info: max token length = 1024
0.00.051.561 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.561 I load_tensors: offloading output layer to GPU
0.00.051.562 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.572 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.573 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.051.877 I llama_init_from_model: n_seq_max     = 1
0.00.051.878 I llama_init_from_model: n_ctx         = 128
0.00.051.878 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.878 I llama_init_from_model: n_batch       = 128
0.00.051.878 I llama_init_from_model: n_ubatch      = 128
0.00.051.878 I llama_init_from_model: flash_attn    = 0
0.00.051.878 I llama_init_from_model: freq_base     = 10000.0
0.00.051.879 I llama_init_from_model: freq_scale    = 1
0.00.051.879 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.879 I ggml_metal_init: allocating
0.00.051.882 I ggml_metal_init: found device: Apple M4
0.00.051.884 I ggml_metal_init: picking default device: Apple M4
0.00.052.450 I ggml_metal_init: using embedded metal library
0.00.054.778 I ggml_metal_init: GPU name:   Apple M4
0.00.054.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.781 I ggml_metal_init: simdgroup reduction   = true
0.00.054.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.781 I ggml_metal_init: has bfloat            = true
0.00.054.781 I ggml_metal_init: use bfloat            = true
0.00.054.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.519 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.776 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.781 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.795 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.692 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.693 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.693 I llama_init_from_model: graph nodes  = 967
0.00.066.693 I llama_init_from_model: graph splits = 2
0.00.066.694 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.695 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.571.260 I 
0.00.571.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.571.311 I perplexity: tokenizing the input ..
0.00.579.410 I perplexity: tokenization took 8.097 ms
0.00.579.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.702.353 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.703.507 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.703.535 I llama_perf_context_print:        load time =     561.87 ms
0.00.703.536 I llama_perf_context_print: prompt eval time =     122.71 ms /   128 tokens (    0.96 ms per token,  1043.10 tokens per second)
0.00.703.537 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.703.537 I llama_perf_context_print:       total time =     132.28 ms /   129 tokens
0.00.704.098 I ggml_metal_free: deallocating

real	0m0.719s
user	0m0.077s
sys	0m0.103s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.016.710 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.291 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.032.296 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.300 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.300 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.301 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.302 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.302 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.302 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.303 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.303 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.303 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.304 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.326 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.042.326 I llama_model_loader: - type  f32:  194 tensors
0.00.042.327 I llama_model_loader: - type q4_1:   97 tensors
0.00.042.327 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.327 I print_info: file format = GGUF V3 (latest)
0.00.042.328 I print_info: file type   = Q4_1
0.00.042.329 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.068.585 I load: special tokens cache size = 25
0.00.077.986 I load: token to piece cache size = 0.2984 MB
0.00.077.990 I print_info: arch             = gptneox
0.00.077.990 I print_info: vocab_only       = 0
0.00.077.990 I print_info: n_ctx_train      = 2048
0.00.077.991 I print_info: n_embd           = 2048
0.00.077.991 I print_info: n_layer          = 24
0.00.077.994 I print_info: n_head           = 16
0.00.077.995 I print_info: n_head_kv        = 16
0.00.077.996 I print_info: n_rot            = 32
0.00.077.996 I print_info: n_swa            = 0
0.00.077.996 I print_info: n_embd_head_k    = 128
0.00.077.996 I print_info: n_embd_head_v    = 128
0.00.077.997 I print_info: n_gqa            = 1
0.00.077.998 I print_info: n_embd_k_gqa     = 2048
0.00.078.001 I print_info: n_embd_v_gqa     = 2048
0.00.078.002 I print_info: f_norm_eps       = 1.0e-05
0.00.078.002 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.078.004 I print_info: f_clamp_kqv      = 0.0e+00
0.00.078.004 I print_info: f_max_alibi_bias = 0.0e+00
0.00.078.005 I print_info: f_logit_scale    = 0.0e+00
0.00.078.005 I print_info: n_ff             = 8192
0.00.078.006 I print_info: n_expert         = 0
0.00.078.006 I print_info: n_expert_used    = 0
0.00.078.006 I print_info: causal attn      = 1
0.00.078.006 I print_info: pooling type     = 0
0.00.078.006 I print_info: rope type        = 2
0.00.078.007 I print_info: rope scaling     = linear
0.00.078.007 I print_info: freq_base_train  = 10000.0
0.00.078.008 I print_info: freq_scale_train = 1
0.00.078.008 I print_info: n_ctx_orig_yarn  = 2048
0.00.078.009 I print_info: rope_finetuned   = unknown
0.00.078.009 I print_info: ssm_d_conv       = 0
0.00.078.010 I print_info: ssm_d_inner      = 0
0.00.078.010 I print_info: ssm_d_state      = 0
0.00.078.010 I print_info: ssm_dt_rank      = 0
0.00.078.010 I print_info: ssm_dt_b_c_rms   = 0
0.00.078.010 I print_info: model type       = 1.4B
0.00.078.011 I print_info: model params     = 1.41 B
0.00.078.011 I print_info: general.name     = 1.4B
0.00.078.012 I print_info: vocab type       = BPE
0.00.078.012 I print_info: n_vocab          = 50304
0.00.078.012 I print_info: n_merges         = 50009
0.00.078.012 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.078.013 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.078.013 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.078.013 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.078.014 I print_info: LF token         = 128 'Ä'
0.00.078.014 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.078.014 I print_info: max token length = 1024
0.00.080.442 I load_tensors: offloading 24 repeating layers to GPU
0.00.080.442 I load_tensors: offloading output layer to GPU
0.00.080.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.080.449 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.080.450 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.080.879 I llama_init_from_model: n_seq_max     = 1
0.00.080.881 I llama_init_from_model: n_ctx         = 2048
0.00.080.881 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.080.881 I llama_init_from_model: n_batch       = 2048
0.00.080.882 I llama_init_from_model: n_ubatch      = 512
0.00.080.882 I llama_init_from_model: flash_attn    = 0
0.00.080.883 I llama_init_from_model: freq_base     = 10000.0
0.00.080.883 I llama_init_from_model: freq_scale    = 1
0.00.080.884 I ggml_metal_init: allocating
0.00.080.888 I ggml_metal_init: found device: Apple M4
0.00.080.891 I ggml_metal_init: picking default device: Apple M4
0.00.081.771 I ggml_metal_init: using embedded metal library
0.00.085.579 I ggml_metal_init: GPU name:   Apple M4
0.00.085.582 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.582 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.583 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.583 I ggml_metal_init: simdgroup reduction   = true
0.00.085.583 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.583 I ggml_metal_init: has bfloat            = true
0.00.085.584 I ggml_metal_init: use bfloat            = true
0.00.085.584 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.586 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.646 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.119.329 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.337 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.120.452 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.120.453 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.120.454 I llama_init_from_model: graph nodes  = 967
0.00.120.454 I llama_init_from_model: graph splits = 2
0.00.120.460 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.588 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.589 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.827 I main: llama threadpool init, n_threads = 4
0.00.857.866 I 
0.00.857.886 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.886 I 
0.00.858.112 I sampler seed: 1234
0.00.858.116 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.127 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.127 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.127 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.588.822 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.01.588.823 I llama_perf_context_print:        load time =     841.11 ms
0.01.588.824 I llama_perf_context_print: prompt eval time =      46.58 ms /     7 tokens (    6.65 ms per token,   150.28 tokens per second)
0.01.588.824 I llama_perf_context_print:        eval time =     681.19 ms /    63 runs   (   10.81 ms per token,    92.49 tokens per second)
0.01.588.826 I llama_perf_context_print:       total time =     731.00 ms /    70 tokens
0.01.589.014 I ggml_metal_free: deallocating

real	0m1.609s
user	0m0.130s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.927 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.017 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.022 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.023 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.024 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.024 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.025 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.026 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.026 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.027 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.027 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.029 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.030 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.030 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.034 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.035 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.035 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.809 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.644 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.644 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.645 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.645 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.646 I llama_model_loader: - type  f32:  194 tensors
0.00.024.646 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.646 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.647 I print_info: file format = GGUF V3 (latest)
0.00.024.648 I print_info: file type   = Q4_1
0.00.024.649 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.158 I load: special tokens cache size = 25
0.00.049.905 I load: token to piece cache size = 0.2984 MB
0.00.049.908 I print_info: arch             = gptneox
0.00.049.908 I print_info: vocab_only       = 0
0.00.049.908 I print_info: n_ctx_train      = 2048
0.00.049.909 I print_info: n_embd           = 2048
0.00.049.909 I print_info: n_layer          = 24
0.00.049.912 I print_info: n_head           = 16
0.00.049.912 I print_info: n_head_kv        = 16
0.00.049.913 I print_info: n_rot            = 32
0.00.049.913 I print_info: n_swa            = 0
0.00.049.913 I print_info: n_embd_head_k    = 128
0.00.049.913 I print_info: n_embd_head_v    = 128
0.00.049.916 I print_info: n_gqa            = 1
0.00.049.917 I print_info: n_embd_k_gqa     = 2048
0.00.049.924 I print_info: n_embd_v_gqa     = 2048
0.00.049.926 I print_info: f_norm_eps       = 1.0e-05
0.00.049.930 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.930 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.930 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.930 I print_info: f_logit_scale    = 0.0e+00
0.00.049.933 I print_info: n_ff             = 8192
0.00.049.934 I print_info: n_expert         = 0
0.00.049.934 I print_info: n_expert_used    = 0
0.00.049.934 I print_info: causal attn      = 1
0.00.049.934 I print_info: pooling type     = 0
0.00.049.934 I print_info: rope type        = 2
0.00.049.934 I print_info: rope scaling     = linear
0.00.049.936 I print_info: freq_base_train  = 10000.0
0.00.049.937 I print_info: freq_scale_train = 1
0.00.049.937 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.938 I print_info: rope_finetuned   = unknown
0.00.049.938 I print_info: ssm_d_conv       = 0
0.00.049.938 I print_info: ssm_d_inner      = 0
0.00.049.938 I print_info: ssm_d_state      = 0
0.00.049.939 I print_info: ssm_dt_rank      = 0
0.00.049.939 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.939 I print_info: model type       = 1.4B
0.00.049.940 I print_info: model params     = 1.41 B
0.00.049.940 I print_info: general.name     = 1.4B
0.00.049.940 I print_info: vocab type       = BPE
0.00.049.941 I print_info: n_vocab          = 50304
0.00.049.941 I print_info: n_merges         = 50009
0.00.049.941 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.941 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.941 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.941 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.942 I print_info: LF token         = 128 'Ä'
0.00.049.944 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.944 I print_info: max token length = 1024
0.00.051.870 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.871 I load_tensors: offloading output layer to GPU
0.00.051.871 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.881 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.883 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.052.166 I llama_init_from_model: n_seq_max     = 1
0.00.052.166 I llama_init_from_model: n_ctx         = 128
0.00.052.167 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.167 I llama_init_from_model: n_batch       = 128
0.00.052.167 I llama_init_from_model: n_ubatch      = 128
0.00.052.167 I llama_init_from_model: flash_attn    = 0
0.00.052.167 I llama_init_from_model: freq_base     = 10000.0
0.00.052.168 I llama_init_from_model: freq_scale    = 1
0.00.052.168 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.168 I ggml_metal_init: allocating
0.00.052.171 I ggml_metal_init: found device: Apple M4
0.00.052.173 I ggml_metal_init: picking default device: Apple M4
0.00.052.750 I ggml_metal_init: using embedded metal library
0.00.055.124 I ggml_metal_init: GPU name:   Apple M4
0.00.055.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.126 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.127 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.127 I ggml_metal_init: simdgroup reduction   = true
0.00.055.127 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.127 I ggml_metal_init: has bfloat            = true
0.00.055.127 I ggml_metal_init: use bfloat            = true
0.00.055.128 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.128 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.513 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.776 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.781 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.796 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.659 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.660 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.660 I llama_init_from_model: graph nodes  = 967
0.00.066.660 I llama_init_from_model: graph splits = 2
0.00.066.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.428 I 
0.00.654.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.478 I perplexity: tokenizing the input ..
0.00.662.634 I perplexity: tokenization took 8.154 ms
0.00.662.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.785.588 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.786.827 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.786.846 I llama_perf_context_print:        load time =     645.50 ms
0.00.786.847 I llama_perf_context_print: prompt eval time =     122.72 ms /   128 tokens (    0.96 ms per token,  1043.01 tokens per second)
0.00.786.849 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.786.850 I llama_perf_context_print:       total time =     132.42 ms /   129 tokens
0.00.787.221 I ggml_metal_free: deallocating

real	0m0.800s
user	0m0.077s
sys	0m0.101s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.789 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.025.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.076 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.078 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.078 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.082 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.083 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.083 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.083 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.088 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.090 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.091 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.091 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.837 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.855 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.607 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.608 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.608 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.608 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.609 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.609 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.610 I llama_model_loader: - type  f32:  194 tensors
0.00.033.610 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.610 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.611 I print_info: file format = GGUF V3 (latest)
0.00.033.611 I print_info: file type   = Q5_0
0.00.033.612 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.055.476 I load: special tokens cache size = 25
0.00.061.713 I load: token to piece cache size = 0.2984 MB
0.00.061.716 I print_info: arch             = gptneox
0.00.061.716 I print_info: vocab_only       = 0
0.00.061.717 I print_info: n_ctx_train      = 2048
0.00.061.717 I print_info: n_embd           = 2048
0.00.061.717 I print_info: n_layer          = 24
0.00.061.720 I print_info: n_head           = 16
0.00.061.721 I print_info: n_head_kv        = 16
0.00.061.723 I print_info: n_rot            = 32
0.00.061.723 I print_info: n_swa            = 0
0.00.061.723 I print_info: n_embd_head_k    = 128
0.00.061.723 I print_info: n_embd_head_v    = 128
0.00.061.724 I print_info: n_gqa            = 1
0.00.061.725 I print_info: n_embd_k_gqa     = 2048
0.00.061.726 I print_info: n_embd_v_gqa     = 2048
0.00.061.726 I print_info: f_norm_eps       = 1.0e-05
0.00.061.726 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.061.727 I print_info: f_clamp_kqv      = 0.0e+00
0.00.061.729 I print_info: f_max_alibi_bias = 0.0e+00
0.00.061.729 I print_info: f_logit_scale    = 0.0e+00
0.00.061.730 I print_info: n_ff             = 8192
0.00.061.730 I print_info: n_expert         = 0
0.00.061.730 I print_info: n_expert_used    = 0
0.00.061.731 I print_info: causal attn      = 1
0.00.061.733 I print_info: pooling type     = 0
0.00.061.733 I print_info: rope type        = 2
0.00.061.733 I print_info: rope scaling     = linear
0.00.061.733 I print_info: freq_base_train  = 10000.0
0.00.061.734 I print_info: freq_scale_train = 1
0.00.061.734 I print_info: n_ctx_orig_yarn  = 2048
0.00.061.734 I print_info: rope_finetuned   = unknown
0.00.061.734 I print_info: ssm_d_conv       = 0
0.00.061.734 I print_info: ssm_d_inner      = 0
0.00.061.735 I print_info: ssm_d_state      = 0
0.00.061.735 I print_info: ssm_dt_rank      = 0
0.00.061.735 I print_info: ssm_dt_b_c_rms   = 0
0.00.061.735 I print_info: model type       = 1.4B
0.00.061.735 I print_info: model params     = 1.41 B
0.00.061.736 I print_info: general.name     = 1.4B
0.00.061.736 I print_info: vocab type       = BPE
0.00.061.736 I print_info: n_vocab          = 50304
0.00.061.737 I print_info: n_merges         = 50009
0.00.061.737 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.061.737 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.061.737 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.061.737 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.061.738 I print_info: LF token         = 128 'Ä'
0.00.061.738 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.061.738 I print_info: max token length = 1024
0.00.063.827 I load_tensors: offloading 24 repeating layers to GPU
0.00.063.827 I load_tensors: offloading output layer to GPU
0.00.063.828 I load_tensors: offloaded 25/25 layers to GPU
0.00.063.839 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.063.840 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.064.147 I llama_init_from_model: n_seq_max     = 1
0.00.064.148 I llama_init_from_model: n_ctx         = 2048
0.00.064.148 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.148 I llama_init_from_model: n_batch       = 2048
0.00.064.148 I llama_init_from_model: n_ubatch      = 512
0.00.064.148 I llama_init_from_model: flash_attn    = 0
0.00.064.149 I llama_init_from_model: freq_base     = 10000.0
0.00.064.149 I llama_init_from_model: freq_scale    = 1
0.00.064.149 I ggml_metal_init: allocating
0.00.064.153 I ggml_metal_init: found device: Apple M4
0.00.064.155 I ggml_metal_init: picking default device: Apple M4
0.00.064.763 I ggml_metal_init: using embedded metal library
0.00.067.249 I ggml_metal_init: GPU name:   Apple M4
0.00.067.250 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.067.251 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.067.251 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.067.252 I ggml_metal_init: simdgroup reduction   = true
0.00.067.252 I ggml_metal_init: simdgroup matrix mul. = true
0.00.067.252 I ggml_metal_init: has bfloat            = true
0.00.067.252 I ggml_metal_init: use bfloat            = true
0.00.067.253 I ggml_metal_init: hasUnifiedMemory      = true
0.00.067.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.077.254 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.098.135 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.098.145 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.098.174 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.099.275 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.099.276 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.099.276 I llama_init_from_model: graph nodes  = 967
0.00.099.277 I llama_init_from_model: graph splits = 2
0.00.099.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.396 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.055 I main: llama threadpool init, n_threads = 4
0.00.831.094 I 
0.00.831.119 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.831.119 I 
0.00.831.347 I sampler seed: 1234
0.00.831.352 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.378 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.379 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.379 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.624.107 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56037.88 tokens per second)
0.01.624.107 I llama_perf_context_print:        load time =     822.26 ms
0.01.624.108 I llama_perf_context_print: prompt eval time =      43.04 ms /     7 tokens (    6.15 ms per token,   162.64 tokens per second)
0.01.624.109 I llama_perf_context_print:        eval time =     746.57 ms /    63 runs   (   11.85 ms per token,    84.39 tokens per second)
0.01.624.110 I llama_perf_context_print:       total time =     793.05 ms /    70 tokens
0.01.624.311 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.113s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.767 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.974 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.980 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.981 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.981 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.982 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.982 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.983 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.983 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.984 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.984 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.984 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.985 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.985 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.987 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.987 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.987 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.833 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.647 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.648 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.648 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.649 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.649 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.649 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.650 I llama_model_loader: - type  f32:  194 tensors
0.00.026.650 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.650 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.651 I print_info: file format = GGUF V3 (latest)
0.00.026.651 I print_info: file type   = Q5_0
0.00.026.653 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.046.146 I load: special tokens cache size = 25
0.00.051.994 I load: token to piece cache size = 0.2984 MB
0.00.051.997 I print_info: arch             = gptneox
0.00.051.997 I print_info: vocab_only       = 0
0.00.051.998 I print_info: n_ctx_train      = 2048
0.00.051.998 I print_info: n_embd           = 2048
0.00.051.998 I print_info: n_layer          = 24
0.00.052.001 I print_info: n_head           = 16
0.00.052.007 I print_info: n_head_kv        = 16
0.00.052.007 I print_info: n_rot            = 32
0.00.052.008 I print_info: n_swa            = 0
0.00.052.008 I print_info: n_embd_head_k    = 128
0.00.052.008 I print_info: n_embd_head_v    = 128
0.00.052.009 I print_info: n_gqa            = 1
0.00.052.010 I print_info: n_embd_k_gqa     = 2048
0.00.052.010 I print_info: n_embd_v_gqa     = 2048
0.00.052.011 I print_info: f_norm_eps       = 1.0e-05
0.00.052.012 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.012 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.014 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.014 I print_info: f_logit_scale    = 0.0e+00
0.00.052.015 I print_info: n_ff             = 8192
0.00.052.015 I print_info: n_expert         = 0
0.00.052.015 I print_info: n_expert_used    = 0
0.00.052.015 I print_info: causal attn      = 1
0.00.052.015 I print_info: pooling type     = 0
0.00.052.016 I print_info: rope type        = 2
0.00.052.016 I print_info: rope scaling     = linear
0.00.052.018 I print_info: freq_base_train  = 10000.0
0.00.052.019 I print_info: freq_scale_train = 1
0.00.052.019 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.020 I print_info: rope_finetuned   = unknown
0.00.052.020 I print_info: ssm_d_conv       = 0
0.00.052.020 I print_info: ssm_d_inner      = 0
0.00.052.020 I print_info: ssm_d_state      = 0
0.00.052.020 I print_info: ssm_dt_rank      = 0
0.00.052.020 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.021 I print_info: model type       = 1.4B
0.00.052.022 I print_info: model params     = 1.41 B
0.00.052.022 I print_info: general.name     = 1.4B
0.00.052.023 I print_info: vocab type       = BPE
0.00.052.023 I print_info: n_vocab          = 50304
0.00.052.023 I print_info: n_merges         = 50009
0.00.052.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.023 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.023 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.024 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.024 I print_info: LF token         = 128 'Ä'
0.00.052.024 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.024 I print_info: max token length = 1024
0.00.053.793 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.793 I load_tensors: offloading output layer to GPU
0.00.053.793 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.799 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.799 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.204 I llama_init_from_model: n_seq_max     = 1
0.00.054.205 I llama_init_from_model: n_ctx         = 128
0.00.054.205 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.205 I llama_init_from_model: n_batch       = 128
0.00.054.205 I llama_init_from_model: n_ubatch      = 128
0.00.054.205 I llama_init_from_model: flash_attn    = 0
0.00.054.206 I llama_init_from_model: freq_base     = 10000.0
0.00.054.206 I llama_init_from_model: freq_scale    = 1
0.00.054.206 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.207 I ggml_metal_init: allocating
0.00.054.209 I ggml_metal_init: found device: Apple M4
0.00.054.211 I ggml_metal_init: picking default device: Apple M4
0.00.054.764 I ggml_metal_init: using embedded metal library
0.00.057.050 I ggml_metal_init: GPU name:   Apple M4
0.00.057.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.052 I ggml_metal_init: simdgroup reduction   = true
0.00.057.052 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.052 I ggml_metal_init: has bfloat            = true
0.00.057.053 I ggml_metal_init: use bfloat            = true
0.00.057.053 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.452 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.714 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.718 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.732 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.694 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.695 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.695 I llama_init_from_model: graph nodes  = 967
0.00.068.695 I llama_init_from_model: graph splits = 2
0.00.068.696 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.697 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.729.445 I 
0.00.729.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.729.493 I perplexity: tokenizing the input ..
0.00.736.817 I perplexity: tokenization took 7.321 ms
0.00.736.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.842 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.872.216 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.872.244 I llama_perf_context_print:        load time =     718.67 ms
0.00.872.245 I llama_perf_context_print: prompt eval time =     133.79 ms /   128 tokens (    1.05 ms per token,   956.72 tokens per second)
0.00.872.246 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.246 I llama_perf_context_print:       total time =     142.80 ms /   129 tokens
0.00.872.602 I ggml_metal_free: deallocating

real	0m0.888s
user	0m0.078s
sys	0m0.105s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.016.650 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.027.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.782 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.783 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.783 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.783 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.784 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.785 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.785 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.785 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.788 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.788 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.788 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.790 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.790 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.791 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.500 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.868 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.799 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.801 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.801 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.802 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.802 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.802 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.038.803 I llama_model_loader: - type  f32:  194 tensors
0.00.038.803 I llama_model_loader: - type q5_1:   97 tensors
0.00.038.804 I llama_model_loader: - type q6_K:    1 tensors
0.00.038.804 I print_info: file format = GGUF V3 (latest)
0.00.038.805 I print_info: file type   = Q5_1
0.00.038.806 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.068.439 I load: special tokens cache size = 25
0.00.079.692 I load: token to piece cache size = 0.2984 MB
0.00.079.698 I print_info: arch             = gptneox
0.00.079.698 I print_info: vocab_only       = 0
0.00.079.700 I print_info: n_ctx_train      = 2048
0.00.079.701 I print_info: n_embd           = 2048
0.00.079.701 I print_info: n_layer          = 24
0.00.079.705 I print_info: n_head           = 16
0.00.079.706 I print_info: n_head_kv        = 16
0.00.079.707 I print_info: n_rot            = 32
0.00.079.707 I print_info: n_swa            = 0
0.00.079.707 I print_info: n_embd_head_k    = 128
0.00.079.707 I print_info: n_embd_head_v    = 128
0.00.079.708 I print_info: n_gqa            = 1
0.00.079.709 I print_info: n_embd_k_gqa     = 2048
0.00.079.710 I print_info: n_embd_v_gqa     = 2048
0.00.079.711 I print_info: f_norm_eps       = 1.0e-05
0.00.079.712 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.712 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.712 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.712 I print_info: f_logit_scale    = 0.0e+00
0.00.079.713 I print_info: n_ff             = 8192
0.00.079.713 I print_info: n_expert         = 0
0.00.079.714 I print_info: n_expert_used    = 0
0.00.079.714 I print_info: causal attn      = 1
0.00.079.714 I print_info: pooling type     = 0
0.00.079.714 I print_info: rope type        = 2
0.00.079.715 I print_info: rope scaling     = linear
0.00.079.718 I print_info: freq_base_train  = 10000.0
0.00.079.718 I print_info: freq_scale_train = 1
0.00.079.718 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.719 I print_info: rope_finetuned   = unknown
0.00.079.719 I print_info: ssm_d_conv       = 0
0.00.079.719 I print_info: ssm_d_inner      = 0
0.00.079.719 I print_info: ssm_d_state      = 0
0.00.079.720 I print_info: ssm_dt_rank      = 0
0.00.079.720 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.720 I print_info: model type       = 1.4B
0.00.079.721 I print_info: model params     = 1.41 B
0.00.079.726 I print_info: general.name     = 1.4B
0.00.079.727 I print_info: vocab type       = BPE
0.00.079.727 I print_info: n_vocab          = 50304
0.00.079.727 I print_info: n_merges         = 50009
0.00.079.728 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.728 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.728 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.729 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.729 I print_info: LF token         = 128 'Ä'
0.00.079.729 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.730 I print_info: max token length = 1024
0.00.082.509 I load_tensors: offloading 24 repeating layers to GPU
0.00.082.509 I load_tensors: offloading output layer to GPU
0.00.082.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.082.521 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.082.523 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.082.937 I llama_init_from_model: n_seq_max     = 1
0.00.082.938 I llama_init_from_model: n_ctx         = 2048
0.00.082.938 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.082.938 I llama_init_from_model: n_batch       = 2048
0.00.082.939 I llama_init_from_model: n_ubatch      = 512
0.00.082.939 I llama_init_from_model: flash_attn    = 0
0.00.082.940 I llama_init_from_model: freq_base     = 10000.0
0.00.082.940 I llama_init_from_model: freq_scale    = 1
0.00.082.941 I ggml_metal_init: allocating
0.00.082.945 I ggml_metal_init: found device: Apple M4
0.00.082.947 I ggml_metal_init: picking default device: Apple M4
0.00.083.842 I ggml_metal_init: using embedded metal library
0.00.087.593 I ggml_metal_init: GPU name:   Apple M4
0.00.087.596 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.596 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.597 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.597 I ggml_metal_init: simdgroup reduction   = true
0.00.087.597 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.597 I ggml_metal_init: has bfloat            = true
0.00.087.598 I ggml_metal_init: use bfloat            = true
0.00.087.598 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.748 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.122.922 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.122.928 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.122.948 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.124.055 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.124.057 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.124.057 I llama_init_from_model: graph nodes  = 967
0.00.124.057 I llama_init_from_model: graph splits = 2
0.00.124.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.124.181 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.124.182 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.951.502 I main: llama threadpool init, n_threads = 4
0.00.951.566 I 
0.00.951.610 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.951.612 I 
0.00.951.963 I sampler seed: 1234
0.00.951.969 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.951.996 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.951.998 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.951.998 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.797.711 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.797.712 I llama_perf_context_print:        load time =     934.84 ms
0.01.797.712 I llama_perf_context_print: prompt eval time =      42.81 ms /     7 tokens (    6.12 ms per token,   163.50 tokens per second)
0.01.797.714 I llama_perf_context_print:        eval time =     799.89 ms /    63 runs   (   12.70 ms per token,    78.76 tokens per second)
0.01.797.714 I llama_perf_context_print:       total time =     846.22 ms /    70 tokens
0.01.797.934 I ggml_metal_free: deallocating

real	0m1.833s
user	0m0.137s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.213 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.633 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.646 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.646 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.647 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.647 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.649 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.650 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.650 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.653 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.653 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.491 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.343 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.343 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.344 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.344 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.344 I llama_model_loader: - type  f32:  194 tensors
0.00.025.345 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.345 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.346 I print_info: file format = GGUF V3 (latest)
0.00.025.346 I print_info: file type   = Q5_1
0.00.025.347 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.381 I load: special tokens cache size = 25
0.00.050.584 I load: token to piece cache size = 0.2984 MB
0.00.050.587 I print_info: arch             = gptneox
0.00.050.587 I print_info: vocab_only       = 0
0.00.050.587 I print_info: n_ctx_train      = 2048
0.00.050.588 I print_info: n_embd           = 2048
0.00.050.588 I print_info: n_layer          = 24
0.00.050.591 I print_info: n_head           = 16
0.00.050.592 I print_info: n_head_kv        = 16
0.00.050.595 I print_info: n_rot            = 32
0.00.050.595 I print_info: n_swa            = 0
0.00.050.595 I print_info: n_embd_head_k    = 128
0.00.050.595 I print_info: n_embd_head_v    = 128
0.00.050.596 I print_info: n_gqa            = 1
0.00.050.598 I print_info: n_embd_k_gqa     = 2048
0.00.050.599 I print_info: n_embd_v_gqa     = 2048
0.00.050.599 I print_info: f_norm_eps       = 1.0e-05
0.00.050.600 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.600 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.600 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.600 I print_info: f_logit_scale    = 0.0e+00
0.00.050.601 I print_info: n_ff             = 8192
0.00.050.623 I print_info: n_expert         = 0
0.00.050.625 I print_info: n_expert_used    = 0
0.00.050.626 I print_info: causal attn      = 1
0.00.050.626 I print_info: pooling type     = 0
0.00.050.626 I print_info: rope type        = 2
0.00.050.626 I print_info: rope scaling     = linear
0.00.050.627 I print_info: freq_base_train  = 10000.0
0.00.050.627 I print_info: freq_scale_train = 1
0.00.050.628 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.629 I print_info: rope_finetuned   = unknown
0.00.050.629 I print_info: ssm_d_conv       = 0
0.00.050.629 I print_info: ssm_d_inner      = 0
0.00.050.629 I print_info: ssm_d_state      = 0
0.00.050.629 I print_info: ssm_dt_rank      = 0
0.00.050.629 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.629 I print_info: model type       = 1.4B
0.00.050.630 I print_info: model params     = 1.41 B
0.00.050.630 I print_info: general.name     = 1.4B
0.00.050.631 I print_info: vocab type       = BPE
0.00.050.631 I print_info: n_vocab          = 50304
0.00.050.631 I print_info: n_merges         = 50009
0.00.050.631 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.631 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.631 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.632 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.632 I print_info: LF token         = 128 'Ä'
0.00.050.632 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.632 I print_info: max token length = 1024
0.00.052.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.622 I load_tensors: offloading output layer to GPU
0.00.052.622 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.633 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.634 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.979 I llama_init_from_model: n_seq_max     = 1
0.00.052.980 I llama_init_from_model: n_ctx         = 128
0.00.052.980 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.981 I llama_init_from_model: n_batch       = 128
0.00.052.981 I llama_init_from_model: n_ubatch      = 128
0.00.052.981 I llama_init_from_model: flash_attn    = 0
0.00.052.981 I llama_init_from_model: freq_base     = 10000.0
0.00.052.981 I llama_init_from_model: freq_scale    = 1
0.00.052.982 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.982 I ggml_metal_init: allocating
0.00.052.986 I ggml_metal_init: found device: Apple M4
0.00.052.988 I ggml_metal_init: picking default device: Apple M4
0.00.053.635 I ggml_metal_init: using embedded metal library
0.00.056.287 I ggml_metal_init: GPU name:   Apple M4
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.290 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.290 I ggml_metal_init: simdgroup reduction   = true
0.00.056.291 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.291 I ggml_metal_init: has bfloat            = true
0.00.056.291 I ggml_metal_init: use bfloat            = true
0.00.056.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.397 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.774 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.780 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.798 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.675 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.676 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.677 I llama_init_from_model: graph nodes  = 967
0.00.067.677 I llama_init_from_model: graph splits = 2
0.00.067.678 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.678 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.755.996 I 
0.00.756.033 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.048 I perplexity: tokenizing the input ..
0.00.763.778 I perplexity: tokenization took 7.729 ms
0.00.763.782 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.897.834 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.899.247 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.899.265 I llama_perf_context_print:        load time =     746.78 ms
0.00.899.266 I llama_perf_context_print: prompt eval time =     133.80 ms /   128 tokens (    1.05 ms per token,   956.67 tokens per second)
0.00.899.267 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.899.267 I llama_perf_context_print:       total time =     143.27 ms /   129 tokens
0.00.899.637 I ggml_metal_free: deallocating

real	0m0.917s
user	0m0.079s
sys	0m0.096s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.012.517 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.327 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.019.332 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.334 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.335 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.338 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.339 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.339 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.340 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.136 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.135 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.938 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.939 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.939 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.940 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.940 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.940 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.027.941 I llama_model_loader: - type  f32:  194 tensors
0.00.027.941 I llama_model_loader: - type q2_K:   49 tensors
0.00.027.941 I llama_model_loader: - type q3_K:   48 tensors
0.00.027.941 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.942 I print_info: file format = GGUF V3 (latest)
0.00.027.942 I print_info: file type   = Q2_K - Medium
0.00.027.943 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.046.779 I load: special tokens cache size = 25
0.00.052.677 I load: token to piece cache size = 0.2984 MB
0.00.052.680 I print_info: arch             = gptneox
0.00.052.681 I print_info: vocab_only       = 0
0.00.052.681 I print_info: n_ctx_train      = 2048
0.00.052.681 I print_info: n_embd           = 2048
0.00.052.681 I print_info: n_layer          = 24
0.00.052.684 I print_info: n_head           = 16
0.00.052.685 I print_info: n_head_kv        = 16
0.00.052.685 I print_info: n_rot            = 32
0.00.052.687 I print_info: n_swa            = 0
0.00.052.687 I print_info: n_embd_head_k    = 128
0.00.052.687 I print_info: n_embd_head_v    = 128
0.00.052.688 I print_info: n_gqa            = 1
0.00.052.689 I print_info: n_embd_k_gqa     = 2048
0.00.052.689 I print_info: n_embd_v_gqa     = 2048
0.00.052.690 I print_info: f_norm_eps       = 1.0e-05
0.00.052.690 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.690 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.691 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.691 I print_info: f_logit_scale    = 0.0e+00
0.00.052.691 I print_info: n_ff             = 8192
0.00.052.692 I print_info: n_expert         = 0
0.00.052.692 I print_info: n_expert_used    = 0
0.00.052.692 I print_info: causal attn      = 1
0.00.052.692 I print_info: pooling type     = 0
0.00.052.692 I print_info: rope type        = 2
0.00.052.693 I print_info: rope scaling     = linear
0.00.052.695 I print_info: freq_base_train  = 10000.0
0.00.052.695 I print_info: freq_scale_train = 1
0.00.052.695 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.695 I print_info: rope_finetuned   = unknown
0.00.052.696 I print_info: ssm_d_conv       = 0
0.00.052.696 I print_info: ssm_d_inner      = 0
0.00.052.696 I print_info: ssm_d_state      = 0
0.00.052.696 I print_info: ssm_dt_rank      = 0
0.00.052.696 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.698 I print_info: model type       = 1.4B
0.00.052.698 I print_info: model params     = 1.41 B
0.00.052.698 I print_info: general.name     = 1.4B
0.00.052.699 I print_info: vocab type       = BPE
0.00.052.699 I print_info: n_vocab          = 50304
0.00.052.699 I print_info: n_merges         = 50009
0.00.052.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.700 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.700 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.700 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.700 I print_info: LF token         = 128 'Ä'
0.00.052.701 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.701 I print_info: max token length = 1024
0.00.054.540 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.540 I load_tensors: offloading output layer to GPU
0.00.054.540 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.551 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.552 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.843 I llama_init_from_model: n_seq_max     = 1
0.00.054.844 I llama_init_from_model: n_ctx         = 2048
0.00.054.844 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.844 I llama_init_from_model: n_batch       = 2048
0.00.054.844 I llama_init_from_model: n_ubatch      = 512
0.00.054.845 I llama_init_from_model: flash_attn    = 0
0.00.054.845 I llama_init_from_model: freq_base     = 10000.0
0.00.054.845 I llama_init_from_model: freq_scale    = 1
0.00.054.846 I ggml_metal_init: allocating
0.00.054.849 I ggml_metal_init: found device: Apple M4
0.00.054.851 I ggml_metal_init: picking default device: Apple M4
0.00.055.435 I ggml_metal_init: using embedded metal library
0.00.057.778 I ggml_metal_init: GPU name:   Apple M4
0.00.057.779 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.780 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.781 I ggml_metal_init: simdgroup reduction   = true
0.00.057.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.781 I ggml_metal_init: has bfloat            = true
0.00.057.781 I ggml_metal_init: use bfloat            = true
0.00.057.781 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.580 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.678 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.683 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.705 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.089.875 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.089.876 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.089.877 I llama_init_from_model: graph nodes  = 967
0.00.089.877 I llama_init_from_model: graph splits = 2
0.00.089.880 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.032 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.033 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.208 I main: llama threadpool init, n_threads = 4
0.00.510.246 I 
0.00.510.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.272 I 
0.00.510.511 I sampler seed: 1234
0.00.510.516 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.558 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.560 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.560 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.189.913 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.01.189.913 I llama_perf_context_print:        load time =     497.69 ms
0.01.189.914 I llama_perf_context_print: prompt eval time =      35.92 ms /     7 tokens (    5.13 ms per token,   194.88 tokens per second)
0.01.189.915 I llama_perf_context_print:        eval time =     640.42 ms /    63 runs   (   10.17 ms per token,    98.37 tokens per second)
0.01.189.915 I llama_perf_context_print:       total time =     679.71 ms /    70 tokens
0.01.190.139 I ggml_metal_free: deallocating

real	0m1.212s
user	0m0.109s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.895 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.155 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.161 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.163 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.164 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.164 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.165 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.166 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.166 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.167 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.167 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.167 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.168 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.168 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.170 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.170 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.975 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.027 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.952 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.952 I llama_model_loader: - type  f32:  194 tensors
0.00.025.953 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.953 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.953 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.954 I print_info: file format = GGUF V3 (latest)
0.00.025.954 I print_info: file type   = Q2_K - Medium
0.00.025.956 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.825 I load: special tokens cache size = 25
0.00.052.210 I load: token to piece cache size = 0.2984 MB
0.00.052.216 I print_info: arch             = gptneox
0.00.052.217 I print_info: vocab_only       = 0
0.00.052.217 I print_info: n_ctx_train      = 2048
0.00.052.217 I print_info: n_embd           = 2048
0.00.052.223 I print_info: n_layer          = 24
0.00.052.227 I print_info: n_head           = 16
0.00.052.228 I print_info: n_head_kv        = 16
0.00.052.229 I print_info: n_rot            = 32
0.00.052.229 I print_info: n_swa            = 0
0.00.052.230 I print_info: n_embd_head_k    = 128
0.00.052.230 I print_info: n_embd_head_v    = 128
0.00.052.230 I print_info: n_gqa            = 1
0.00.052.231 I print_info: n_embd_k_gqa     = 2048
0.00.052.232 I print_info: n_embd_v_gqa     = 2048
0.00.052.233 I print_info: f_norm_eps       = 1.0e-05
0.00.052.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.234 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.234 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.234 I print_info: f_logit_scale    = 0.0e+00
0.00.052.235 I print_info: n_ff             = 8192
0.00.052.235 I print_info: n_expert         = 0
0.00.052.235 I print_info: n_expert_used    = 0
0.00.052.236 I print_info: causal attn      = 1
0.00.052.236 I print_info: pooling type     = 0
0.00.052.236 I print_info: rope type        = 2
0.00.052.236 I print_info: rope scaling     = linear
0.00.052.237 I print_info: freq_base_train  = 10000.0
0.00.052.238 I print_info: freq_scale_train = 1
0.00.052.238 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.238 I print_info: rope_finetuned   = unknown
0.00.052.238 I print_info: ssm_d_conv       = 0
0.00.052.238 I print_info: ssm_d_inner      = 0
0.00.052.238 I print_info: ssm_d_state      = 0
0.00.052.238 I print_info: ssm_dt_rank      = 0
0.00.052.238 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.239 I print_info: model type       = 1.4B
0.00.052.239 I print_info: model params     = 1.41 B
0.00.052.240 I print_info: general.name     = 1.4B
0.00.052.241 I print_info: vocab type       = BPE
0.00.052.241 I print_info: n_vocab          = 50304
0.00.052.241 I print_info: n_merges         = 50009
0.00.052.241 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.242 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.242 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.242 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.242 I print_info: LF token         = 128 'Ä'
0.00.052.242 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.243 I print_info: max token length = 1024
0.00.054.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.161 I load_tensors: offloading output layer to GPU
0.00.054.162 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.172 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.174 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.054.460 I llama_init_from_model: n_seq_max     = 1
0.00.054.461 I llama_init_from_model: n_ctx         = 128
0.00.054.461 I llama_init_from_model: n_ctx_per_seq = 128
0.00.054.461 I llama_init_from_model: n_batch       = 128
0.00.054.461 I llama_init_from_model: n_ubatch      = 128
0.00.054.462 I llama_init_from_model: flash_attn    = 0
0.00.054.462 I llama_init_from_model: freq_base     = 10000.0
0.00.054.462 I llama_init_from_model: freq_scale    = 1
0.00.054.463 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.463 I ggml_metal_init: allocating
0.00.054.466 I ggml_metal_init: found device: Apple M4
0.00.054.468 I ggml_metal_init: picking default device: Apple M4
0.00.055.073 I ggml_metal_init: using embedded metal library
0.00.057.446 I ggml_metal_init: GPU name:   Apple M4
0.00.057.447 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.448 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.449 I ggml_metal_init: simdgroup reduction   = true
0.00.057.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.449 I ggml_metal_init: has bfloat            = true
0.00.057.449 I ggml_metal_init: use bfloat            = true
0.00.057.450 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.451 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.695 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.050 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.933 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.934 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.934 I llama_init_from_model: graph nodes  = 967
0.00.069.935 I llama_init_from_model: graph splits = 2
0.00.069.936 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.936 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.456.246 I 
0.00.456.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.456.292 I perplexity: tokenizing the input ..
0.00.464.672 I perplexity: tokenization took 8.378 ms
0.00.464.678 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.597.172 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.598.336 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.598.363 I llama_perf_context_print:        load time =     446.34 ms
0.00.598.364 I llama_perf_context_print: prompt eval time =     132.27 ms /   128 tokens (    1.03 ms per token,   967.74 tokens per second)
0.00.598.365 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.598.365 I llama_perf_context_print:       total time =     142.12 ms /   129 tokens
0.00.598.942 I ggml_metal_free: deallocating

real	0m0.615s
user	0m0.080s
sys	0m0.070s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.697 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.100 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.029.105 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.107 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.107 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.111 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.112 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.112 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.115 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.116 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.116 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.117 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.117 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.117 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.121 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.864 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.878 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.798 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.799 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.037.799 I llama_model_loader: - type  f32:  194 tensors
0.00.037.800 I llama_model_loader: - type q3_K:   25 tensors
0.00.037.800 I llama_model_loader: - type q4_K:   71 tensors
0.00.037.800 I llama_model_loader: - type q5_K:    1 tensors
0.00.037.800 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.801 I print_info: file format = GGUF V3 (latest)
0.00.037.801 I print_info: file type   = Q3_K - Medium
0.00.037.802 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.060.260 I load: special tokens cache size = 25
0.00.066.948 I load: token to piece cache size = 0.2984 MB
0.00.066.950 I print_info: arch             = gptneox
0.00.066.951 I print_info: vocab_only       = 0
0.00.066.951 I print_info: n_ctx_train      = 2048
0.00.066.951 I print_info: n_embd           = 2048
0.00.066.951 I print_info: n_layer          = 24
0.00.066.954 I print_info: n_head           = 16
0.00.066.955 I print_info: n_head_kv        = 16
0.00.066.955 I print_info: n_rot            = 32
0.00.066.955 I print_info: n_swa            = 0
0.00.066.955 I print_info: n_embd_head_k    = 128
0.00.066.955 I print_info: n_embd_head_v    = 128
0.00.066.958 I print_info: n_gqa            = 1
0.00.066.958 I print_info: n_embd_k_gqa     = 2048
0.00.066.959 I print_info: n_embd_v_gqa     = 2048
0.00.066.960 I print_info: f_norm_eps       = 1.0e-05
0.00.066.960 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.066.960 I print_info: f_clamp_kqv      = 0.0e+00
0.00.066.960 I print_info: f_max_alibi_bias = 0.0e+00
0.00.066.960 I print_info: f_logit_scale    = 0.0e+00
0.00.066.961 I print_info: n_ff             = 8192
0.00.066.964 I print_info: n_expert         = 0
0.00.066.964 I print_info: n_expert_used    = 0
0.00.066.964 I print_info: causal attn      = 1
0.00.066.964 I print_info: pooling type     = 0
0.00.066.964 I print_info: rope type        = 2
0.00.066.965 I print_info: rope scaling     = linear
0.00.066.965 I print_info: freq_base_train  = 10000.0
0.00.066.965 I print_info: freq_scale_train = 1
0.00.066.966 I print_info: n_ctx_orig_yarn  = 2048
0.00.066.966 I print_info: rope_finetuned   = unknown
0.00.066.966 I print_info: ssm_d_conv       = 0
0.00.066.966 I print_info: ssm_d_inner      = 0
0.00.066.966 I print_info: ssm_d_state      = 0
0.00.066.966 I print_info: ssm_dt_rank      = 0
0.00.066.966 I print_info: ssm_dt_b_c_rms   = 0
0.00.066.967 I print_info: model type       = 1.4B
0.00.066.970 I print_info: model params     = 1.41 B
0.00.066.971 I print_info: general.name     = 1.4B
0.00.066.971 I print_info: vocab type       = BPE
0.00.066.971 I print_info: n_vocab          = 50304
0.00.066.971 I print_info: n_merges         = 50009
0.00.066.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.066.972 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.066.972 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.066.972 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.066.972 I print_info: LF token         = 128 'Ä'
0.00.066.973 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.066.973 I print_info: max token length = 1024
0.00.069.004 I load_tensors: offloading 24 repeating layers to GPU
0.00.069.004 I load_tensors: offloading output layer to GPU
0.00.069.004 I load_tensors: offloaded 25/25 layers to GPU
0.00.069.015 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.069.016 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.069.328 I llama_init_from_model: n_seq_max     = 1
0.00.069.329 I llama_init_from_model: n_ctx         = 2048
0.00.069.329 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.069.329 I llama_init_from_model: n_batch       = 2048
0.00.069.330 I llama_init_from_model: n_ubatch      = 512
0.00.069.330 I llama_init_from_model: flash_attn    = 0
0.00.069.330 I llama_init_from_model: freq_base     = 10000.0
0.00.069.330 I llama_init_from_model: freq_scale    = 1
0.00.069.331 I ggml_metal_init: allocating
0.00.069.334 I ggml_metal_init: found device: Apple M4
0.00.069.336 I ggml_metal_init: picking default device: Apple M4
0.00.069.977 I ggml_metal_init: using embedded metal library
0.00.072.641 I ggml_metal_init: GPU name:   Apple M4
0.00.072.643 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.643 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.644 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.644 I ggml_metal_init: simdgroup reduction   = true
0.00.072.644 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.644 I ggml_metal_init: has bfloat            = true
0.00.072.644 I ggml_metal_init: use bfloat            = true
0.00.072.645 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.645 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.404 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.103.350 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.103.356 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.103.374 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.104.403 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.104.405 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.104.405 I llama_init_from_model: graph nodes  = 967
0.00.104.405 I llama_init_from_model: graph splits = 2
0.00.104.408 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.350 I main: llama threadpool init, n_threads = 4
0.00.600.387 I 
0.00.600.409 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.409 I 
0.00.600.642 I sampler seed: 1234
0.00.600.646 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.600.676 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.600.679 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.600.679 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.342.345 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.342.345 I llama_perf_context_print:        load time =     591.65 ms
0.01.342.346 I llama_perf_context_print: prompt eval time =      40.39 ms /     7 tokens (    5.77 ms per token,   173.31 tokens per second)
0.01.342.347 I llama_perf_context_print:        eval time =     698.27 ms /    63 runs   (   11.08 ms per token,    90.22 tokens per second)
0.01.342.347 I llama_perf_context_print:       total time =     742.00 ms /    70 tokens
0.01.342.601 I ggml_metal_free: deallocating

real	0m1.359s
user	0m0.115s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.789 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.940 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.946 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.947 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.948 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.949 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.949 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.950 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.950 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.950 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.951 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.951 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.953 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.953 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.953 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.632 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.606 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.272 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.273 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.273 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.274 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.275 I llama_model_loader: - type  f32:  194 tensors
0.00.024.275 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.276 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.276 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.277 I print_info: file format = GGUF V3 (latest)
0.00.024.277 I print_info: file type   = Q3_K - Medium
0.00.024.278 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.100 I load: special tokens cache size = 25
0.00.048.860 I load: token to piece cache size = 0.2984 MB
0.00.048.863 I print_info: arch             = gptneox
0.00.048.863 I print_info: vocab_only       = 0
0.00.048.863 I print_info: n_ctx_train      = 2048
0.00.048.863 I print_info: n_embd           = 2048
0.00.048.863 I print_info: n_layer          = 24
0.00.048.866 I print_info: n_head           = 16
0.00.048.867 I print_info: n_head_kv        = 16
0.00.048.867 I print_info: n_rot            = 32
0.00.048.867 I print_info: n_swa            = 0
0.00.048.867 I print_info: n_embd_head_k    = 128
0.00.048.870 I print_info: n_embd_head_v    = 128
0.00.048.871 I print_info: n_gqa            = 1
0.00.048.872 I print_info: n_embd_k_gqa     = 2048
0.00.048.872 I print_info: n_embd_v_gqa     = 2048
0.00.048.873 I print_info: f_norm_eps       = 1.0e-05
0.00.048.873 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.873 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.874 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.874 I print_info: f_logit_scale    = 0.0e+00
0.00.048.875 I print_info: n_ff             = 8192
0.00.048.875 I print_info: n_expert         = 0
0.00.048.875 I print_info: n_expert_used    = 0
0.00.048.875 I print_info: causal attn      = 1
0.00.048.875 I print_info: pooling type     = 0
0.00.048.875 I print_info: rope type        = 2
0.00.048.880 I print_info: rope scaling     = linear
0.00.048.881 I print_info: freq_base_train  = 10000.0
0.00.048.882 I print_info: freq_scale_train = 1
0.00.048.883 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.883 I print_info: rope_finetuned   = unknown
0.00.048.883 I print_info: ssm_d_conv       = 0
0.00.048.883 I print_info: ssm_d_inner      = 0
0.00.048.883 I print_info: ssm_d_state      = 0
0.00.048.883 I print_info: ssm_dt_rank      = 0
0.00.048.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.884 I print_info: model type       = 1.4B
0.00.048.884 I print_info: model params     = 1.41 B
0.00.048.884 I print_info: general.name     = 1.4B
0.00.048.885 I print_info: vocab type       = BPE
0.00.048.885 I print_info: n_vocab          = 50304
0.00.048.885 I print_info: n_merges         = 50009
0.00.048.885 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.885 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.886 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.886 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.886 I print_info: LF token         = 128 'Ä'
0.00.048.886 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.886 I print_info: max token length = 1024
0.00.050.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.821 I load_tensors: offloading output layer to GPU
0.00.050.822 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.832 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.833 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.051.137 I llama_init_from_model: n_seq_max     = 1
0.00.051.138 I llama_init_from_model: n_ctx         = 128
0.00.051.138 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.138 I llama_init_from_model: n_batch       = 128
0.00.051.138 I llama_init_from_model: n_ubatch      = 128
0.00.051.138 I llama_init_from_model: flash_attn    = 0
0.00.051.139 I llama_init_from_model: freq_base     = 10000.0
0.00.051.139 I llama_init_from_model: freq_scale    = 1
0.00.051.139 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.140 I ggml_metal_init: allocating
0.00.051.143 I ggml_metal_init: found device: Apple M4
0.00.051.145 I ggml_metal_init: picking default device: Apple M4
0.00.051.698 I ggml_metal_init: using embedded metal library
0.00.054.072 I ggml_metal_init: GPU name:   Apple M4
0.00.054.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.074 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.074 I ggml_metal_init: simdgroup reduction   = true
0.00.054.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.074 I ggml_metal_init: has bfloat            = true
0.00.054.074 I ggml_metal_init: use bfloat            = true
0.00.054.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.718 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.076 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.081 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.098 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.969 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.970 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.970 I llama_init_from_model: graph nodes  = 967
0.00.065.970 I llama_init_from_model: graph splits = 2
0.00.065.971 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.972 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.478.254 I 
0.00.478.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.478.302 I perplexity: tokenizing the input ..
0.00.485.988 I perplexity: tokenization took 7.684 ms
0.00.485.992 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.618.143 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.619.296 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.619.337 I llama_perf_context_print:        load time =     469.46 ms
0.00.619.338 I llama_perf_context_print: prompt eval time =     131.90 ms /   128 tokens (    1.03 ms per token,   970.43 tokens per second)
0.00.619.339 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.339 I llama_perf_context_print:       total time =     141.09 ms /   129 tokens
0.00.619.887 I ggml_metal_free: deallocating

real	0m0.634s
user	0m0.077s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.982 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.650 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.655 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.657 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.658 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.658 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.662 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.664 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.664 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.664 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.665 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.665 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.668 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.668 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.552 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.556 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.376 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.377 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.378 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.378 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.378 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.379 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.379 I llama_model_loader: - type  f32:  194 tensors
0.00.027.379 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.380 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.380 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.381 I print_info: file format = GGUF V3 (latest)
0.00.027.381 I print_info: file type   = Q4_K - Medium
0.00.027.382 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.047.089 I load: special tokens cache size = 25
0.00.053.283 I load: token to piece cache size = 0.2984 MB
0.00.053.286 I print_info: arch             = gptneox
0.00.053.286 I print_info: vocab_only       = 0
0.00.053.286 I print_info: n_ctx_train      = 2048
0.00.053.286 I print_info: n_embd           = 2048
0.00.053.287 I print_info: n_layer          = 24
0.00.053.289 I print_info: n_head           = 16
0.00.053.290 I print_info: n_head_kv        = 16
0.00.053.290 I print_info: n_rot            = 32
0.00.053.290 I print_info: n_swa            = 0
0.00.053.291 I print_info: n_embd_head_k    = 128
0.00.053.291 I print_info: n_embd_head_v    = 128
0.00.053.292 I print_info: n_gqa            = 1
0.00.053.292 I print_info: n_embd_k_gqa     = 2048
0.00.053.293 I print_info: n_embd_v_gqa     = 2048
0.00.053.294 I print_info: f_norm_eps       = 1.0e-05
0.00.053.294 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.297 I print_info: f_logit_scale    = 0.0e+00
0.00.053.297 I print_info: n_ff             = 8192
0.00.053.298 I print_info: n_expert         = 0
0.00.053.300 I print_info: n_expert_used    = 0
0.00.053.302 I print_info: causal attn      = 1
0.00.053.302 I print_info: pooling type     = 0
0.00.053.302 I print_info: rope type        = 2
0.00.053.302 I print_info: rope scaling     = linear
0.00.053.303 I print_info: freq_base_train  = 10000.0
0.00.053.303 I print_info: freq_scale_train = 1
0.00.053.303 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.303 I print_info: rope_finetuned   = unknown
0.00.053.304 I print_info: ssm_d_conv       = 0
0.00.053.304 I print_info: ssm_d_inner      = 0
0.00.053.304 I print_info: ssm_d_state      = 0
0.00.053.304 I print_info: ssm_dt_rank      = 0
0.00.053.304 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.305 I print_info: model type       = 1.4B
0.00.053.305 I print_info: model params     = 1.41 B
0.00.053.306 I print_info: general.name     = 1.4B
0.00.053.306 I print_info: vocab type       = BPE
0.00.053.306 I print_info: n_vocab          = 50304
0.00.053.306 I print_info: n_merges         = 50009
0.00.053.307 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.307 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.307 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.307 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.307 I print_info: LF token         = 128 'Ä'
0.00.053.308 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.308 I print_info: max token length = 1024
0.00.055.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.055.312 I load_tensors: offloading output layer to GPU
0.00.055.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.055.323 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.324 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.616 I llama_init_from_model: n_seq_max     = 1
0.00.055.617 I llama_init_from_model: n_ctx         = 2048
0.00.055.617 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.617 I llama_init_from_model: n_batch       = 2048
0.00.055.617 I llama_init_from_model: n_ubatch      = 512
0.00.055.617 I llama_init_from_model: flash_attn    = 0
0.00.055.618 I llama_init_from_model: freq_base     = 10000.0
0.00.055.618 I llama_init_from_model: freq_scale    = 1
0.00.055.618 I ggml_metal_init: allocating
0.00.055.622 I ggml_metal_init: found device: Apple M4
0.00.055.624 I ggml_metal_init: picking default device: Apple M4
0.00.056.231 I ggml_metal_init: using embedded metal library
0.00.058.622 I ggml_metal_init: GPU name:   Apple M4
0.00.058.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.625 I ggml_metal_init: simdgroup reduction   = true
0.00.058.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.625 I ggml_metal_init: has bfloat            = true
0.00.058.625 I ggml_metal_init: use bfloat            = true
0.00.058.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.596 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.605 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.613 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.634 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.571 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.572 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.573 I llama_init_from_model: graph nodes  = 967
0.00.088.573 I llama_init_from_model: graph splits = 2
0.00.088.576 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.697 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.035 I main: llama threadpool init, n_threads = 4
0.00.622.085 I 
0.00.622.106 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.106 I 
0.00.622.334 I sampler seed: 1234
0.00.622.339 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.622.380 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.622.380 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.622.380 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.377.447 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.377.447 I llama_perf_context_print:        load time =     611.05 ms
0.01.377.448 I llama_perf_context_print: prompt eval time =      47.07 ms /     7 tokens (    6.72 ms per token,   148.71 tokens per second)
0.01.377.449 I llama_perf_context_print:        eval time =     704.93 ms /    63 runs   (   11.19 ms per token,    89.37 tokens per second)
0.01.377.449 I llama_perf_context_print:       total time =     755.41 ms /    70 tokens
0.01.377.644 I ggml_metal_free: deallocating

real	0m1.395s
user	0m0.110s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.909 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.982 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.989 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.992 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.993 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.997 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.997 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.802 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.850 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.657 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.657 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.658 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.658 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.659 I llama_model_loader: - type  f32:  194 tensors
0.00.025.659 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.659 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.659 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.660 I print_info: file format = GGUF V3 (latest)
0.00.025.660 I print_info: file type   = Q4_K - Medium
0.00.025.663 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.642 I load: special tokens cache size = 25
0.00.050.647 I load: token to piece cache size = 0.2984 MB
0.00.050.650 I print_info: arch             = gptneox
0.00.050.650 I print_info: vocab_only       = 0
0.00.050.650 I print_info: n_ctx_train      = 2048
0.00.050.651 I print_info: n_embd           = 2048
0.00.050.651 I print_info: n_layer          = 24
0.00.050.654 I print_info: n_head           = 16
0.00.050.655 I print_info: n_head_kv        = 16
0.00.050.655 I print_info: n_rot            = 32
0.00.050.655 I print_info: n_swa            = 0
0.00.050.655 I print_info: n_embd_head_k    = 128
0.00.050.658 I print_info: n_embd_head_v    = 128
0.00.050.659 I print_info: n_gqa            = 1
0.00.050.660 I print_info: n_embd_k_gqa     = 2048
0.00.050.660 I print_info: n_embd_v_gqa     = 2048
0.00.050.661 I print_info: f_norm_eps       = 1.0e-05
0.00.050.662 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.662 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.662 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.662 I print_info: f_logit_scale    = 0.0e+00
0.00.050.663 I print_info: n_ff             = 8192
0.00.050.663 I print_info: n_expert         = 0
0.00.050.663 I print_info: n_expert_used    = 0
0.00.050.663 I print_info: causal attn      = 1
0.00.050.664 I print_info: pooling type     = 0
0.00.050.665 I print_info: rope type        = 2
0.00.050.665 I print_info: rope scaling     = linear
0.00.050.666 I print_info: freq_base_train  = 10000.0
0.00.050.666 I print_info: freq_scale_train = 1
0.00.050.666 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.666 I print_info: rope_finetuned   = unknown
0.00.050.666 I print_info: ssm_d_conv       = 0
0.00.050.667 I print_info: ssm_d_inner      = 0
0.00.050.667 I print_info: ssm_d_state      = 0
0.00.050.667 I print_info: ssm_dt_rank      = 0
0.00.050.667 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.667 I print_info: model type       = 1.4B
0.00.050.668 I print_info: model params     = 1.41 B
0.00.050.668 I print_info: general.name     = 1.4B
0.00.050.672 I print_info: vocab type       = BPE
0.00.050.672 I print_info: n_vocab          = 50304
0.00.050.672 I print_info: n_merges         = 50009
0.00.050.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.673 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.673 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.673 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.673 I print_info: LF token         = 128 'Ä'
0.00.050.673 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.674 I print_info: max token length = 1024
0.00.052.643 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.643 I load_tensors: offloading output layer to GPU
0.00.052.643 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.654 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.655 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.950 I llama_init_from_model: n_seq_max     = 1
0.00.052.950 I llama_init_from_model: n_ctx         = 128
0.00.052.950 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.951 I llama_init_from_model: n_batch       = 128
0.00.052.951 I llama_init_from_model: n_ubatch      = 128
0.00.052.951 I llama_init_from_model: flash_attn    = 0
0.00.052.951 I llama_init_from_model: freq_base     = 10000.0
0.00.052.951 I llama_init_from_model: freq_scale    = 1
0.00.052.952 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.952 I ggml_metal_init: allocating
0.00.052.955 I ggml_metal_init: found device: Apple M4
0.00.052.957 I ggml_metal_init: picking default device: Apple M4
0.00.053.537 I ggml_metal_init: using embedded metal library
0.00.055.886 I ggml_metal_init: GPU name:   Apple M4
0.00.055.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.888 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.888 I ggml_metal_init: simdgroup reduction   = true
0.00.055.888 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.889 I ggml_metal_init: has bfloat            = true
0.00.055.889 I ggml_metal_init: use bfloat            = true
0.00.055.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.523 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.827 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.829 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.844 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.695 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.696 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.697 I llama_init_from_model: graph nodes  = 967
0.00.067.697 I llama_init_from_model: graph splits = 2
0.00.067.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.667 I 
0.00.544.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.714 I perplexity: tokenizing the input ..
0.00.552.448 I perplexity: tokenization took 7.733 ms
0.00.552.451 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.686.692 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.687.878 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.687.908 I llama_perf_context_print:        load time =     534.75 ms
0.00.687.909 I llama_perf_context_print: prompt eval time =     134.02 ms /   128 tokens (    1.05 ms per token,   955.11 tokens per second)
0.00.687.910 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.687.910 I llama_perf_context_print:       total time =     143.25 ms /   129 tokens
0.00.688.426 I ggml_metal_free: deallocating

real	0m0.703s
user	0m0.078s
sys	0m0.092s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.153 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.702 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.709 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.711 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.716 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.717 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.717 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.572 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.621 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.396 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.398 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.399 I llama_model_loader: - type  f32:  194 tensors
0.00.026.399 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.399 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.400 I print_info: file format = GGUF V3 (latest)
0.00.026.401 I print_info: file type   = Q5_K - Medium
0.00.026.401 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.046.207 I load: special tokens cache size = 25
0.00.052.144 I load: token to piece cache size = 0.2984 MB
0.00.052.147 I print_info: arch             = gptneox
0.00.052.147 I print_info: vocab_only       = 0
0.00.052.148 I print_info: n_ctx_train      = 2048
0.00.052.148 I print_info: n_embd           = 2048
0.00.052.148 I print_info: n_layer          = 24
0.00.052.151 I print_info: n_head           = 16
0.00.052.151 I print_info: n_head_kv        = 16
0.00.052.152 I print_info: n_rot            = 32
0.00.052.152 I print_info: n_swa            = 0
0.00.052.152 I print_info: n_embd_head_k    = 128
0.00.052.152 I print_info: n_embd_head_v    = 128
0.00.052.153 I print_info: n_gqa            = 1
0.00.052.154 I print_info: n_embd_k_gqa     = 2048
0.00.052.154 I print_info: n_embd_v_gqa     = 2048
0.00.052.155 I print_info: f_norm_eps       = 1.0e-05
0.00.052.157 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.158 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.158 I print_info: f_logit_scale    = 0.0e+00
0.00.052.159 I print_info: n_ff             = 8192
0.00.052.159 I print_info: n_expert         = 0
0.00.052.159 I print_info: n_expert_used    = 0
0.00.052.159 I print_info: causal attn      = 1
0.00.052.159 I print_info: pooling type     = 0
0.00.052.160 I print_info: rope type        = 2
0.00.052.160 I print_info: rope scaling     = linear
0.00.052.162 I print_info: freq_base_train  = 10000.0
0.00.052.162 I print_info: freq_scale_train = 1
0.00.052.162 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.162 I print_info: rope_finetuned   = unknown
0.00.052.162 I print_info: ssm_d_conv       = 0
0.00.052.163 I print_info: ssm_d_inner      = 0
0.00.052.163 I print_info: ssm_d_state      = 0
0.00.052.163 I print_info: ssm_dt_rank      = 0
0.00.052.163 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.163 I print_info: model type       = 1.4B
0.00.052.164 I print_info: model params     = 1.41 B
0.00.052.164 I print_info: general.name     = 1.4B
0.00.052.164 I print_info: vocab type       = BPE
0.00.052.164 I print_info: n_vocab          = 50304
0.00.052.169 I print_info: n_merges         = 50009
0.00.052.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.169 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.170 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.170 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.170 I print_info: LF token         = 128 'Ä'
0.00.052.171 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.171 I print_info: max token length = 1024
0.00.053.861 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.861 I load_tensors: offloading output layer to GPU
0.00.053.861 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.872 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.873 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.054.163 I llama_init_from_model: n_seq_max     = 1
0.00.054.164 I llama_init_from_model: n_ctx         = 2048
0.00.054.164 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.165 I llama_init_from_model: n_batch       = 2048
0.00.054.165 I llama_init_from_model: n_ubatch      = 512
0.00.054.165 I llama_init_from_model: flash_attn    = 0
0.00.054.165 I llama_init_from_model: freq_base     = 10000.0
0.00.054.165 I llama_init_from_model: freq_scale    = 1
0.00.054.166 I ggml_metal_init: allocating
0.00.054.169 I ggml_metal_init: found device: Apple M4
0.00.054.171 I ggml_metal_init: picking default device: Apple M4
0.00.054.794 I ggml_metal_init: using embedded metal library
0.00.057.182 I ggml_metal_init: GPU name:   Apple M4
0.00.057.183 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.183 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.184 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.184 I ggml_metal_init: simdgroup reduction   = true
0.00.057.184 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.185 I ggml_metal_init: has bfloat            = true
0.00.057.185 I ggml_metal_init: use bfloat            = true
0.00.057.185 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.186 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.221 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.065 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.070 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.089 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.157 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.158 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.159 I llama_init_from_model: graph nodes  = 967
0.00.088.159 I llama_init_from_model: graph splits = 2
0.00.088.162 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.280 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.522 I main: llama threadpool init, n_threads = 4
0.00.692.564 I 
0.00.692.597 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.597 I 
0.00.692.758 I sampler seed: 1234
0.00.692.764 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.692.774 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.692.774 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.692.774 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.574.264 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.574.265 I llama_perf_context_print:        load time =     683.36 ms
0.01.574.266 I llama_perf_context_print: prompt eval time =      51.67 ms /     7 tokens (    7.38 ms per token,   135.46 tokens per second)
0.01.574.267 I llama_perf_context_print:        eval time =     826.70 ms /    63 runs   (   13.12 ms per token,    76.21 tokens per second)
0.01.574.268 I llama_perf_context_print:       total time =     881.74 ms /    70 tokens
0.01.574.502 I ggml_metal_free: deallocating

real	0m1.591s
user	0m0.111s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.755 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.716 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.725 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.726 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.727 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.727 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.728 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.728 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.728 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.729 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.729 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.731 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.731 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.731 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.289 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.290 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.292 I llama_model_loader: - type  f32:  194 tensors
0.00.024.292 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.292 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.293 I print_info: file format = GGUF V3 (latest)
0.00.024.293 I print_info: file type   = Q5_K - Medium
0.00.024.294 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.031 I load: special tokens cache size = 25
0.00.049.007 I load: token to piece cache size = 0.2984 MB
0.00.049.010 I print_info: arch             = gptneox
0.00.049.010 I print_info: vocab_only       = 0
0.00.049.010 I print_info: n_ctx_train      = 2048
0.00.049.011 I print_info: n_embd           = 2048
0.00.049.011 I print_info: n_layer          = 24
0.00.049.014 I print_info: n_head           = 16
0.00.049.014 I print_info: n_head_kv        = 16
0.00.049.015 I print_info: n_rot            = 32
0.00.049.015 I print_info: n_swa            = 0
0.00.049.015 I print_info: n_embd_head_k    = 128
0.00.049.015 I print_info: n_embd_head_v    = 128
0.00.049.016 I print_info: n_gqa            = 1
0.00.049.017 I print_info: n_embd_k_gqa     = 2048
0.00.049.017 I print_info: n_embd_v_gqa     = 2048
0.00.049.018 I print_info: f_norm_eps       = 1.0e-05
0.00.049.021 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.021 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.021 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.022 I print_info: f_logit_scale    = 0.0e+00
0.00.049.022 I print_info: n_ff             = 8192
0.00.049.023 I print_info: n_expert         = 0
0.00.049.023 I print_info: n_expert_used    = 0
0.00.049.023 I print_info: causal attn      = 1
0.00.049.023 I print_info: pooling type     = 0
0.00.049.023 I print_info: rope type        = 2
0.00.049.024 I print_info: rope scaling     = linear
0.00.049.024 I print_info: freq_base_train  = 10000.0
0.00.049.024 I print_info: freq_scale_train = 1
0.00.049.025 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.025 I print_info: rope_finetuned   = unknown
0.00.049.025 I print_info: ssm_d_conv       = 0
0.00.049.025 I print_info: ssm_d_inner      = 0
0.00.049.027 I print_info: ssm_d_state      = 0
0.00.049.027 I print_info: ssm_dt_rank      = 0
0.00.049.027 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.028 I print_info: model type       = 1.4B
0.00.049.028 I print_info: model params     = 1.41 B
0.00.049.028 I print_info: general.name     = 1.4B
0.00.049.029 I print_info: vocab type       = BPE
0.00.049.029 I print_info: n_vocab          = 50304
0.00.049.029 I print_info: n_merges         = 50009
0.00.049.029 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.030 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.030 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.030 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.030 I print_info: LF token         = 128 'Ä'
0.00.049.031 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.031 I print_info: max token length = 1024
0.00.051.069 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.069 I load_tensors: offloading output layer to GPU
0.00.051.069 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.079 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.081 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.051.356 I llama_init_from_model: n_seq_max     = 1
0.00.051.357 I llama_init_from_model: n_ctx         = 128
0.00.051.357 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.357 I llama_init_from_model: n_batch       = 128
0.00.051.357 I llama_init_from_model: n_ubatch      = 128
0.00.051.358 I llama_init_from_model: flash_attn    = 0
0.00.051.358 I llama_init_from_model: freq_base     = 10000.0
0.00.051.358 I llama_init_from_model: freq_scale    = 1
0.00.051.359 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.359 I ggml_metal_init: allocating
0.00.051.362 I ggml_metal_init: found device: Apple M4
0.00.051.364 I ggml_metal_init: picking default device: Apple M4
0.00.051.928 I ggml_metal_init: using embedded metal library
0.00.054.255 I ggml_metal_init: GPU name:   Apple M4
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.256 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.257 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.257 I ggml_metal_init: simdgroup reduction   = true
0.00.054.257 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.257 I ggml_metal_init: has bfloat            = true
0.00.054.257 I ggml_metal_init: use bfloat            = true
0.00.054.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.258 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.865 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.098 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.100 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.114 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.982 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.984 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.984 I llama_init_from_model: graph nodes  = 967
0.00.065.984 I llama_init_from_model: graph splits = 2
0.00.065.986 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.986 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.544 I 
0.00.630.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.630.610 I perplexity: tokenizing the input ..
0.00.638.812 I perplexity: tokenization took 8.201 ms
0.00.638.821 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.779.430 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.780.609 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.780.627 I llama_perf_context_print:        load time =     621.78 ms
0.00.780.628 I llama_perf_context_print: prompt eval time =     140.38 ms /   128 tokens (    1.10 ms per token,   911.80 tokens per second)
0.00.780.629 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.780.629 I llama_perf_context_print:       total time =     150.09 ms /   129 tokens
0.00.780.892 I ggml_metal_free: deallocating

real	0m0.795s
user	0m0.077s
sys	0m0.115s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.010.157 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.146 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.019.151 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.152 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.157 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.157 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.161 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.165 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.165 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.166 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.169 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.171 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.921 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.633 I llama_model_loader: - type  f32:  194 tensors
0.00.027.633 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.634 I print_info: file format = GGUF V3 (latest)
0.00.027.634 I print_info: file type   = Q6_K
0.00.027.635 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.046.580 I load: special tokens cache size = 25
0.00.052.465 I load: token to piece cache size = 0.2984 MB
0.00.052.468 I print_info: arch             = gptneox
0.00.052.468 I print_info: vocab_only       = 0
0.00.052.468 I print_info: n_ctx_train      = 2048
0.00.052.468 I print_info: n_embd           = 2048
0.00.052.469 I print_info: n_layer          = 24
0.00.052.472 I print_info: n_head           = 16
0.00.052.473 I print_info: n_head_kv        = 16
0.00.052.473 I print_info: n_rot            = 32
0.00.052.473 I print_info: n_swa            = 0
0.00.052.473 I print_info: n_embd_head_k    = 128
0.00.052.473 I print_info: n_embd_head_v    = 128
0.00.052.476 I print_info: n_gqa            = 1
0.00.052.477 I print_info: n_embd_k_gqa     = 2048
0.00.052.477 I print_info: n_embd_v_gqa     = 2048
0.00.052.478 I print_info: f_norm_eps       = 1.0e-05
0.00.052.478 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.478 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.478 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.479 I print_info: f_logit_scale    = 0.0e+00
0.00.052.479 I print_info: n_ff             = 8192
0.00.052.480 I print_info: n_expert         = 0
0.00.052.480 I print_info: n_expert_used    = 0
0.00.052.480 I print_info: causal attn      = 1
0.00.052.481 I print_info: pooling type     = 0
0.00.052.482 I print_info: rope type        = 2
0.00.052.482 I print_info: rope scaling     = linear
0.00.052.482 I print_info: freq_base_train  = 10000.0
0.00.052.483 I print_info: freq_scale_train = 1
0.00.052.483 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.483 I print_info: rope_finetuned   = unknown
0.00.052.483 I print_info: ssm_d_conv       = 0
0.00.052.483 I print_info: ssm_d_inner      = 0
0.00.052.484 I print_info: ssm_d_state      = 0
0.00.052.484 I print_info: ssm_dt_rank      = 0
0.00.052.484 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.484 I print_info: model type       = 1.4B
0.00.052.485 I print_info: model params     = 1.41 B
0.00.052.485 I print_info: general.name     = 1.4B
0.00.052.487 I print_info: vocab type       = BPE
0.00.052.487 I print_info: n_vocab          = 50304
0.00.052.487 I print_info: n_merges         = 50009
0.00.052.487 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.487 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.488 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.488 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.488 I print_info: LF token         = 128 'Ä'
0.00.052.488 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.489 I print_info: max token length = 1024
0.00.054.135 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.135 I load_tensors: offloading output layer to GPU
0.00.054.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.146 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.147 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.054.440 I llama_init_from_model: n_seq_max     = 1
0.00.054.441 I llama_init_from_model: n_ctx         = 2048
0.00.054.441 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.054.441 I llama_init_from_model: n_batch       = 2048
0.00.054.441 I llama_init_from_model: n_ubatch      = 512
0.00.054.441 I llama_init_from_model: flash_attn    = 0
0.00.054.442 I llama_init_from_model: freq_base     = 10000.0
0.00.054.442 I llama_init_from_model: freq_scale    = 1
0.00.054.443 I ggml_metal_init: allocating
0.00.054.446 I ggml_metal_init: found device: Apple M4
0.00.054.448 I ggml_metal_init: picking default device: Apple M4
0.00.055.045 I ggml_metal_init: using embedded metal library
0.00.057.397 I ggml_metal_init: GPU name:   Apple M4
0.00.057.399 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.399 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.400 I ggml_metal_init: simdgroup reduction   = true
0.00.057.400 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.400 I ggml_metal_init: has bfloat            = true
0.00.057.400 I ggml_metal_init: use bfloat            = true
0.00.057.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.401 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.306 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.992 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.999 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.019 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.110 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.112 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.112 I llama_init_from_model: graph nodes  = 967
0.00.088.113 I llama_init_from_model: graph splits = 2
0.00.088.115 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.250 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.250 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.413 I main: llama threadpool init, n_threads = 4
0.00.760.456 I 
0.00.760.483 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.483 I 
0.00.760.659 I sampler seed: 1234
0.00.760.665 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.698 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.702 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.684.378 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.01.684.378 I llama_perf_context_print:        load time =     750.25 ms
0.01.684.379 I llama_perf_context_print: prompt eval time =      54.54 ms /     7 tokens (    7.79 ms per token,   128.35 tokens per second)
0.01.684.380 I llama_perf_context_print:        eval time =     866.23 ms /    63 runs   (   13.75 ms per token,    72.73 tokens per second)
0.01.684.383 I llama_perf_context_print:       total time =     923.97 ms /    70 tokens
0.01.684.616 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.110s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4469 (7426a26b) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.892 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.755 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.768 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.768 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.618 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.626 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.481 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.482 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.483 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.484 I llama_model_loader: - type  f32:  194 tensors
0.00.025.484 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.485 I print_info: file format = GGUF V3 (latest)
0.00.025.485 I print_info: file type   = Q6_K
0.00.025.486 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.940 I load: special tokens cache size = 25
0.00.050.942 I load: token to piece cache size = 0.2984 MB
0.00.050.945 I print_info: arch             = gptneox
0.00.050.945 I print_info: vocab_only       = 0
0.00.050.945 I print_info: n_ctx_train      = 2048
0.00.050.945 I print_info: n_embd           = 2048
0.00.050.946 I print_info: n_layer          = 24
0.00.050.949 I print_info: n_head           = 16
0.00.050.950 I print_info: n_head_kv        = 16
0.00.050.950 I print_info: n_rot            = 32
0.00.050.951 I print_info: n_swa            = 0
0.00.050.952 I print_info: n_embd_head_k    = 128
0.00.050.952 I print_info: n_embd_head_v    = 128
0.00.050.954 I print_info: n_gqa            = 1
0.00.050.955 I print_info: n_embd_k_gqa     = 2048
0.00.050.955 I print_info: n_embd_v_gqa     = 2048
0.00.050.956 I print_info: f_norm_eps       = 1.0e-05
0.00.050.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.958 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.958 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.958 I print_info: f_logit_scale    = 0.0e+00
0.00.050.959 I print_info: n_ff             = 8192
0.00.050.959 I print_info: n_expert         = 0
0.00.050.959 I print_info: n_expert_used    = 0
0.00.050.959 I print_info: causal attn      = 1
0.00.050.960 I print_info: pooling type     = 0
0.00.050.960 I print_info: rope type        = 2
0.00.050.960 I print_info: rope scaling     = linear
0.00.050.960 I print_info: freq_base_train  = 10000.0
0.00.050.961 I print_info: freq_scale_train = 1
0.00.050.961 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.961 I print_info: rope_finetuned   = unknown
0.00.050.961 I print_info: ssm_d_conv       = 0
0.00.050.961 I print_info: ssm_d_inner      = 0
0.00.050.961 I print_info: ssm_d_state      = 0
0.00.050.962 I print_info: ssm_dt_rank      = 0
0.00.050.962 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.962 I print_info: model type       = 1.4B
0.00.050.962 I print_info: model params     = 1.41 B
0.00.050.962 I print_info: general.name     = 1.4B
0.00.050.969 I print_info: vocab type       = BPE
0.00.050.969 I print_info: n_vocab          = 50304
0.00.050.970 I print_info: n_merges         = 50009
0.00.050.970 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.970 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.970 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.970 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.971 I print_info: LF token         = 128 'Ä'
0.00.050.971 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.971 I print_info: max token length = 1024
0.00.052.965 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.965 I load_tensors: offloading output layer to GPU
0.00.052.965 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.975 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.977 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.053.274 I llama_init_from_model: n_seq_max     = 1
0.00.053.275 I llama_init_from_model: n_ctx         = 128
0.00.053.275 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.275 I llama_init_from_model: n_batch       = 128
0.00.053.276 I llama_init_from_model: n_ubatch      = 128
0.00.053.276 I llama_init_from_model: flash_attn    = 0
0.00.053.276 I llama_init_from_model: freq_base     = 10000.0
0.00.053.276 I llama_init_from_model: freq_scale    = 1
0.00.053.277 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.277 I ggml_metal_init: allocating
0.00.053.280 I ggml_metal_init: found device: Apple M4
0.00.053.283 I ggml_metal_init: picking default device: Apple M4
0.00.053.839 I ggml_metal_init: using embedded metal library
0.00.056.199 I ggml_metal_init: GPU name:   Apple M4
0.00.056.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.201 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.201 I ggml_metal_init: simdgroup reduction   = true
0.00.056.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.201 I ggml_metal_init: has bfloat            = true
0.00.056.201 I ggml_metal_init: use bfloat            = true
0.00.056.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.011 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.443 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.446 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.462 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.426 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.427 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.427 I llama_init_from_model: graph nodes  = 967
0.00.068.427 I llama_init_from_model: graph splits = 2
0.00.068.428 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.429 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.167.869 I 
0.00.167.914 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.167.930 I perplexity: tokenizing the input ..
0.00.175.909 I perplexity: tokenization took 7.977 ms
0.00.175.914 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.315.355 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.316.485 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.316.509 I llama_perf_context_print:        load time =     157.97 ms
0.00.316.510 I llama_perf_context_print: prompt eval time =     139.13 ms /   128 tokens (    1.09 ms per token,   920.02 tokens per second)
0.00.316.511 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.316.511 I llama_perf_context_print:       total time =     148.65 ms /   129 tokens
0.00.316.945 I ggml_metal_free: deallocating

real	0m0.332s
user	0m0.079s
sys	0m0.045s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4469 (7426a26b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c60a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c60a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c60af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c60b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c60baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c60c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c60c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c60cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c60d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c60d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c60db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c60e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c60eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c60f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c60fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c610260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c610980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c6110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c6117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c611f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c6126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c612dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c6134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c613d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c6144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c614770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c614d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c6159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c6161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c616690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c616950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c6171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c617720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c6179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c6187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c618c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c619100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c6195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c619a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c619ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c61a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c61a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c61ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c61b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c61bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c61c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c61c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c61cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c61d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c61d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c61dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c61e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c61ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c61f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c61f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c61f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c6201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c620dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c621270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c621710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c6224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c622e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c6232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c623770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c623c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c624160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c6246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c625150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c6256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c625bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c626690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c626be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c627130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c627bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c628120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c628670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c628bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c629110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c629660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c629bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c62a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c62a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c62aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c62b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c62b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c62bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c61b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c62c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c62c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c62cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c62d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c62d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c62dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c62e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c62e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c62ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c62f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c62f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c62fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c630220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c630cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c631160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c631600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c631aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c631f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c6323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c632880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c632d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c6331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c633660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c633b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c633fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c634440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c6348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c634d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c635220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c6356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c635b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c636000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c6364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c636940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c636de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c637280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c637720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c637bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c638060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c638500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c6389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c638e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c6392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c639780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c639c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c63a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c63a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c63aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c63aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c63b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c63b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c63bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c63c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c63c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c63ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c63cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c63d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c63d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c63dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c63e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c63e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c63eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c63ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c63f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c63f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c63fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c6401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c640680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c640b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c640fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c641460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c641900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c641da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c642240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c6426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c642b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c643020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c6434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c643960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c643e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c6442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c644740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c644be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c645520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c6459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c646300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c6467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c646c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c6470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c647580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c647a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c647ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c648410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c648960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c648eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c649400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c6496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c649cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c64a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c64a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c64b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c64b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c64b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c64be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c64c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c64cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c64d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c64d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c64da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c64e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c64e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c64ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c64f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c64f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c64fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c6501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c650710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c650c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c6511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c651700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c651c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c6521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c6526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c652c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c653190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c6536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c653c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c654180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c6546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c654c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c655170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c6556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c655c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c656160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c6566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c657150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c6576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c658140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c658690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c658be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c659130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c659680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c659bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c65a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c65a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c65abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c65b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c65b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c65bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c65c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c65c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c65cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c65d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c65d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c65db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c65e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c65e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c65eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c65f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c65f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c65fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c6600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c660610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c660b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c661000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c6614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c661940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c661de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c662280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c662720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c662bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c663060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c6639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c663e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c6642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c664780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c664c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c6650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c665610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c665d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c666450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c666b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c667290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c667550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c667d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c668000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c668610 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.139.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.139.607 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10af04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10af04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10af05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10af05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10af05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10af06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10af065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10af06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10af06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10af07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10af07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10af07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10af08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10af09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10af09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10af0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10af0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10af0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10af0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10af0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10af0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10af0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10af0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10af0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10af0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10af0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10af0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10af0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10af0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10af0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10af0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10af0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10af10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10af10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10af108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10af10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10af11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10af11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10af11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10af11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10af12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10af127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10af12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10af130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10af13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10af13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10af13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10af14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10af146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10af14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10af14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10af15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10af15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10af15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10af16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10af165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10af16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10af17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10af174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10af17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10af17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10af18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10af18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10af18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10af18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10af193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10af19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10af19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10af1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10af1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10af1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10af1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10af1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10af1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10af1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10af1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10af1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10af1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10af1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10af1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10af1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10af1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10af1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10af1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10af1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10af1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10af1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10af1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10af1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10af1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10af202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10af20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10af20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10af21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10af21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10af218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10af21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10af221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10af22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10af22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10af22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10af23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10af23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10af23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10af240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10af24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10af249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10af24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10af252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10af25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10af25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10af25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10af26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10af268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10af26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10af271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10af27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10af27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10af27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10af28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10af287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10af28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10af290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10af29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10af299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10af29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10af2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10af2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10af2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10af2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10af2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10af2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10af2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10af2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10af2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10af2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10af2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10af2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10af2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10af2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10af2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10af2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10af2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10af2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10af2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10af2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10af2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10af2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10af30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10af30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10af30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10af31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10af315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10af31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10af31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10af32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10af327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10af32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10af33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10af334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10af33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10af33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10af34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10af346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10af34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10af34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10af35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10af35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10af36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10af365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10af36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10af36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10af37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10af37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10af37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10af38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10af384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10af38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10af38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10af39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10af39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10af39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10af39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10af3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10af3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10af3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10af3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10af3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10af3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10af3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10af3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10af3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10af3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10af3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10af3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c64bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c649f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c6682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c649980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c64a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c61d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c61d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c61f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c64c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c614a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c61b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c61be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c61c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c61af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c61a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c61ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c613a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c60e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c61fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c62c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c667810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c616c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c616ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c64c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c64abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c615040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c615300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c6155c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c668a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c668d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c668ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c6692b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c669570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c669830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c669af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c669db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c66a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c66a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c66a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c66a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c66ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c66ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c66b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c66b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c66b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c66b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c66bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c66beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c66c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c66c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c66c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c66c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c66cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c66cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c66d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c66d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c66d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c66da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c66dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c66dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c66e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c66e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c66e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c66eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c66ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c66f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c66f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c66f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c66f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c66fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c66fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c6700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c670370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c670630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c6708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c670bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c670e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c671130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c6713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c6716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c671970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c671c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c671ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c6721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c672470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c672730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c6729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c672cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c672f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c673230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c6734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c6737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c673a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c673d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c673ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c6742b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10c674820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10c674ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10c674f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10c6751e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10c6754a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10c675760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10c675a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10c675ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10c675fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10c676260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10c676520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10c6767e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10c676db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10c677380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10c6779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10c677c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10c677f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10c6781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10c6784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10c678770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10c678a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10c678cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10c678fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10c679270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10c679530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10c6797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10c679ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10c679d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10c67a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10c67a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10c67a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10c67a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10c67ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10c67adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10c67b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10c67b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10c67b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10c67b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10c67bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10c67be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10c67c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10c67c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10c67c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10c67c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10c67cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10c67cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10c67d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10c67d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10c67d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10c67d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10c67dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10c67df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10c67e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10c67e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10c67e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10c67ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10c67ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10c67eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10c67f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10c67f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10c67f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10c67faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10c67fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10c680070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10c680330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10c6805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10c6808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10c680b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10c680e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10c6810f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10c6813b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10c681670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10c681930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10c681bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10c681eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10c682170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10c682430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10c6826f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10c6829b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10c682c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10c682f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10c6831f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10c6834b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10c683770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10c683a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10c683cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10c683fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10c684270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10c684530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10c6847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10c684ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10c684d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10c685030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10c6852f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10c6855b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10c685870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10c685b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10c685df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10c6860b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10c686370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10c686630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10c6868f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10c686bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10c686e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10c687130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10c6873f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10c6876b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10c687970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10c687c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10c687ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10c6881b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10c688470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10c688730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10c6889f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10c688cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10c688f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10c689230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10c6894f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10c6897b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10c689a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10c689d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10c689ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10c68a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10c68a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10c68a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10c68aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10c68adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10c68b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10c68b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10c68b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10c68b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10c68bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10c68be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10c68c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10c68c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10c68c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10c68c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10c68cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10c68ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10c68d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10c68d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10c68d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10c68d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10c68dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10c68df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10c68e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10c68e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10c68e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10c68ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10c68ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10c68efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10c68f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10c68f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10c68f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10c68fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10c68fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10c690030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10c6902f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10c6905b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10c690870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10c690b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10c690df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10c6910b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10c691370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10c691630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10c6918f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10c691bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10c691e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10c692130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10c6923f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10c6926b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10c692970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10c692c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10c692ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10c6931b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10c693470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10c693730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10c6939f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10c693cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10c693f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10c694230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10c6944f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10c6947b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10c694a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10c694d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10c694ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10c6952b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10c695570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10c695830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10c695af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10c695db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10c696070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10c696330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10c6965f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10c6968b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10c696b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10c696e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10c6970f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10c6973b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10c697670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10c697930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10c697bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10c697eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10c698170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10c6986b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10c698bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10c698eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10c6992b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10c699750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10c699bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10c69a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10c69a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10c69ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10c69b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10c69b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10c69bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10c69c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10c69c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10c69ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10c69d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10c69d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10c69df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10c69e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10c69eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10c69f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10c69f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10c69fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10c6a0160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10c6a0710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10c6a0cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10c6a1270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10c6a1820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10c6a1dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10c6a2380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10c6a2930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10c6a2ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10c6a3490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10c6a3a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10c6a3ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10c6a45a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10c6a4b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10c6a5100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10c6a56b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10c6a5c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10c6a6210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10c6a67c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10c6a6d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10c6a7320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10c6a78d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10c6a7e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10c6a8430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10c6a89e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10c6a8f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10c6a9540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10c6a9af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10c6aa0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10c6aa650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10c6aac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10c6ab1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10c6ab760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10c6abd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10c6ac2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10c6ac870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10c6ace20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10c6ad3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10c6ad980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10c6adf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10c6ae4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10c6ae9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10c6aeee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10c6af3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10c6af8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10c6afde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10c6b02e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10c6b07e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10c6b0ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10c6b11e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10c6b16e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10c6b1be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10c6b20e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10c6b25e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10c6b2ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10c6b2fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10c6b39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10c6b4110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10c6b4830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10c6b4f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10c6b5210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10c6b5a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10c6b5cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10c6b62d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.769s
user	0m0.293s
sys	0m0.316s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4469 (7426a26b)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13bf0d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13bf0dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13bf0e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13bf0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13bf0ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13bf0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13bf0f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13bf0fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13bf10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13bf10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13bf10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13bf11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13bf11e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13bf125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13bf12df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13bf13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13bf13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13bf14350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13bf14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13bf15240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13bf15960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13bf16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13bf167a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13bf17040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13bf17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13bf17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13bf18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13bf18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13bf191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13bf194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13bf19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13bf19c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13bf1a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13bf1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13bf1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13bf1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13bf1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13bf1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13bf1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13bf1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13bf1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13bf1ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13bf1d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13bf1d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13bf1d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13bf1df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13bf1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13bf1ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13bf1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13bf1fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13bf20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13bf20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13bf20c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13bf21290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13bf21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13bf21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13bf223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13bf22680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13bf22c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13bf23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13bf23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13bf23be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13bf24080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13bf24520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13bf249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13bf24e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13bf25300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13bf257a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13bf25c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13bf260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13bf26580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13bf26a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13bf26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13bf27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13bf27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13bf27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13bf28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13bf28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13bf28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13bf293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13bf29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13bf29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13bf2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13bf2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13bf2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13bf2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13bf2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13bf2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13bf2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13bf2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13bf2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13bf2d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13bf2d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13bf2de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13bf2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13bf2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13bf2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13bf1eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13bf2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13bf2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13bf2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13bf30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13bf30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13bf30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13bf314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13bf31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13bf31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13bf324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13bf32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13bf32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13bf334d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13bf33a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13bf33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13bf34410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13bf348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13bf34d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13bf351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13bf35690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13bf35b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13bf35fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13bf36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13bf36910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13bf36db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13bf37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13bf376f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13bf37b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13bf38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13bf384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13bf38970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13bf38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13bf392b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13bf39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13bf39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13bf3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13bf3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13bf3a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13bf3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13bf3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13bf3b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13bf3bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13bf3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13bf3c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13bf3ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13bf3ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13bf3d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13bf3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13bf3dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13bf3e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13bf3e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13bf3ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13bf3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13bf3f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13bf3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13bf3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13bf401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13bf40650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13bf40af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13bf40f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13bf41430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13bf418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13bf41d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13bf42210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13bf426b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13bf42b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13bf42ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13bf43490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13bf43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13bf43dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13bf44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13bf44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13bf44bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13bf45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13bf454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13bf45990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13bf45e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13bf462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13bf46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13bf46c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13bf470b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13bf47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13bf479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13bf47e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13bf48330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13bf487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13bf48c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13bf49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13bf495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13bf49a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13bf49ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13bf4a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13bf4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13bf4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13bf4b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13bf4b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13bf4bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13bf4c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13bf4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13bf4c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13bf4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13bf4d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13bf4dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13bf4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13bf4e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13bf4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13bf4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13bf4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13bf4ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13bf503a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13bf50840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13bf50ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13bf51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13bf519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13bf51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13bf52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13bf529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13bf52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13bf53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13bf539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13bf53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13bf54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13bf549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13bf54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13bf55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13bf559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13bf55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13bf56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13bf56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13bf56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13bf57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13bf57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13bf57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13bf58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13bf58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13bf58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13bf59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13bf59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13bf59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13bf5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13bf5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13bf5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13bf5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13bf5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13bf5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13bf5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13bf5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13bf5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13bf5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13bf5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13bf5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13bf5e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13bf5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13bf5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13bf5f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13bf5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13bf5fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13bf603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13bf608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13bf60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13bf61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13bf618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13bf61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13bf62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13bf628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13bf62e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13bf63370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13bf638c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13bf63e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13bf642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13bf64750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13bf64bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13bf65090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13bf65530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13bf659d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13bf65e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13bf66310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13bf667b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13bf66c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13bf670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13bf67590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13bf67a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13bf67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13bf68370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13bf688c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13bf68fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13bf69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13bf69e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13bf6a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13bf6a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13bf6aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13bf6b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13bf6b8c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.088.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.434 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13d004d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13d0051f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13d005660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13d005ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13d005f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13d0063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13d006820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13d006c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13d007100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13d007570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13d0079e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13d0080d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13d008bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13d0093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13d009bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13d00a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13d00a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13d00b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13d00b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13d00bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13d00c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13d00cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13d00d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13d00dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13d00e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13d00e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13d00e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13d00ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13d00f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13d00f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13d00fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13d00ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13d0103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13d0106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13d010b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13d010f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13d0113f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13d011860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13d011cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13d012140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13d0125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13d012a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13d012e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13d013300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13d013770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13d013be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13d014050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13d0144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13d014930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13d014da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13d015210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13d015680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13d015af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13d015f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13d0163d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13d016840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13d016db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13d0172b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13d017720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13d017b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13d018000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13d018470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13d0188e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13d018d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13d0191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13d019630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13d019aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13d019f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13d01a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13d01a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13d01ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13d01b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13d01b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13d01b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13d01be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13d01c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13d01c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13d01cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13d01cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13d01d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13d01d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13d01dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13d01e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13d01e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13d01ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13d01eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13d01f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13d01f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13d01fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13d0200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13d020520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13d020990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13d020e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13d021270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13d0216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13d021b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13d021fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13d022430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13d0228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13d022d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13d023180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13d0235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13d023a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13d023ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13d024340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13d0247b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13d024c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13d025090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13d025500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13d025970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13d025de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13d026250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13d0266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13d026b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13d026fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13d027410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13d027880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13d027cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13d028160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13d0285d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13d028a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13d028eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13d029320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13d029790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13d029c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13d02a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13d02a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13d02a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13d02adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13d02b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13d02b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13d02bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13d02bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13d02c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13d02c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13d02ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13d02d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13d02d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13d02da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13d02de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13d02e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13d02e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13d02ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13d02f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13d02f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13d02f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13d02fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13d030210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13d030680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13d030af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13d030f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13d0313d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13d031840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13d031cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13d032120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13d032590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13d032a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13d032e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13d0332e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13d033750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13d033bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13d034030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13d0344a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13d034910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13d034d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13d0351f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13d035e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13d0360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13d0363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13d036810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13d036c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13d0370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13d037560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13d0379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13d037e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13d0382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13d038720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13d038b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13d039000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13d039470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13d0398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13d039d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13d03a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13d03a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13d03aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13d03af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13d03b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13d03b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13d03bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13d03c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13d03c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13d03c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13d03ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13d03d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13d03d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13d03db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13d03dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13d03e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13d03e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13d03ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13d03f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13d03f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13d03fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13d040080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13d0404f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13d040960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13d040dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13d041240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13d041760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13d041c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13d0427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13d042aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13d043060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13d043620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13d043be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13d0441a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13d044760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13d044d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13d0452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13d0458a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13d045e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13d046420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13d0469e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13d046fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13d047560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13d047b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13d0480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13d0486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13d048c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13d049220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13d0497e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13d049da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13d04a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13d04a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13d04aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13d04b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13d04ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13d04c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13d04c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13d04cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13d04d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13d04d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13d04dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13d04e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13d04e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13d04ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13d04f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13d04f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13d04ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13d050520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13d050ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13d0510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13d051660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13d051c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13d0521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13d0527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13d052d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13d053320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13d0538e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13d053ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13d054460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13d054a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13d054fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13d0555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13d055b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13d056120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13d0566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13d056ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13d0571a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13d0576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13d057ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13d0580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13d0585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13d058aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13d058fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13d0594a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13d0599a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13d059ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13d05a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13d05a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13d05ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13d05b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13d05b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13d05c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13d05c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13d05cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13d05d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13d05d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13d05e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13d05e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13d05ea90 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13bf6b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13bf4edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13bf4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13bf4d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13bf20930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13bf20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13bf22940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13bf4f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13bf17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13bf1e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13bf1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13bf1f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13bf1dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13bf1fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13bf16ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13bf22f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13bf2f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13bf6aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13bf19ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13bf1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13bf4f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13bf4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13bf182f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13bf185b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13bf18870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13bf6bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13bf6bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13bf6c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13bf6c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13bf6c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13bf6cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13bf6cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13bf6d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13bf6d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13bf6d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13bf6d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13bf6db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13bf6de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13bf6e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13bf6e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13bf6e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13bf6e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13bf6ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13bf6eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13bf6f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13bf6f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13bf6f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13bf6f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13bf6fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13bf6ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13bf701e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13bf704a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13bf70760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13bf70a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13bf70ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13bf70fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13bf71260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13bf71520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13bf717e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13bf71aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13bf71d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13bf72020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13bf722e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13bf725a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13bf72860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13bf72b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13bf72de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13bf730a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13bf73360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13bf73620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13bf738e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13bf73ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13bf73e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13bf74120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13bf743e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13bf746a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13bf74960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13bf74c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13bf74ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13bf751a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13bf75460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13bf75720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13bf759e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13bf75ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13bf75f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13bf76220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13bf764e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13bf767a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13bf76a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13bf76d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13bf76fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13bf772a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13bf77560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13bf77820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13bf77ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13bf77da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13bf78060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13bf78320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13bf785e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13bf788a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13bf78b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13bf78e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13bf790e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13bf793a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13bf79660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13bf79920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13bf79be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13bf79ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13bf7a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13bf7a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13bf7a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13bf7a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13bf7ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13bf7af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13bf7b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13bf7b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13bf7b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13bf7ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13bf7bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13bf7bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13bf7c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13bf7c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13bf7c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13bf7caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13bf7cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13bf7d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13bf7d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13bf7d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13bf7d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13bf7db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13bf7dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13bf7e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13bf7e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13bf7e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13bf7e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13bf7eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13bf7ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13bf7f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13bf7f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13bf7f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13bf7f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13bf7fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13bf7fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13bf801a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13bf80460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13bf80720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13bf809e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13bf80ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13bf80f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13bf81220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13bf814e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13bf817a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13bf81a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13bf81d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13bf81fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13bf822a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13bf82560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13bf82820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13bf82ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13bf82da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13bf83060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13bf83320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13bf835e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13bf838a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13bf83b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13bf83e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13bf840e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13bf843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13bf84660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13bf84920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13bf84be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13bf84ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13bf85160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13bf85420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13bf856e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13bf859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13bf85c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13bf85f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13bf861e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13bf864a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13bf86760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13bf86a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13bf86ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13bf86fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13bf87260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13bf87520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13bf877e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13bf87aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13bf87d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13bf88020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13bf882e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13bf885a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13bf88860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13bf88b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13bf88de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13bf890a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13bf89360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13bf89620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13bf898e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13bf89ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13bf89e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13bf8a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13bf8a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13bf8a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13bf8a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13bf8ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13bf8aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13bf8b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13bf8b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13bf8b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13bf8bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13bf8bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13bf8c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13bf8c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13bf8cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13bf8d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13bf8d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13bf8dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13bf8e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13bf8e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13bf8ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13bf8f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13bf8f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13bf8fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13bf90230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13bf90780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13bf90cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13bf91220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13bf91770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13bf91cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13bf92210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13bf92760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13bf92cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13bf93200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13bf93750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13bf93ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13bf941f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13bf94740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13bf94c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13bf951e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13bf95730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13bf95c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13bf961d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13bf96720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13bf96c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13bf971c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13bf97710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13bf97c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13bf981b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13bf98700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13bf98c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13bf991a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13bf996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13bf99c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13bf9a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13bf9a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13bf9ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13bf9b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13bf9b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13bf9bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13bf9c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13bf9c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13bf9cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13bf9d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13bf9d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13bf9dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13bf9e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13bf9e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13bf9e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13bf9e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13bf9ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13bf9f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13bf9f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13bf9fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13bf9ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13bfa0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13bfa08a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13bfa0d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13bfa1180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13bfa15f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13bfa1a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13bfa1ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13bfa2340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13bfa27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13bfa34a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13bfa3bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13bfa42e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13bfa45a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13bfa4a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13bfa5010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13bfa5620 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.912s
user	0m0.242s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.53 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.11 sec*proc (2 tests)

Total Test time (real) =   1.12 sec
        1.14 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.31 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.58 sec*proc (2 tests)

Total Test time (real) =   0.59 sec
        0.59 real         0.16 user         0.05 sys
```
