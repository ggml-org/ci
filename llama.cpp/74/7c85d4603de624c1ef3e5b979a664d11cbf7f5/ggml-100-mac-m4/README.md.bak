### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.87 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.71 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.44 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.51 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.03 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.34 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.26 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.26 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.41 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  181.42 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.83 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.34 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 225.53 sec*proc (28 tests)

Total Test time (real) = 225.54 sec

real	3m45.569s
user	7m40.630s
sys	0m6.433s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.94 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.18 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.38 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.77 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.40 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.28 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.11 sec*proc (28 tests)

Total Test time (real) =  52.12 sec

real	0m52.133s
user	1m13.142s
sys	0m5.349s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.152 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.364 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.675 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.023.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.684 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.023.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.685 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.023.686 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.023.687 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.023.688 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.023.689 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.023.690 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.023.692 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.023.693 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.023.696 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.023.697 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.023.698 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.023.698 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.023.699 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.023.699 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.023.700 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.199 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.201 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.202 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.202 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.203 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.203 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.030.204 I llama_model_loader: - type  f32:  124 tensors
0.00.030.205 I llama_model_loader: - type  f16:   73 tensors
0.00.035.019 I llm_load_vocab: special tokens cache size = 5
0.00.037.325 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.329 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.329 I llm_load_print_meta: arch             = bert
0.00.037.330 I llm_load_print_meta: vocab type       = WPM
0.00.037.330 I llm_load_print_meta: n_vocab          = 30522
0.00.037.330 I llm_load_print_meta: n_merges         = 0
0.00.037.331 I llm_load_print_meta: vocab_only       = 0
0.00.037.331 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.331 I llm_load_print_meta: n_embd           = 384
0.00.037.331 I llm_load_print_meta: n_layer          = 12
0.00.037.335 I llm_load_print_meta: n_head           = 12
0.00.037.336 I llm_load_print_meta: n_head_kv        = 12
0.00.037.336 I llm_load_print_meta: n_rot            = 32
0.00.037.336 I llm_load_print_meta: n_swa            = 0
0.00.037.337 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.337 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.338 I llm_load_print_meta: n_gqa            = 1
0.00.037.338 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.339 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.340 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.341 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.343 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.343 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.344 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.344 I llm_load_print_meta: n_ff             = 1536
0.00.037.345 I llm_load_print_meta: n_expert         = 0
0.00.037.345 I llm_load_print_meta: n_expert_used    = 0
0.00.037.345 I llm_load_print_meta: causal attn      = 0
0.00.037.347 I llm_load_print_meta: pooling type     = 2
0.00.037.347 I llm_load_print_meta: rope type        = 2
0.00.037.348 I llm_load_print_meta: rope scaling     = linear
0.00.037.348 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.349 I llm_load_print_meta: freq_scale_train = 1
0.00.037.349 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.349 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.349 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.350 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.350 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.350 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.350 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.351 I llm_load_print_meta: model type       = 33M
0.00.037.351 I llm_load_print_meta: model ftype      = F16
0.00.037.352 I llm_load_print_meta: model params     = 33.21 M
0.00.037.353 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.355 I llm_load_print_meta: general.name     = Bge Small
0.00.037.356 I llm_load_print_meta: BOS token        = 101 '[CLS]'
0.00.037.356 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.356 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.357 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.357 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.357 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.359 I llm_load_print_meta: max token length = 21
0.00.039.458 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.458 I llm_load_tensors: offloading output layer to GPU
0.00.039.460 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.488 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.490 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.054 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.055 I llama_new_context_with_model: n_ctx         = 512
0.00.040.056 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.056 I llama_new_context_with_model: n_batch       = 2048
0.00.040.056 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.057 I llama_new_context_with_model: flash_attn    = 0
0.00.040.057 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.057 I llama_new_context_with_model: freq_scale    = 1
0.00.040.058 I ggml_metal_init: allocating
0.00.040.062 I ggml_metal_init: found device: Apple M4
0.00.040.065 I ggml_metal_init: picking default device: Apple M4
0.00.040.919 I ggml_metal_init: using embedded metal library
0.00.045.292 I ggml_metal_init: GPU name:   Apple M4
0.00.045.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.296 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.297 I ggml_metal_init: simdgroup reduction   = true
0.00.045.297 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.297 I ggml_metal_init: has bfloat            = true
0.00.045.297 I ggml_metal_init: use bfloat            = true
0.00.045.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.148 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.058.781 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.784 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.785 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.548 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.549 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.550 I llama_new_context_with_model: graph nodes  = 429
0.00.059.550 I llama_new_context_with_model: graph splits = 2
0.00.059.551 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.551 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.065.987 I 
0.00.066.001 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.066.692 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.071.547 I llama_perf_context_print:        load time =      46.62 ms
0.00.071.548 I llama_perf_context_print: prompt eval time =       4.68 ms /     9 tokens (    0.52 ms per token,  1921.43 tokens per second)
0.00.071.549 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.071.549 I llama_perf_context_print:       total time =       5.56 ms /    10 tokens
0.00.071.703 I ggml_metal_free: deallocating

real	0m0.254s
user	0m0.051s
sys	0m0.033s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.427 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.495 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.498 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.500 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.500 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.501 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.501 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.501 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.502 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.503 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.503 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.503 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.503 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.506 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.506 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.507 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.507 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.507 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.508 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.508 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.045 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.740 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.741 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.742 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.742 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.742 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.743 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.014.743 I llama_model_loader: - type  f32:  124 tensors
0.00.014.744 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.257 I llm_load_vocab: special tokens cache size = 5
0.00.018.552 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.555 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.555 I llm_load_print_meta: arch             = bert
0.00.018.556 I llm_load_print_meta: vocab type       = WPM
0.00.018.556 I llm_load_print_meta: n_vocab          = 30522
0.00.018.556 I llm_load_print_meta: n_merges         = 0
0.00.018.557 I llm_load_print_meta: vocab_only       = 0
0.00.018.557 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.557 I llm_load_print_meta: n_embd           = 384
0.00.018.557 I llm_load_print_meta: n_layer          = 12
0.00.018.562 I llm_load_print_meta: n_head           = 12
0.00.018.562 I llm_load_print_meta: n_head_kv        = 12
0.00.018.563 I llm_load_print_meta: n_rot            = 32
0.00.018.563 I llm_load_print_meta: n_swa            = 0
0.00.018.563 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.563 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.563 I llm_load_print_meta: n_gqa            = 1
0.00.018.564 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.565 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.565 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.567 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.567 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.567 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.567 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.568 I llm_load_print_meta: n_ff             = 1536
0.00.018.568 I llm_load_print_meta: n_expert         = 0
0.00.018.568 I llm_load_print_meta: n_expert_used    = 0
0.00.018.568 I llm_load_print_meta: causal attn      = 0
0.00.018.568 I llm_load_print_meta: pooling type     = 2
0.00.018.568 I llm_load_print_meta: rope type        = 2
0.00.018.569 I llm_load_print_meta: rope scaling     = linear
0.00.018.570 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.570 I llm_load_print_meta: freq_scale_train = 1
0.00.018.570 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.570 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.572 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.572 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.572 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.572 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.572 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.572 I llm_load_print_meta: model type       = 33M
0.00.018.573 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.573 I llm_load_print_meta: model params     = 33.21 M
0.00.018.574 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.574 I llm_load_print_meta: general.name     = Bge Small
0.00.018.574 I llm_load_print_meta: BOS token        = 101 '[CLS]'
0.00.018.574 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.574 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.574 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.575 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.576 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.577 I llm_load_print_meta: max token length = 21
0.00.019.856 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.856 I llm_load_tensors: offloading output layer to GPU
0.00.019.859 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.867 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.868 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.225 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.226 I llama_new_context_with_model: n_ctx         = 512
0.00.020.226 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.226 I llama_new_context_with_model: n_batch       = 2048
0.00.020.226 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.227 I llama_new_context_with_model: flash_attn    = 0
0.00.020.227 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.227 I llama_new_context_with_model: freq_scale    = 1
0.00.020.228 I ggml_metal_init: allocating
0.00.020.230 I ggml_metal_init: found device: Apple M4
0.00.020.232 I ggml_metal_init: picking default device: Apple M4
0.00.020.832 I ggml_metal_init: using embedded metal library
0.00.023.352 I ggml_metal_init: GPU name:   Apple M4
0.00.023.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.356 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.357 I ggml_metal_init: simdgroup reduction   = true
0.00.023.357 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.357 I ggml_metal_init: has bfloat            = true
0.00.023.357 I ggml_metal_init: use bfloat            = true
0.00.023.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.358 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.539 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.020 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.023 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.024 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.635 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.636 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.636 I llama_new_context_with_model: graph nodes  = 429
0.00.034.637 I llama_new_context_with_model: graph splits = 2
0.00.034.638 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.036 I 
0.00.039.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.588 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.994 I llama_perf_context_print:        load time =      29.61 ms
0.00.043.995 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2106.25 tokens per second)
0.00.043.996 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.997 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.044.199 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.204 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.500 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.179 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.188 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.189 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.199 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.200 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.200 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.202 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.202 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.203 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.204 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.204 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.208 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.208 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.209 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.986 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.342 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.344 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.344 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.345 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.346 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.346 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.347 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.347 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.348 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.348 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.349 I llama_model_loader: - type  f32:   40 tensors
0.00.050.351 I llama_model_loader: - type  f16:   30 tensors
0.00.069.100 W llm_load_vocab: empty token at index 5
0.00.073.661 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.943 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.975 I llm_load_vocab: special tokens cache size = 5
0.00.338.894 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.338.899 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.338.900 I llm_load_print_meta: arch             = jina-bert-v2
0.00.338.900 I llm_load_print_meta: vocab type       = BPE
0.00.338.901 I llm_load_print_meta: n_vocab          = 61056
0.00.338.901 I llm_load_print_meta: n_merges         = 39382
0.00.338.901 I llm_load_print_meta: vocab_only       = 0
0.00.338.901 I llm_load_print_meta: n_ctx_train      = 8192
0.00.338.902 I llm_load_print_meta: n_embd           = 384
0.00.338.903 I llm_load_print_meta: n_layer          = 4
0.00.338.907 I llm_load_print_meta: n_head           = 12
0.00.338.908 I llm_load_print_meta: n_head_kv        = 12
0.00.338.908 I llm_load_print_meta: n_rot            = 32
0.00.338.908 I llm_load_print_meta: n_swa            = 0
0.00.338.908 I llm_load_print_meta: n_embd_head_k    = 32
0.00.338.909 I llm_load_print_meta: n_embd_head_v    = 32
0.00.338.909 I llm_load_print_meta: n_gqa            = 1
0.00.338.911 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.338.912 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.338.912 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.338.913 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.338.913 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.338.913 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.338.915 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.338.915 I llm_load_print_meta: n_ff             = 1536
0.00.338.916 I llm_load_print_meta: n_expert         = 0
0.00.338.916 I llm_load_print_meta: n_expert_used    = 0
0.00.338.916 I llm_load_print_meta: causal attn      = 0
0.00.338.916 I llm_load_print_meta: pooling type     = -1
0.00.338.916 I llm_load_print_meta: rope type        = -1
0.00.338.917 I llm_load_print_meta: rope scaling     = linear
0.00.338.917 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.338.917 I llm_load_print_meta: freq_scale_train = 1
0.00.338.918 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.338.918 I llm_load_print_meta: rope_finetuned   = unknown
0.00.338.918 I llm_load_print_meta: ssm_d_conv       = 0
0.00.338.918 I llm_load_print_meta: ssm_d_inner      = 0
0.00.338.918 I llm_load_print_meta: ssm_d_state      = 0
0.00.338.918 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.338.919 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.338.920 I llm_load_print_meta: model type       = 33M
0.00.338.920 I llm_load_print_meta: model ftype      = F16
0.00.338.921 I llm_load_print_meta: model params     = 32.90 M
0.00.338.921 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.338.922 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.338.923 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.338.923 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.338.923 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.338.923 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.338.923 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.338.923 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.338.923 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.338.924 I llm_load_print_meta: max token length = 45
0.00.339.953 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.339.953 I llm_load_tensors: offloading output layer to GPU
0.00.339.953 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.339.973 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.339.974 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.340.741 I llama_new_context_with_model: n_seq_max     = 1
0.00.340.743 I llama_new_context_with_model: n_ctx         = 8192
0.00.340.743 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.340.743 I llama_new_context_with_model: n_batch       = 2048
0.00.340.743 I llama_new_context_with_model: n_ubatch      = 2048
0.00.340.743 I llama_new_context_with_model: flash_attn    = 0
0.00.340.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.340.744 I llama_new_context_with_model: freq_scale    = 1
0.00.340.744 I ggml_metal_init: allocating
0.00.340.748 I ggml_metal_init: found device: Apple M4
0.00.340.749 I ggml_metal_init: picking default device: Apple M4
0.00.341.546 I ggml_metal_init: using embedded metal library
0.00.344.340 I ggml_metal_init: GPU name:   Apple M4
0.00.344.342 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.344.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.344.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.344.343 I ggml_metal_init: simdgroup reduction   = true
0.00.344.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.344.343 I ggml_metal_init: has bfloat            = true
0.00.344.343 I ggml_metal_init: use bfloat            = true
0.00.344.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.344.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.911 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.356.291 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.356.293 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.356.294 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.356.825 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.356.826 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.356.827 I llama_new_context_with_model: graph nodes  = 154
0.00.356.827 I llama_new_context_with_model: graph splits = 2
0.00.356.828 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.356.828 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.366.336 I 
0.00.366.358 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.366.511 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.366.512 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.366.515 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.366.515 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.366.519 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.366.519 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.367.006 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.669 I llama_perf_context_print:        load time =     342.83 ms
0.00.370.671 I llama_perf_context_print: prompt eval time =       3.66 ms /    62 tokens (    0.06 ms per token, 16963.06 tokens per second)
0.00.370.673 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.674 I llama_perf_context_print:       total time =       4.33 ms /    63 tokens
0.00.370.934 I ggml_metal_free: deallocating

real	0m1.107s
user	0m0.345s
sys	0m0.041s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.193 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.350 I main: llama backend init
0.00.000.361 I main: load the model and apply lora adapter, if any
0.00.032.937 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.915 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.927 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.930 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.932 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.938 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.943 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.943 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.944 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.053.186 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.794 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.059.796 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.796 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.797 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.797 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.798 I llama_model_loader: - type  f32:  194 tensors
0.00.059.799 I llama_model_loader: - type  f16:   98 tensors
0.00.092.246 I llm_load_vocab: special tokens cache size = 25
0.00.099.283 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.286 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.287 I llm_load_print_meta: arch             = gptneox
0.00.099.287 I llm_load_print_meta: vocab type       = BPE
0.00.099.287 I llm_load_print_meta: n_vocab          = 50304
0.00.099.287 I llm_load_print_meta: n_merges         = 50009
0.00.099.287 I llm_load_print_meta: vocab_only       = 0
0.00.099.288 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.288 I llm_load_print_meta: n_embd           = 2048
0.00.099.288 I llm_load_print_meta: n_layer          = 24
0.00.099.291 I llm_load_print_meta: n_head           = 16
0.00.099.292 I llm_load_print_meta: n_head_kv        = 16
0.00.099.292 I llm_load_print_meta: n_rot            = 32
0.00.099.292 I llm_load_print_meta: n_swa            = 0
0.00.099.293 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.293 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.293 I llm_load_print_meta: n_gqa            = 1
0.00.099.294 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.296 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.297 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.297 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.297 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.297 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.298 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.298 I llm_load_print_meta: n_ff             = 8192
0.00.099.299 I llm_load_print_meta: n_expert         = 0
0.00.099.299 I llm_load_print_meta: n_expert_used    = 0
0.00.099.299 I llm_load_print_meta: causal attn      = 1
0.00.099.299 I llm_load_print_meta: pooling type     = 0
0.00.099.299 I llm_load_print_meta: rope type        = 2
0.00.099.299 I llm_load_print_meta: rope scaling     = linear
0.00.099.300 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.300 I llm_load_print_meta: freq_scale_train = 1
0.00.099.300 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.300 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.300 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.301 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.301 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.301 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.301 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.301 I llm_load_print_meta: model type       = 1.4B
0.00.099.302 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.302 I llm_load_print_meta: model params     = 1.41 B
0.00.099.303 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.303 I llm_load_print_meta: general.name     = 1.4B
0.00.099.303 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.303 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.304 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.304 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.304 I llm_load_print_meta: LF token         = 128 ''
0.00.099.304 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.304 I llm_load_print_meta: max token length = 1024
0.00.101.930 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.930 I llm_load_tensors: offloading output layer to GPU
0.00.101.930 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.949 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.950 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.910 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.911 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.911 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.911 I llama_new_context_with_model: n_batch       = 2048
0.00.102.911 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.912 I llama_new_context_with_model: flash_attn    = 0
0.00.102.912 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.912 I llama_new_context_with_model: freq_scale    = 1
0.00.102.913 I ggml_metal_init: allocating
0.00.102.922 I ggml_metal_init: found device: Apple M4
0.00.102.925 I ggml_metal_init: picking default device: Apple M4
0.00.103.612 I ggml_metal_init: using embedded metal library
0.00.119.489 I ggml_metal_init: GPU name:   Apple M4
0.00.119.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.119.493 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.119.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.119.494 I ggml_metal_init: simdgroup reduction   = true
0.00.119.494 I ggml_metal_init: simdgroup matrix mul. = true
0.00.119.494 I ggml_metal_init: has bfloat            = true
0.00.119.494 I ggml_metal_init: use bfloat            = true
0.00.119.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.119.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.142.894 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.164.288 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.164.294 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.164.313 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.165.326 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.165.329 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.165.329 I llama_new_context_with_model: graph nodes  = 967
0.00.165.330 I llama_new_context_with_model: graph splits = 2
0.00.165.333 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.165.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.165.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.252.251 I main: llama threadpool init, n_threads = 4
0.00.252.296 I 
0.00.252.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.252.319 I 
0.00.252.583 I sampler seed: 1234
0.00.252.590 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.252.626 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.252.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.252.628 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.091.052 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.02.091.052 I llama_perf_context_print:        load time =     219.30 ms
0.02.091.053 I llama_perf_context_print: prompt eval time =      43.79 ms /     7 tokens (    6.26 ms per token,   159.87 tokens per second)
0.02.091.054 I llama_perf_context_print:        eval time =    1791.71 ms /    63 runs   (   28.44 ms per token,    35.16 tokens per second)
0.02.091.054 I llama_perf_context_print:       total time =    1838.81 ms /    70 tokens
0.02.091.311 I ggml_metal_free: deallocating

real	0m2.381s
user	0m0.145s
sys	0m0.111s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.538 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.887 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.944 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.951 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.961 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.969 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.969 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.970 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.970 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.971 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.972 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.972 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.976 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.095 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.120 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.096 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.050.098 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.099 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.099 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.100 I llama_model_loader: - type  f32:  194 tensors
0.00.050.100 I llama_model_loader: - type  f16:   98 tensors
0.00.078.345 I llm_load_vocab: special tokens cache size = 25
0.00.084.783 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.786 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.786 I llm_load_print_meta: arch             = gptneox
0.00.084.787 I llm_load_print_meta: vocab type       = BPE
0.00.084.787 I llm_load_print_meta: n_vocab          = 50304
0.00.084.787 I llm_load_print_meta: n_merges         = 50009
0.00.084.787 I llm_load_print_meta: vocab_only       = 0
0.00.084.787 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.787 I llm_load_print_meta: n_embd           = 2048
0.00.084.788 I llm_load_print_meta: n_layer          = 24
0.00.084.791 I llm_load_print_meta: n_head           = 16
0.00.084.792 I llm_load_print_meta: n_head_kv        = 16
0.00.084.792 I llm_load_print_meta: n_rot            = 32
0.00.084.792 I llm_load_print_meta: n_swa            = 0
0.00.084.792 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.792 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.793 I llm_load_print_meta: n_gqa            = 1
0.00.084.793 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.794 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.796 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.797 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.797 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.797 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.798 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.800 I llm_load_print_meta: n_ff             = 8192
0.00.084.800 I llm_load_print_meta: n_expert         = 0
0.00.084.800 I llm_load_print_meta: n_expert_used    = 0
0.00.084.800 I llm_load_print_meta: causal attn      = 1
0.00.084.801 I llm_load_print_meta: pooling type     = 0
0.00.084.801 I llm_load_print_meta: rope type        = 2
0.00.084.801 I llm_load_print_meta: rope scaling     = linear
0.00.084.801 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.802 I llm_load_print_meta: freq_scale_train = 1
0.00.084.802 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.802 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.802 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.803 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.804 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.804 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.804 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.804 I llm_load_print_meta: model type       = 1.4B
0.00.084.805 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.805 I llm_load_print_meta: model params     = 1.41 B
0.00.084.806 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.806 I llm_load_print_meta: general.name     = 1.4B
0.00.084.806 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.807 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.808 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.808 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.808 I llm_load_print_meta: LF token         = 128 ''
0.00.084.808 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.808 I llm_load_print_meta: max token length = 1024
0.00.087.463 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.463 I llm_load_tensors: offloading output layer to GPU
0.00.087.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.474 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.087.475 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.088.400 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.401 I llama_new_context_with_model: n_ctx         = 128
0.00.088.401 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.401 I llama_new_context_with_model: n_batch       = 128
0.00.088.401 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.401 I llama_new_context_with_model: flash_attn    = 0
0.00.088.402 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.402 I llama_new_context_with_model: freq_scale    = 1
0.00.088.403 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.403 I ggml_metal_init: allocating
0.00.088.407 I ggml_metal_init: found device: Apple M4
0.00.088.409 I ggml_metal_init: picking default device: Apple M4
0.00.089.007 I ggml_metal_init: using embedded metal library
0.00.091.488 I ggml_metal_init: GPU name:   Apple M4
0.00.091.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.490 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.491 I ggml_metal_init: simdgroup reduction   = true
0.00.091.491 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.491 I ggml_metal_init: has bfloat            = true
0.00.091.491 I ggml_metal_init: use bfloat            = true
0.00.091.492 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.492 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.326 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.772 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.774 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.798 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.637 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.638 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.638 I llama_new_context_with_model: graph nodes  = 967
0.00.102.639 I llama_new_context_with_model: graph splits = 2
0.00.102.640 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.102.640 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.442.151 I 
0.01.442.207 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.442.263 I perplexity: tokenizing the input ..
0.01.454.308 I perplexity: tokenization took 12.041 ms
0.01.454.314 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.574.932 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.576.763 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.576.782 I llama_perf_context_print:        load time =    1422.25 ms
0.01.576.784 I llama_perf_context_print: prompt eval time =     120.22 ms /   128 tokens (    0.94 ms per token,  1064.70 tokens per second)
0.01.576.785 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.576.786 I llama_perf_context_print:       total time =     134.64 ms /   129 tokens
0.01.577.368 I ggml_metal_free: deallocating

real	0m1.775s
user	0m0.121s
sys	0m0.242s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.596 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.138 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.145 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.146 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.150 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.150 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.150 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.151 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.151 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.152 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.152 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.153 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.155 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.238 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.240 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.240 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.240 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.241 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.242 I llama_model_loader: - type  f32:  194 tensors
0.00.037.242 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.955 I llm_load_vocab: special tokens cache size = 25
0.00.068.605 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.609 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.609 I llm_load_print_meta: arch             = gptneox
0.00.068.610 I llm_load_print_meta: vocab type       = BPE
0.00.068.610 I llm_load_print_meta: n_vocab          = 50304
0.00.068.610 I llm_load_print_meta: n_merges         = 50009
0.00.068.612 I llm_load_print_meta: vocab_only       = 0
0.00.068.612 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.612 I llm_load_print_meta: n_embd           = 2048
0.00.068.617 I llm_load_print_meta: n_layer          = 24
0.00.068.623 I llm_load_print_meta: n_head           = 16
0.00.068.624 I llm_load_print_meta: n_head_kv        = 16
0.00.068.625 I llm_load_print_meta: n_rot            = 32
0.00.068.625 I llm_load_print_meta: n_swa            = 0
0.00.068.625 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.625 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.626 I llm_load_print_meta: n_gqa            = 1
0.00.068.627 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.628 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.628 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.629 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.629 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.629 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.629 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.630 I llm_load_print_meta: n_ff             = 8192
0.00.068.630 I llm_load_print_meta: n_expert         = 0
0.00.068.630 I llm_load_print_meta: n_expert_used    = 0
0.00.068.631 I llm_load_print_meta: causal attn      = 1
0.00.068.631 I llm_load_print_meta: pooling type     = 0
0.00.068.633 I llm_load_print_meta: rope type        = 2
0.00.068.633 I llm_load_print_meta: rope scaling     = linear
0.00.068.634 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.634 I llm_load_print_meta: freq_scale_train = 1
0.00.068.634 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.634 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.635 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.635 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.635 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.635 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.635 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.635 I llm_load_print_meta: model type       = 1.4B
0.00.068.639 I llm_load_print_meta: model ftype      = Q8_0
0.00.068.640 I llm_load_print_meta: model params     = 1.41 B
0.00.068.640 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.068.640 I llm_load_print_meta: general.name     = 1.4B
0.00.068.641 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.641 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.641 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.641 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.642 I llm_load_print_meta: LF token         = 128 ''
0.00.068.642 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.642 I llm_load_print_meta: max token length = 1024
0.00.071.109 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.071.109 I llm_load_tensors: offloading output layer to GPU
0.00.071.109 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.071.116 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.071.116 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.072.275 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.276 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.277 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.277 I llama_new_context_with_model: n_batch       = 2048
0.00.072.277 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.277 I llama_new_context_with_model: flash_attn    = 0
0.00.072.278 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.278 I llama_new_context_with_model: freq_scale    = 1
0.00.072.278 I ggml_metal_init: allocating
0.00.072.281 I ggml_metal_init: found device: Apple M4
0.00.072.284 I ggml_metal_init: picking default device: Apple M4
0.00.073.083 I ggml_metal_init: using embedded metal library
0.00.076.104 I ggml_metal_init: GPU name:   Apple M4
0.00.076.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.076.106 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.076.106 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.076.107 I ggml_metal_init: simdgroup reduction   = true
0.00.076.107 I ggml_metal_init: simdgroup matrix mul. = true
0.00.076.107 I ggml_metal_init: has bfloat            = true
0.00.076.107 I ggml_metal_init: use bfloat            = true
0.00.076.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.076.108 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.567 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.443 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.452 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.480 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.114.629 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.114.631 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.114.632 I llama_new_context_with_model: graph nodes  = 967
0.00.114.632 I llama_new_context_with_model: graph splits = 2
0.00.114.636 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.777 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.778 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.314.941 I main: llama threadpool init, n_threads = 4
0.01.315.007 I 
0.01.315.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.315.039 I 
0.01.315.412 I sampler seed: 1234
0.01.315.418 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.315.457 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.315.458 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.315.459 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.412.819 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.02.412.820 I llama_perf_context_print:        load time =    1305.34 ms
0.02.412.821 I llama_perf_context_print: prompt eval time =      50.05 ms /     7 tokens (    7.15 ms per token,   139.87 tokens per second)
0.02.412.821 I llama_perf_context_print:        eval time =    1044.15 ms /    63 runs   (   16.57 ms per token,    60.34 tokens per second)
0.02.412.822 I llama_perf_context_print:       total time =    1097.88 ms /    70 tokens
0.02.413.040 I ggml_metal_free: deallocating

real	0m2.431s
user	0m0.127s
sys	0m0.245s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.132 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.907 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.282 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.299 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.300 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.300 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.301 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.303 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.304 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.234 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.887 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.042 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.043 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.044 I llama_model_loader: - type  f32:  194 tensors
0.00.034.045 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.834 I llm_load_vocab: special tokens cache size = 25
0.00.066.943 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.947 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.947 I llm_load_print_meta: arch             = gptneox
0.00.066.947 I llm_load_print_meta: vocab type       = BPE
0.00.066.948 I llm_load_print_meta: n_vocab          = 50304
0.00.066.948 I llm_load_print_meta: n_merges         = 50009
0.00.066.948 I llm_load_print_meta: vocab_only       = 0
0.00.066.948 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.948 I llm_load_print_meta: n_embd           = 2048
0.00.066.949 I llm_load_print_meta: n_layer          = 24
0.00.066.953 I llm_load_print_meta: n_head           = 16
0.00.066.954 I llm_load_print_meta: n_head_kv        = 16
0.00.066.954 I llm_load_print_meta: n_rot            = 32
0.00.066.956 I llm_load_print_meta: n_swa            = 0
0.00.066.956 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.956 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.957 I llm_load_print_meta: n_gqa            = 1
0.00.066.958 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.959 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.959 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.960 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.960 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.960 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.960 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.961 I llm_load_print_meta: n_ff             = 8192
0.00.066.961 I llm_load_print_meta: n_expert         = 0
0.00.066.961 I llm_load_print_meta: n_expert_used    = 0
0.00.066.961 I llm_load_print_meta: causal attn      = 1
0.00.066.962 I llm_load_print_meta: pooling type     = 0
0.00.066.962 I llm_load_print_meta: rope type        = 2
0.00.066.962 I llm_load_print_meta: rope scaling     = linear
0.00.066.963 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.963 I llm_load_print_meta: freq_scale_train = 1
0.00.066.963 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.963 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.964 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.964 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.965 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.965 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.965 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.966 I llm_load_print_meta: model type       = 1.4B
0.00.066.966 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.966 I llm_load_print_meta: model params     = 1.41 B
0.00.066.967 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.967 I llm_load_print_meta: general.name     = 1.4B
0.00.066.967 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.968 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.968 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.968 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.968 I llm_load_print_meta: LF token         = 128 ''
0.00.066.968 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.969 I llm_load_print_meta: max token length = 1024
0.00.069.309 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.309 I llm_load_tensors: offloading output layer to GPU
0.00.069.309 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.321 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.322 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.227 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.227 I llama_new_context_with_model: n_ctx         = 128
0.00.070.227 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.228 I llama_new_context_with_model: n_batch       = 128
0.00.070.228 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.228 I llama_new_context_with_model: flash_attn    = 0
0.00.070.228 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.229 I llama_new_context_with_model: freq_scale    = 1
0.00.070.229 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.229 I ggml_metal_init: allocating
0.00.070.233 I ggml_metal_init: found device: Apple M4
0.00.070.235 I ggml_metal_init: picking default device: Apple M4
0.00.070.866 I ggml_metal_init: using embedded metal library
0.00.073.405 I ggml_metal_init: GPU name:   Apple M4
0.00.073.407 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.408 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.408 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.408 I ggml_metal_init: simdgroup reduction   = true
0.00.073.408 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.408 I ggml_metal_init: has bfloat            = true
0.00.073.409 I ggml_metal_init: use bfloat            = true
0.00.073.409 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.410 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.708 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.182 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.084.184 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.084.200 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.114 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.085.115 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.085.116 I llama_new_context_with_model: graph nodes  = 967
0.00.085.116 I llama_new_context_with_model: graph splits = 2
0.00.085.118 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.085.118 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.946.823 I 
0.00.946.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.946.862 I perplexity: tokenizing the input ..
0.00.954.629 I perplexity: tokenization took 7.766 ms
0.00.954.633 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.079.440 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.080.716 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.080.736 I llama_perf_context_print:        load time =     934.91 ms
0.01.080.742 I llama_perf_context_print: prompt eval time =     124.53 ms /   128 tokens (    0.97 ms per token,  1027.86 tokens per second)
0.01.080.743 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.080.743 I llama_perf_context_print:       total time =     133.91 ms /   129 tokens
0.01.081.299 I ggml_metal_free: deallocating

real	0m1.100s
user	0m0.095s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.016.477 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.962 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.025.967 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.968 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.970 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.971 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.971 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.971 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.973 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.973 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.973 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.973 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.974 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.974 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.976 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.978 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.978 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.979 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.960 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.962 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.962 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.963 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.963 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.963 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.035.964 I llama_model_loader: - type  f32:  194 tensors
0.00.035.964 I llama_model_loader: - type q4_0:   97 tensors
0.00.035.964 I llama_model_loader: - type q6_K:    1 tensors
0.00.063.785 I llm_load_vocab: special tokens cache size = 25
0.00.072.252 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.255 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.255 I llm_load_print_meta: arch             = gptneox
0.00.072.256 I llm_load_print_meta: vocab type       = BPE
0.00.072.256 I llm_load_print_meta: n_vocab          = 50304
0.00.072.256 I llm_load_print_meta: n_merges         = 50009
0.00.072.256 I llm_load_print_meta: vocab_only       = 0
0.00.072.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.257 I llm_load_print_meta: n_embd           = 2048
0.00.072.257 I llm_load_print_meta: n_layer          = 24
0.00.072.260 I llm_load_print_meta: n_head           = 16
0.00.072.261 I llm_load_print_meta: n_head_kv        = 16
0.00.072.261 I llm_load_print_meta: n_rot            = 32
0.00.072.262 I llm_load_print_meta: n_swa            = 0
0.00.072.262 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.262 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.263 I llm_load_print_meta: n_gqa            = 1
0.00.072.264 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.265 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.265 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.266 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.266 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.266 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.266 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.267 I llm_load_print_meta: n_ff             = 8192
0.00.072.267 I llm_load_print_meta: n_expert         = 0
0.00.072.271 I llm_load_print_meta: n_expert_used    = 0
0.00.072.271 I llm_load_print_meta: causal attn      = 1
0.00.072.271 I llm_load_print_meta: pooling type     = 0
0.00.072.271 I llm_load_print_meta: rope type        = 2
0.00.072.271 I llm_load_print_meta: rope scaling     = linear
0.00.072.272 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.272 I llm_load_print_meta: freq_scale_train = 1
0.00.072.272 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.273 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.273 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.273 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.273 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.273 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.273 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.274 I llm_load_print_meta: model type       = 1.4B
0.00.072.274 I llm_load_print_meta: model ftype      = Q4_0
0.00.072.275 I llm_load_print_meta: model params     = 1.41 B
0.00.072.279 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.072.280 I llm_load_print_meta: general.name     = 1.4B
0.00.072.280 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.282 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.282 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.282 I llm_load_print_meta: LF token         = 128 ''
0.00.072.282 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.282 I llm_load_print_meta: max token length = 1024
0.00.075.035 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.075.035 I llm_load_tensors: offloading output layer to GPU
0.00.075.035 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.075.049 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.075.050 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.076.460 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.461 I llama_new_context_with_model: n_ctx         = 2048
0.00.076.461 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.076.462 I llama_new_context_with_model: n_batch       = 2048
0.00.076.462 I llama_new_context_with_model: n_ubatch      = 512
0.00.076.462 I llama_new_context_with_model: flash_attn    = 0
0.00.076.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.463 I llama_new_context_with_model: freq_scale    = 1
0.00.076.464 I ggml_metal_init: allocating
0.00.076.472 I ggml_metal_init: found device: Apple M4
0.00.076.475 I ggml_metal_init: picking default device: Apple M4
0.00.077.413 I ggml_metal_init: using embedded metal library
0.00.081.343 I ggml_metal_init: GPU name:   Apple M4
0.00.081.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.081.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.081.346 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.081.347 I ggml_metal_init: simdgroup reduction   = true
0.00.081.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.081.347 I ggml_metal_init: has bfloat            = true
0.00.081.347 I ggml_metal_init: use bfloat            = true
0.00.081.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.081.349 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.894 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.119.059 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.067 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.091 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.136 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.138 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.138 I llama_new_context_with_model: graph nodes  = 967
0.00.120.139 I llama_new_context_with_model: graph splits = 2
0.00.120.143 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.120.287 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.534 I main: llama threadpool init, n_threads = 4
0.00.721.575 I 
0.00.721.598 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.721.598 I 
0.00.721.835 I sampler seed: 1234
0.00.721.841 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.721.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.721.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.721.889 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.399.959 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.399.960 I llama_perf_context_print:        load time =     705.05 ms
0.01.399.961 I llama_perf_context_print: prompt eval time =      44.27 ms /     7 tokens (    6.32 ms per token,   158.12 tokens per second)
0.01.399.961 I llama_perf_context_print:        eval time =     630.86 ms /    63 runs   (   10.01 ms per token,    99.86 tokens per second)
0.01.399.963 I llama_perf_context_print:       total time =     678.43 ms /    70 tokens
0.01.400.192 I ggml_metal_free: deallocating

real	0m1.426s
user	0m0.127s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.027 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.983 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.990 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.991 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.991 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.993 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.993 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.994 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.892 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.023 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.886 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.886 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.887 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.887 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.887 I llama_model_loader: - type  f32:  194 tensors
0.00.024.888 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.888 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.952 I llm_load_vocab: special tokens cache size = 25
0.00.051.905 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.908 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.908 I llm_load_print_meta: arch             = gptneox
0.00.051.908 I llm_load_print_meta: vocab type       = BPE
0.00.051.909 I llm_load_print_meta: n_vocab          = 50304
0.00.051.909 I llm_load_print_meta: n_merges         = 50009
0.00.051.909 I llm_load_print_meta: vocab_only       = 0
0.00.051.909 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.909 I llm_load_print_meta: n_embd           = 2048
0.00.051.910 I llm_load_print_meta: n_layer          = 24
0.00.051.912 I llm_load_print_meta: n_head           = 16
0.00.051.913 I llm_load_print_meta: n_head_kv        = 16
0.00.051.913 I llm_load_print_meta: n_rot            = 32
0.00.051.914 I llm_load_print_meta: n_swa            = 0
0.00.051.914 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.914 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.915 I llm_load_print_meta: n_gqa            = 1
0.00.051.916 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.916 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.917 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.917 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.917 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.917 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.917 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.918 I llm_load_print_meta: n_ff             = 8192
0.00.051.918 I llm_load_print_meta: n_expert         = 0
0.00.051.918 I llm_load_print_meta: n_expert_used    = 0
0.00.051.919 I llm_load_print_meta: causal attn      = 1
0.00.051.919 I llm_load_print_meta: pooling type     = 0
0.00.051.919 I llm_load_print_meta: rope type        = 2
0.00.051.919 I llm_load_print_meta: rope scaling     = linear
0.00.051.920 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.920 I llm_load_print_meta: freq_scale_train = 1
0.00.051.920 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.920 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.921 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.921 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.921 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.921 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.921 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.921 I llm_load_print_meta: model type       = 1.4B
0.00.051.922 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.922 I llm_load_print_meta: model params     = 1.41 B
0.00.051.925 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.925 I llm_load_print_meta: general.name     = 1.4B
0.00.051.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.926 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: LF token         = 128 ''
0.00.051.927 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.927 I llm_load_print_meta: max token length = 1024
0.00.053.890 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.890 I llm_load_tensors: offloading output layer to GPU
0.00.053.890 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.901 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.902 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.795 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.796 I llama_new_context_with_model: n_ctx         = 128
0.00.054.796 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.796 I llama_new_context_with_model: n_batch       = 128
0.00.054.796 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.797 I llama_new_context_with_model: flash_attn    = 0
0.00.054.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.797 I llama_new_context_with_model: freq_scale    = 1
0.00.054.798 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.798 I ggml_metal_init: allocating
0.00.054.801 I ggml_metal_init: found device: Apple M4
0.00.054.803 I ggml_metal_init: picking default device: Apple M4
0.00.055.373 I ggml_metal_init: using embedded metal library
0.00.057.713 I ggml_metal_init: GPU name:   Apple M4
0.00.057.715 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.715 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.715 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.715 I ggml_metal_init: simdgroup reduction   = true
0.00.057.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.716 I ggml_metal_init: has bfloat            = true
0.00.057.716 I ggml_metal_init: use bfloat            = true
0.00.057.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.717 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.643 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.929 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.933 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.950 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.803 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.804 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.804 I llama_new_context_with_model: graph nodes  = 967
0.00.069.805 I llama_new_context_with_model: graph splits = 2
0.00.069.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.806 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.603.975 I 
0.00.604.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.604.039 I perplexity: tokenizing the input ..
0.00.611.692 I perplexity: tokenization took 7.651 ms
0.00.611.696 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.734.376 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.735.501 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.735.519 I llama_perf_context_print:        load time =     593.94 ms
0.00.735.520 I llama_perf_context_print: prompt eval time =     122.45 ms /   128 tokens (    0.96 ms per token,  1045.29 tokens per second)
0.00.735.521 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.521 I llama_perf_context_print:       total time =     131.55 ms /   129 tokens
0.00.735.964 I ggml_metal_free: deallocating

real	0m0.751s
user	0m0.079s
sys	0m0.094s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.082 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.944 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.018.948 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.949 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.950 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.950 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.951 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.951 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.952 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.952 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.952 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.953 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.953 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.953 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.954 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.957 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.957 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.957 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.936 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.037 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.986 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.987 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.987 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.988 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.988 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.988 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.027.989 I llama_model_loader: - type  f32:  194 tensors
0.00.027.989 I llama_model_loader: - type q4_1:   97 tensors
0.00.027.990 I llama_model_loader: - type q6_K:    1 tensors
0.00.048.565 I llm_load_vocab: special tokens cache size = 25
0.00.054.472 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.054.475 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.054.476 I llm_load_print_meta: arch             = gptneox
0.00.054.476 I llm_load_print_meta: vocab type       = BPE
0.00.054.476 I llm_load_print_meta: n_vocab          = 50304
0.00.054.476 I llm_load_print_meta: n_merges         = 50009
0.00.054.477 I llm_load_print_meta: vocab_only       = 0
0.00.054.477 I llm_load_print_meta: n_ctx_train      = 2048
0.00.054.477 I llm_load_print_meta: n_embd           = 2048
0.00.054.477 I llm_load_print_meta: n_layer          = 24
0.00.054.480 I llm_load_print_meta: n_head           = 16
0.00.054.481 I llm_load_print_meta: n_head_kv        = 16
0.00.054.481 I llm_load_print_meta: n_rot            = 32
0.00.054.481 I llm_load_print_meta: n_swa            = 0
0.00.054.482 I llm_load_print_meta: n_embd_head_k    = 128
0.00.054.482 I llm_load_print_meta: n_embd_head_v    = 128
0.00.054.483 I llm_load_print_meta: n_gqa            = 1
0.00.054.483 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.054.484 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.054.484 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.054.485 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.054.485 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.054.485 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.054.485 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.054.488 I llm_load_print_meta: n_ff             = 8192
0.00.054.488 I llm_load_print_meta: n_expert         = 0
0.00.054.489 I llm_load_print_meta: n_expert_used    = 0
0.00.054.490 I llm_load_print_meta: causal attn      = 1
0.00.054.492 I llm_load_print_meta: pooling type     = 0
0.00.054.492 I llm_load_print_meta: rope type        = 2
0.00.054.492 I llm_load_print_meta: rope scaling     = linear
0.00.054.493 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.054.493 I llm_load_print_meta: freq_scale_train = 1
0.00.054.493 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.054.493 I llm_load_print_meta: rope_finetuned   = unknown
0.00.054.494 I llm_load_print_meta: ssm_d_conv       = 0
0.00.054.494 I llm_load_print_meta: ssm_d_inner      = 0
0.00.054.494 I llm_load_print_meta: ssm_d_state      = 0
0.00.054.494 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.054.494 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.054.494 I llm_load_print_meta: model type       = 1.4B
0.00.054.495 I llm_load_print_meta: model ftype      = Q4_1
0.00.054.498 I llm_load_print_meta: model params     = 1.41 B
0.00.054.499 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.054.499 I llm_load_print_meta: general.name     = 1.4B
0.00.054.499 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.054.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.054.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.054.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.054.501 I llm_load_print_meta: LF token         = 128 ''
0.00.054.502 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.054.502 I llm_load_print_meta: max token length = 1024
0.00.056.539 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.056.539 I llm_load_tensors: offloading output layer to GPU
0.00.056.539 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.056.550 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.056.551 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.057.487 I llama_new_context_with_model: n_seq_max     = 1
0.00.057.487 I llama_new_context_with_model: n_ctx         = 2048
0.00.057.488 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.057.488 I llama_new_context_with_model: n_batch       = 2048
0.00.057.488 I llama_new_context_with_model: n_ubatch      = 512
0.00.057.488 I llama_new_context_with_model: flash_attn    = 0
0.00.057.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.057.489 I llama_new_context_with_model: freq_scale    = 1
0.00.057.489 I ggml_metal_init: allocating
0.00.057.496 I ggml_metal_init: found device: Apple M4
0.00.057.499 I ggml_metal_init: picking default device: Apple M4
0.00.058.128 I ggml_metal_init: using embedded metal library
0.00.060.521 I ggml_metal_init: GPU name:   Apple M4
0.00.060.522 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.523 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.523 I ggml_metal_init: simdgroup reduction   = true
0.00.060.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.524 I ggml_metal_init: has bfloat            = true
0.00.060.525 I ggml_metal_init: use bfloat            = true
0.00.060.526 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.527 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.282 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.567 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.573 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.595 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.608 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.609 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.610 I llama_new_context_with_model: graph nodes  = 967
0.00.091.610 I llama_new_context_with_model: graph splits = 2
0.00.091.613 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.746 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.730.273 I main: llama threadpool init, n_threads = 4
0.00.730.312 I 
0.00.730.334 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.730.334 I 
0.00.730.566 I sampler seed: 1234
0.00.730.570 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.730.607 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.730.609 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.730.609 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.462.384 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66046.51 tokens per second)
0.01.462.385 I llama_perf_context_print:        load time =     721.19 ms
0.01.462.385 I llama_perf_context_print: prompt eval time =      43.60 ms /     7 tokens (    6.23 ms per token,   160.54 tokens per second)
0.01.462.386 I llama_perf_context_print:        eval time =     685.33 ms /    63 runs   (   10.88 ms per token,    91.93 tokens per second)
0.01.462.387 I llama_perf_context_print:       total time =     732.12 ms /    70 tokens
0.01.462.621 I ggml_metal_free: deallocating

real	0m1.479s
user	0m0.111s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.852 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.803 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.808 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.809 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.810 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.810 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.811 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.811 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.813 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.813 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.813 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.814 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.814 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.816 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.816 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.816 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.650 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.651 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.651 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.651 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.652 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.652 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.653 I llama_model_loader: - type  f32:  194 tensors
0.00.023.653 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.653 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.068 I llm_load_vocab: special tokens cache size = 25
0.00.050.222 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.225 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.225 I llm_load_print_meta: arch             = gptneox
0.00.050.225 I llm_load_print_meta: vocab type       = BPE
0.00.050.226 I llm_load_print_meta: n_vocab          = 50304
0.00.050.226 I llm_load_print_meta: n_merges         = 50009
0.00.050.226 I llm_load_print_meta: vocab_only       = 0
0.00.050.226 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.226 I llm_load_print_meta: n_embd           = 2048
0.00.050.226 I llm_load_print_meta: n_layer          = 24
0.00.050.229 I llm_load_print_meta: n_head           = 16
0.00.050.230 I llm_load_print_meta: n_head_kv        = 16
0.00.050.230 I llm_load_print_meta: n_rot            = 32
0.00.050.230 I llm_load_print_meta: n_swa            = 0
0.00.050.231 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.231 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.231 I llm_load_print_meta: n_gqa            = 1
0.00.050.232 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.236 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.236 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.238 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.238 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.238 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.238 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.239 I llm_load_print_meta: n_ff             = 8192
0.00.050.239 I llm_load_print_meta: n_expert         = 0
0.00.050.239 I llm_load_print_meta: n_expert_used    = 0
0.00.050.239 I llm_load_print_meta: causal attn      = 1
0.00.050.240 I llm_load_print_meta: pooling type     = 0
0.00.050.240 I llm_load_print_meta: rope type        = 2
0.00.050.244 I llm_load_print_meta: rope scaling     = linear
0.00.050.245 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.245 I llm_load_print_meta: freq_scale_train = 1
0.00.050.245 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.245 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.246 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.246 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.246 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.246 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.246 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.246 I llm_load_print_meta: model type       = 1.4B
0.00.050.247 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.247 I llm_load_print_meta: model params     = 1.41 B
0.00.050.248 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.248 I llm_load_print_meta: general.name     = 1.4B
0.00.050.248 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.248 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.249 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.250 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.250 I llm_load_print_meta: LF token         = 128 ''
0.00.050.250 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.251 I llm_load_print_meta: max token length = 1024
0.00.052.223 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.223 I llm_load_tensors: offloading output layer to GPU
0.00.052.223 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.234 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.235 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.140 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.141 I llama_new_context_with_model: n_ctx         = 128
0.00.053.141 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.141 I llama_new_context_with_model: n_batch       = 128
0.00.053.141 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.142 I llama_new_context_with_model: flash_attn    = 0
0.00.053.142 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.142 I llama_new_context_with_model: freq_scale    = 1
0.00.053.143 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.143 I ggml_metal_init: allocating
0.00.053.147 I ggml_metal_init: found device: Apple M4
0.00.053.149 I ggml_metal_init: picking default device: Apple M4
0.00.053.709 I ggml_metal_init: using embedded metal library
0.00.056.018 I ggml_metal_init: GPU name:   Apple M4
0.00.056.020 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.020 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.021 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.021 I ggml_metal_init: simdgroup reduction   = true
0.00.056.021 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.021 I ggml_metal_init: has bfloat            = true
0.00.056.021 I ggml_metal_init: use bfloat            = true
0.00.056.022 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.022 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.764 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.992 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.997 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.012 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.910 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.911 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.911 I llama_new_context_with_model: graph nodes  = 967
0.00.067.911 I llama_new_context_with_model: graph splits = 2
0.00.067.912 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.913 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.683.005 I 
0.00.683.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.683.059 I perplexity: tokenizing the input ..
0.00.690.823 I perplexity: tokenization took 7.762 ms
0.00.690.826 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.237 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.813.662 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.813.678 I llama_perf_context_print:        load time =     674.15 ms
0.00.813.678 I llama_perf_context_print: prompt eval time =     121.17 ms /   128 tokens (    0.95 ms per token,  1056.36 tokens per second)
0.00.813.679 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.679 I llama_perf_context_print:       total time =     130.67 ms /   129 tokens
0.00.814.040 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.079s
sys	0m0.111s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.250 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.026.905 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.910 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.910 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.911 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.911 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.911 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.912 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.912 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.913 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.913 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.913 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.915 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.060 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.191 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.371 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.372 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.373 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.373 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.373 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.373 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.036.374 I llama_model_loader: - type  f32:  194 tensors
0.00.036.374 I llama_model_loader: - type q5_0:   97 tensors
0.00.036.374 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.161 I llm_load_vocab: special tokens cache size = 25
0.00.065.109 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.112 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.112 I llm_load_print_meta: arch             = gptneox
0.00.065.112 I llm_load_print_meta: vocab type       = BPE
0.00.065.112 I llm_load_print_meta: n_vocab          = 50304
0.00.065.112 I llm_load_print_meta: n_merges         = 50009
0.00.065.113 I llm_load_print_meta: vocab_only       = 0
0.00.065.113 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.113 I llm_load_print_meta: n_embd           = 2048
0.00.065.113 I llm_load_print_meta: n_layer          = 24
0.00.065.116 I llm_load_print_meta: n_head           = 16
0.00.065.117 I llm_load_print_meta: n_head_kv        = 16
0.00.065.117 I llm_load_print_meta: n_rot            = 32
0.00.065.120 I llm_load_print_meta: n_swa            = 0
0.00.065.120 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.120 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.121 I llm_load_print_meta: n_gqa            = 1
0.00.065.122 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.122 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.123 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.123 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.123 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.124 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.124 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.126 I llm_load_print_meta: n_ff             = 8192
0.00.065.126 I llm_load_print_meta: n_expert         = 0
0.00.065.126 I llm_load_print_meta: n_expert_used    = 0
0.00.065.127 I llm_load_print_meta: causal attn      = 1
0.00.065.128 I llm_load_print_meta: pooling type     = 0
0.00.065.128 I llm_load_print_meta: rope type        = 2
0.00.065.128 I llm_load_print_meta: rope scaling     = linear
0.00.065.129 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.129 I llm_load_print_meta: freq_scale_train = 1
0.00.065.129 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.129 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.129 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.129 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.130 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.130 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.130 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.130 I llm_load_print_meta: model type       = 1.4B
0.00.065.130 I llm_load_print_meta: model ftype      = Q5_0
0.00.065.131 I llm_load_print_meta: model params     = 1.41 B
0.00.065.131 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.065.131 I llm_load_print_meta: general.name     = 1.4B
0.00.065.131 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.132 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.132 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.132 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.132 I llm_load_print_meta: LF token         = 128 ''
0.00.065.132 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.132 I llm_load_print_meta: max token length = 1024
0.00.067.168 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.168 I llm_load_tensors: offloading output layer to GPU
0.00.067.169 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.179 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.067.180 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.068.104 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.105 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.105 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.105 I llama_new_context_with_model: n_batch       = 2048
0.00.068.106 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.106 I llama_new_context_with_model: flash_attn    = 0
0.00.068.106 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.107 I llama_new_context_with_model: freq_scale    = 1
0.00.068.107 I ggml_metal_init: allocating
0.00.068.114 I ggml_metal_init: found device: Apple M4
0.00.068.117 I ggml_metal_init: picking default device: Apple M4
0.00.068.716 I ggml_metal_init: using embedded metal library
0.00.071.093 I ggml_metal_init: GPU name:   Apple M4
0.00.071.095 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.095 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.096 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.096 I ggml_metal_init: simdgroup reduction   = true
0.00.071.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.096 I ggml_metal_init: has bfloat            = true
0.00.071.096 I ggml_metal_init: use bfloat            = true
0.00.071.097 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.098 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.711 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.823 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.830 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.847 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.923 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.924 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.925 I llama_new_context_with_model: graph nodes  = 967
0.00.102.925 I llama_new_context_with_model: graph splits = 2
0.00.102.929 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.369.051 I main: llama threadpool init, n_threads = 4
0.01.369.147 I 
0.01.369.205 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.369.206 I 
0.01.369.836 I sampler seed: 1234
0.01.369.843 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.369.922 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.369.927 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.369.928 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.151.678 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.02.151.678 I llama_perf_context_print:        load time =    1359.78 ms
0.02.151.679 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.15 tokens per second)
0.02.151.680 I llama_perf_context_print:        eval time =     735.00 ms /    63 runs   (   11.67 ms per token,    85.71 tokens per second)
0.02.151.684 I llama_perf_context_print:       total time =     782.65 ms /    70 tokens
0.02.151.987 I ggml_metal_free: deallocating

real	0m2.168s
user	0m0.124s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.721 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.875 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.877 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.880 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.882 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.884 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.886 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.886 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.955 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.955 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.956 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.956 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.956 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.029.957 I llama_model_loader: - type  f32:  194 tensors
0.00.029.957 I llama_model_loader: - type q5_0:   97 tensors
0.00.029.957 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.183 I llm_load_vocab: special tokens cache size = 25
0.00.058.227 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.232 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.233 I llm_load_print_meta: arch             = gptneox
0.00.058.233 I llm_load_print_meta: vocab type       = BPE
0.00.058.233 I llm_load_print_meta: n_vocab          = 50304
0.00.058.237 I llm_load_print_meta: n_merges         = 50009
0.00.058.237 I llm_load_print_meta: vocab_only       = 0
0.00.058.237 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.237 I llm_load_print_meta: n_embd           = 2048
0.00.058.237 I llm_load_print_meta: n_layer          = 24
0.00.058.241 I llm_load_print_meta: n_head           = 16
0.00.058.242 I llm_load_print_meta: n_head_kv        = 16
0.00.058.242 I llm_load_print_meta: n_rot            = 32
0.00.058.242 I llm_load_print_meta: n_swa            = 0
0.00.058.242 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.242 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.243 I llm_load_print_meta: n_gqa            = 1
0.00.058.244 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.245 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.246 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.246 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.246 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.247 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.247 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.247 I llm_load_print_meta: n_ff             = 8192
0.00.058.248 I llm_load_print_meta: n_expert         = 0
0.00.058.248 I llm_load_print_meta: n_expert_used    = 0
0.00.058.248 I llm_load_print_meta: causal attn      = 1
0.00.058.248 I llm_load_print_meta: pooling type     = 0
0.00.058.248 I llm_load_print_meta: rope type        = 2
0.00.058.248 I llm_load_print_meta: rope scaling     = linear
0.00.058.250 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.250 I llm_load_print_meta: freq_scale_train = 1
0.00.058.250 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.250 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.250 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.251 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.251 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.251 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.251 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.251 I llm_load_print_meta: model type       = 1.4B
0.00.058.251 I llm_load_print_meta: model ftype      = Q5_0
0.00.058.253 I llm_load_print_meta: model params     = 1.41 B
0.00.058.253 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.058.254 I llm_load_print_meta: general.name     = 1.4B
0.00.058.254 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.254 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.254 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.254 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.254 I llm_load_print_meta: LF token         = 128 ''
0.00.058.255 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.255 I llm_load_print_meta: max token length = 1024
0.00.060.253 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.253 I llm_load_tensors: offloading output layer to GPU
0.00.060.254 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.265 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.060.266 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.061.179 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.179 I llama_new_context_with_model: n_ctx         = 128
0.00.061.180 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.061.180 I llama_new_context_with_model: n_batch       = 128
0.00.061.180 I llama_new_context_with_model: n_ubatch      = 128
0.00.061.180 I llama_new_context_with_model: flash_attn    = 0
0.00.061.181 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.181 I llama_new_context_with_model: freq_scale    = 1
0.00.061.181 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.061.182 I ggml_metal_init: allocating
0.00.061.185 I ggml_metal_init: found device: Apple M4
0.00.061.187 I ggml_metal_init: picking default device: Apple M4
0.00.061.747 I ggml_metal_init: using embedded metal library
0.00.064.108 I ggml_metal_init: GPU name:   Apple M4
0.00.064.109 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.109 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.110 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.110 I ggml_metal_init: simdgroup reduction   = true
0.00.064.110 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.111 I ggml_metal_init: has bfloat            = true
0.00.064.111 I ggml_metal_init: use bfloat            = true
0.00.064.111 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.112 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.490 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.075.918 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.924 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.940 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.792 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.793 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.794 I llama_new_context_with_model: graph nodes  = 967
0.00.076.794 I llama_new_context_with_model: graph splits = 2
0.00.076.795 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.076.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.433 I 
0.00.705.457 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.471 I perplexity: tokenizing the input ..
0.00.713.170 I perplexity: tokenization took 7.698 ms
0.00.713.175 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.330 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.849.574 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.849.590 I llama_perf_context_print:        load time =     693.71 ms
0.00.849.590 I llama_perf_context_print: prompt eval time =     134.93 ms /   128 tokens (    1.05 ms per token,   948.63 tokens per second)
0.00.849.593 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.849.593 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.850.054 I ggml_metal_free: deallocating

real	0m0.866s
user	0m0.084s
sys	0m0.111s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.010.615 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.455 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.457 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.458 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.458 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.459 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.460 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.460 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.460 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.461 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.461 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.464 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.405 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.523 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.482 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.483 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.484 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.484 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.485 I llama_model_loader: - type  f32:  194 tensors
0.00.026.485 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.828 I llm_load_vocab: special tokens cache size = 25
0.00.053.752 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.755 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.755 I llm_load_print_meta: arch             = gptneox
0.00.053.755 I llm_load_print_meta: vocab type       = BPE
0.00.053.756 I llm_load_print_meta: n_vocab          = 50304
0.00.053.756 I llm_load_print_meta: n_merges         = 50009
0.00.053.756 I llm_load_print_meta: vocab_only       = 0
0.00.053.756 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.756 I llm_load_print_meta: n_embd           = 2048
0.00.053.756 I llm_load_print_meta: n_layer          = 24
0.00.053.759 I llm_load_print_meta: n_head           = 16
0.00.053.760 I llm_load_print_meta: n_head_kv        = 16
0.00.053.761 I llm_load_print_meta: n_rot            = 32
0.00.053.761 I llm_load_print_meta: n_swa            = 0
0.00.053.761 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.761 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.762 I llm_load_print_meta: n_gqa            = 1
0.00.053.764 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.765 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.765 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.766 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.766 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.766 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.766 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.767 I llm_load_print_meta: n_ff             = 8192
0.00.053.767 I llm_load_print_meta: n_expert         = 0
0.00.053.767 I llm_load_print_meta: n_expert_used    = 0
0.00.053.769 I llm_load_print_meta: causal attn      = 1
0.00.053.771 I llm_load_print_meta: pooling type     = 0
0.00.053.771 I llm_load_print_meta: rope type        = 2
0.00.053.771 I llm_load_print_meta: rope scaling     = linear
0.00.053.772 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.772 I llm_load_print_meta: freq_scale_train = 1
0.00.053.772 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.772 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.772 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.773 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.773 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.773 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.773 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.773 I llm_load_print_meta: model type       = 1.4B
0.00.053.774 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.774 I llm_load_print_meta: model params     = 1.41 B
0.00.053.775 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.775 I llm_load_print_meta: general.name     = 1.4B
0.00.053.775 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.775 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.776 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.776 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.776 I llm_load_print_meta: LF token         = 128 ''
0.00.053.776 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.776 I llm_load_print_meta: max token length = 1024
0.00.055.945 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.945 I llm_load_tensors: offloading output layer to GPU
0.00.055.945 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.956 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.957 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.957 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.958 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.958 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.958 I llama_new_context_with_model: n_batch       = 2048
0.00.056.958 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.959 I llama_new_context_with_model: flash_attn    = 0
0.00.056.959 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.959 I llama_new_context_with_model: freq_scale    = 1
0.00.056.960 I ggml_metal_init: allocating
0.00.056.963 I ggml_metal_init: found device: Apple M4
0.00.056.965 I ggml_metal_init: picking default device: Apple M4
0.00.057.573 I ggml_metal_init: using embedded metal library
0.00.059.990 I ggml_metal_init: GPU name:   Apple M4
0.00.059.992 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.992 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.993 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.993 I ggml_metal_init: simdgroup reduction   = true
0.00.059.994 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.994 I ggml_metal_init: has bfloat            = true
0.00.059.994 I ggml_metal_init: use bfloat            = true
0.00.059.995 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.995 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.070.080 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.552 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.557 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.574 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.616 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.618 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.618 I llama_new_context_with_model: graph nodes  = 967
0.00.092.619 I llama_new_context_with_model: graph splits = 2
0.00.092.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.763 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.359 I main: llama threadpool init, n_threads = 4
0.00.715.395 I 
0.00.715.417 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.417 I 
0.00.715.641 I sampler seed: 1234
0.00.715.646 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.715.691 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.715.696 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.715.696 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.555.243 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.555.243 I llama_perf_context_print:        load time =     704.74 ms
0.01.555.245 I llama_perf_context_print: prompt eval time =      42.26 ms /     7 tokens (    6.04 ms per token,   165.66 tokens per second)
0.01.555.246 I llama_perf_context_print:        eval time =     794.24 ms /    63 runs   (   12.61 ms per token,    79.32 tokens per second)
0.01.555.246 I llama_perf_context_print:       total time =     839.89 ms /    70 tokens
0.01.555.495 I ggml_metal_free: deallocating

real	0m1.573s
user	0m0.112s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.722 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.524 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.528 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.530 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.536 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.537 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.538 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.538 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.540 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.541 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.541 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.541 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.543 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.543 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.544 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.498 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.603 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.532 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.532 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.533 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.534 I llama_model_loader: - type  f32:  194 tensors
0.00.024.534 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.534 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.319 I llm_load_vocab: special tokens cache size = 25
0.00.051.263 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.265 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.266 I llm_load_print_meta: arch             = gptneox
0.00.051.266 I llm_load_print_meta: vocab type       = BPE
0.00.051.266 I llm_load_print_meta: n_vocab          = 50304
0.00.051.267 I llm_load_print_meta: n_merges         = 50009
0.00.051.267 I llm_load_print_meta: vocab_only       = 0
0.00.051.267 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.267 I llm_load_print_meta: n_embd           = 2048
0.00.051.267 I llm_load_print_meta: n_layer          = 24
0.00.051.271 I llm_load_print_meta: n_head           = 16
0.00.051.272 I llm_load_print_meta: n_head_kv        = 16
0.00.051.272 I llm_load_print_meta: n_rot            = 32
0.00.051.272 I llm_load_print_meta: n_swa            = 0
0.00.051.272 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.272 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.273 I llm_load_print_meta: n_gqa            = 1
0.00.051.274 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.275 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.275 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.277 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.277 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.278 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.278 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.279 I llm_load_print_meta: n_ff             = 8192
0.00.051.279 I llm_load_print_meta: n_expert         = 0
0.00.051.279 I llm_load_print_meta: n_expert_used    = 0
0.00.051.279 I llm_load_print_meta: causal attn      = 1
0.00.051.279 I llm_load_print_meta: pooling type     = 0
0.00.051.279 I llm_load_print_meta: rope type        = 2
0.00.051.280 I llm_load_print_meta: rope scaling     = linear
0.00.051.280 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.280 I llm_load_print_meta: freq_scale_train = 1
0.00.051.280 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.281 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.281 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.281 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.281 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.281 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.282 I llm_load_print_meta: model type       = 1.4B
0.00.051.282 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.282 I llm_load_print_meta: model params     = 1.41 B
0.00.051.283 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.283 I llm_load_print_meta: general.name     = 1.4B
0.00.051.284 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.284 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.284 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.284 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.284 I llm_load_print_meta: LF token         = 128 ''
0.00.051.285 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.285 I llm_load_print_meta: max token length = 1024
0.00.053.323 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.323 I llm_load_tensors: offloading output layer to GPU
0.00.053.323 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.334 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.335 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.264 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.265 I llama_new_context_with_model: n_ctx         = 128
0.00.054.265 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.266 I llama_new_context_with_model: n_batch       = 128
0.00.054.266 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.266 I llama_new_context_with_model: flash_attn    = 0
0.00.054.266 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.267 I llama_new_context_with_model: freq_scale    = 1
0.00.054.267 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.268 I ggml_metal_init: allocating
0.00.054.274 I ggml_metal_init: found device: Apple M4
0.00.054.276 I ggml_metal_init: picking default device: Apple M4
0.00.054.839 I ggml_metal_init: using embedded metal library
0.00.057.227 I ggml_metal_init: GPU name:   Apple M4
0.00.057.229 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.229 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.230 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.230 I ggml_metal_init: simdgroup reduction   = true
0.00.057.230 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.230 I ggml_metal_init: has bfloat            = true
0.00.057.230 I ggml_metal_init: use bfloat            = true
0.00.057.231 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.231 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.874 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.217 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.220 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.236 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.100 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.101 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.102 I llama_new_context_with_model: graph nodes  = 967
0.00.069.102 I llama_new_context_with_model: graph splits = 2
0.00.069.103 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.103 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.358 I 
0.00.643.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.401 I perplexity: tokenizing the input ..
0.00.651.164 I perplexity: tokenization took 7.761 ms
0.00.651.168 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.786.203 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.787.354 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.787.372 I llama_perf_context_print:        load time =     633.63 ms
0.00.787.373 I llama_perf_context_print: prompt eval time =     134.81 ms /   128 tokens (    1.05 ms per token,   949.48 tokens per second)
0.00.787.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.787.374 I llama_perf_context_print:       total time =     144.02 ms /   129 tokens
0.00.787.854 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.079s
sys	0m0.109s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.613 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.202 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.203 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.204 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.204 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.206 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.206 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.208 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.208 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.211 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.329 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.427 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.324 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.324 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.325 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.326 I llama_model_loader: - type  f32:  194 tensors
0.00.023.326 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.326 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.326 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.776 I llm_load_vocab: special tokens cache size = 25
0.00.049.774 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.777 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.777 I llm_load_print_meta: arch             = gptneox
0.00.049.778 I llm_load_print_meta: vocab type       = BPE
0.00.049.778 I llm_load_print_meta: n_vocab          = 50304
0.00.049.778 I llm_load_print_meta: n_merges         = 50009
0.00.049.778 I llm_load_print_meta: vocab_only       = 0
0.00.049.778 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.778 I llm_load_print_meta: n_embd           = 2048
0.00.049.779 I llm_load_print_meta: n_layer          = 24
0.00.049.781 I llm_load_print_meta: n_head           = 16
0.00.049.784 I llm_load_print_meta: n_head_kv        = 16
0.00.049.784 I llm_load_print_meta: n_rot            = 32
0.00.049.784 I llm_load_print_meta: n_swa            = 0
0.00.049.785 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.785 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.785 I llm_load_print_meta: n_gqa            = 1
0.00.049.786 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.787 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.787 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.788 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.788 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.788 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.788 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.789 I llm_load_print_meta: n_ff             = 8192
0.00.049.789 I llm_load_print_meta: n_expert         = 0
0.00.049.789 I llm_load_print_meta: n_expert_used    = 0
0.00.049.789 I llm_load_print_meta: causal attn      = 1
0.00.049.789 I llm_load_print_meta: pooling type     = 0
0.00.049.790 I llm_load_print_meta: rope type        = 2
0.00.049.790 I llm_load_print_meta: rope scaling     = linear
0.00.049.792 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.792 I llm_load_print_meta: freq_scale_train = 1
0.00.049.792 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.793 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.793 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.793 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.793 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.793 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.793 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.794 I llm_load_print_meta: model type       = 1.4B
0.00.049.794 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.794 I llm_load_print_meta: model params     = 1.41 B
0.00.049.795 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.795 I llm_load_print_meta: general.name     = 1.4B
0.00.049.796 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.796 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.796 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.796 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.796 I llm_load_print_meta: LF token         = 128 ''
0.00.049.797 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.797 I llm_load_print_meta: max token length = 1024
0.00.051.716 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.716 I llm_load_tensors: offloading output layer to GPU
0.00.051.716 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.727 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.728 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.612 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.613 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.613 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.613 I llama_new_context_with_model: n_batch       = 2048
0.00.052.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.613 I llama_new_context_with_model: flash_attn    = 0
0.00.052.614 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.614 I llama_new_context_with_model: freq_scale    = 1
0.00.052.615 I ggml_metal_init: allocating
0.00.052.621 I ggml_metal_init: found device: Apple M4
0.00.052.623 I ggml_metal_init: picking default device: Apple M4
0.00.053.201 I ggml_metal_init: using embedded metal library
0.00.055.563 I ggml_metal_init: GPU name:   Apple M4
0.00.055.564 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.565 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.565 I ggml_metal_init: simdgroup reduction   = true
0.00.055.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.566 I ggml_metal_init: has bfloat            = true
0.00.055.566 I ggml_metal_init: use bfloat            = true
0.00.055.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.567 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.410 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.744 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.762 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.738 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.739 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.739 I llama_new_context_with_model: graph nodes  = 967
0.00.085.740 I llama_new_context_with_model: graph splits = 2
0.00.085.742 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.871 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.871 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.440.781 I main: llama threadpool init, n_threads = 4
0.00.440.827 I 
0.00.440.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.440.855 I 
0.00.441.104 I sampler seed: 1234
0.00.441.112 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.441.127 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.441.128 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.441.128 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.121.172 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64137.31 tokens per second)
0.01.121.173 I llama_perf_context_print:        load time =     432.16 ms
0.01.121.174 I llama_perf_context_print: prompt eval time =      35.83 ms /     7 tokens (    5.12 ms per token,   195.37 tokens per second)
0.01.121.174 I llama_perf_context_print:        eval time =     641.41 ms /    63 runs   (   10.18 ms per token,    98.22 tokens per second)
0.01.121.175 I llama_perf_context_print:       total time =     680.40 ms /    70 tokens
0.01.121.383 I ggml_metal_free: deallocating

real	0m1.137s
user	0m0.110s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.161 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.704 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.708 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.710 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.711 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.716 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.696 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.782 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.704 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.705 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.705 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.706 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.706 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.706 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.707 I llama_model_loader: - type  f32:  194 tensors
0.00.023.707 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.707 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.708 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.102 I llm_load_vocab: special tokens cache size = 25
0.00.050.087 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.090 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.090 I llm_load_print_meta: arch             = gptneox
0.00.050.091 I llm_load_print_meta: vocab type       = BPE
0.00.050.091 I llm_load_print_meta: n_vocab          = 50304
0.00.050.091 I llm_load_print_meta: n_merges         = 50009
0.00.050.091 I llm_load_print_meta: vocab_only       = 0
0.00.050.092 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.092 I llm_load_print_meta: n_embd           = 2048
0.00.050.092 I llm_load_print_meta: n_layer          = 24
0.00.050.095 I llm_load_print_meta: n_head           = 16
0.00.050.095 I llm_load_print_meta: n_head_kv        = 16
0.00.050.096 I llm_load_print_meta: n_rot            = 32
0.00.050.096 I llm_load_print_meta: n_swa            = 0
0.00.050.098 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.098 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.099 I llm_load_print_meta: n_gqa            = 1
0.00.050.100 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.101 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.101 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.102 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.102 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.102 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.102 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.103 I llm_load_print_meta: n_ff             = 8192
0.00.050.103 I llm_load_print_meta: n_expert         = 0
0.00.050.103 I llm_load_print_meta: n_expert_used    = 0
0.00.050.103 I llm_load_print_meta: causal attn      = 1
0.00.050.104 I llm_load_print_meta: pooling type     = 0
0.00.050.104 I llm_load_print_meta: rope type        = 2
0.00.050.104 I llm_load_print_meta: rope scaling     = linear
0.00.050.104 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.105 I llm_load_print_meta: freq_scale_train = 1
0.00.050.105 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.105 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.107 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.107 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.107 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.107 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.107 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.107 I llm_load_print_meta: model type       = 1.4B
0.00.050.108 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.108 I llm_load_print_meta: model params     = 1.41 B
0.00.050.109 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.109 I llm_load_print_meta: general.name     = 1.4B
0.00.050.109 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.110 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.111 I llm_load_print_meta: LF token         = 128 ''
0.00.050.112 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.112 I llm_load_print_meta: max token length = 1024
0.00.052.016 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.016 I llm_load_tensors: offloading output layer to GPU
0.00.052.017 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.027 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.028 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.918 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.919 I llama_new_context_with_model: n_ctx         = 128
0.00.052.919 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.920 I llama_new_context_with_model: n_batch       = 128
0.00.052.920 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.920 I llama_new_context_with_model: flash_attn    = 0
0.00.052.920 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.920 I llama_new_context_with_model: freq_scale    = 1
0.00.052.921 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.921 I ggml_metal_init: allocating
0.00.052.924 I ggml_metal_init: found device: Apple M4
0.00.052.926 I ggml_metal_init: picking default device: Apple M4
0.00.053.462 I ggml_metal_init: using embedded metal library
0.00.055.786 I ggml_metal_init: GPU name:   Apple M4
0.00.055.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.788 I ggml_metal_init: simdgroup reduction   = true
0.00.055.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.788 I ggml_metal_init: has bfloat            = true
0.00.055.789 I ggml_metal_init: use bfloat            = true
0.00.055.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.789 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.486 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.760 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.764 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.780 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.625 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.626 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.627 I llama_new_context_with_model: graph nodes  = 967
0.00.067.627 I llama_new_context_with_model: graph splits = 2
0.00.067.628 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.628 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.383.274 I 
0.00.383.303 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.383.318 I perplexity: tokenizing the input ..
0.00.390.856 I perplexity: tokenization took 7.536 ms
0.00.390.859 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.523.808 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.525.074 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.525.096 I llama_perf_context_print:        load time =     374.11 ms
0.00.525.097 I llama_perf_context_print: prompt eval time =     132.72 ms /   128 tokens (    1.04 ms per token,   964.47 tokens per second)
0.00.525.098 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.525.103 I llama_perf_context_print:       total time =     141.82 ms /   129 tokens
0.00.525.462 I ggml_metal_free: deallocating

real	0m0.538s
user	0m0.078s
sys	0m0.066s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.825 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.260 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.271 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.271 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.271 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.272 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.272 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.272 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.273 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.275 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.276 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.213 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.064 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.064 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.065 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.065 I llama_model_loader: - type  f32:  194 tensors
0.00.024.066 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.066 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.066 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.066 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.657 I llm_load_vocab: special tokens cache size = 25
0.00.050.575 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.577 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.578 I llm_load_print_meta: arch             = gptneox
0.00.050.578 I llm_load_print_meta: vocab type       = BPE
0.00.050.578 I llm_load_print_meta: n_vocab          = 50304
0.00.050.578 I llm_load_print_meta: n_merges         = 50009
0.00.050.578 I llm_load_print_meta: vocab_only       = 0
0.00.050.579 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.579 I llm_load_print_meta: n_embd           = 2048
0.00.050.579 I llm_load_print_meta: n_layer          = 24
0.00.050.582 I llm_load_print_meta: n_head           = 16
0.00.050.583 I llm_load_print_meta: n_head_kv        = 16
0.00.050.583 I llm_load_print_meta: n_rot            = 32
0.00.050.583 I llm_load_print_meta: n_swa            = 0
0.00.050.583 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.584 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.584 I llm_load_print_meta: n_gqa            = 1
0.00.050.585 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.586 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.587 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.587 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.588 I llm_load_print_meta: n_ff             = 8192
0.00.050.590 I llm_load_print_meta: n_expert         = 0
0.00.050.591 I llm_load_print_meta: n_expert_used    = 0
0.00.050.591 I llm_load_print_meta: causal attn      = 1
0.00.050.591 I llm_load_print_meta: pooling type     = 0
0.00.050.591 I llm_load_print_meta: rope type        = 2
0.00.050.592 I llm_load_print_meta: rope scaling     = linear
0.00.050.592 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.593 I llm_load_print_meta: freq_scale_train = 1
0.00.050.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.593 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.593 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.593 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.593 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.594 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.594 I llm_load_print_meta: model type       = 1.4B
0.00.050.595 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.595 I llm_load_print_meta: model params     = 1.41 B
0.00.050.595 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.596 I llm_load_print_meta: general.name     = 1.4B
0.00.050.596 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.596 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.601 I llm_load_print_meta: LF token         = 128 ''
0.00.050.601 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.601 I llm_load_print_meta: max token length = 1024
0.00.052.583 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.583 I llm_load_tensors: offloading output layer to GPU
0.00.052.583 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.594 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.595 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.501 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.502 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.502 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.502 I llama_new_context_with_model: n_batch       = 2048
0.00.053.502 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.502 I llama_new_context_with_model: flash_attn    = 0
0.00.053.503 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.503 I llama_new_context_with_model: freq_scale    = 1
0.00.053.503 I ggml_metal_init: allocating
0.00.053.507 I ggml_metal_init: found device: Apple M4
0.00.053.509 I ggml_metal_init: picking default device: Apple M4
0.00.054.109 I ggml_metal_init: using embedded metal library
0.00.056.438 I ggml_metal_init: GPU name:   Apple M4
0.00.056.440 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.441 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.441 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.441 I ggml_metal_init: simdgroup reduction   = true
0.00.056.441 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.442 I ggml_metal_init: has bfloat            = true
0.00.056.442 I ggml_metal_init: use bfloat            = true
0.00.056.442 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.443 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.321 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.339 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.350 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.372 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.395 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.396 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.396 I llama_new_context_with_model: graph nodes  = 967
0.00.086.396 I llama_new_context_with_model: graph splits = 2
0.00.086.399 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.544 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.535.110 I main: llama threadpool init, n_threads = 4
0.00.535.149 I 
0.00.535.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.535.188 I 
0.00.535.426 I sampler seed: 1234
0.00.535.430 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.535.464 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.535.465 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.535.465 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.285.739 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.285.739 I llama_perf_context_print:        load time =     526.28 ms
0.01.285.740 I llama_perf_context_print: prompt eval time =      44.41 ms /     7 tokens (    6.34 ms per token,   157.63 tokens per second)
0.01.285.741 I llama_perf_context_print:        eval time =     702.82 ms /    63 runs   (   11.16 ms per token,    89.64 tokens per second)
0.01.285.741 I llama_perf_context_print:       total time =     750.63 ms /    70 tokens
0.01.285.946 I ggml_metal_free: deallocating

real	0m1.303s
user	0m0.110s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.965 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.685 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.686 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.576 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.600 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.601 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.601 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.602 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.602 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.603 I llama_model_loader: - type  f32:  194 tensors
0.00.025.603 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.603 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.604 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.604 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.141 I llm_load_vocab: special tokens cache size = 25
0.00.052.054 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.057 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.057 I llm_load_print_meta: arch             = gptneox
0.00.052.057 I llm_load_print_meta: vocab type       = BPE
0.00.052.058 I llm_load_print_meta: n_vocab          = 50304
0.00.052.058 I llm_load_print_meta: n_merges         = 50009
0.00.052.058 I llm_load_print_meta: vocab_only       = 0
0.00.052.058 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.058 I llm_load_print_meta: n_embd           = 2048
0.00.052.058 I llm_load_print_meta: n_layer          = 24
0.00.052.061 I llm_load_print_meta: n_head           = 16
0.00.052.062 I llm_load_print_meta: n_head_kv        = 16
0.00.052.062 I llm_load_print_meta: n_rot            = 32
0.00.052.064 I llm_load_print_meta: n_swa            = 0
0.00.052.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.065 I llm_load_print_meta: n_gqa            = 1
0.00.052.066 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.067 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.071 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.072 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.072 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.073 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.073 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.073 I llm_load_print_meta: n_ff             = 8192
0.00.052.074 I llm_load_print_meta: n_expert         = 0
0.00.052.074 I llm_load_print_meta: n_expert_used    = 0
0.00.052.076 I llm_load_print_meta: causal attn      = 1
0.00.052.076 I llm_load_print_meta: pooling type     = 0
0.00.052.076 I llm_load_print_meta: rope type        = 2
0.00.052.077 I llm_load_print_meta: rope scaling     = linear
0.00.052.077 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.077 I llm_load_print_meta: freq_scale_train = 1
0.00.052.078 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.078 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.078 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.078 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.078 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.078 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.079 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.079 I llm_load_print_meta: model type       = 1.4B
0.00.052.080 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.080 I llm_load_print_meta: model params     = 1.41 B
0.00.052.081 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.081 I llm_load_print_meta: general.name     = 1.4B
0.00.052.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.082 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.083 I llm_load_print_meta: LF token         = 128 ''
0.00.052.083 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.084 I llm_load_print_meta: max token length = 1024
0.00.054.074 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.075 I llm_load_tensors: offloading output layer to GPU
0.00.054.075 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.085 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.086 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.987 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.988 I llama_new_context_with_model: n_ctx         = 128
0.00.054.988 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.988 I llama_new_context_with_model: n_batch       = 128
0.00.054.988 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.988 I llama_new_context_with_model: flash_attn    = 0
0.00.054.989 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.989 I llama_new_context_with_model: freq_scale    = 1
0.00.054.989 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.990 I ggml_metal_init: allocating
0.00.054.993 I ggml_metal_init: found device: Apple M4
0.00.054.995 I ggml_metal_init: picking default device: Apple M4
0.00.055.567 I ggml_metal_init: using embedded metal library
0.00.057.954 I ggml_metal_init: GPU name:   Apple M4
0.00.057.956 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.956 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.956 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.957 I ggml_metal_init: simdgroup reduction   = true
0.00.057.957 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.957 I ggml_metal_init: has bfloat            = true
0.00.057.957 I ggml_metal_init: use bfloat            = true
0.00.057.958 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.958 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.762 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.091 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.093 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.107 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.032 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.033 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.034 I llama_new_context_with_model: graph nodes  = 967
0.00.070.034 I llama_new_context_with_model: graph splits = 2
0.00.070.035 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.035 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.470 I 
0.00.483.499 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.483.512 I perplexity: tokenizing the input ..
0.00.490.941 I perplexity: tokenization took 7.427 ms
0.00.490.944 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.623.202 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.624.370 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.624.387 I llama_perf_context_print:        load time =     474.50 ms
0.00.624.387 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.46 tokens per second)
0.00.624.390 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.624.390 I llama_perf_context_print:       total time =     140.92 ms /   129 tokens
0.00.624.882 I ggml_metal_free: deallocating

real	0m0.639s
user	0m0.078s
sys	0m0.088s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.692 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.205 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.210 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.211 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.212 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.212 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.213 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.213 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.214 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.214 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.215 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.215 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.215 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.216 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.216 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.218 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.179 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.284 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.247 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.248 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.249 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.249 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.249 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.249 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.250 I llama_model_loader: - type  f32:  194 tensors
0.00.024.250 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.250 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.250 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.673 I llm_load_vocab: special tokens cache size = 25
0.00.050.606 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.609 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.610 I llm_load_print_meta: arch             = gptneox
0.00.050.610 I llm_load_print_meta: vocab type       = BPE
0.00.050.610 I llm_load_print_meta: n_vocab          = 50304
0.00.050.610 I llm_load_print_meta: n_merges         = 50009
0.00.050.611 I llm_load_print_meta: vocab_only       = 0
0.00.050.611 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.611 I llm_load_print_meta: n_embd           = 2048
0.00.050.611 I llm_load_print_meta: n_layer          = 24
0.00.050.614 I llm_load_print_meta: n_head           = 16
0.00.050.615 I llm_load_print_meta: n_head_kv        = 16
0.00.050.615 I llm_load_print_meta: n_rot            = 32
0.00.050.615 I llm_load_print_meta: n_swa            = 0
0.00.050.615 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.616 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.616 I llm_load_print_meta: n_gqa            = 1
0.00.050.617 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.618 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.618 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.619 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.619 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.622 I llm_load_print_meta: n_ff             = 8192
0.00.050.622 I llm_load_print_meta: n_expert         = 0
0.00.050.623 I llm_load_print_meta: n_expert_used    = 0
0.00.050.623 I llm_load_print_meta: causal attn      = 1
0.00.050.623 I llm_load_print_meta: pooling type     = 0
0.00.050.623 I llm_load_print_meta: rope type        = 2
0.00.050.623 I llm_load_print_meta: rope scaling     = linear
0.00.050.624 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.624 I llm_load_print_meta: freq_scale_train = 1
0.00.050.624 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.625 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.625 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.625 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.627 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.627 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.627 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.627 I llm_load_print_meta: model type       = 1.4B
0.00.050.628 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.628 I llm_load_print_meta: model params     = 1.41 B
0.00.050.629 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.629 I llm_load_print_meta: general.name     = 1.4B
0.00.050.629 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.629 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.630 I llm_load_print_meta: LF token         = 128 ''
0.00.050.631 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.631 I llm_load_print_meta: max token length = 1024
0.00.052.654 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.654 I llm_load_tensors: offloading output layer to GPU
0.00.052.654 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.665 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.666 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.572 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.573 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.573 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.573 I llama_new_context_with_model: n_batch       = 2048
0.00.053.574 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.574 I llama_new_context_with_model: flash_attn    = 0
0.00.053.574 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.574 I llama_new_context_with_model: freq_scale    = 1
0.00.053.575 I ggml_metal_init: allocating
0.00.053.582 I ggml_metal_init: found device: Apple M4
0.00.053.585 I ggml_metal_init: picking default device: Apple M4
0.00.054.167 I ggml_metal_init: using embedded metal library
0.00.056.481 I ggml_metal_init: GPU name:   Apple M4
0.00.056.482 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.483 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.483 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.483 I ggml_metal_init: simdgroup reduction   = true
0.00.056.484 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.484 I ggml_metal_init: has bfloat            = true
0.00.056.484 I ggml_metal_init: use bfloat            = true
0.00.056.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.261 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.802 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.807 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.825 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.852 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.854 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.854 I llama_new_context_with_model: graph nodes  = 967
0.00.086.855 I llama_new_context_with_model: graph splits = 2
0.00.086.857 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.001 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.002 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.631.410 I main: llama threadpool init, n_threads = 4
0.00.631.454 I 
0.00.631.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.631.474 I 
0.00.631.712 I sampler seed: 1234
0.00.631.718 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.631.751 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.631.754 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.631.754 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.382.600 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47972.97 tokens per second)
0.01.382.601 I llama_perf_context_print:        load time =     621.71 ms
0.01.382.602 I llama_perf_context_print: prompt eval time =      50.91 ms /     7 tokens (    7.27 ms per token,   137.49 tokens per second)
0.01.382.602 I llama_perf_context_print:        eval time =     697.45 ms /    63 runs   (   11.07 ms per token,    90.33 tokens per second)
0.01.382.603 I llama_perf_context_print:       total time =     751.19 ms /    70 tokens
0.01.382.870 I ggml_metal_free: deallocating

real	0m1.401s
user	0m0.110s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.893 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.355 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.359 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.364 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.368 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.218 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.312 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.141 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.144 I llama_model_loader: - type  f32:  194 tensors
0.00.024.144 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.145 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.145 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.549 I llm_load_vocab: special tokens cache size = 25
0.00.050.482 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.485 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.485 I llm_load_print_meta: arch             = gptneox
0.00.050.486 I llm_load_print_meta: vocab type       = BPE
0.00.050.486 I llm_load_print_meta: n_vocab          = 50304
0.00.050.486 I llm_load_print_meta: n_merges         = 50009
0.00.050.486 I llm_load_print_meta: vocab_only       = 0
0.00.050.486 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.487 I llm_load_print_meta: n_embd           = 2048
0.00.050.487 I llm_load_print_meta: n_layer          = 24
0.00.050.490 I llm_load_print_meta: n_head           = 16
0.00.050.493 I llm_load_print_meta: n_head_kv        = 16
0.00.050.493 I llm_load_print_meta: n_rot            = 32
0.00.050.494 I llm_load_print_meta: n_swa            = 0
0.00.050.494 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.494 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.495 I llm_load_print_meta: n_gqa            = 1
0.00.050.495 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.496 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.496 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.497 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.497 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.497 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.497 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.498 I llm_load_print_meta: n_ff             = 8192
0.00.050.498 I llm_load_print_meta: n_expert         = 0
0.00.050.498 I llm_load_print_meta: n_expert_used    = 0
0.00.050.498 I llm_load_print_meta: causal attn      = 1
0.00.050.499 I llm_load_print_meta: pooling type     = 0
0.00.050.499 I llm_load_print_meta: rope type        = 2
0.00.050.499 I llm_load_print_meta: rope scaling     = linear
0.00.050.500 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.500 I llm_load_print_meta: freq_scale_train = 1
0.00.050.500 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.500 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.500 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.501 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.501 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.501 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.501 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.502 I llm_load_print_meta: model type       = 1.4B
0.00.050.502 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.503 I llm_load_print_meta: model params     = 1.41 B
0.00.050.504 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.504 I llm_load_print_meta: general.name     = 1.4B
0.00.050.504 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.504 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.505 I llm_load_print_meta: LF token         = 128 ''
0.00.050.505 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.506 I llm_load_print_meta: max token length = 1024
0.00.052.520 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.520 I llm_load_tensors: offloading output layer to GPU
0.00.052.520 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.531 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.532 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.422 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.422 I llama_new_context_with_model: n_ctx         = 128
0.00.053.423 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.423 I llama_new_context_with_model: n_batch       = 128
0.00.053.423 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.423 I llama_new_context_with_model: flash_attn    = 0
0.00.053.424 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.424 I llama_new_context_with_model: freq_scale    = 1
0.00.053.424 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.425 I ggml_metal_init: allocating
0.00.053.430 I ggml_metal_init: found device: Apple M4
0.00.053.433 I ggml_metal_init: picking default device: Apple M4
0.00.053.972 I ggml_metal_init: using embedded metal library
0.00.056.381 I ggml_metal_init: GPU name:   Apple M4
0.00.056.382 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.383 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.383 I ggml_metal_init: simdgroup reduction   = true
0.00.056.383 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.383 I ggml_metal_init: has bfloat            = true
0.00.056.383 I ggml_metal_init: use bfloat            = true
0.00.056.384 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.105 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.361 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.364 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.381 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.325 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.326 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.326 I llama_new_context_with_model: graph nodes  = 967
0.00.068.326 I llama_new_context_with_model: graph splits = 2
0.00.068.328 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.328 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.556.467 I 
0.00.556.511 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.556.525 I perplexity: tokenizing the input ..
0.00.564.021 I perplexity: tokenization took 7.493 ms
0.00.564.024 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.698.841 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.075 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.088 I llama_perf_context_print:        load time =     546.57 ms
0.00.700.089 I llama_perf_context_print: prompt eval time =     134.58 ms /   128 tokens (    1.05 ms per token,   951.10 tokens per second)
0.00.700.090 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.091 I llama_perf_context_print:       total time =     143.62 ms /   129 tokens
0.00.700.513 I ggml_metal_free: deallocating

real	0m0.715s
user	0m0.078s
sys	0m0.099s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.685 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.418 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.423 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.425 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.427 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.428 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.429 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.431 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.277 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.380 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.385 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.386 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.386 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.387 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.387 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.387 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.388 I llama_model_loader: - type  f32:  194 tensors
0.00.023.388 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.389 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.905 I llm_load_vocab: special tokens cache size = 25
0.00.051.075 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.080 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.080 I llm_load_print_meta: arch             = gptneox
0.00.051.081 I llm_load_print_meta: vocab type       = BPE
0.00.051.081 I llm_load_print_meta: n_vocab          = 50304
0.00.051.081 I llm_load_print_meta: n_merges         = 50009
0.00.051.081 I llm_load_print_meta: vocab_only       = 0
0.00.051.081 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.082 I llm_load_print_meta: n_embd           = 2048
0.00.051.082 I llm_load_print_meta: n_layer          = 24
0.00.051.086 I llm_load_print_meta: n_head           = 16
0.00.051.086 I llm_load_print_meta: n_head_kv        = 16
0.00.051.087 I llm_load_print_meta: n_rot            = 32
0.00.051.087 I llm_load_print_meta: n_swa            = 0
0.00.051.087 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.087 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.090 I llm_load_print_meta: n_gqa            = 1
0.00.051.091 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.091 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.092 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.092 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.092 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.092 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.092 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.093 I llm_load_print_meta: n_ff             = 8192
0.00.051.093 I llm_load_print_meta: n_expert         = 0
0.00.051.093 I llm_load_print_meta: n_expert_used    = 0
0.00.051.093 I llm_load_print_meta: causal attn      = 1
0.00.051.094 I llm_load_print_meta: pooling type     = 0
0.00.051.094 I llm_load_print_meta: rope type        = 2
0.00.051.094 I llm_load_print_meta: rope scaling     = linear
0.00.051.095 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.095 I llm_load_print_meta: freq_scale_train = 1
0.00.051.095 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.097 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.097 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.097 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.097 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.097 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.097 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.098 I llm_load_print_meta: model type       = 1.4B
0.00.051.099 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.099 I llm_load_print_meta: model params     = 1.41 B
0.00.051.101 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.101 I llm_load_print_meta: general.name     = 1.4B
0.00.051.101 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.101 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.101 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.101 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.102 I llm_load_print_meta: LF token         = 128 ''
0.00.051.102 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.102 I llm_load_print_meta: max token length = 1024
0.00.052.935 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.935 I llm_load_tensors: offloading output layer to GPU
0.00.052.936 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.946 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.947 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.799 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.800 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.801 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.801 I llama_new_context_with_model: n_batch       = 2048
0.00.053.801 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.801 I llama_new_context_with_model: flash_attn    = 0
0.00.053.802 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.802 I llama_new_context_with_model: freq_scale    = 1
0.00.053.803 I ggml_metal_init: allocating
0.00.053.806 I ggml_metal_init: found device: Apple M4
0.00.053.809 I ggml_metal_init: picking default device: Apple M4
0.00.054.431 I ggml_metal_init: using embedded metal library
0.00.056.890 I ggml_metal_init: GPU name:   Apple M4
0.00.056.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.892 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.893 I ggml_metal_init: simdgroup reduction   = true
0.00.056.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.893 I ggml_metal_init: has bfloat            = true
0.00.056.893 I ggml_metal_init: use bfloat            = true
0.00.056.894 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.895 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.208 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.439 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.448 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.468 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.485 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.486 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.486 I llama_new_context_with_model: graph nodes  = 967
0.00.088.487 I llama_new_context_with_model: graph splits = 2
0.00.088.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.620 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.621 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.821 I main: llama threadpool init, n_threads = 4
0.00.686.863 I 
0.00.686.901 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.686.903 I 
0.00.687.155 I sampler seed: 1234
0.00.687.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.687.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.687.178 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.687.178 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.533.905 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.01.533.905 I llama_perf_context_print:        load time =     678.13 ms
0.01.533.906 I llama_perf_context_print: prompt eval time =      51.70 ms /     7 tokens (    7.39 ms per token,   135.40 tokens per second)
0.01.533.906 I llama_perf_context_print:        eval time =     792.08 ms /    63 runs   (   12.57 ms per token,    79.54 tokens per second)
0.01.533.907 I llama_perf_context_print:       total time =     847.09 ms /    70 tokens
0.01.534.136 I ggml_metal_free: deallocating

real	0m1.550s
user	0m0.113s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.673 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.137 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.141 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.145 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.146 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.149 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.149 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.150 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.150 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.156 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.156 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.017.981 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.021 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.902 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.903 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.904 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.904 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.022.905 I llama_model_loader: - type  f32:  194 tensors
0.00.022.905 I llama_model_loader: - type q5_K:   61 tensors
0.00.022.906 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.153 I llm_load_vocab: special tokens cache size = 25
0.00.049.077 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.079 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.080 I llm_load_print_meta: arch             = gptneox
0.00.049.080 I llm_load_print_meta: vocab type       = BPE
0.00.049.080 I llm_load_print_meta: n_vocab          = 50304
0.00.049.081 I llm_load_print_meta: n_merges         = 50009
0.00.049.081 I llm_load_print_meta: vocab_only       = 0
0.00.049.081 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.081 I llm_load_print_meta: n_embd           = 2048
0.00.049.081 I llm_load_print_meta: n_layer          = 24
0.00.049.084 I llm_load_print_meta: n_head           = 16
0.00.049.085 I llm_load_print_meta: n_head_kv        = 16
0.00.049.086 I llm_load_print_meta: n_rot            = 32
0.00.049.086 I llm_load_print_meta: n_swa            = 0
0.00.049.086 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.087 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.088 I llm_load_print_meta: n_gqa            = 1
0.00.049.089 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.090 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.090 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.090 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.091 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.091 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.091 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.092 I llm_load_print_meta: n_ff             = 8192
0.00.049.092 I llm_load_print_meta: n_expert         = 0
0.00.049.092 I llm_load_print_meta: n_expert_used    = 0
0.00.049.092 I llm_load_print_meta: causal attn      = 1
0.00.049.092 I llm_load_print_meta: pooling type     = 0
0.00.049.092 I llm_load_print_meta: rope type        = 2
0.00.049.093 I llm_load_print_meta: rope scaling     = linear
0.00.049.097 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.097 I llm_load_print_meta: freq_scale_train = 1
0.00.049.097 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.098 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.098 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.098 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.098 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.098 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.098 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.100 I llm_load_print_meta: model type       = 1.4B
0.00.049.100 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.101 I llm_load_print_meta: model params     = 1.41 B
0.00.049.101 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.102 I llm_load_print_meta: general.name     = 1.4B
0.00.049.102 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.102 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.102 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.102 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.103 I llm_load_print_meta: LF token         = 128 ''
0.00.049.103 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.104 I llm_load_print_meta: max token length = 1024
0.00.051.064 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.064 I llm_load_tensors: offloading output layer to GPU
0.00.051.064 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.075 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.076 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.051.967 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.967 I llama_new_context_with_model: n_ctx         = 128
0.00.051.968 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.968 I llama_new_context_with_model: n_batch       = 128
0.00.051.968 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.968 I llama_new_context_with_model: flash_attn    = 0
0.00.051.969 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.969 I llama_new_context_with_model: freq_scale    = 1
0.00.051.969 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.970 I ggml_metal_init: allocating
0.00.051.979 I ggml_metal_init: found device: Apple M4
0.00.051.981 I ggml_metal_init: picking default device: Apple M4
0.00.052.532 I ggml_metal_init: using embedded metal library
0.00.054.876 I ggml_metal_init: GPU name:   Apple M4
0.00.054.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.878 I ggml_metal_init: simdgroup reduction   = true
0.00.054.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.879 I ggml_metal_init: has bfloat            = true
0.00.054.879 I ggml_metal_init: use bfloat            = true
0.00.054.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.557 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.828 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.831 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.846 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.682 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.683 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.684 I llama_new_context_with_model: graph nodes  = 967
0.00.066.684 I llama_new_context_with_model: graph splits = 2
0.00.066.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.618.642 I 
0.00.618.682 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.618.695 I perplexity: tokenizing the input ..
0.00.626.417 I perplexity: tokenization took 7.721 ms
0.00.626.421 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.767.226 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.768.404 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.768.416 I llama_perf_context_print:        load time =     609.96 ms
0.00.768.417 I llama_perf_context_print: prompt eval time =     140.58 ms /   128 tokens (    1.10 ms per token,   910.53 tokens per second)
0.00.768.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.768.418 I llama_perf_context_print:       total time =     149.78 ms /   129 tokens
0.00.768.767 I ggml_metal_free: deallocating

real	0m0.781s
user	0m0.078s
sys	0m0.105s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.675 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.299 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.300 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.302 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.302 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.314 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.282 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.282 I llama_model_loader: - type  f32:  194 tensors
0.00.023.283 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.889 I llm_load_vocab: special tokens cache size = 25
0.00.049.906 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.909 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.910 I llm_load_print_meta: arch             = gptneox
0.00.049.910 I llm_load_print_meta: vocab type       = BPE
0.00.049.910 I llm_load_print_meta: n_vocab          = 50304
0.00.049.910 I llm_load_print_meta: n_merges         = 50009
0.00.049.911 I llm_load_print_meta: vocab_only       = 0
0.00.049.911 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.911 I llm_load_print_meta: n_embd           = 2048
0.00.049.911 I llm_load_print_meta: n_layer          = 24
0.00.049.913 I llm_load_print_meta: n_head           = 16
0.00.049.914 I llm_load_print_meta: n_head_kv        = 16
0.00.049.914 I llm_load_print_meta: n_rot            = 32
0.00.049.915 I llm_load_print_meta: n_swa            = 0
0.00.049.915 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.915 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.918 I llm_load_print_meta: n_gqa            = 1
0.00.049.918 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.919 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.920 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.920 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.920 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.921 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.921 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.921 I llm_load_print_meta: n_ff             = 8192
0.00.049.922 I llm_load_print_meta: n_expert         = 0
0.00.049.922 I llm_load_print_meta: n_expert_used    = 0
0.00.049.922 I llm_load_print_meta: causal attn      = 1
0.00.049.922 I llm_load_print_meta: pooling type     = 0
0.00.049.922 I llm_load_print_meta: rope type        = 2
0.00.049.923 I llm_load_print_meta: rope scaling     = linear
0.00.049.923 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.923 I llm_load_print_meta: freq_scale_train = 1
0.00.049.924 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.924 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.924 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.924 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.924 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.924 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.925 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.925 I llm_load_print_meta: model type       = 1.4B
0.00.049.925 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.926 I llm_load_print_meta: model params     = 1.41 B
0.00.049.926 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.926 I llm_load_print_meta: general.name     = 1.4B
0.00.049.927 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.927 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: LF token         = 128 ''
0.00.049.929 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: max token length = 1024
0.00.052.028 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.028 I llm_load_tensors: offloading output layer to GPU
0.00.052.028 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.039 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.040 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.983 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.984 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.984 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.984 I llama_new_context_with_model: n_batch       = 2048
0.00.052.984 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.984 I llama_new_context_with_model: flash_attn    = 0
0.00.052.985 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.985 I llama_new_context_with_model: freq_scale    = 1
0.00.052.986 I ggml_metal_init: allocating
0.00.052.989 I ggml_metal_init: found device: Apple M4
0.00.052.991 I ggml_metal_init: picking default device: Apple M4
0.00.053.604 I ggml_metal_init: using embedded metal library
0.00.055.947 I ggml_metal_init: GPU name:   Apple M4
0.00.055.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.949 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.949 I ggml_metal_init: simdgroup reduction   = true
0.00.055.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.949 I ggml_metal_init: has bfloat            = true
0.00.055.950 I ggml_metal_init: use bfloat            = true
0.00.055.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.951 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.850 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.260 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.281 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.264 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.266 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.266 I llama_new_context_with_model: graph nodes  = 967
0.00.086.266 I llama_new_context_with_model: graph splits = 2
0.00.086.269 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.738.797 I main: llama threadpool init, n_threads = 4
0.00.738.855 I 
0.00.738.907 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.738.908 I 
0.00.739.157 I sampler seed: 1234
0.00.739.163 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.207 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.210 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.210 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.622.429 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52592.59 tokens per second)
0.01.622.429 I llama_perf_context_print:        load time =     730.11 ms
0.01.622.430 I llama_perf_context_print: prompt eval time =      54.54 ms /     7 tokens (    7.79 ms per token,   128.36 tokens per second)
0.01.622.431 I llama_perf_context_print:        eval time =     825.63 ms /    63 runs   (   13.11 ms per token,    76.31 tokens per second)
0.01.622.431 I llama_perf_context_print:       total time =     883.64 ms /    70 tokens
0.01.622.710 I ggml_metal_free: deallocating

real	0m1.639s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4425 (747c85d4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.679 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.090 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.094 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.096 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.097 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.097 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.097 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.098 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.099 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.099 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.100 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.101 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.102 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.103 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.103 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.011 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.012 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.012 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.013 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.013 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.013 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.014 I llama_model_loader: - type  f32:  194 tensors
0.00.023.014 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.481 I llm_load_vocab: special tokens cache size = 25
0.00.049.364 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.366 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.366 I llm_load_print_meta: arch             = gptneox
0.00.049.367 I llm_load_print_meta: vocab type       = BPE
0.00.049.367 I llm_load_print_meta: n_vocab          = 50304
0.00.049.367 I llm_load_print_meta: n_merges         = 50009
0.00.049.367 I llm_load_print_meta: vocab_only       = 0
0.00.049.367 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.368 I llm_load_print_meta: n_embd           = 2048
0.00.049.368 I llm_load_print_meta: n_layer          = 24
0.00.049.371 I llm_load_print_meta: n_head           = 16
0.00.049.371 I llm_load_print_meta: n_head_kv        = 16
0.00.049.372 I llm_load_print_meta: n_rot            = 32
0.00.049.372 I llm_load_print_meta: n_swa            = 0
0.00.049.372 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.373 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.374 I llm_load_print_meta: n_gqa            = 1
0.00.049.375 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.375 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.376 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.376 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.378 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.379 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.379 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.379 I llm_load_print_meta: n_ff             = 8192
0.00.049.380 I llm_load_print_meta: n_expert         = 0
0.00.049.380 I llm_load_print_meta: n_expert_used    = 0
0.00.049.380 I llm_load_print_meta: causal attn      = 1
0.00.049.380 I llm_load_print_meta: pooling type     = 0
0.00.049.380 I llm_load_print_meta: rope type        = 2
0.00.049.380 I llm_load_print_meta: rope scaling     = linear
0.00.049.381 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.381 I llm_load_print_meta: freq_scale_train = 1
0.00.049.381 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.382 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.382 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.382 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.382 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.382 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.382 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.383 I llm_load_print_meta: model type       = 1.4B
0.00.049.384 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.388 I llm_load_print_meta: model params     = 1.41 B
0.00.049.388 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.388 I llm_load_print_meta: general.name     = 1.4B
0.00.049.389 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.389 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.389 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.389 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.389 I llm_load_print_meta: LF token         = 128 ''
0.00.049.391 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.391 I llm_load_print_meta: max token length = 1024
0.00.051.398 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.398 I llm_load_tensors: offloading output layer to GPU
0.00.051.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.409 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.410 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.284 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.285 I llama_new_context_with_model: n_ctx         = 128
0.00.052.285 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.285 I llama_new_context_with_model: n_batch       = 128
0.00.052.285 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.286 I llama_new_context_with_model: flash_attn    = 0
0.00.052.286 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.286 I llama_new_context_with_model: freq_scale    = 1
0.00.052.287 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.287 I ggml_metal_init: allocating
0.00.052.294 I ggml_metal_init: found device: Apple M4
0.00.052.296 I ggml_metal_init: picking default device: Apple M4
0.00.052.850 I ggml_metal_init: using embedded metal library
0.00.055.195 I ggml_metal_init: GPU name:   Apple M4
0.00.055.196 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.197 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.197 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.197 I ggml_metal_init: simdgroup reduction   = true
0.00.055.197 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.197 I ggml_metal_init: has bfloat            = true
0.00.055.198 I ggml_metal_init: use bfloat            = true
0.00.055.198 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.199 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.885 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.334 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.337 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.358 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.319 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.320 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.320 I llama_new_context_with_model: graph nodes  = 967
0.00.067.321 I llama_new_context_with_model: graph splits = 2
0.00.067.322 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.653.976 I 
0.00.654.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.036 I perplexity: tokenizing the input ..
0.00.661.857 I perplexity: tokenization took 7.819 ms
0.00.661.864 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.801.669 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.802.847 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.802.863 I llama_perf_context_print:        load time =     645.29 ms
0.00.802.864 I llama_perf_context_print: prompt eval time =     139.58 ms /   128 tokens (    1.09 ms per token,   917.05 tokens per second)
0.00.802.864 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.802.865 I llama_perf_context_print:       total time =     148.89 ms /   129 tokens
0.00.803.308 I ggml_metal_free: deallocating

real	0m0.817s
user	0m0.078s
sys	0m0.118s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4425 (747c85d4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145f0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145f0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145f0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145f0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145f0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145f0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145f0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145f0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145f0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145f0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145f0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145f0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145f0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145f101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145f10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145f11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145f11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145f11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145f12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145f12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145f13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145f13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145f14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145f14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145f14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145f15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145f15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145f16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145f168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145f17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145f176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145f17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145f17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145f182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145f18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145f18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145f19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145f19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145f199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145f19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145f1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145f1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145f1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145f1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145f1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145f1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145f1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145f1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145f1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145f1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145f1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145f1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145f1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145f1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145f1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145f20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145f20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145f208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145f20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145f21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145f216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145f21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145f21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145f22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145f22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145f22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145f23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145f23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145f23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145f240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145f24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145f24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145f250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145f25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145f260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145f26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145f26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145f270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145f27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145f27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145f28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145f28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145f290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145f295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145f29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145f2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145f2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145f2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145f2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145f2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145f2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145f1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145f2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145f2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145f2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145f2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145f2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145f2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145f2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145f2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145f2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145f2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145f2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145f2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145f301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145f30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145f30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145f310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145f31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145f31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145f31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145f32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145f32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145f32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145f33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145f335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145f33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145f33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145f343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145f34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145f34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145f351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145f35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145f35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145f35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145f36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145f368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145f36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145f37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145f376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145f37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145f37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145f38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145f38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145f39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145f39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145f39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145f3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145f3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145f3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145f3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145f3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145f3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145f3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145f3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145f3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145f3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145f3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145f3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145f3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145f3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145f3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145f3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145f3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145f3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145f3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145f3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145f40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145f40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145f40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145f40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145f413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145f41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145f41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145f421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145f42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145f42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145f42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145f43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145f438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145f43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145f44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145f446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145f44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145f45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145f454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145f45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145f45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145f46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145f46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145f46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145f47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145f47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145f479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145f47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145f483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145f488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145f48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145f49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145f49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145f49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145f4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145f4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145f4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145f4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145f4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145f4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145f4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145f4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145f4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145f4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145f4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145f4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145f4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145f4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145f4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145f4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145f4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145f50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145f506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145f50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145f51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145f51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145f51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145f52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145f52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145f52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145f53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145f53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145f53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145f54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145f54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145f54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145f55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145f55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145f55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145f560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145f56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145f56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145f570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145f57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145f57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145f580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145f58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145f58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145f590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145f59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145f59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145f5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145f5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145f5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145f5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145f5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145f5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145f5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145f5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145f5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145f5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145f5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145f5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145f5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145f5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145f5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145f5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145f5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145f5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145f60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145f605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145f60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145f60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145f61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145f618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145f61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145f62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145f626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145f62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145f62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145f63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145f63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145f63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145f64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145f64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145f64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145f65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145f655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145f65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145f663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145f66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145f67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145f674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145f67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145f67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145f685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.770 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x136704b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136704f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136705400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136705870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x136705ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136706150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1367065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x136706a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136706ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136707310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136707780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136707e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136708990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136709140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x136709950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13670a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13670a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13670aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13670b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13670bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13670c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13670cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13670d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13670d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13670e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13670e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13670e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13670ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13670ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13670f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13670f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13670fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x136710180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x136710440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1367108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136710d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136711190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136711600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x136711a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136711ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1367127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136712c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1367130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136713510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x136713980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136713df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136714260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1367146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136714b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x136714fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136715420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x136715890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136715d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136716170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1367165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136716b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136717050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1367174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136717930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x136717da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136718210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x136718680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136718af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x136718f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1367193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136719840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x136719cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13671a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13671a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13671aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13671ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13671b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13671b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13671bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13671c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13671c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13671c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13671cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13671d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13671d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13671dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13671df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13671e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13671e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13671ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13671f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13671f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13671f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13671fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1367202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136720730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x136720ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136721010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136721480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1367218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x136721d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1367221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136722640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x136722ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136722f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136723390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136723800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136723c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1367240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136724550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1367249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136724e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1367252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x136725710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136725ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136726460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1367268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x136726d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1367271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x136727620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136727a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136727f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136728370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1367287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136728c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1367290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x136729530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1367299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136729e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13672a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13672a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13672ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13672afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13672b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13672b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13672bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13672c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13672c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13672ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13672cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13672d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13672d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13672dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13672e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13672e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13672e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13672edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13672f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13672f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13672fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13672ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x136730420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136730890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x136730d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136731170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1367315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136731a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136731ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136732330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1367327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x136732c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136733080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1367334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136733960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136733dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136734240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1367346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x136734b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x136735bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136735e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136736140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1367365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136736a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x136736e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136737300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136737be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x136738050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1367384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x136738da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136739210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136739680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136739af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x136739f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13673a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13673a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13673acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13673b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13673b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13673ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13673be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13673c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13673c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13673cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13673d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13673d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13673d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13673dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13673e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13673e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13673ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13673ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13673f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13673f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13673fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x136740290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x136740700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136740b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x136740fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136741500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136741a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136742840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136742e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1367433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136743980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136744500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x136744ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136745080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136745640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136745c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1367461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136746780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136746d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x136747300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1367478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136747e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136748440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136748a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136748fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136749580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x136749b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13674a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13674a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13674ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13674b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13674b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13674bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13674c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13674c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13674cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13674d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13674da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13674e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13674e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13674ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13674f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13674f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13674fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1367502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x136750880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x136750e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x136751400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1367519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x136751f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x136752540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x136752b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1367530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x136753680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x136753c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x136754200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1367547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x136754d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x136755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136755900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136755ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136756480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136756a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136756f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136757440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x136757940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136757e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136758340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136758840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x136758d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136759240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136759740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x136759c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13675a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13675a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13675ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13675b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13675b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13675bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13675c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13675cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13675d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13675d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13675df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13675e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13675e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145e046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145e04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145e04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145e05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145e058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145e05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145e06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145e065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145e06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145e06fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145e07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145e07aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145e085c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145e08d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145e09580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145e09ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145e0a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145e0aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145e0b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145e0b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145e0c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145e0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145e0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145e0d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145e0dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145e0e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145e0e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145e0e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145e0ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145e0f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145e0f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145e0f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145e0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145e10110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145e10580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145e109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145e10e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145e112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145e11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145e11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145e12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145e12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145e12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145e12d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145e131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145e13ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145e13f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145e143a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145e14810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145e14c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145e150f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145e15560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145e159d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145e15e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145e162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145e16820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145e16d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145e17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145e17600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145e17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145e17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145e18350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145e187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145e18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145e190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145e19510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145e19980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145e19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145e1a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145e1a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145e1ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145e1afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145e1b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145e1b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145e1bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145e1c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145e1c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145e1ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145e1cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145e1d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145e1d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145e1dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145e1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145e1e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145e1e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145e1edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145e1f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145e1f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145e1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145e1ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145e20400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145e20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145e20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145e21150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145e215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145e21a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145e21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145e22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145e22780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145e22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145e23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145e234d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145e23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145e24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145e24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145e24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145e24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145e251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145e25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145e25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145e25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145e26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145e26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145e27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145e279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145e27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145e282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145e28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145e28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145e29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145e29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145e298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145e29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145e2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145e2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145e2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145e2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145e2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145e2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145e2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145e2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145e2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145e2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145e2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145e2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145e2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145e2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145e2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145e2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145e2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145e2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145e2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145e2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145e2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145e2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145e30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145e307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145e30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145e310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145e31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145e31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145e31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145e32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145e326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145e32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145e32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145e33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145e338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145e33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145e34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145e345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145e34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145e34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145e35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145e35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145e36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145e36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145e36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145e37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145e376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145e37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145e37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145e38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145e39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145e395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145e39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145e39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145e3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145e3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145e3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145e3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145e3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145e3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145e3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145e3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145e3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145e3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145e3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145e3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145e3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145e3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145e3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145e3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145e3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145e404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145e40930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145e40da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145e41210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145e41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145e42050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145e42310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145e42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145e42bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145e43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145e434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145e43940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145e43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145e44220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145e44690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145e44b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145e44f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145e45850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145e45cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145e46130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145e465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145e46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145e46e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145e472f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145e47760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145e47bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145e48040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145e484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145e48920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145e48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145e49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145e49670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145e49ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145e49f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145e4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145e4a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145e4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145e4b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145e4b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145e4b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145e4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145e4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145e4c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145e4cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145e4d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145e4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145e4d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145e4dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145e4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145e4e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145e4eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145e4ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145e4f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145e4f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145e4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145e500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145e50560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145e509d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145e50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145e512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145e51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145e51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145e52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145e52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145e528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145e52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145e531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145e53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145e53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145e53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145e54380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145e547f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145e54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145e550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145e55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145e559b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145e56420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145e56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145e57260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145e57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145e57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145e580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145e586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145e58cc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.793s
user	0m0.302s
sys	0m0.325s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4425 (747c85d4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f60a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f60a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f60aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f60ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f60b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f60c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f60c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f60cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f60d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f60d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f60dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f60e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f60ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f60f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f60fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f610310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f610a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f611870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f612760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f612e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f6135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f613e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f614560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f614820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f614e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f615aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f615fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f6162a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f616740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f616a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f617290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f6177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f617f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f6183d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f618870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f618d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f6191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f619650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f619af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f619f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f61a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f61a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f61ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f61b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f61bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f61c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f61c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f61ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f61d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f61da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f61e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f61e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f61ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f61f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f61f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f61fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f620280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f620540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f609620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f609a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f609f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f60a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f60a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f60ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f60b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f60b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f60b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f60be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f60c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f60c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f60cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f60cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f60d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f60d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f60dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f60e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f60e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f60ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f60eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f60f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f60fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f60ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f610400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f610870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f610ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f611150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f6115c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f611a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f611ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f612310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f612780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f612bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f613060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f6134d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f613940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f613db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f614220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f614690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f614b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f614f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f6153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f615850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f615cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f616130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f6165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f616a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f616e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f6172f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f617760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f617bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f618040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f6184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f618920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f618d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f619200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f619670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f619ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f619f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f61a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f61a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f61b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f61b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f61b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f61be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f61c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f61c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f61cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f61d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f61d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f61d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f61dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f61e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f61e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f61eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f61ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f61f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f61f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f61fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f6200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f620550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f6209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f620e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f6212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f621710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f621b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f621ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f6228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f622d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f6231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f623620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f623a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f623f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f624370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f6247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f624c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f6250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f625530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f6259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f625e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f626280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f6266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f626b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f626fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f627440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f6278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f627d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f628190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f628a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f628ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f6297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f629c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f62a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f62a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f62a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f62adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f62b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f62b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f62bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f62bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f62c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f62c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f62cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f62d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f62d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f62da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f62dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f62e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f62e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f62ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f62f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f62f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f62f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f62fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f630240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f6306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f630b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f630f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f6314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f631940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f631db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f632220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f632690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f632bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f6330c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f633c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f633ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f6344b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f634a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f635030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f6355f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f635bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f636170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f636730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f636cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f6372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f637870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f637e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f6383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f6389b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f638f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f639530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f639af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f63a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f63a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f63ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f63b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f63b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f63bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f63c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f63c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f63ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f63d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f63da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f63dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f63e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f63eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f63f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f63f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f63fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f640270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f640830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f640df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f6413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f641970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f641f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f6424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f642ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f643070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f643630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f643bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f6441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f644770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f644d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f6452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f6458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f645e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f646430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f6469f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f646fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f647570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11f7047c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11f704c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11f705150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11f7055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11f705a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11f705ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11f706310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11f706780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11f706bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11f707060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11f7074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11f707940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11f707db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11f708220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11f708690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11f708b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11f708f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11f709ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11f70a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11f70a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11f70b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11f70b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11f70b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11f70ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11f70be80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.091.209 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10f63cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10f6397f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10f636fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10f6466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10f643eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10f641c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10f63f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10f637b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10f6352f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10f63a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10f63b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10f640af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10f63d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10f6455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10f6380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10f639230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10f6427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10f63c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10f63e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10f639db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10f644470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10f63f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10f634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10f647270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10f644a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10f63a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10f63d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10f6410b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10f6386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10f642d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10f637570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10f645b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10f63e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10f643330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10f63ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10f636430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10f6358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10f646130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10f63ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10f6421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10f644ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10f6438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10f63ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10f60f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10f647830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10f647af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10f647db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10f648070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10f648330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10f6485f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10f6488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10f648b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10f6490b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10f649370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10f649630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10f6498f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10f649bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10f649e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10f64a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10f64a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10f64a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10f64a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10f64ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10f64aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10f64b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10f64b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10f64b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10f64b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10f64bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10f64bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f64c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f64c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f64c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f64cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f64cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f64d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f64d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f64d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f64dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f64dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f64e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f64e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f64e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f64e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f64eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f64ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f64f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f64f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f64f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f64f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f64fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f64fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f650140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f6506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f650980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f650c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f650f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f6511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f651480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f651740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f651a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f651cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f651f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f652240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f652500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f6527c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f652d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f653000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f6532c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f653580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f653840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f653b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f654080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f654340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f654600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f6548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f654b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f654e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f655100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f6553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f655680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f655940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f655c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f655ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f656180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f656440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f656700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f6569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f656c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f656f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f657200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f6574c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f657780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f657a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f657d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f657fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f658280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f658540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f658800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f658c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f658ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f6593c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f6598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f659dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f65a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f65a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f65acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f65b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f65b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f65bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f65c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f65c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f65cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f65cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f65d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f65d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f65dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f65e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f65e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f65edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f65f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f65f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f65fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f6602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f660800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f660d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f661200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f661700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f661c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f662100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f662600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f662b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f663000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f663a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f663f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f664400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f664900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f664e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f665300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f665800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f665d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f666200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f666700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f666c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f667100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f667600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f667b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f668000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f668500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f668a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f668fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f669560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f669b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f66a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f66a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f66ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f66b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f66bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f66bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f66c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f66c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f66ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f66d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f66daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f66df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f66e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f66ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f66f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f66f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f66fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f670120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f670670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f670bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f671110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f671660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f671bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f672100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f672650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f672ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f6730f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f673640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f673b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f6740e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f674630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f674b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f6750d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f675620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f675b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f6760c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f676610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f676b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f6770b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f677600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f677b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f6780a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f6785f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f678b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f679090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f6795e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f679b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f67a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f67a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f67ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f67b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f67b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f67bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f67c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f67c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f67cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f67d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f67d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f67daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f67e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f67e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f67eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f67f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f67f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f67fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f680020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f680570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f680ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f681010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f681560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f681a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f681ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f682340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f6827e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f682c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f683120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f6835c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f683a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f683f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f6843a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f684840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f684ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f685180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f685620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f685ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f686010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f686730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f686e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f687570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f687c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f687f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f688740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f688a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f689010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11f60b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11f61d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11f61d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11f61f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11f61bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11f614ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11f61a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11f61dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11f613ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11f616cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11f616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11f6152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11f61b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11f620b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11f621320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11f621a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11f622160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11f622880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11f622fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11f623950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11f624070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11f624790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11f624eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11f6255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11f625cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11f625fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11f6265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11f626bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11f6271e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11f6279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11f627e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11f628130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11f6289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11f628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11f6291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11f629660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11f629b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11f629fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11f62a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11f62a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11f62ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11f62b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11f62b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11f62bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11f62be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11f62c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11f62ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11f62d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11f62d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11f62dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11f62e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11f62e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11f62eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11f62f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11f62fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11f630140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11f6305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11f6308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11f630eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11f6316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11f631b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11f631fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11f632480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11f632920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11f632dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11f633260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11f633700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11f633ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11f634040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11f6344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10f669820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10f669270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10f668cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10f669dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10f63dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10f66c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10f66a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10f688cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10f66a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10f66afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10f66cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10f688210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10f66d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10f66b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10f689470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10f689730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10f6899f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10f689cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10f689f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10f68a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10f68a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10f68a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10f68aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10f68ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10f68aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10f68b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10f68b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10f68b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10f68baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10f68bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10f68c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10f68c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10f68c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10f68c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10f68cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10f68ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10f68d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10f68d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10f68d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10f68d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10f68dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10f68deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10f68e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10f68e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10f68e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10f68e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10f68ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10f68ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10f68f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10f68f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10f68f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10f68fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10f68fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10f68ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10f690270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10f690530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10f6907f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10f690ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10f690d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10f691030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10f6912f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10f6915b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10f691870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10f691b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10f691df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10f6920b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10f692370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10f692630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10f6928f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10f692bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10f692e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10f693130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10f6933f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10f6936b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10f693970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10f693c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10f693ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10f6941b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10f694470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10f694730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10f6949f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10f694cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10f694f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10f695230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10f6954f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10f6957b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10f695a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10f695d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10f695ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10f6962b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10f696570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10f696830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10f696af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10f696db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10f697070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10f697330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10f6975f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10f6978b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10f697b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10f697e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10f6980f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10f6983b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10f698670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10f698930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10f698bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10f698eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10f699170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10f699430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10f6996f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10f6999b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10f699c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10f699f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10f69a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10f69a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10f69a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10f69aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10f69acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10f69afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10f69b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10f69b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10f69b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10f69bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10f69bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10f69c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10f69c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10f69c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10f69c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10f69cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10f69cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f69d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f69d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f69d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f69d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f69dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f69de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f69e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f69e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f69e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f69e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f69ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f69f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f69f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f69f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f69fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f69fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f69ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f6a0280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f6a0540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f6a0800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f6a0ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f6a0d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f6a1040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f6a1300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f6a15c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f6a1880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f6a1b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f6a1e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f6a20c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f6a2380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f6a2640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f6a2900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f6a2bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f6a2e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f6a3140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f6a3400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f6a36c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f6a3980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f6a3c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f6a3f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f6a41c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f6a4480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f6a4740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f6a4a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f6a4cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f6a4f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f6a5240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f6a5500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f6a57c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f6a5a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f6a5d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f6a6000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f6a62c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f6a6580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f6a6840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f6a6b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f6a6dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f6a7080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f6a7340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f6a7600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f6a78c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f6a7b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f6a7e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f6a8100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f6a83c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f6a8680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f6a8940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f6a8c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f6a8ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f6a9180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f6a9440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f6a9700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f6a99c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f6a9c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f6a9f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f6aa200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f6aa4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f6aa780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f6aaa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f6aad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f6aafc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f6ab280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f6ab540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f6ab800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f6abac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f6abd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f6ac040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f6ac300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f6ac5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f6ac880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f6acb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f6ace00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.917s
user	0m0.248s
sys	0m0.140s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.33 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.59 sec*proc (2 tests)

Total Test time (real) =   0.60 sec
        0.60 real         0.15 user         0.05 sys
```
