Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.090s
user	0m1.028s
sys	0m1.463s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking C executable ../bin/test-c
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-quantize-stats
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Built target llava
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 35%] Built target test-c
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target llava_shared
[ 36%] Built target common
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-log
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-chat
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-sampling
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Built target test-chat-template
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-gguf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-arg-parser
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 62%] Built target test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target llama-batched-bench
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Built target llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Built target llama-imatrix
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-infill
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookahead
[ 76%] Linking CXX executable ../../bin/llama-lookup
[ 76%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Built target llama-gritlm
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Generating loading.html.hpp
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-cli
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-parallel
[ 81%] Generating index.html.gz.hpp
[ 81%] Built target llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Built target llama-perplexity
[ 85%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 85%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-tokenize
[ 87%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-tts
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Built target llama-quantize
[ 89%] Built target llama-retrieval
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-tokenize
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tts
[ 92%] Built target llama-run
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Built target llama-gen-docs
[ 95%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.204s
user	0m6.439s
sys	0m9.935s

main: quantize time =  2434.83 ms
main:    total time =  2434.83 ms

main: quantize time =  1881.82 ms
main:    total time =  1881.82 ms

main: quantize time =  1946.04 ms
main:    total time =  1946.04 ms

main: quantize time =  2744.87 ms
main:    total time =  2744.87 ms

main: quantize time =  2151.56 ms
main:    total time =  2151.56 ms

main: quantize time =  6371.70 ms
main:    total time =  6371.70 ms

main: quantize time =  5901.77 ms
main:    total time =  5901.77 ms

main: quantize time =  7029.70 ms
main:    total time =  7029.70 ms

main: quantize time =  6083.89 ms
main:    total time =  6083.89 ms

main: quantize time =  4620.86 ms
main:    total time =  4620.86 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.179 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.370 I main: llama backend init
0.00.000.376 I main: load the model and apply lora adapter, if any
0.00.055.597 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.068.587 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.068.608 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.614 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.615 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.618 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.619 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.619 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.621 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.626 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.627 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.844 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.078.178 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.087.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.087.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.087.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.087.546 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.087.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.087.548 I llama_model_loader: - type  f32:  194 tensors
0.00.087.549 I llama_model_loader: - type  f16:   98 tensors
0.00.087.550 I print_info: file format = GGUF V3 (latest)
0.00.087.552 I print_info: file type   = all F32 (guessed)
0.00.087.554 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.103.774 I load: special tokens cache size = 25
0.00.113.182 I load: token to piece cache size = 0.2984 MB
0.00.113.186 I print_info: arch             = gptneox
0.00.113.186 I print_info: vocab_only       = 0
0.00.113.186 I print_info: n_ctx_train      = 2048
0.00.113.186 I print_info: n_embd           = 2048
0.00.113.187 I print_info: n_layer          = 24
0.00.113.192 I print_info: n_head           = 16
0.00.113.192 I print_info: n_head_kv        = 16
0.00.113.193 I print_info: n_rot            = 32
0.00.113.195 I print_info: n_swa            = 0
0.00.113.196 I print_info: n_embd_head_k    = 128
0.00.113.196 I print_info: n_embd_head_v    = 128
0.00.113.197 I print_info: n_gqa            = 1
0.00.113.198 I print_info: n_embd_k_gqa     = 2048
0.00.113.198 I print_info: n_embd_v_gqa     = 2048
0.00.113.199 I print_info: f_norm_eps       = 1.0e-05
0.00.113.199 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.113.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.113.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.113.201 I print_info: f_logit_scale    = 0.0e+00
0.00.113.202 I print_info: n_ff             = 8192
0.00.113.202 I print_info: n_expert         = 0
0.00.113.203 I print_info: n_expert_used    = 0
0.00.113.203 I print_info: causal attn      = 1
0.00.113.203 I print_info: pooling type     = 0
0.00.113.203 I print_info: rope type        = 2
0.00.113.204 I print_info: rope scaling     = linear
0.00.113.204 I print_info: freq_base_train  = 10000.0
0.00.113.204 I print_info: freq_scale_train = 1
0.00.113.205 I print_info: n_ctx_orig_yarn  = 2048
0.00.113.205 I print_info: rope_finetuned   = unknown
0.00.113.205 I print_info: ssm_d_conv       = 0
0.00.113.205 I print_info: ssm_d_inner      = 0
0.00.113.205 I print_info: ssm_d_state      = 0
0.00.113.205 I print_info: ssm_dt_rank      = 0
0.00.113.205 I print_info: ssm_dt_b_c_rms   = 0
0.00.113.206 I print_info: model type       = 1.4B
0.00.113.211 I print_info: model params     = 1.41 B
0.00.113.211 I print_info: general.name     = 1.4B
0.00.113.212 I print_info: vocab type       = BPE
0.00.113.213 I print_info: n_vocab          = 50304
0.00.113.213 I print_info: n_merges         = 50009
0.00.113.213 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.113.213 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.113.214 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.113.214 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.113.214 I print_info: LF token         = 187 'Ċ'
0.00.113.214 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.113.215 I print_info: max token length = 1024
0.00.113.215 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.167.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.167.440 I load_tensors: offloading output layer to GPU
0.00.167.441 I load_tensors: offloaded 25/25 layers to GPU
0.00.167.469 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.167.470 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.167.963 I llama_init_from_model: n_seq_max     = 1
0.00.167.964 I llama_init_from_model: n_ctx         = 2048
0.00.167.964 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.167.964 I llama_init_from_model: n_batch       = 2048
0.00.167.965 I llama_init_from_model: n_ubatch      = 512
0.00.167.965 I llama_init_from_model: flash_attn    = 0
0.00.167.965 I llama_init_from_model: freq_base     = 10000.0
0.00.167.966 I llama_init_from_model: freq_scale    = 1
0.00.167.967 I ggml_metal_init: allocating
0.00.168.040 I ggml_metal_init: found device: Apple M4
0.00.168.046 I ggml_metal_init: picking default device: Apple M4
0.00.168.820 I ggml_metal_init: using embedded metal library
0.00.243.926 I ggml_metal_init: GPU name:   Apple M4
0.00.243.931 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.243.932 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.243.932 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.243.932 I ggml_metal_init: simdgroup reduction   = true
0.00.243.932 I ggml_metal_init: simdgroup matrix mul. = true
0.00.243.933 I ggml_metal_init: has residency sets    = true
0.00.243.933 I ggml_metal_init: has bfloat            = true
0.00.243.933 I ggml_metal_init: use bfloat            = true
0.00.243.934 I ggml_metal_init: hasUnifiedMemory      = true
0.00.243.937 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.392.965 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.433.666 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.433.675 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.722 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.304 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.437.307 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.437.308 I llama_init_from_model: graph nodes  = 967
0.00.437.308 I llama_init_from_model: graph splits = 2
0.00.437.313 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.437.442 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.437.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.503.880 I main: llama threadpool init, n_threads = 4
0.00.503.920 I 
0.00.503.953 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.503.954 I 
0.00.504.130 I sampler seed: 1234
0.00.504.134 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.504.161 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.504.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.504.163 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.337.123 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.02.337.123 I llama_perf_context_print:        load time =     447.10 ms
0.02.337.124 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.65 tokens per second)
0.02.337.125 I llama_perf_context_print:        eval time =    1786.37 ms /    63 runs   (   28.36 ms per token,    35.27 tokens per second)
0.02.337.126 I llama_perf_context_print:       total time =    1834.41 ms /    70 tokens
0.02.337.390 I ggml_metal_free: deallocating

real	0m2.651s
user	0m0.146s
sys	0m0.164s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.098 I main: llama backend init
0.00.000.100 I main: load the model and apply lora adapter, if any
0.00.010.542 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.029.374 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.382 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.386 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.387 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.387 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.388 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.388 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.389 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.389 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.390 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.390 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.390 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.391 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.391 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.393 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.393 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.393 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.284 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.436 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.499 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.501 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.501 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.502 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.502 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.502 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.503 I llama_model_loader: - type  f32:  194 tensors
0.00.038.504 I llama_model_loader: - type q8_0:   98 tensors
0.00.038.505 I print_info: file format = GGUF V3 (latest)
0.00.038.510 I print_info: file type   = Q8_0
0.00.038.511 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.047.614 I load: special tokens cache size = 25
0.00.054.491 I load: token to piece cache size = 0.2984 MB
0.00.054.495 I print_info: arch             = gptneox
0.00.054.496 I print_info: vocab_only       = 0
0.00.054.496 I print_info: n_ctx_train      = 2048
0.00.054.496 I print_info: n_embd           = 2048
0.00.054.496 I print_info: n_layer          = 24
0.00.054.502 I print_info: n_head           = 16
0.00.054.502 I print_info: n_head_kv        = 16
0.00.054.504 I print_info: n_rot            = 32
0.00.054.504 I print_info: n_swa            = 0
0.00.054.504 I print_info: n_embd_head_k    = 128
0.00.054.505 I print_info: n_embd_head_v    = 128
0.00.054.505 I print_info: n_gqa            = 1
0.00.054.506 I print_info: n_embd_k_gqa     = 2048
0.00.054.507 I print_info: n_embd_v_gqa     = 2048
0.00.054.507 I print_info: f_norm_eps       = 1.0e-05
0.00.054.508 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.054.508 I print_info: f_clamp_kqv      = 0.0e+00
0.00.054.508 I print_info: f_max_alibi_bias = 0.0e+00
0.00.054.508 I print_info: f_logit_scale    = 0.0e+00
0.00.054.517 I print_info: n_ff             = 8192
0.00.054.519 I print_info: n_expert         = 0
0.00.054.519 I print_info: n_expert_used    = 0
0.00.054.519 I print_info: causal attn      = 1
0.00.054.519 I print_info: pooling type     = 0
0.00.054.519 I print_info: rope type        = 2
0.00.054.520 I print_info: rope scaling     = linear
0.00.054.520 I print_info: freq_base_train  = 10000.0
0.00.054.521 I print_info: freq_scale_train = 1
0.00.054.522 I print_info: n_ctx_orig_yarn  = 2048
0.00.054.522 I print_info: rope_finetuned   = unknown
0.00.054.522 I print_info: ssm_d_conv       = 0
0.00.054.523 I print_info: ssm_d_inner      = 0
0.00.054.523 I print_info: ssm_d_state      = 0
0.00.054.523 I print_info: ssm_dt_rank      = 0
0.00.054.523 I print_info: ssm_dt_b_c_rms   = 0
0.00.054.523 I print_info: model type       = 1.4B
0.00.054.524 I print_info: model params     = 1.41 B
0.00.054.524 I print_info: general.name     = 1.4B
0.00.054.525 I print_info: vocab type       = BPE
0.00.054.525 I print_info: n_vocab          = 50304
0.00.054.525 I print_info: n_merges         = 50009
0.00.054.525 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.054.526 I print_info: LF token         = 187 'Ċ'
0.00.054.527 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.054.527 I print_info: max token length = 1024
0.00.054.527 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.156.440 I load_tensors: offloading 24 repeating layers to GPU
0.01.156.445 I load_tensors: offloading output layer to GPU
0.01.156.446 I load_tensors: offloaded 25/25 layers to GPU
0.01.156.471 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.156.473 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.157.848 I llama_init_from_model: n_seq_max     = 1
0.01.157.850 I llama_init_from_model: n_ctx         = 2048
0.01.157.850 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.157.851 I llama_init_from_model: n_batch       = 2048
0.01.157.851 I llama_init_from_model: n_ubatch      = 512
0.01.157.851 I llama_init_from_model: flash_attn    = 0
0.01.157.852 I llama_init_from_model: freq_base     = 10000.0
0.01.157.853 I llama_init_from_model: freq_scale    = 1
0.01.157.854 I ggml_metal_init: allocating
0.01.157.870 I ggml_metal_init: found device: Apple M4
0.01.157.878 I ggml_metal_init: picking default device: Apple M4
0.01.159.258 I ggml_metal_init: using embedded metal library
0.01.165.243 I ggml_metal_init: GPU name:   Apple M4
0.01.165.246 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.165.247 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.165.248 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.165.248 I ggml_metal_init: simdgroup reduction   = true
0.01.165.248 I ggml_metal_init: simdgroup matrix mul. = true
0.01.165.249 I ggml_metal_init: has residency sets    = true
0.01.165.249 I ggml_metal_init: has bfloat            = true
0.01.165.249 I ggml_metal_init: use bfloat            = true
0.01.165.250 I ggml_metal_init: hasUnifiedMemory      = true
0.01.165.251 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.180.703 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.236.227 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.236.234 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.236.270 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.241.368 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.241.371 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.241.371 I llama_init_from_model: graph nodes  = 967
0.01.241.371 I llama_init_from_model: graph splits = 2
0.01.241.378 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.241.502 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.241.502 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.299.782 I main: llama threadpool init, n_threads = 4
0.01.299.825 I 
0.01.299.845 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.299.845 I 
0.01.300.006 I sampler seed: 1234
0.01.300.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.300.021 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.300.021 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.300.023 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.391.393 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52788.10 tokens per second)
0.02.391.394 I llama_perf_context_print:        load time =    1288.48 ms
0.02.391.394 I llama_perf_context_print: prompt eval time =      49.53 ms /     7 tokens (    7.08 ms per token,   141.33 tokens per second)
0.02.391.395 I llama_perf_context_print:        eval time =    1038.86 ms /    63 runs   (   16.49 ms per token,    60.64 tokens per second)
0.02.391.396 I llama_perf_context_print:       total time =    1092.37 ms /    70 tokens
0.02.391.620 I ggml_metal_free: deallocating

real	0m2.410s
user	0m0.109s
sys	0m0.270s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.016.276 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.807 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.032.825 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.828 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.830 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.831 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.832 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.832 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.834 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.835 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.835 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.295 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.042.296 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.297 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.297 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.297 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.298 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.042.298 I llama_model_loader: - type  f32:  194 tensors
0.00.042.298 I llama_model_loader: - type q4_0:   97 tensors
0.00.042.298 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.299 I print_info: file format = GGUF V3 (latest)
0.00.042.300 I print_info: file type   = Q4_0
0.00.042.301 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.051.429 I load: special tokens cache size = 25
0.00.058.991 I load: token to piece cache size = 0.2984 MB
0.00.058.995 I print_info: arch             = gptneox
0.00.058.995 I print_info: vocab_only       = 0
0.00.058.995 I print_info: n_ctx_train      = 2048
0.00.058.996 I print_info: n_embd           = 2048
0.00.058.996 I print_info: n_layer          = 24
0.00.059.000 I print_info: n_head           = 16
0.00.059.001 I print_info: n_head_kv        = 16
0.00.059.001 I print_info: n_rot            = 32
0.00.059.001 I print_info: n_swa            = 0
0.00.059.001 I print_info: n_embd_head_k    = 128
0.00.059.002 I print_info: n_embd_head_v    = 128
0.00.059.003 I print_info: n_gqa            = 1
0.00.059.003 I print_info: n_embd_k_gqa     = 2048
0.00.059.004 I print_info: n_embd_v_gqa     = 2048
0.00.059.005 I print_info: f_norm_eps       = 1.0e-05
0.00.059.005 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.005 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.005 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.006 I print_info: f_logit_scale    = 0.0e+00
0.00.059.006 I print_info: n_ff             = 8192
0.00.059.006 I print_info: n_expert         = 0
0.00.059.007 I print_info: n_expert_used    = 0
0.00.059.007 I print_info: causal attn      = 1
0.00.059.007 I print_info: pooling type     = 0
0.00.059.007 I print_info: rope type        = 2
0.00.059.010 I print_info: rope scaling     = linear
0.00.059.010 I print_info: freq_base_train  = 10000.0
0.00.059.011 I print_info: freq_scale_train = 1
0.00.059.011 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.011 I print_info: rope_finetuned   = unknown
0.00.059.011 I print_info: ssm_d_conv       = 0
0.00.059.011 I print_info: ssm_d_inner      = 0
0.00.059.011 I print_info: ssm_d_state      = 0
0.00.059.011 I print_info: ssm_dt_rank      = 0
0.00.059.012 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.012 I print_info: model type       = 1.4B
0.00.059.012 I print_info: model params     = 1.41 B
0.00.059.012 I print_info: general.name     = 1.4B
0.00.059.013 I print_info: vocab type       = BPE
0.00.059.013 I print_info: n_vocab          = 50304
0.00.059.013 I print_info: n_merges         = 50009
0.00.059.014 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.014 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.014 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.014 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.015 I print_info: LF token         = 187 'Ċ'
0.00.059.021 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.023 I print_info: max token length = 1024
0.00.059.024 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.786.462 I load_tensors: offloading 24 repeating layers to GPU
0.00.786.473 I load_tensors: offloading output layer to GPU
0.00.786.480 I load_tensors: offloaded 25/25 layers to GPU
0.00.786.512 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.786.514 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.787.820 I llama_init_from_model: n_seq_max     = 1
0.00.787.832 I llama_init_from_model: n_ctx         = 2048
0.00.787.833 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.787.833 I llama_init_from_model: n_batch       = 2048
0.00.787.834 I llama_init_from_model: n_ubatch      = 512
0.00.787.834 I llama_init_from_model: flash_attn    = 0
0.00.787.836 I llama_init_from_model: freq_base     = 10000.0
0.00.787.837 I llama_init_from_model: freq_scale    = 1
0.00.787.839 I ggml_metal_init: allocating
0.00.787.913 I ggml_metal_init: found device: Apple M4
0.00.787.925 I ggml_metal_init: picking default device: Apple M4
0.00.789.709 I ggml_metal_init: using embedded metal library
0.00.795.506 I ggml_metal_init: GPU name:   Apple M4
0.00.795.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.795.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.795.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.795.524 I ggml_metal_init: simdgroup reduction   = true
0.00.795.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.795.524 I ggml_metal_init: has residency sets    = true
0.00.795.525 I ggml_metal_init: has bfloat            = true
0.00.795.525 I ggml_metal_init: use bfloat            = true
0.00.795.527 I ggml_metal_init: hasUnifiedMemory      = true
0.00.795.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.807.832 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.838.461 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.838.466 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.838.504 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.843.733 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.843.735 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.843.736 I llama_init_from_model: graph nodes  = 967
0.00.843.736 I llama_init_from_model: graph splits = 2
0.00.843.743 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.843.872 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.843.873 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.897.353 I main: llama threadpool init, n_threads = 4
0.00.897.397 I 
0.00.897.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.897.420 I 
0.00.897.595 I sampler seed: 1234
0.00.897.600 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.897.610 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.897.611 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.897.611 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.577.366 I llama_perf_sampler_print:    sampling time =       1.51 ms /    71 runs   (    0.02 ms per token, 47176.08 tokens per second)
0.01.577.367 I llama_perf_context_print:        load time =     880.32 ms
0.01.577.367 I llama_perf_context_print: prompt eval time =      48.95 ms /     7 tokens (    6.99 ms per token,   143.00 tokens per second)
0.01.577.368 I llama_perf_context_print:        eval time =     628.27 ms /    63 runs   (    9.97 ms per token,   100.28 tokens per second)
0.01.577.368 I llama_perf_context_print:       total time =     680.77 ms /    70 tokens
0.01.577.587 I ggml_metal_free: deallocating

real	0m1.595s
user	0m0.107s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.094 I main: load the model and apply lora adapter, if any
0.00.008.895 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.773 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.779 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.785 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.785 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.786 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.786 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.786 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.788 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.788 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.789 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.789 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.790 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.790 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.791 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.793 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.794 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.559 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.329 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.330 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.331 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.331 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.331 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.332 I llama_model_loader: - type  f32:  194 tensors
0.00.025.332 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.333 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.333 I print_info: file format = GGUF V3 (latest)
0.00.025.334 I print_info: file type   = Q4_1
0.00.025.335 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.307 I load: special tokens cache size = 25
0.00.039.294 I load: token to piece cache size = 0.2984 MB
0.00.039.298 I print_info: arch             = gptneox
0.00.039.298 I print_info: vocab_only       = 0
0.00.039.299 I print_info: n_ctx_train      = 2048
0.00.039.299 I print_info: n_embd           = 2048
0.00.039.299 I print_info: n_layer          = 24
0.00.039.304 I print_info: n_head           = 16
0.00.039.304 I print_info: n_head_kv        = 16
0.00.039.305 I print_info: n_rot            = 32
0.00.039.307 I print_info: n_swa            = 0
0.00.039.308 I print_info: n_embd_head_k    = 128
0.00.039.308 I print_info: n_embd_head_v    = 128
0.00.039.308 I print_info: n_gqa            = 1
0.00.039.309 I print_info: n_embd_k_gqa     = 2048
0.00.039.309 I print_info: n_embd_v_gqa     = 2048
0.00.039.310 I print_info: f_norm_eps       = 1.0e-05
0.00.039.311 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.311 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.311 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.313 I print_info: f_logit_scale    = 0.0e+00
0.00.039.313 I print_info: n_ff             = 8192
0.00.039.313 I print_info: n_expert         = 0
0.00.039.313 I print_info: n_expert_used    = 0
0.00.039.313 I print_info: causal attn      = 1
0.00.039.314 I print_info: pooling type     = 0
0.00.039.314 I print_info: rope type        = 2
0.00.039.315 I print_info: rope scaling     = linear
0.00.039.315 I print_info: freq_base_train  = 10000.0
0.00.039.315 I print_info: freq_scale_train = 1
0.00.039.315 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.316 I print_info: rope_finetuned   = unknown
0.00.039.316 I print_info: ssm_d_conv       = 0
0.00.039.316 I print_info: ssm_d_inner      = 0
0.00.039.316 I print_info: ssm_d_state      = 0
0.00.039.316 I print_info: ssm_dt_rank      = 0
0.00.039.316 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.317 I print_info: model type       = 1.4B
0.00.039.317 I print_info: model params     = 1.41 B
0.00.039.317 I print_info: general.name     = 1.4B
0.00.039.318 I print_info: vocab type       = BPE
0.00.039.319 I print_info: n_vocab          = 50304
0.00.039.319 I print_info: n_merges         = 50009
0.00.039.319 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.319 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: LF token         = 187 'Ċ'
0.00.039.320 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.320 I print_info: max token length = 1024
0.00.039.321 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.730.104 I load_tensors: offloading 24 repeating layers to GPU
0.00.730.116 I load_tensors: offloading output layer to GPU
0.00.730.117 I load_tensors: offloaded 25/25 layers to GPU
0.00.730.151 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.730.156 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.731.397 I llama_init_from_model: n_seq_max     = 1
0.00.731.410 I llama_init_from_model: n_ctx         = 2048
0.00.731.411 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.731.411 I llama_init_from_model: n_batch       = 2048
0.00.731.412 I llama_init_from_model: n_ubatch      = 512
0.00.731.412 I llama_init_from_model: flash_attn    = 0
0.00.731.415 I llama_init_from_model: freq_base     = 10000.0
0.00.731.415 I llama_init_from_model: freq_scale    = 1
0.00.731.418 I ggml_metal_init: allocating
0.00.731.500 I ggml_metal_init: found device: Apple M4
0.00.731.515 I ggml_metal_init: picking default device: Apple M4
0.00.733.354 I ggml_metal_init: using embedded metal library
0.00.740.135 I ggml_metal_init: GPU name:   Apple M4
0.00.740.142 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.740.143 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.740.143 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.740.147 I ggml_metal_init: simdgroup reduction   = true
0.00.740.148 I ggml_metal_init: simdgroup matrix mul. = true
0.00.740.148 I ggml_metal_init: has residency sets    = true
0.00.740.148 I ggml_metal_init: has bfloat            = true
0.00.740.148 I ggml_metal_init: use bfloat            = true
0.00.740.150 I ggml_metal_init: hasUnifiedMemory      = true
0.00.740.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.759.276 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.816.964 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.816.970 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.817.004 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.822.593 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.822.596 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.822.596 I llama_init_from_model: graph nodes  = 967
0.00.822.596 I llama_init_from_model: graph splits = 2
0.00.822.601 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.822.735 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.822.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.390 I main: llama threadpool init, n_threads = 4
0.00.879.438 I 
0.00.879.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.465 I 
0.00.879.620 I sampler seed: 1234
0.00.879.624 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.879.674 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.879.675 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.879.675 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.605.269 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54489.64 tokens per second)
0.01.605.270 I llama_perf_context_print:        load time =     869.76 ms
0.01.605.270 I llama_perf_context_print: prompt eval time =      49.14 ms /     7 tokens (    7.02 ms per token,   142.44 tokens per second)
0.01.605.271 I llama_perf_context_print:        eval time =     673.63 ms /    63 runs   (   10.69 ms per token,    93.52 tokens per second)
0.01.605.271 I llama_perf_context_print:       total time =     726.61 ms /    70 tokens
0.01.605.514 I ggml_metal_free: deallocating

real	0m1.623s
user	0m0.111s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.009.786 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.032 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.037 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.039 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.039 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.039 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.048 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.050 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.051 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.799 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.483 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.484 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.484 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.485 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.485 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.485 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.485 I llama_model_loader: - type  f32:  194 tensors
0.00.025.486 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.486 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.486 I print_info: file format = GGUF V3 (latest)
0.00.025.487 I print_info: file type   = Q5_0
0.00.025.487 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.602 I load: special tokens cache size = 25
0.00.039.769 I load: token to piece cache size = 0.2984 MB
0.00.039.772 I print_info: arch             = gptneox
0.00.039.772 I print_info: vocab_only       = 0
0.00.039.772 I print_info: n_ctx_train      = 2048
0.00.039.773 I print_info: n_embd           = 2048
0.00.039.773 I print_info: n_layer          = 24
0.00.039.776 I print_info: n_head           = 16
0.00.039.777 I print_info: n_head_kv        = 16
0.00.039.777 I print_info: n_rot            = 32
0.00.039.777 I print_info: n_swa            = 0
0.00.039.777 I print_info: n_embd_head_k    = 128
0.00.039.779 I print_info: n_embd_head_v    = 128
0.00.039.780 I print_info: n_gqa            = 1
0.00.039.781 I print_info: n_embd_k_gqa     = 2048
0.00.039.782 I print_info: n_embd_v_gqa     = 2048
0.00.039.782 I print_info: f_norm_eps       = 1.0e-05
0.00.039.787 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.787 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.787 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.787 I print_info: f_logit_scale    = 0.0e+00
0.00.039.788 I print_info: n_ff             = 8192
0.00.039.788 I print_info: n_expert         = 0
0.00.039.789 I print_info: n_expert_used    = 0
0.00.039.789 I print_info: causal attn      = 1
0.00.039.790 I print_info: pooling type     = 0
0.00.039.792 I print_info: rope type        = 2
0.00.039.792 I print_info: rope scaling     = linear
0.00.039.793 I print_info: freq_base_train  = 10000.0
0.00.039.793 I print_info: freq_scale_train = 1
0.00.039.793 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.793 I print_info: rope_finetuned   = unknown
0.00.039.793 I print_info: ssm_d_conv       = 0
0.00.039.794 I print_info: ssm_d_inner      = 0
0.00.039.794 I print_info: ssm_d_state      = 0
0.00.039.794 I print_info: ssm_dt_rank      = 0
0.00.039.794 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.794 I print_info: model type       = 1.4B
0.00.039.795 I print_info: model params     = 1.41 B
0.00.039.795 I print_info: general.name     = 1.4B
0.00.039.795 I print_info: vocab type       = BPE
0.00.039.796 I print_info: n_vocab          = 50304
0.00.039.796 I print_info: n_merges         = 50009
0.00.039.796 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.796 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: LF token         = 187 'Ċ'
0.00.039.797 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: max token length = 1024
0.00.039.797 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.717.266 I load_tensors: offloading 24 repeating layers to GPU
0.00.717.278 I load_tensors: offloading output layer to GPU
0.00.717.279 I load_tensors: offloaded 25/25 layers to GPU
0.00.717.312 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.717.314 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.718.848 I llama_init_from_model: n_seq_max     = 1
0.00.718.851 I llama_init_from_model: n_ctx         = 2048
0.00.718.852 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.718.853 I llama_init_from_model: n_batch       = 2048
0.00.718.853 I llama_init_from_model: n_ubatch      = 512
0.00.718.853 I llama_init_from_model: flash_attn    = 0
0.00.718.855 I llama_init_from_model: freq_base     = 10000.0
0.00.718.856 I llama_init_from_model: freq_scale    = 1
0.00.718.858 I ggml_metal_init: allocating
0.00.718.933 I ggml_metal_init: found device: Apple M4
0.00.718.946 I ggml_metal_init: picking default device: Apple M4
0.00.720.822 I ggml_metal_init: using embedded metal library
0.00.727.535 I ggml_metal_init: GPU name:   Apple M4
0.00.727.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.727.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.727.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.727.542 I ggml_metal_init: simdgroup reduction   = true
0.00.727.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.727.543 I ggml_metal_init: has residency sets    = true
0.00.727.543 I ggml_metal_init: has bfloat            = true
0.00.727.543 I ggml_metal_init: use bfloat            = true
0.00.727.544 I ggml_metal_init: hasUnifiedMemory      = true
0.00.727.546 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.744.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.796.916 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.796.922 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.796.960 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.801.301 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.801.303 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.801.303 I llama_init_from_model: graph nodes  = 967
0.00.801.304 I llama_init_from_model: graph splits = 2
0.00.801.310 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.801.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.801.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.860.502 I main: llama threadpool init, n_threads = 4
0.00.860.545 I 
0.00.860.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.860.567 I 
0.00.860.709 I sampler seed: 1234
0.00.860.713 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.860.733 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.860.733 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.860.733 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.647.540 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.647.540 I llama_perf_context_print:        load time =     849.96 ms
0.01.647.541 I llama_perf_context_print: prompt eval time =      48.76 ms /     7 tokens (    6.97 ms per token,   143.55 tokens per second)
0.01.647.542 I llama_perf_context_print:        eval time =     735.17 ms /    63 runs   (   11.67 ms per token,    85.69 tokens per second)
0.01.647.542 I llama_perf_context_print:       total time =     787.79 ms /    70 tokens
0.01.647.803 I ggml_metal_free: deallocating

real	0m1.666s
user	0m0.109s
sys	0m0.226s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.756 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.294 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.305 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.306 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.307 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.307 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.309 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.309 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.309 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.313 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.025 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.036 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.758 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.758 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.759 I print_info: file format = GGUF V3 (latest)
0.00.024.760 I print_info: file type   = Q5_1
0.00.024.761 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.911 I load: special tokens cache size = 25
0.00.038.912 I load: token to piece cache size = 0.2984 MB
0.00.038.915 I print_info: arch             = gptneox
0.00.038.915 I print_info: vocab_only       = 0
0.00.038.915 I print_info: n_ctx_train      = 2048
0.00.038.915 I print_info: n_embd           = 2048
0.00.038.916 I print_info: n_layer          = 24
0.00.038.918 I print_info: n_head           = 16
0.00.038.919 I print_info: n_head_kv        = 16
0.00.038.919 I print_info: n_rot            = 32
0.00.038.919 I print_info: n_swa            = 0
0.00.038.920 I print_info: n_embd_head_k    = 128
0.00.038.922 I print_info: n_embd_head_v    = 128
0.00.038.923 I print_info: n_gqa            = 1
0.00.038.923 I print_info: n_embd_k_gqa     = 2048
0.00.038.924 I print_info: n_embd_v_gqa     = 2048
0.00.038.925 I print_info: f_norm_eps       = 1.0e-05
0.00.038.925 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.925 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.926 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.926 I print_info: f_logit_scale    = 0.0e+00
0.00.038.926 I print_info: n_ff             = 8192
0.00.038.927 I print_info: n_expert         = 0
0.00.038.927 I print_info: n_expert_used    = 0
0.00.038.927 I print_info: causal attn      = 1
0.00.038.927 I print_info: pooling type     = 0
0.00.038.927 I print_info: rope type        = 2
0.00.038.928 I print_info: rope scaling     = linear
0.00.038.928 I print_info: freq_base_train  = 10000.0
0.00.038.928 I print_info: freq_scale_train = 1
0.00.038.928 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.929 I print_info: rope_finetuned   = unknown
0.00.038.929 I print_info: ssm_d_conv       = 0
0.00.038.931 I print_info: ssm_d_inner      = 0
0.00.038.931 I print_info: ssm_d_state      = 0
0.00.038.931 I print_info: ssm_dt_rank      = 0
0.00.038.931 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.931 I print_info: model type       = 1.4B
0.00.038.932 I print_info: model params     = 1.41 B
0.00.038.932 I print_info: general.name     = 1.4B
0.00.038.932 I print_info: vocab type       = BPE
0.00.038.932 I print_info: n_vocab          = 50304
0.00.038.933 I print_info: n_merges         = 50009
0.00.038.933 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.933 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.937 I print_info: LF token         = 187 'Ċ'
0.00.038.938 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.938 I print_info: max token length = 1024
0.00.038.938 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.611.728 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.742 I load_tensors: offloading output layer to GPU
0.00.611.742 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.771 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.611.772 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.613.208 I llama_init_from_model: n_seq_max     = 1
0.00.613.213 I llama_init_from_model: n_ctx         = 2048
0.00.613.213 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.613.214 I llama_init_from_model: n_batch       = 2048
0.00.613.214 I llama_init_from_model: n_ubatch      = 512
0.00.613.215 I llama_init_from_model: flash_attn    = 0
0.00.613.217 I llama_init_from_model: freq_base     = 10000.0
0.00.613.217 I llama_init_from_model: freq_scale    = 1
0.00.613.221 I ggml_metal_init: allocating
0.00.613.270 I ggml_metal_init: found device: Apple M4
0.00.613.282 I ggml_metal_init: picking default device: Apple M4
0.00.615.073 I ggml_metal_init: using embedded metal library
0.00.621.029 I ggml_metal_init: GPU name:   Apple M4
0.00.621.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.037 I ggml_metal_init: simdgroup reduction   = true
0.00.621.037 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.037 I ggml_metal_init: has residency sets    = true
0.00.621.037 I ggml_metal_init: has bfloat            = true
0.00.621.038 I ggml_metal_init: use bfloat            = true
0.00.621.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.324 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.456 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.461 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.495 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.025 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.027 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.028 I llama_init_from_model: graph nodes  = 967
0.00.703.028 I llama_init_from_model: graph splits = 2
0.00.703.034 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.166 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.167 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.762.158 I main: llama threadpool init, n_threads = 4
0.00.762.199 I 
0.00.762.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.762.223 I 
0.00.762.401 I sampler seed: 1234
0.00.762.406 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.762.440 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.762.443 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.762.443 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.604.466 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52709.73 tokens per second)
0.01.604.466 I llama_perf_context_print:        load time =     752.68 ms
0.01.604.467 I llama_perf_context_print: prompt eval time =      52.25 ms /     7 tokens (    7.46 ms per token,   133.97 tokens per second)
0.01.604.469 I llama_perf_context_print:        eval time =     786.88 ms /    63 runs   (   12.49 ms per token,    80.06 tokens per second)
0.01.604.469 I llama_perf_context_print:       total time =     843.02 ms /    70 tokens
0.01.604.712 I ggml_metal_free: deallocating

real	0m1.622s
user	0m0.110s
sys	0m0.222s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.897 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.528 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.533 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.535 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.535 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.536 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.536 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.537 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.537 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.538 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.538 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.538 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.539 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.539 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.540 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.541 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.541 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.542 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.124 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.125 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.126 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.126 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.126 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.127 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.127 I llama_model_loader: - type  f32:  194 tensors
0.00.025.128 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.128 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.128 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.129 I print_info: file format = GGUF V3 (latest)
0.00.025.129 I print_info: file type   = Q2_K - Medium
0.00.025.130 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.301 I load: special tokens cache size = 25
0.00.039.271 I load: token to piece cache size = 0.2984 MB
0.00.039.274 I print_info: arch             = gptneox
0.00.039.274 I print_info: vocab_only       = 0
0.00.039.274 I print_info: n_ctx_train      = 2048
0.00.039.275 I print_info: n_embd           = 2048
0.00.039.275 I print_info: n_layer          = 24
0.00.039.278 I print_info: n_head           = 16
0.00.039.279 I print_info: n_head_kv        = 16
0.00.039.279 I print_info: n_rot            = 32
0.00.039.281 I print_info: n_swa            = 0
0.00.039.281 I print_info: n_embd_head_k    = 128
0.00.039.282 I print_info: n_embd_head_v    = 128
0.00.039.282 I print_info: n_gqa            = 1
0.00.039.283 I print_info: n_embd_k_gqa     = 2048
0.00.039.284 I print_info: n_embd_v_gqa     = 2048
0.00.039.284 I print_info: f_norm_eps       = 1.0e-05
0.00.039.285 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.285 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.285 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.285 I print_info: f_logit_scale    = 0.0e+00
0.00.039.286 I print_info: n_ff             = 8192
0.00.039.286 I print_info: n_expert         = 0
0.00.039.286 I print_info: n_expert_used    = 0
0.00.039.286 I print_info: causal attn      = 1
0.00.039.287 I print_info: pooling type     = 0
0.00.039.287 I print_info: rope type        = 2
0.00.039.288 I print_info: rope scaling     = linear
0.00.039.290 I print_info: freq_base_train  = 10000.0
0.00.039.290 I print_info: freq_scale_train = 1
0.00.039.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.290 I print_info: rope_finetuned   = unknown
0.00.039.290 I print_info: ssm_d_conv       = 0
0.00.039.291 I print_info: ssm_d_inner      = 0
0.00.039.291 I print_info: ssm_d_state      = 0
0.00.039.291 I print_info: ssm_dt_rank      = 0
0.00.039.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.291 I print_info: model type       = 1.4B
0.00.039.292 I print_info: model params     = 1.41 B
0.00.039.292 I print_info: general.name     = 1.4B
0.00.039.292 I print_info: vocab type       = BPE
0.00.039.292 I print_info: n_vocab          = 50304
0.00.039.296 I print_info: n_merges         = 50009
0.00.039.297 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.297 I print_info: LF token         = 187 'Ċ'
0.00.039.298 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.298 I print_info: max token length = 1024
0.00.039.298 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.342.567 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.584 I load_tensors: offloading output layer to GPU
0.00.342.585 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.623 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.624 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.344.263 I llama_init_from_model: n_seq_max     = 1
0.00.344.266 I llama_init_from_model: n_ctx         = 2048
0.00.344.267 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.344.267 I llama_init_from_model: n_batch       = 2048
0.00.344.268 I llama_init_from_model: n_ubatch      = 512
0.00.344.268 I llama_init_from_model: flash_attn    = 0
0.00.344.271 I llama_init_from_model: freq_base     = 10000.0
0.00.344.272 I llama_init_from_model: freq_scale    = 1
0.00.344.276 I ggml_metal_init: allocating
0.00.344.379 I ggml_metal_init: found device: Apple M4
0.00.344.393 I ggml_metal_init: picking default device: Apple M4
0.00.346.391 I ggml_metal_init: using embedded metal library
0.00.351.983 I ggml_metal_init: GPU name:   Apple M4
0.00.351.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.998 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.999 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.352.000 I ggml_metal_init: simdgroup reduction   = true
0.00.352.000 I ggml_metal_init: simdgroup matrix mul. = true
0.00.352.001 I ggml_metal_init: has residency sets    = true
0.00.352.001 I ggml_metal_init: has bfloat            = true
0.00.352.001 I ggml_metal_init: use bfloat            = true
0.00.352.003 I ggml_metal_init: hasUnifiedMemory      = true
0.00.352.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.888 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.433.702 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.433.708 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.433.743 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.965 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.437.967 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.437.967 I llama_init_from_model: graph nodes  = 967
0.00.437.967 I llama_init_from_model: graph splits = 2
0.00.437.974 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.438.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.438.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.499.805 I main: llama threadpool init, n_threads = 4
0.00.499.850 I 
0.00.499.876 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.499.877 I 
0.00.500.053 I sampler seed: 1234
0.00.500.058 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.500.078 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.500.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.500.078 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.185.773 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.185.774 I llama_perf_context_print:        load time =     489.18 ms
0.01.185.775 I llama_perf_context_print: prompt eval time =      44.16 ms /     7 tokens (    6.31 ms per token,   158.51 tokens per second)
0.01.185.775 I llama_perf_context_print:        eval time =     638.72 ms /    63 runs   (   10.14 ms per token,    98.63 tokens per second)
0.01.185.776 I llama_perf_context_print:       total time =     686.70 ms /    70 tokens
0.01.185.966 I ggml_metal_free: deallocating

real	0m1.205s
user	0m0.112s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.744 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.372 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.375 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.378 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.378 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.380 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.099 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.085 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.755 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.755 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.755 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.756 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.756 I llama_model_loader: - type  f32:  194 tensors
0.00.024.757 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.757 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.757 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.757 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.758 I print_info: file format = GGUF V3 (latest)
0.00.024.758 I print_info: file type   = Q3_K - Medium
0.00.024.760 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.560 I load: special tokens cache size = 25
0.00.038.538 I load: token to piece cache size = 0.2984 MB
0.00.038.541 I print_info: arch             = gptneox
0.00.038.541 I print_info: vocab_only       = 0
0.00.038.541 I print_info: n_ctx_train      = 2048
0.00.038.541 I print_info: n_embd           = 2048
0.00.038.542 I print_info: n_layer          = 24
0.00.038.544 I print_info: n_head           = 16
0.00.038.545 I print_info: n_head_kv        = 16
0.00.038.545 I print_info: n_rot            = 32
0.00.038.545 I print_info: n_swa            = 0
0.00.038.546 I print_info: n_embd_head_k    = 128
0.00.038.546 I print_info: n_embd_head_v    = 128
0.00.038.546 I print_info: n_gqa            = 1
0.00.038.547 I print_info: n_embd_k_gqa     = 2048
0.00.038.548 I print_info: n_embd_v_gqa     = 2048
0.00.038.548 I print_info: f_norm_eps       = 1.0e-05
0.00.038.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.549 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.549 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.549 I print_info: f_logit_scale    = 0.0e+00
0.00.038.550 I print_info: n_ff             = 8192
0.00.038.550 I print_info: n_expert         = 0
0.00.038.550 I print_info: n_expert_used    = 0
0.00.038.551 I print_info: causal attn      = 1
0.00.038.551 I print_info: pooling type     = 0
0.00.038.551 I print_info: rope type        = 2
0.00.038.551 I print_info: rope scaling     = linear
0.00.038.552 I print_info: freq_base_train  = 10000.0
0.00.038.552 I print_info: freq_scale_train = 1
0.00.038.552 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.552 I print_info: rope_finetuned   = unknown
0.00.038.552 I print_info: ssm_d_conv       = 0
0.00.038.553 I print_info: ssm_d_inner      = 0
0.00.038.553 I print_info: ssm_d_state      = 0
0.00.038.553 I print_info: ssm_dt_rank      = 0
0.00.038.553 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.553 I print_info: model type       = 1.4B
0.00.038.554 I print_info: model params     = 1.41 B
0.00.038.554 I print_info: general.name     = 1.4B
0.00.038.554 I print_info: vocab type       = BPE
0.00.038.554 I print_info: n_vocab          = 50304
0.00.038.555 I print_info: n_merges         = 50009
0.00.038.557 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.557 I print_info: LF token         = 187 'Ċ'
0.00.038.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.558 I print_info: max token length = 1024
0.00.038.558 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.455.871 I load_tensors: offloading 24 repeating layers to GPU
0.00.455.884 I load_tensors: offloading output layer to GPU
0.00.455.885 I load_tensors: offloaded 25/25 layers to GPU
0.00.455.918 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.455.919 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.457.514 I llama_init_from_model: n_seq_max     = 1
0.00.457.520 I llama_init_from_model: n_ctx         = 2048
0.00.457.521 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.457.522 I llama_init_from_model: n_batch       = 2048
0.00.457.522 I llama_init_from_model: n_ubatch      = 512
0.00.457.523 I llama_init_from_model: flash_attn    = 0
0.00.457.525 I llama_init_from_model: freq_base     = 10000.0
0.00.457.526 I llama_init_from_model: freq_scale    = 1
0.00.457.530 I ggml_metal_init: allocating
0.00.457.602 I ggml_metal_init: found device: Apple M4
0.00.457.614 I ggml_metal_init: picking default device: Apple M4
0.00.459.524 I ggml_metal_init: using embedded metal library
0.00.465.257 I ggml_metal_init: GPU name:   Apple M4
0.00.465.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.465.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.465.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.465.266 I ggml_metal_init: simdgroup reduction   = true
0.00.465.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.465.266 I ggml_metal_init: has residency sets    = true
0.00.465.267 I ggml_metal_init: has bfloat            = true
0.00.465.267 I ggml_metal_init: use bfloat            = true
0.00.465.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.465.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.485.053 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.536.618 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.536.625 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.536.657 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.541.575 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.541.578 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.541.578 I llama_init_from_model: graph nodes  = 967
0.00.541.578 I llama_init_from_model: graph splits = 2
0.00.541.584 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.541.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.541.710 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.597.941 I main: llama threadpool init, n_threads = 4
0.00.597.987 I 
0.00.598.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.598.011 I 
0.00.598.162 I sampler seed: 1234
0.00.598.167 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.598.187 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.598.187 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.598.188 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.345.242 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51337.67 tokens per second)
0.01.345.243 I llama_perf_context_print:        load time =     588.48 ms
0.01.345.243 I llama_perf_context_print: prompt eval time =      48.56 ms /     7 tokens (    6.94 ms per token,   144.14 tokens per second)
0.01.345.245 I llama_perf_context_print:        eval time =     695.57 ms /    63 runs   (   11.04 ms per token,    90.57 tokens per second)
0.01.345.245 I llama_perf_context_print:       total time =     748.02 ms /    70 tokens
0.01.345.496 I ggml_metal_free: deallocating

real	0m1.362s
user	0m0.108s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.011.028 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.879 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.884 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.884 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.885 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.885 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.886 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.888 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.889 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.889 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.889 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.890 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.890 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.891 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.892 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.892 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.706 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.453 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.454 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.455 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.455 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.455 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.456 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.456 I llama_model_loader: - type  f32:  194 tensors
0.00.027.456 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.457 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.457 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.457 I print_info: file format = GGUF V3 (latest)
0.00.027.458 I print_info: file type   = Q4_K - Medium
0.00.027.459 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.035.635 I load: special tokens cache size = 25
0.00.041.944 I load: token to piece cache size = 0.2984 MB
0.00.041.947 I print_info: arch             = gptneox
0.00.041.947 I print_info: vocab_only       = 0
0.00.041.947 I print_info: n_ctx_train      = 2048
0.00.041.947 I print_info: n_embd           = 2048
0.00.041.947 I print_info: n_layer          = 24
0.00.041.950 I print_info: n_head           = 16
0.00.041.951 I print_info: n_head_kv        = 16
0.00.041.951 I print_info: n_rot            = 32
0.00.041.951 I print_info: n_swa            = 0
0.00.041.951 I print_info: n_embd_head_k    = 128
0.00.041.951 I print_info: n_embd_head_v    = 128
0.00.041.952 I print_info: n_gqa            = 1
0.00.041.953 I print_info: n_embd_k_gqa     = 2048
0.00.041.954 I print_info: n_embd_v_gqa     = 2048
0.00.041.954 I print_info: f_norm_eps       = 1.0e-05
0.00.041.955 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.955 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.955 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.955 I print_info: f_logit_scale    = 0.0e+00
0.00.041.957 I print_info: n_ff             = 8192
0.00.041.957 I print_info: n_expert         = 0
0.00.041.958 I print_info: n_expert_used    = 0
0.00.041.958 I print_info: causal attn      = 1
0.00.041.958 I print_info: pooling type     = 0
0.00.041.958 I print_info: rope type        = 2
0.00.041.958 I print_info: rope scaling     = linear
0.00.041.959 I print_info: freq_base_train  = 10000.0
0.00.041.959 I print_info: freq_scale_train = 1
0.00.041.959 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.959 I print_info: rope_finetuned   = unknown
0.00.041.959 I print_info: ssm_d_conv       = 0
0.00.041.960 I print_info: ssm_d_inner      = 0
0.00.041.960 I print_info: ssm_d_state      = 0
0.00.041.960 I print_info: ssm_dt_rank      = 0
0.00.041.960 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.961 I print_info: model type       = 1.4B
0.00.041.962 I print_info: model params     = 1.41 B
0.00.041.962 I print_info: general.name     = 1.4B
0.00.041.962 I print_info: vocab type       = BPE
0.00.041.963 I print_info: n_vocab          = 50304
0.00.041.964 I print_info: n_merges         = 50009
0.00.041.964 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.965 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.965 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.965 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.965 I print_info: LF token         = 187 'Ċ'
0.00.041.966 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.967 I print_info: max token length = 1024
0.00.041.967 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.078 I load_tensors: offloading output layer to GPU
0.00.528.078 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.115 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.121 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.645 I llama_init_from_model: n_seq_max     = 1
0.00.529.647 I llama_init_from_model: n_ctx         = 2048
0.00.529.647 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.529.648 I llama_init_from_model: n_batch       = 2048
0.00.529.648 I llama_init_from_model: n_ubatch      = 512
0.00.529.648 I llama_init_from_model: flash_attn    = 0
0.00.529.650 I llama_init_from_model: freq_base     = 10000.0
0.00.529.651 I llama_init_from_model: freq_scale    = 1
0.00.529.653 I ggml_metal_init: allocating
0.00.529.722 I ggml_metal_init: found device: Apple M4
0.00.529.736 I ggml_metal_init: picking default device: Apple M4
0.00.531.629 I ggml_metal_init: using embedded metal library
0.00.537.593 I ggml_metal_init: GPU name:   Apple M4
0.00.537.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.601 I ggml_metal_init: simdgroup reduction   = true
0.00.537.601 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.601 I ggml_metal_init: has residency sets    = true
0.00.537.601 I ggml_metal_init: has bfloat            = true
0.00.537.602 I ggml_metal_init: use bfloat            = true
0.00.537.603 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.613 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.906 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.756 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.614.763 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.798 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.618.908 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.618.910 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.618.910 I llama_init_from_model: graph nodes  = 967
0.00.618.911 I llama_init_from_model: graph splits = 2
0.00.618.917 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.050 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.394 I main: llama threadpool init, n_threads = 4
0.00.679.441 I 
0.00.679.465 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.466 I 
0.00.679.617 I sampler seed: 1234
0.00.679.621 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.679.632 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.679.632 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.679.632 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.441.921 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.441.922 I llama_perf_context_print:        load time =     667.64 ms
0.01.441.923 I llama_perf_context_print: prompt eval time =      58.00 ms /     7 tokens (    8.29 ms per token,   120.70 tokens per second)
0.01.441.923 I llama_perf_context_print:        eval time =     701.36 ms /    63 runs   (   11.13 ms per token,    89.83 tokens per second)
0.01.441.924 I llama_perf_context_print:       total time =     763.25 ms /    70 tokens
0.01.442.133 I ggml_metal_free: deallocating

real	0m1.458s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.960 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.911 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.912 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.913 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.913 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.914 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.914 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.915 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.915 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.915 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.916 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.916 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.916 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.917 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.920 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.920 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.921 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.690 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.662 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.342 I llama_model_loader: - type  f32:  194 tensors
0.00.026.342 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.342 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.343 I print_info: file format = GGUF V3 (latest)
0.00.026.343 I print_info: file type   = Q5_K - Medium
0.00.026.344 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.497 I load: special tokens cache size = 25
0.00.040.756 I load: token to piece cache size = 0.2984 MB
0.00.040.758 I print_info: arch             = gptneox
0.00.040.759 I print_info: vocab_only       = 0
0.00.040.759 I print_info: n_ctx_train      = 2048
0.00.040.759 I print_info: n_embd           = 2048
0.00.040.759 I print_info: n_layer          = 24
0.00.040.762 I print_info: n_head           = 16
0.00.040.762 I print_info: n_head_kv        = 16
0.00.040.763 I print_info: n_rot            = 32
0.00.040.763 I print_info: n_swa            = 0
0.00.040.763 I print_info: n_embd_head_k    = 128
0.00.040.763 I print_info: n_embd_head_v    = 128
0.00.040.766 I print_info: n_gqa            = 1
0.00.040.767 I print_info: n_embd_k_gqa     = 2048
0.00.040.767 I print_info: n_embd_v_gqa     = 2048
0.00.040.768 I print_info: f_norm_eps       = 1.0e-05
0.00.040.773 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.773 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.773 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.773 I print_info: f_logit_scale    = 0.0e+00
0.00.040.776 I print_info: n_ff             = 8192
0.00.040.776 I print_info: n_expert         = 0
0.00.040.776 I print_info: n_expert_used    = 0
0.00.040.776 I print_info: causal attn      = 1
0.00.040.776 I print_info: pooling type     = 0
0.00.040.778 I print_info: rope type        = 2
0.00.040.779 I print_info: rope scaling     = linear
0.00.040.779 I print_info: freq_base_train  = 10000.0
0.00.040.780 I print_info: freq_scale_train = 1
0.00.040.780 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.780 I print_info: rope_finetuned   = unknown
0.00.040.780 I print_info: ssm_d_conv       = 0
0.00.040.780 I print_info: ssm_d_inner      = 0
0.00.040.782 I print_info: ssm_d_state      = 0
0.00.040.782 I print_info: ssm_dt_rank      = 0
0.00.040.782 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.782 I print_info: model type       = 1.4B
0.00.040.782 I print_info: model params     = 1.41 B
0.00.040.783 I print_info: general.name     = 1.4B
0.00.040.783 I print_info: vocab type       = BPE
0.00.040.783 I print_info: n_vocab          = 50304
0.00.040.783 I print_info: n_merges         = 50009
0.00.040.784 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.784 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.786 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.786 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.787 I print_info: LF token         = 187 'Ċ'
0.00.040.787 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.787 I print_info: max token length = 1024
0.00.040.787 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.622.060 I load_tensors: offloading 24 repeating layers to GPU
0.00.622.074 I load_tensors: offloading output layer to GPU
0.00.622.075 I load_tensors: offloaded 25/25 layers to GPU
0.00.622.108 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.622.110 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.623.601 I llama_init_from_model: n_seq_max     = 1
0.00.623.604 I llama_init_from_model: n_ctx         = 2048
0.00.623.605 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.623.605 I llama_init_from_model: n_batch       = 2048
0.00.623.606 I llama_init_from_model: n_ubatch      = 512
0.00.623.606 I llama_init_from_model: flash_attn    = 0
0.00.623.608 I llama_init_from_model: freq_base     = 10000.0
0.00.623.609 I llama_init_from_model: freq_scale    = 1
0.00.623.611 I ggml_metal_init: allocating
0.00.623.690 I ggml_metal_init: found device: Apple M4
0.00.623.703 I ggml_metal_init: picking default device: Apple M4
0.00.625.692 I ggml_metal_init: using embedded metal library
0.00.632.290 I ggml_metal_init: GPU name:   Apple M4
0.00.632.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.632.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.632.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.632.296 I ggml_metal_init: simdgroup reduction   = true
0.00.632.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.632.296 I ggml_metal_init: has residency sets    = true
0.00.632.296 I ggml_metal_init: has bfloat            = true
0.00.632.297 I ggml_metal_init: use bfloat            = true
0.00.632.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.632.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.042 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.704.843 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.704.849 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.704.885 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.709.644 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.709.646 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.709.646 I llama_init_from_model: graph nodes  = 967
0.00.709.646 I llama_init_from_model: graph splits = 2
0.00.709.652 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.783 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.758 I main: llama threadpool init, n_threads = 4
0.00.776.799 I 
0.00.776.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.822 I 
0.00.776.967 I sampler seed: 1234
0.00.776.972 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.777.018 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.777.022 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.777.023 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.622.136 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55512.12 tokens per second)
0.01.622.137 I llama_perf_context_print:        load time =     766.09 ms
0.01.622.138 I llama_perf_context_print: prompt eval time =      56.77 ms /     7 tokens (    8.11 ms per token,   123.30 tokens per second)
0.01.622.139 I llama_perf_context_print:        eval time =     785.55 ms /    63 runs   (   12.47 ms per token,    80.20 tokens per second)
0.01.622.139 I llama_perf_context_print:       total time =     846.09 ms /    70 tokens
0.01.622.387 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.109s
sys	0m0.232s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.656 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.895 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.899 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.901 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.902 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.903 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.908 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.909 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.586 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.583 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.232 I llama_model_loader: - type  f32:  194 tensors
0.00.025.232 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.233 I print_info: file format = GGUF V3 (latest)
0.00.025.233 I print_info: file type   = Q6_K
0.00.025.234 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.014 I load: special tokens cache size = 25
0.00.039.091 I load: token to piece cache size = 0.2984 MB
0.00.039.094 I print_info: arch             = gptneox
0.00.039.094 I print_info: vocab_only       = 0
0.00.039.094 I print_info: n_ctx_train      = 2048
0.00.039.094 I print_info: n_embd           = 2048
0.00.039.094 I print_info: n_layer          = 24
0.00.039.097 I print_info: n_head           = 16
0.00.039.098 I print_info: n_head_kv        = 16
0.00.039.098 I print_info: n_rot            = 32
0.00.039.098 I print_info: n_swa            = 0
0.00.039.099 I print_info: n_embd_head_k    = 128
0.00.039.099 I print_info: n_embd_head_v    = 128
0.00.039.099 I print_info: n_gqa            = 1
0.00.039.100 I print_info: n_embd_k_gqa     = 2048
0.00.039.101 I print_info: n_embd_v_gqa     = 2048
0.00.039.101 I print_info: f_norm_eps       = 1.0e-05
0.00.039.102 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.102 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.102 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.102 I print_info: f_logit_scale    = 0.0e+00
0.00.039.103 I print_info: n_ff             = 8192
0.00.039.103 I print_info: n_expert         = 0
0.00.039.103 I print_info: n_expert_used    = 0
0.00.039.104 I print_info: causal attn      = 1
0.00.039.104 I print_info: pooling type     = 0
0.00.039.104 I print_info: rope type        = 2
0.00.039.104 I print_info: rope scaling     = linear
0.00.039.104 I print_info: freq_base_train  = 10000.0
0.00.039.105 I print_info: freq_scale_train = 1
0.00.039.105 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.105 I print_info: rope_finetuned   = unknown
0.00.039.105 I print_info: ssm_d_conv       = 0
0.00.039.106 I print_info: ssm_d_inner      = 0
0.00.039.106 I print_info: ssm_d_state      = 0
0.00.039.106 I print_info: ssm_dt_rank      = 0
0.00.039.106 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.106 I print_info: model type       = 1.4B
0.00.039.107 I print_info: model params     = 1.41 B
0.00.039.107 I print_info: general.name     = 1.4B
0.00.039.107 I print_info: vocab type       = BPE
0.00.039.107 I print_info: n_vocab          = 50304
0.00.039.107 I print_info: n_merges         = 50009
0.00.039.108 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.108 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.109 I print_info: LF token         = 187 'Ċ'
0.00.039.109 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.109 I print_info: max token length = 1024
0.00.039.109 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.693.223 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.228 I load_tensors: offloading output layer to GPU
0.00.693.229 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.256 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.693.257 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.694.655 I llama_init_from_model: n_seq_max     = 1
0.00.694.657 I llama_init_from_model: n_ctx         = 2048
0.00.694.657 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.694.658 I llama_init_from_model: n_batch       = 2048
0.00.694.658 I llama_init_from_model: n_ubatch      = 512
0.00.694.659 I llama_init_from_model: flash_attn    = 0
0.00.694.659 I llama_init_from_model: freq_base     = 10000.0
0.00.694.660 I llama_init_from_model: freq_scale    = 1
0.00.694.661 I ggml_metal_init: allocating
0.00.694.710 I ggml_metal_init: found device: Apple M4
0.00.694.721 I ggml_metal_init: picking default device: Apple M4
0.00.696.254 I ggml_metal_init: using embedded metal library
0.00.702.459 I ggml_metal_init: GPU name:   Apple M4
0.00.702.462 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.465 I ggml_metal_init: simdgroup reduction   = true
0.00.702.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.465 I ggml_metal_init: has residency sets    = true
0.00.702.466 I ggml_metal_init: has bfloat            = true
0.00.702.466 I ggml_metal_init: use bfloat            = true
0.00.702.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.719.010 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.771.717 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.771.723 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.771.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.775.874 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.775.876 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.775.876 I llama_init_from_model: graph nodes  = 967
0.00.775.877 I llama_init_from_model: graph splits = 2
0.00.775.883 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.776.007 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.776.007 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.843.980 I main: llama threadpool init, n_threads = 4
0.00.844.018 I 
0.00.844.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.844.041 I 
0.00.844.220 I sampler seed: 1234
0.00.844.224 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.844.235 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.844.235 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.844.235 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.725.023 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48102.98 tokens per second)
0.01.725.023 I llama_perf_context_print:        load time =     834.55 ms
0.01.725.028 I llama_perf_context_print: prompt eval time =      57.47 ms /     7 tokens (    8.21 ms per token,   121.81 tokens per second)
0.01.725.029 I llama_perf_context_print:        eval time =     820.84 ms /    63 runs   (   13.03 ms per token,    76.75 tokens per second)
0.01.725.029 I llama_perf_context_print:       total time =     881.82 ms /    70 tokens
0.01.725.280 I ggml_metal_free: deallocating

real	0m1.740s
user	0m0.108s
sys	0m0.250s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.001.542 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.615 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.397 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.032.409 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.411 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.412 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.426 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.430 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.431 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.431 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.432 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.440 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.041.462 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.208 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.208 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.209 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.209 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.210 I llama_model_loader: - type  f32:  194 tensors
0.00.045.210 I llama_model_loader: - type  f16:   98 tensors
0.00.045.211 I print_info: file format = GGUF V3 (latest)
0.00.045.212 I print_info: file type   = all F32 (guessed)
0.00.045.214 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.053.191 I load: special tokens cache size = 25
0.00.059.023 I load: token to piece cache size = 0.2984 MB
0.00.059.027 I print_info: arch             = gptneox
0.00.059.028 I print_info: vocab_only       = 0
0.00.059.028 I print_info: n_ctx_train      = 2048
0.00.059.028 I print_info: n_embd           = 2048
0.00.059.028 I print_info: n_layer          = 24
0.00.059.032 I print_info: n_head           = 16
0.00.059.032 I print_info: n_head_kv        = 16
0.00.059.032 I print_info: n_rot            = 32
0.00.059.033 I print_info: n_swa            = 0
0.00.059.033 I print_info: n_embd_head_k    = 128
0.00.059.033 I print_info: n_embd_head_v    = 128
0.00.059.034 I print_info: n_gqa            = 1
0.00.059.034 I print_info: n_embd_k_gqa     = 2048
0.00.059.035 I print_info: n_embd_v_gqa     = 2048
0.00.059.036 I print_info: f_norm_eps       = 1.0e-05
0.00.059.036 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.036 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.037 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.037 I print_info: f_logit_scale    = 0.0e+00
0.00.059.037 I print_info: n_ff             = 8192
0.00.059.038 I print_info: n_expert         = 0
0.00.059.038 I print_info: n_expert_used    = 0
0.00.059.038 I print_info: causal attn      = 1
0.00.059.038 I print_info: pooling type     = 0
0.00.059.038 I print_info: rope type        = 2
0.00.059.040 I print_info: rope scaling     = linear
0.00.059.040 I print_info: freq_base_train  = 10000.0
0.00.059.041 I print_info: freq_scale_train = 1
0.00.059.041 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.041 I print_info: rope_finetuned   = unknown
0.00.059.041 I print_info: ssm_d_conv       = 0
0.00.059.042 I print_info: ssm_d_inner      = 0
0.00.059.043 I print_info: ssm_d_state      = 0
0.00.059.043 I print_info: ssm_dt_rank      = 0
0.00.059.043 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.043 I print_info: model type       = 1.4B
0.00.059.044 I print_info: model params     = 1.41 B
0.00.059.046 I print_info: general.name     = 1.4B
0.00.059.046 I print_info: vocab type       = BPE
0.00.059.046 I print_info: n_vocab          = 50304
0.00.059.046 I print_info: n_merges         = 50009
0.00.059.047 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.047 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.047 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.047 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.047 I print_info: LF token         = 187 'Ċ'
0.00.059.047 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.048 I print_info: max token length = 1024
0.00.059.048 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.311.538 I load_tensors: offloading 24 repeating layers to GPU
0.01.311.546 I load_tensors: offloading output layer to GPU
0.01.311.546 I load_tensors: offloaded 25/25 layers to GPU
0.01.311.579 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.311.581 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.312.609 I llama_init_from_model: n_seq_max     = 1
0.01.312.611 I llama_init_from_model: n_ctx         = 128
0.01.312.611 I llama_init_from_model: n_ctx_per_seq = 128
0.01.312.611 I llama_init_from_model: n_batch       = 128
0.01.312.612 I llama_init_from_model: n_ubatch      = 128
0.01.312.612 I llama_init_from_model: flash_attn    = 0
0.01.312.612 I llama_init_from_model: freq_base     = 10000.0
0.01.312.613 I llama_init_from_model: freq_scale    = 1
0.01.312.613 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.312.614 I ggml_metal_init: allocating
0.01.312.679 I ggml_metal_init: found device: Apple M4
0.01.312.685 I ggml_metal_init: picking default device: Apple M4
0.01.313.959 I ggml_metal_init: using embedded metal library
0.01.317.970 I ggml_metal_init: GPU name:   Apple M4
0.01.317.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.317.974 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.317.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.317.975 I ggml_metal_init: simdgroup reduction   = true
0.01.317.975 I ggml_metal_init: simdgroup matrix mul. = true
0.01.317.975 I ggml_metal_init: has residency sets    = true
0.01.317.976 I ggml_metal_init: has bfloat            = true
0.01.317.976 I ggml_metal_init: use bfloat            = true
0.01.317.977 I ggml_metal_init: hasUnifiedMemory      = true
0.01.317.981 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.328.836 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.330.536 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.330.538 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.330.569 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.332.278 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.332.279 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.332.279 I llama_init_from_model: graph nodes  = 967
0.01.332.280 I llama_init_from_model: graph splits = 2
0.01.332.281 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.332.281 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.367.880 I 
0.01.367.923 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.367.927 I perplexity: tokenizing the input ..
0.01.373.079 I perplexity: tokenization took 5.15 ms
0.01.373.086 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.491.576 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.492.909 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.492.945 I llama_perf_context_print:        load time =    1348.26 ms
0.01.492.946 I llama_perf_context_print: prompt eval time =     118.18 ms /   128 tokens (    0.92 ms per token,  1083.08 tokens per second)
0.01.492.947 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.492.947 I llama_perf_context_print:       total time =     125.07 ms /   129 tokens
0.01.493.325 I ggml_metal_free: deallocating

real	0m1.673s
user	0m0.082s
sys	0m0.226s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.355 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.580 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.588 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.596 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.597 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.598 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.598 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.599 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.601 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.601 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.424 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.430 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.277 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.280 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.280 I llama_model_loader: - type  f32:  194 tensors
0.00.025.281 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.281 I print_info: file format = GGUF V3 (latest)
0.00.025.282 I print_info: file type   = Q8_0
0.00.025.283 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.747 I load: special tokens cache size = 25
0.00.039.861 I load: token to piece cache size = 0.2984 MB
0.00.039.865 I print_info: arch             = gptneox
0.00.039.865 I print_info: vocab_only       = 0
0.00.039.866 I print_info: n_ctx_train      = 2048
0.00.039.866 I print_info: n_embd           = 2048
0.00.039.866 I print_info: n_layer          = 24
0.00.039.870 I print_info: n_head           = 16
0.00.039.871 I print_info: n_head_kv        = 16
0.00.039.871 I print_info: n_rot            = 32
0.00.039.871 I print_info: n_swa            = 0
0.00.039.871 I print_info: n_embd_head_k    = 128
0.00.039.872 I print_info: n_embd_head_v    = 128
0.00.039.872 I print_info: n_gqa            = 1
0.00.039.873 I print_info: n_embd_k_gqa     = 2048
0.00.039.876 I print_info: n_embd_v_gqa     = 2048
0.00.039.877 I print_info: f_norm_eps       = 1.0e-05
0.00.039.877 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.877 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.879 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.879 I print_info: f_logit_scale    = 0.0e+00
0.00.039.879 I print_info: n_ff             = 8192
0.00.039.880 I print_info: n_expert         = 0
0.00.039.880 I print_info: n_expert_used    = 0
0.00.039.880 I print_info: causal attn      = 1
0.00.039.880 I print_info: pooling type     = 0
0.00.039.880 I print_info: rope type        = 2
0.00.039.880 I print_info: rope scaling     = linear
0.00.039.881 I print_info: freq_base_train  = 10000.0
0.00.039.881 I print_info: freq_scale_train = 1
0.00.039.881 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.881 I print_info: rope_finetuned   = unknown
0.00.039.883 I print_info: ssm_d_conv       = 0
0.00.039.883 I print_info: ssm_d_inner      = 0
0.00.039.883 I print_info: ssm_d_state      = 0
0.00.039.883 I print_info: ssm_dt_rank      = 0
0.00.039.883 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.884 I print_info: model type       = 1.4B
0.00.039.884 I print_info: model params     = 1.41 B
0.00.039.884 I print_info: general.name     = 1.4B
0.00.039.885 I print_info: vocab type       = BPE
0.00.039.885 I print_info: n_vocab          = 50304
0.00.039.885 I print_info: n_merges         = 50009
0.00.039.886 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.886 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.887 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.887 I print_info: LF token         = 187 'Ċ'
0.00.039.887 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.887 I print_info: max token length = 1024
0.00.039.888 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.885.384 I load_tensors: offloading 24 repeating layers to GPU
0.00.885.390 I load_tensors: offloading output layer to GPU
0.00.885.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.885.425 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.885.429 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.886.740 I llama_init_from_model: n_seq_max     = 1
0.00.886.743 I llama_init_from_model: n_ctx         = 128
0.00.886.743 I llama_init_from_model: n_ctx_per_seq = 128
0.00.886.744 I llama_init_from_model: n_batch       = 128
0.00.886.744 I llama_init_from_model: n_ubatch      = 128
0.00.886.744 I llama_init_from_model: flash_attn    = 0
0.00.886.745 I llama_init_from_model: freq_base     = 10000.0
0.00.886.746 I llama_init_from_model: freq_scale    = 1
0.00.886.746 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.886.748 I ggml_metal_init: allocating
0.00.886.821 I ggml_metal_init: found device: Apple M4
0.00.886.831 I ggml_metal_init: picking default device: Apple M4
0.00.888.255 I ggml_metal_init: using embedded metal library
0.00.893.733 I ggml_metal_init: GPU name:   Apple M4
0.00.893.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.893.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.893.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.893.739 I ggml_metal_init: simdgroup reduction   = true
0.00.893.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.893.740 I ggml_metal_init: has residency sets    = true
0.00.893.740 I ggml_metal_init: has bfloat            = true
0.00.893.740 I ggml_metal_init: use bfloat            = true
0.00.893.741 I ggml_metal_init: hasUnifiedMemory      = true
0.00.893.742 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.909.590 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.912.938 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.912.941 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.912.978 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.916.197 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.916.199 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.916.199 I llama_init_from_model: graph nodes  = 967
0.00.916.199 I llama_init_from_model: graph splits = 2
0.00.916.202 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.916.203 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.942.867 I 
0.00.942.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.942.952 I perplexity: tokenizing the input ..
0.00.950.263 I perplexity: tokenization took 7.309 ms
0.00.950.270 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.075.000 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.076.687 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.076.707 I llama_perf_context_print:        load time =     933.50 ms
0.01.076.708 I llama_perf_context_print: prompt eval time =     123.96 ms /   128 tokens (    0.97 ms per token,  1032.56 tokens per second)
0.01.076.709 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.076.709 I llama_perf_context_print:       total time =     133.85 ms /   129 tokens
0.01.077.073 I ggml_metal_free: deallocating

real	0m1.092s
user	0m0.076s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.129 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.140 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.988 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.048 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.874 I llama_model_loader: - type  f32:  194 tensors
0.00.025.874 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.874 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.875 I print_info: file format = GGUF V3 (latest)
0.00.025.876 I print_info: file type   = Q4_0
0.00.025.877 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.954 I load: special tokens cache size = 25
0.00.040.052 I load: token to piece cache size = 0.2984 MB
0.00.040.056 I print_info: arch             = gptneox
0.00.040.056 I print_info: vocab_only       = 0
0.00.040.056 I print_info: n_ctx_train      = 2048
0.00.040.057 I print_info: n_embd           = 2048
0.00.040.057 I print_info: n_layer          = 24
0.00.040.061 I print_info: n_head           = 16
0.00.040.062 I print_info: n_head_kv        = 16
0.00.040.062 I print_info: n_rot            = 32
0.00.040.062 I print_info: n_swa            = 0
0.00.040.062 I print_info: n_embd_head_k    = 128
0.00.040.062 I print_info: n_embd_head_v    = 128
0.00.040.063 I print_info: n_gqa            = 1
0.00.040.064 I print_info: n_embd_k_gqa     = 2048
0.00.040.066 I print_info: n_embd_v_gqa     = 2048
0.00.040.067 I print_info: f_norm_eps       = 1.0e-05
0.00.040.067 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.068 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.068 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.068 I print_info: f_logit_scale    = 0.0e+00
0.00.040.069 I print_info: n_ff             = 8192
0.00.040.070 I print_info: n_expert         = 0
0.00.040.070 I print_info: n_expert_used    = 0
0.00.040.070 I print_info: causal attn      = 1
0.00.040.071 I print_info: pooling type     = 0
0.00.040.071 I print_info: rope type        = 2
0.00.040.071 I print_info: rope scaling     = linear
0.00.040.071 I print_info: freq_base_train  = 10000.0
0.00.040.072 I print_info: freq_scale_train = 1
0.00.040.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.072 I print_info: rope_finetuned   = unknown
0.00.040.072 I print_info: ssm_d_conv       = 0
0.00.040.072 I print_info: ssm_d_inner      = 0
0.00.040.072 I print_info: ssm_d_state      = 0
0.00.040.073 I print_info: ssm_dt_rank      = 0
0.00.040.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.073 I print_info: model type       = 1.4B
0.00.040.073 I print_info: model params     = 1.41 B
0.00.040.075 I print_info: general.name     = 1.4B
0.00.040.076 I print_info: vocab type       = BPE
0.00.040.076 I print_info: n_vocab          = 50304
0.00.040.076 I print_info: n_merges         = 50009
0.00.040.076 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.076 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.077 I print_info: LF token         = 187 'Ċ'
0.00.040.077 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.077 I print_info: max token length = 1024
0.00.040.077 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.770 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.786 I load_tensors: offloading output layer to GPU
0.00.600.786 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.824 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.600.825 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.602.282 I llama_init_from_model: n_seq_max     = 1
0.00.602.285 I llama_init_from_model: n_ctx         = 128
0.00.602.286 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.286 I llama_init_from_model: n_batch       = 128
0.00.602.286 I llama_init_from_model: n_ubatch      = 128
0.00.602.287 I llama_init_from_model: flash_attn    = 0
0.00.602.289 I llama_init_from_model: freq_base     = 10000.0
0.00.602.290 I llama_init_from_model: freq_scale    = 1
0.00.602.290 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.293 I ggml_metal_init: allocating
0.00.602.410 I ggml_metal_init: found device: Apple M4
0.00.602.424 I ggml_metal_init: picking default device: Apple M4
0.00.604.338 I ggml_metal_init: using embedded metal library
0.00.609.777 I ggml_metal_init: GPU name:   Apple M4
0.00.609.793 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.609.794 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.609.795 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.609.796 I ggml_metal_init: simdgroup reduction   = true
0.00.609.797 I ggml_metal_init: simdgroup matrix mul. = true
0.00.609.797 I ggml_metal_init: has residency sets    = true
0.00.609.797 I ggml_metal_init: has bfloat            = true
0.00.609.797 I ggml_metal_init: use bfloat            = true
0.00.609.800 I ggml_metal_init: hasUnifiedMemory      = true
0.00.609.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.004 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.633.588 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.633.595 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.633.652 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.636.935 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.636.937 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.636.937 I llama_init_from_model: graph nodes  = 967
0.00.636.938 I llama_init_from_model: graph splits = 2
0.00.636.941 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.636.941 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.238 I 
0.00.662.318 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.332 I perplexity: tokenizing the input ..
0.00.669.598 I perplexity: tokenization took 7.268 ms
0.00.669.607 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.804.445 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.805.779 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.805.804 I llama_perf_context_print:        load time =     652.10 ms
0.00.805.805 I llama_perf_context_print: prompt eval time =     133.96 ms /   128 tokens (    1.05 ms per token,   955.52 tokens per second)
0.00.805.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.805.806 I llama_perf_context_print:       total time =     143.57 ms /   129 tokens
0.00.806.193 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.081s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.855 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.271 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.274 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.275 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.275 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.276 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.277 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.279 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.280 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.281 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.100 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.865 I llama_model_loader: - type  f32:  194 tensors
0.00.024.865 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.866 I print_info: file format = GGUF V3 (latest)
0.00.024.867 I print_info: file type   = Q4_1
0.00.024.868 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.905 I load: special tokens cache size = 25
0.00.038.846 I load: token to piece cache size = 0.2984 MB
0.00.038.852 I print_info: arch             = gptneox
0.00.038.852 I print_info: vocab_only       = 0
0.00.038.853 I print_info: n_ctx_train      = 2048
0.00.038.855 I print_info: n_embd           = 2048
0.00.038.855 I print_info: n_layer          = 24
0.00.038.859 I print_info: n_head           = 16
0.00.038.860 I print_info: n_head_kv        = 16
0.00.038.861 I print_info: n_rot            = 32
0.00.038.861 I print_info: n_swa            = 0
0.00.038.862 I print_info: n_embd_head_k    = 128
0.00.038.862 I print_info: n_embd_head_v    = 128
0.00.038.862 I print_info: n_gqa            = 1
0.00.038.863 I print_info: n_embd_k_gqa     = 2048
0.00.038.863 I print_info: n_embd_v_gqa     = 2048
0.00.038.866 I print_info: f_norm_eps       = 1.0e-05
0.00.038.866 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.866 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.868 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.868 I print_info: f_logit_scale    = 0.0e+00
0.00.038.868 I print_info: n_ff             = 8192
0.00.038.869 I print_info: n_expert         = 0
0.00.038.871 I print_info: n_expert_used    = 0
0.00.038.871 I print_info: causal attn      = 1
0.00.038.871 I print_info: pooling type     = 0
0.00.038.871 I print_info: rope type        = 2
0.00.038.871 I print_info: rope scaling     = linear
0.00.038.872 I print_info: freq_base_train  = 10000.0
0.00.038.872 I print_info: freq_scale_train = 1
0.00.038.872 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.872 I print_info: rope_finetuned   = unknown
0.00.038.873 I print_info: ssm_d_conv       = 0
0.00.038.873 I print_info: ssm_d_inner      = 0
0.00.038.873 I print_info: ssm_d_state      = 0
0.00.038.873 I print_info: ssm_dt_rank      = 0
0.00.038.873 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.873 I print_info: model type       = 1.4B
0.00.038.874 I print_info: model params     = 1.41 B
0.00.038.874 I print_info: general.name     = 1.4B
0.00.038.878 I print_info: vocab type       = BPE
0.00.038.878 I print_info: n_vocab          = 50304
0.00.038.878 I print_info: n_merges         = 50009
0.00.038.879 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.879 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: LF token         = 187 'Ċ'
0.00.038.880 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.880 I print_info: max token length = 1024
0.00.038.880 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.663.829 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.847 I load_tensors: offloading output layer to GPU
0.00.663.847 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.888 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.663.889 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.665.697 I llama_init_from_model: n_seq_max     = 1
0.00.665.699 I llama_init_from_model: n_ctx         = 128
0.00.665.700 I llama_init_from_model: n_ctx_per_seq = 128
0.00.665.700 I llama_init_from_model: n_batch       = 128
0.00.665.701 I llama_init_from_model: n_ubatch      = 128
0.00.665.701 I llama_init_from_model: flash_attn    = 0
0.00.665.704 I llama_init_from_model: freq_base     = 10000.0
0.00.665.705 I llama_init_from_model: freq_scale    = 1
0.00.665.707 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.665.710 I ggml_metal_init: allocating
0.00.665.818 I ggml_metal_init: found device: Apple M4
0.00.665.832 I ggml_metal_init: picking default device: Apple M4
0.00.667.786 I ggml_metal_init: using embedded metal library
0.00.674.543 I ggml_metal_init: GPU name:   Apple M4
0.00.674.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.674.557 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.674.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.674.559 I ggml_metal_init: simdgroup reduction   = true
0.00.674.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.674.559 I ggml_metal_init: has residency sets    = true
0.00.674.560 I ggml_metal_init: has bfloat            = true
0.00.674.572 I ggml_metal_init: use bfloat            = true
0.00.674.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.674.578 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.692.445 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.695.864 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.695.927 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.699.089 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.699.091 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.699.091 I llama_init_from_model: graph nodes  = 967
0.00.699.091 I llama_init_from_model: graph splits = 2
0.00.699.094 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.699.095 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.708 I 
0.00.724.781 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.789 I perplexity: tokenizing the input ..
0.00.732.220 I perplexity: tokenization took 7.427 ms
0.00.732.228 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.868.690 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.870.026 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.870.047 I llama_perf_context_print:        load time =     715.85 ms
0.00.870.048 I llama_perf_context_print: prompt eval time =     135.58 ms /   128 tokens (    1.06 ms per token,   944.11 tokens per second)
0.00.870.049 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.870.049 I llama_perf_context_print:       total time =     145.34 ms /   129 tokens
0.00.870.434 I ggml_metal_free: deallocating

real	0m0.885s
user	0m0.079s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.991 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.434 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.443 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.444 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.446 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.447 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.449 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.449 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.449 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.238 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.257 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.085 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.085 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.085 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.086 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.086 I llama_model_loader: - type  f32:  194 tensors
0.00.026.087 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.087 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.088 I print_info: file format = GGUF V3 (latest)
0.00.026.088 I print_info: file type   = Q5_0
0.00.026.089 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.436 I load: special tokens cache size = 25
0.00.040.651 I load: token to piece cache size = 0.2984 MB
0.00.040.655 I print_info: arch             = gptneox
0.00.040.655 I print_info: vocab_only       = 0
0.00.040.656 I print_info: n_ctx_train      = 2048
0.00.040.656 I print_info: n_embd           = 2048
0.00.040.656 I print_info: n_layer          = 24
0.00.040.661 I print_info: n_head           = 16
0.00.040.663 I print_info: n_head_kv        = 16
0.00.040.663 I print_info: n_rot            = 32
0.00.040.664 I print_info: n_swa            = 0
0.00.040.664 I print_info: n_embd_head_k    = 128
0.00.040.664 I print_info: n_embd_head_v    = 128
0.00.040.665 I print_info: n_gqa            = 1
0.00.040.665 I print_info: n_embd_k_gqa     = 2048
0.00.040.666 I print_info: n_embd_v_gqa     = 2048
0.00.040.666 I print_info: f_norm_eps       = 1.0e-05
0.00.040.667 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.667 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.667 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.667 I print_info: f_logit_scale    = 0.0e+00
0.00.040.668 I print_info: n_ff             = 8192
0.00.040.668 I print_info: n_expert         = 0
0.00.040.668 I print_info: n_expert_used    = 0
0.00.040.668 I print_info: causal attn      = 1
0.00.040.668 I print_info: pooling type     = 0
0.00.040.668 I print_info: rope type        = 2
0.00.040.669 I print_info: rope scaling     = linear
0.00.040.669 I print_info: freq_base_train  = 10000.0
0.00.040.669 I print_info: freq_scale_train = 1
0.00.040.669 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.670 I print_info: rope_finetuned   = unknown
0.00.040.670 I print_info: ssm_d_conv       = 0
0.00.040.670 I print_info: ssm_d_inner      = 0
0.00.040.670 I print_info: ssm_d_state      = 0
0.00.040.670 I print_info: ssm_dt_rank      = 0
0.00.040.670 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.671 I print_info: model type       = 1.4B
0.00.040.671 I print_info: model params     = 1.41 B
0.00.040.671 I print_info: general.name     = 1.4B
0.00.040.672 I print_info: vocab type       = BPE
0.00.040.672 I print_info: n_vocab          = 50304
0.00.040.672 I print_info: n_merges         = 50009
0.00.040.672 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.672 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.672 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.673 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.673 I print_info: LF token         = 187 'Ċ'
0.00.040.673 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.673 I print_info: max token length = 1024
0.00.040.674 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.726.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.726.329 I load_tensors: offloading output layer to GPU
0.00.726.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.726.365 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.726.366 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.728.108 I llama_init_from_model: n_seq_max     = 1
0.00.728.111 I llama_init_from_model: n_ctx         = 128
0.00.728.112 I llama_init_from_model: n_ctx_per_seq = 128
0.00.728.112 I llama_init_from_model: n_batch       = 128
0.00.728.112 I llama_init_from_model: n_ubatch      = 128
0.00.728.113 I llama_init_from_model: flash_attn    = 0
0.00.728.115 I llama_init_from_model: freq_base     = 10000.0
0.00.728.115 I llama_init_from_model: freq_scale    = 1
0.00.728.116 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.728.119 I ggml_metal_init: allocating
0.00.728.221 I ggml_metal_init: found device: Apple M4
0.00.728.235 I ggml_metal_init: picking default device: Apple M4
0.00.730.184 I ggml_metal_init: using embedded metal library
0.00.736.968 I ggml_metal_init: GPU name:   Apple M4
0.00.736.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.978 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.979 I ggml_metal_init: simdgroup reduction   = true
0.00.736.979 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.979 I ggml_metal_init: has residency sets    = true
0.00.736.979 I ggml_metal_init: has bfloat            = true
0.00.736.980 I ggml_metal_init: use bfloat            = true
0.00.736.981 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.984 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.755.232 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.758.801 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.758.805 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.758.852 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.762.216 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.762.218 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.762.218 I llama_init_from_model: graph nodes  = 967
0.00.762.218 I llama_init_from_model: graph splits = 2
0.00.762.222 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.762.223 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.611 I 
0.00.791.694 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.702 I perplexity: tokenizing the input ..
0.00.798.933 I perplexity: tokenization took 7.227 ms
0.00.798.942 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.934.396 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.935.728 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.935.761 I llama_perf_context_print:        load time =     781.61 ms
0.00.935.761 I llama_perf_context_print: prompt eval time =     134.57 ms /   128 tokens (    1.05 ms per token,   951.18 tokens per second)
0.00.935.762 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.935.763 I llama_perf_context_print:       total time =     144.16 ms /   129 tokens
0.00.936.129 I ggml_metal_free: deallocating

real	0m0.952s
user	0m0.081s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.994 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.770 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.779 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.780 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.781 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.782 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.783 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.783 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.784 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.786 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.525 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.562 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.349 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.351 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.351 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.352 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.352 I llama_model_loader: - type  f32:  194 tensors
0.00.024.353 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.353 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.354 I print_info: file format = GGUF V3 (latest)
0.00.024.354 I print_info: file type   = Q5_1
0.00.024.355 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.327 I load: special tokens cache size = 25
0.00.038.253 I load: token to piece cache size = 0.2984 MB
0.00.038.257 I print_info: arch             = gptneox
0.00.038.257 I print_info: vocab_only       = 0
0.00.038.257 I print_info: n_ctx_train      = 2048
0.00.038.258 I print_info: n_embd           = 2048
0.00.038.258 I print_info: n_layer          = 24
0.00.038.262 I print_info: n_head           = 16
0.00.038.263 I print_info: n_head_kv        = 16
0.00.038.263 I print_info: n_rot            = 32
0.00.038.267 I print_info: n_swa            = 0
0.00.038.267 I print_info: n_embd_head_k    = 128
0.00.038.267 I print_info: n_embd_head_v    = 128
0.00.038.268 I print_info: n_gqa            = 1
0.00.038.269 I print_info: n_embd_k_gqa     = 2048
0.00.038.269 I print_info: n_embd_v_gqa     = 2048
0.00.038.270 I print_info: f_norm_eps       = 1.0e-05
0.00.038.270 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.270 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.270 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.271 I print_info: f_logit_scale    = 0.0e+00
0.00.038.272 I print_info: n_ff             = 8192
0.00.038.272 I print_info: n_expert         = 0
0.00.038.273 I print_info: n_expert_used    = 0
0.00.038.273 I print_info: causal attn      = 1
0.00.038.274 I print_info: pooling type     = 0
0.00.038.274 I print_info: rope type        = 2
0.00.038.274 I print_info: rope scaling     = linear
0.00.038.275 I print_info: freq_base_train  = 10000.0
0.00.038.275 I print_info: freq_scale_train = 1
0.00.038.276 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.276 I print_info: rope_finetuned   = unknown
0.00.038.277 I print_info: ssm_d_conv       = 0
0.00.038.277 I print_info: ssm_d_inner      = 0
0.00.038.277 I print_info: ssm_d_state      = 0
0.00.038.277 I print_info: ssm_dt_rank      = 0
0.00.038.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.277 I print_info: model type       = 1.4B
0.00.038.278 I print_info: model params     = 1.41 B
0.00.038.278 I print_info: general.name     = 1.4B
0.00.038.278 I print_info: vocab type       = BPE
0.00.038.278 I print_info: n_vocab          = 50304
0.00.038.278 I print_info: n_merges         = 50009
0.00.038.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.279 I print_info: LF token         = 187 'Ċ'
0.00.038.284 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.284 I print_info: max token length = 1024
0.00.038.285 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.607.456 I load_tensors: offloading 24 repeating layers to GPU
0.00.607.464 I load_tensors: offloading output layer to GPU
0.00.607.466 I load_tensors: offloaded 25/25 layers to GPU
0.00.607.495 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.607.499 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.609.024 I llama_init_from_model: n_seq_max     = 1
0.00.609.026 I llama_init_from_model: n_ctx         = 128
0.00.609.026 I llama_init_from_model: n_ctx_per_seq = 128
0.00.609.027 I llama_init_from_model: n_batch       = 128
0.00.609.027 I llama_init_from_model: n_ubatch      = 128
0.00.609.027 I llama_init_from_model: flash_attn    = 0
0.00.609.029 I llama_init_from_model: freq_base     = 10000.0
0.00.609.029 I llama_init_from_model: freq_scale    = 1
0.00.609.030 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.609.032 I ggml_metal_init: allocating
0.00.609.060 I ggml_metal_init: found device: Apple M4
0.00.609.069 I ggml_metal_init: picking default device: Apple M4
0.00.610.514 I ggml_metal_init: using embedded metal library
0.00.616.547 I ggml_metal_init: GPU name:   Apple M4
0.00.616.551 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.616.552 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.616.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.616.554 I ggml_metal_init: simdgroup reduction   = true
0.00.616.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.616.554 I ggml_metal_init: has residency sets    = true
0.00.616.555 I ggml_metal_init: has bfloat            = true
0.00.616.555 I ggml_metal_init: use bfloat            = true
0.00.616.556 I ggml_metal_init: hasUnifiedMemory      = true
0.00.616.557 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.633.701 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.637.193 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.637.199 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.637.263 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.640.474 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.640.476 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.640.477 I llama_init_from_model: graph nodes  = 967
0.00.640.477 I llama_init_from_model: graph splits = 2
0.00.640.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.640.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.596 I 
0.00.670.673 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.670.681 I perplexity: tokenizing the input ..
0.00.678.587 I perplexity: tokenization took 7.902 ms
0.00.678.602 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.086 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.827.426 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.827.464 I llama_perf_context_print:        load time =     661.59 ms
0.00.827.466 I llama_perf_context_print: prompt eval time =     146.59 ms /   128 tokens (    1.15 ms per token,   873.15 tokens per second)
0.00.827.466 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.467 I llama_perf_context_print:       total time =     156.87 ms /   129 tokens
0.00.827.815 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.079s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.981 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.987 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.989 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.990 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.990 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.990 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.991 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.992 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.992 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.992 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.993 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.993 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.996 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.996 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.997 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.714 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.734 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.543 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.543 I llama_model_loader: - type  f32:  194 tensors
0.00.025.543 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.544 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.544 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.545 I print_info: file format = GGUF V3 (latest)
0.00.025.545 I print_info: file type   = Q2_K - Medium
0.00.025.547 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.239 I load: special tokens cache size = 25
0.00.040.471 I load: token to piece cache size = 0.2984 MB
0.00.040.475 I print_info: arch             = gptneox
0.00.040.476 I print_info: vocab_only       = 0
0.00.040.476 I print_info: n_ctx_train      = 2048
0.00.040.476 I print_info: n_embd           = 2048
0.00.040.476 I print_info: n_layer          = 24
0.00.040.481 I print_info: n_head           = 16
0.00.040.482 I print_info: n_head_kv        = 16
0.00.040.482 I print_info: n_rot            = 32
0.00.040.482 I print_info: n_swa            = 0
0.00.040.482 I print_info: n_embd_head_k    = 128
0.00.040.484 I print_info: n_embd_head_v    = 128
0.00.040.485 I print_info: n_gqa            = 1
0.00.040.486 I print_info: n_embd_k_gqa     = 2048
0.00.040.486 I print_info: n_embd_v_gqa     = 2048
0.00.040.487 I print_info: f_norm_eps       = 1.0e-05
0.00.040.487 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.487 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.487 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.487 I print_info: f_logit_scale    = 0.0e+00
0.00.040.488 I print_info: n_ff             = 8192
0.00.040.488 I print_info: n_expert         = 0
0.00.040.488 I print_info: n_expert_used    = 0
0.00.040.490 I print_info: causal attn      = 1
0.00.040.491 I print_info: pooling type     = 0
0.00.040.491 I print_info: rope type        = 2
0.00.040.491 I print_info: rope scaling     = linear
0.00.040.491 I print_info: freq_base_train  = 10000.0
0.00.040.492 I print_info: freq_scale_train = 1
0.00.040.492 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.492 I print_info: rope_finetuned   = unknown
0.00.040.492 I print_info: ssm_d_conv       = 0
0.00.040.492 I print_info: ssm_d_inner      = 0
0.00.040.492 I print_info: ssm_d_state      = 0
0.00.040.492 I print_info: ssm_dt_rank      = 0
0.00.040.493 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.493 I print_info: model type       = 1.4B
0.00.040.493 I print_info: model params     = 1.41 B
0.00.040.494 I print_info: general.name     = 1.4B
0.00.040.495 I print_info: vocab type       = BPE
0.00.040.495 I print_info: n_vocab          = 50304
0.00.040.495 I print_info: n_merges         = 50009
0.00.040.495 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.496 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: LF token         = 187 'Ċ'
0.00.040.497 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.497 I print_info: max token length = 1024
0.00.040.498 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.342.232 I load_tensors: offloading 24 repeating layers to GPU
0.00.342.246 I load_tensors: offloading output layer to GPU
0.00.342.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.342.277 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.342.278 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.343.984 I llama_init_from_model: n_seq_max     = 1
0.00.343.987 I llama_init_from_model: n_ctx         = 128
0.00.343.987 I llama_init_from_model: n_ctx_per_seq = 128
0.00.343.988 I llama_init_from_model: n_batch       = 128
0.00.343.988 I llama_init_from_model: n_ubatch      = 128
0.00.343.989 I llama_init_from_model: flash_attn    = 0
0.00.343.991 I llama_init_from_model: freq_base     = 10000.0
0.00.343.991 I llama_init_from_model: freq_scale    = 1
0.00.343.992 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.343.996 I ggml_metal_init: allocating
0.00.344.067 I ggml_metal_init: found device: Apple M4
0.00.344.081 I ggml_metal_init: picking default device: Apple M4
0.00.345.865 I ggml_metal_init: using embedded metal library
0.00.351.270 I ggml_metal_init: GPU name:   Apple M4
0.00.351.287 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.288 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.290 I ggml_metal_init: simdgroup reduction   = true
0.00.351.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.290 I ggml_metal_init: has residency sets    = true
0.00.351.291 I ggml_metal_init: has bfloat            = true
0.00.351.291 I ggml_metal_init: use bfloat            = true
0.00.351.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.298 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.372.388 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.375.860 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.375.870 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.375.936 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.379.348 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.379.350 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.379.351 I llama_init_from_model: graph nodes  = 967
0.00.379.351 I llama_init_from_model: graph splits = 2
0.00.379.355 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.379.355 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.410.349 I 
0.00.410.428 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.410.435 I perplexity: tokenizing the input ..
0.00.417.407 I perplexity: tokenization took 6.969 ms
0.00.417.414 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.560.014 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.561.352 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.561.375 I llama_perf_context_print:        load time =     400.35 ms
0.00.561.376 I llama_perf_context_print: prompt eval time =     141.63 ms /   128 tokens (    1.11 ms per token,   903.75 tokens per second)
0.00.561.378 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.561.379 I llama_perf_context_print:       total time =     151.03 ms /   129 tokens
0.00.561.769 I ggml_metal_free: deallocating

real	0m0.577s
user	0m0.082s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.901 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.908 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.914 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.915 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.915 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.916 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.916 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.917 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.918 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.920 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.920 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.920 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.921 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.921 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.924 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.924 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.925 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.580 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.235 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.237 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.237 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.238 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.238 I llama_model_loader: - type  f32:  194 tensors
0.00.024.238 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.239 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.239 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.239 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.240 I print_info: file format = GGUF V3 (latest)
0.00.024.244 I print_info: file type   = Q3_K - Medium
0.00.024.245 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.452 I load: special tokens cache size = 25
0.00.038.367 I load: token to piece cache size = 0.2984 MB
0.00.038.371 I print_info: arch             = gptneox
0.00.038.371 I print_info: vocab_only       = 0
0.00.038.371 I print_info: n_ctx_train      = 2048
0.00.038.371 I print_info: n_embd           = 2048
0.00.038.372 I print_info: n_layer          = 24
0.00.038.376 I print_info: n_head           = 16
0.00.038.376 I print_info: n_head_kv        = 16
0.00.038.377 I print_info: n_rot            = 32
0.00.038.377 I print_info: n_swa            = 0
0.00.038.377 I print_info: n_embd_head_k    = 128
0.00.038.377 I print_info: n_embd_head_v    = 128
0.00.038.380 I print_info: n_gqa            = 1
0.00.038.381 I print_info: n_embd_k_gqa     = 2048
0.00.038.381 I print_info: n_embd_v_gqa     = 2048
0.00.038.382 I print_info: f_norm_eps       = 1.0e-05
0.00.038.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.382 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.382 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.383 I print_info: f_logit_scale    = 0.0e+00
0.00.038.383 I print_info: n_ff             = 8192
0.00.038.384 I print_info: n_expert         = 0
0.00.038.384 I print_info: n_expert_used    = 0
0.00.038.384 I print_info: causal attn      = 1
0.00.038.384 I print_info: pooling type     = 0
0.00.038.384 I print_info: rope type        = 2
0.00.038.384 I print_info: rope scaling     = linear
0.00.038.385 I print_info: freq_base_train  = 10000.0
0.00.038.385 I print_info: freq_scale_train = 1
0.00.038.385 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.385 I print_info: rope_finetuned   = unknown
0.00.038.385 I print_info: ssm_d_conv       = 0
0.00.038.385 I print_info: ssm_d_inner      = 0
0.00.038.386 I print_info: ssm_d_state      = 0
0.00.038.386 I print_info: ssm_dt_rank      = 0
0.00.038.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.387 I print_info: model type       = 1.4B
0.00.038.388 I print_info: model params     = 1.41 B
0.00.038.388 I print_info: general.name     = 1.4B
0.00.038.388 I print_info: vocab type       = BPE
0.00.038.388 I print_info: n_vocab          = 50304
0.00.038.388 I print_info: n_merges         = 50009
0.00.038.389 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.389 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.389 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.389 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.389 I print_info: LF token         = 187 'Ċ'
0.00.038.389 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.390 I print_info: max token length = 1024
0.00.038.390 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.473.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.473.919 I load_tensors: offloading output layer to GPU
0.00.473.920 I load_tensors: offloaded 25/25 layers to GPU
0.00.473.953 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.473.954 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.475.594 I llama_init_from_model: n_seq_max     = 1
0.00.475.599 I llama_init_from_model: n_ctx         = 128
0.00.475.600 I llama_init_from_model: n_ctx_per_seq = 128
0.00.475.600 I llama_init_from_model: n_batch       = 128
0.00.475.601 I llama_init_from_model: n_ubatch      = 128
0.00.475.601 I llama_init_from_model: flash_attn    = 0
0.00.475.602 I llama_init_from_model: freq_base     = 10000.0
0.00.475.603 I llama_init_from_model: freq_scale    = 1
0.00.475.603 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.475.609 I ggml_metal_init: allocating
0.00.475.717 I ggml_metal_init: found device: Apple M4
0.00.475.731 I ggml_metal_init: picking default device: Apple M4
0.00.477.753 I ggml_metal_init: using embedded metal library
0.00.483.519 I ggml_metal_init: GPU name:   Apple M4
0.00.483.524 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.483.525 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.483.526 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.483.527 I ggml_metal_init: simdgroup reduction   = true
0.00.483.527 I ggml_metal_init: simdgroup matrix mul. = true
0.00.483.527 I ggml_metal_init: has residency sets    = true
0.00.483.528 I ggml_metal_init: has bfloat            = true
0.00.483.528 I ggml_metal_init: use bfloat            = true
0.00.483.529 I ggml_metal_init: hasUnifiedMemory      = true
0.00.483.531 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.503.485 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.507.131 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.507.136 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.507.180 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.510.280 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.510.282 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.510.283 I llama_init_from_model: graph nodes  = 967
0.00.510.283 I llama_init_from_model: graph splits = 2
0.00.510.286 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.510.286 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.536.932 I 
0.00.536.996 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.537.002 I perplexity: tokenizing the input ..
0.00.545.543 I perplexity: tokenization took 8.538 ms
0.00.545.555 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.678.058 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.679.398 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.679.419 I llama_perf_context_print:        load time =     528.08 ms
0.00.679.420 I llama_perf_context_print: prompt eval time =     131.46 ms /   128 tokens (    1.03 ms per token,   973.65 tokens per second)
0.00.679.420 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.679.421 I llama_perf_context_print:       total time =     142.49 ms /   129 tokens
0.00.679.801 I ggml_metal_free: deallocating

real	0m0.693s
user	0m0.081s
sys	0m0.136s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.933 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.063 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.070 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.071 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.072 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.072 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.073 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.074 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.074 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.075 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.075 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.081 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.081 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.083 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.083 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.083 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.963 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.798 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.800 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.801 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.801 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.802 I llama_model_loader: - type  f32:  194 tensors
0.00.024.802 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.803 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.803 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.804 I print_info: file format = GGUF V3 (latest)
0.00.024.804 I print_info: file type   = Q4_K - Medium
0.00.024.805 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.152 I load: special tokens cache size = 25
0.00.039.118 I load: token to piece cache size = 0.2984 MB
0.00.039.123 I print_info: arch             = gptneox
0.00.039.123 I print_info: vocab_only       = 0
0.00.039.124 I print_info: n_ctx_train      = 2048
0.00.039.124 I print_info: n_embd           = 2048
0.00.039.124 I print_info: n_layer          = 24
0.00.039.129 I print_info: n_head           = 16
0.00.039.130 I print_info: n_head_kv        = 16
0.00.039.132 I print_info: n_rot            = 32
0.00.039.132 I print_info: n_swa            = 0
0.00.039.132 I print_info: n_embd_head_k    = 128
0.00.039.132 I print_info: n_embd_head_v    = 128
0.00.039.133 I print_info: n_gqa            = 1
0.00.039.134 I print_info: n_embd_k_gqa     = 2048
0.00.039.134 I print_info: n_embd_v_gqa     = 2048
0.00.039.135 I print_info: f_norm_eps       = 1.0e-05
0.00.039.135 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.135 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.136 I print_info: f_logit_scale    = 0.0e+00
0.00.039.136 I print_info: n_ff             = 8192
0.00.039.137 I print_info: n_expert         = 0
0.00.039.137 I print_info: n_expert_used    = 0
0.00.039.137 I print_info: causal attn      = 1
0.00.039.137 I print_info: pooling type     = 0
0.00.039.137 I print_info: rope type        = 2
0.00.039.137 I print_info: rope scaling     = linear
0.00.039.138 I print_info: freq_base_train  = 10000.0
0.00.039.138 I print_info: freq_scale_train = 1
0.00.039.138 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.138 I print_info: rope_finetuned   = unknown
0.00.039.140 I print_info: ssm_d_conv       = 0
0.00.039.140 I print_info: ssm_d_inner      = 0
0.00.039.140 I print_info: ssm_d_state      = 0
0.00.039.140 I print_info: ssm_dt_rank      = 0
0.00.039.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.141 I print_info: model type       = 1.4B
0.00.039.141 I print_info: model params     = 1.41 B
0.00.039.141 I print_info: general.name     = 1.4B
0.00.039.142 I print_info: vocab type       = BPE
0.00.039.142 I print_info: n_vocab          = 50304
0.00.039.142 I print_info: n_merges         = 50009
0.00.039.142 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.142 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.143 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.143 I print_info: LF token         = 187 'Ċ'
0.00.039.143 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.143 I print_info: max token length = 1024
0.00.039.144 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.518.659 I load_tensors: offloading 24 repeating layers to GPU
0.00.518.679 I load_tensors: offloading output layer to GPU
0.00.518.679 I load_tensors: offloaded 25/25 layers to GPU
0.00.518.713 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.518.715 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.520.407 I llama_init_from_model: n_seq_max     = 1
0.00.520.410 I llama_init_from_model: n_ctx         = 128
0.00.520.411 I llama_init_from_model: n_ctx_per_seq = 128
0.00.520.411 I llama_init_from_model: n_batch       = 128
0.00.520.411 I llama_init_from_model: n_ubatch      = 128
0.00.520.412 I llama_init_from_model: flash_attn    = 0
0.00.520.415 I llama_init_from_model: freq_base     = 10000.0
0.00.520.415 I llama_init_from_model: freq_scale    = 1
0.00.520.416 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.520.418 I ggml_metal_init: allocating
0.00.520.506 I ggml_metal_init: found device: Apple M4
0.00.520.521 I ggml_metal_init: picking default device: Apple M4
0.00.522.332 I ggml_metal_init: using embedded metal library
0.00.529.075 I ggml_metal_init: GPU name:   Apple M4
0.00.529.085 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.529.086 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.529.087 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.529.091 I ggml_metal_init: simdgroup reduction   = true
0.00.529.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.529.092 I ggml_metal_init: has residency sets    = true
0.00.529.092 I ggml_metal_init: has bfloat            = true
0.00.529.092 I ggml_metal_init: use bfloat            = true
0.00.529.094 I ggml_metal_init: hasUnifiedMemory      = true
0.00.529.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.548.019 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.551.635 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.551.647 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.551.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.555.020 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.555.022 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.555.022 I llama_init_from_model: graph nodes  = 967
0.00.555.023 I llama_init_from_model: graph splits = 2
0.00.555.025 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.555.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.585.319 I 
0.00.585.404 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.585.412 I perplexity: tokenizing the input ..
0.00.592.324 I perplexity: tokenization took 6.909 ms
0.00.592.331 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.881 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.737.220 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.737.241 I llama_perf_context_print:        load time =     576.38 ms
0.00.737.242 I llama_perf_context_print: prompt eval time =     142.60 ms /   128 tokens (    1.11 ms per token,   897.62 tokens per second)
0.00.737.242 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.243 I llama_perf_context_print:       total time =     151.93 ms /   129 tokens
0.00.737.626 I ggml_metal_free: deallocating

real	0m0.752s
user	0m0.080s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.992 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.657 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.664 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.672 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.672 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.673 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.674 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.674 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.674 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.675 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.675 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.677 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.679 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.502 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.540 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.299 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.301 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.301 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.301 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.302 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.302 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.303 I llama_model_loader: - type  f32:  194 tensors
0.00.025.303 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.303 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.304 I print_info: file format = GGUF V3 (latest)
0.00.025.305 I print_info: file type   = Q5_K - Medium
0.00.025.306 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.187 I load: special tokens cache size = 25
0.00.039.492 I load: token to piece cache size = 0.2984 MB
0.00.039.497 I print_info: arch             = gptneox
0.00.039.497 I print_info: vocab_only       = 0
0.00.039.497 I print_info: n_ctx_train      = 2048
0.00.039.497 I print_info: n_embd           = 2048
0.00.039.498 I print_info: n_layer          = 24
0.00.039.502 I print_info: n_head           = 16
0.00.039.503 I print_info: n_head_kv        = 16
0.00.039.503 I print_info: n_rot            = 32
0.00.039.503 I print_info: n_swa            = 0
0.00.039.503 I print_info: n_embd_head_k    = 128
0.00.039.503 I print_info: n_embd_head_v    = 128
0.00.039.504 I print_info: n_gqa            = 1
0.00.039.505 I print_info: n_embd_k_gqa     = 2048
0.00.039.505 I print_info: n_embd_v_gqa     = 2048
0.00.039.506 I print_info: f_norm_eps       = 1.0e-05
0.00.039.506 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.507 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.507 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.507 I print_info: f_logit_scale    = 0.0e+00
0.00.039.508 I print_info: n_ff             = 8192
0.00.039.508 I print_info: n_expert         = 0
0.00.039.511 I print_info: n_expert_used    = 0
0.00.039.511 I print_info: causal attn      = 1
0.00.039.511 I print_info: pooling type     = 0
0.00.039.511 I print_info: rope type        = 2
0.00.039.511 I print_info: rope scaling     = linear
0.00.039.512 I print_info: freq_base_train  = 10000.0
0.00.039.512 I print_info: freq_scale_train = 1
0.00.039.512 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.512 I print_info: rope_finetuned   = unknown
0.00.039.512 I print_info: ssm_d_conv       = 0
0.00.039.512 I print_info: ssm_d_inner      = 0
0.00.039.513 I print_info: ssm_d_state      = 0
0.00.039.513 I print_info: ssm_dt_rank      = 0
0.00.039.513 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.513 I print_info: model type       = 1.4B
0.00.039.513 I print_info: model params     = 1.41 B
0.00.039.514 I print_info: general.name     = 1.4B
0.00.039.514 I print_info: vocab type       = BPE
0.00.039.514 I print_info: n_vocab          = 50304
0.00.039.514 I print_info: n_merges         = 50009
0.00.039.515 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.515 I print_info: LF token         = 187 'Ċ'
0.00.039.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: max token length = 1024
0.00.039.517 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.592.083 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.099 I load_tensors: offloading output layer to GPU
0.00.592.100 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.134 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.592.135 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.593.829 I llama_init_from_model: n_seq_max     = 1
0.00.593.831 I llama_init_from_model: n_ctx         = 128
0.00.593.832 I llama_init_from_model: n_ctx_per_seq = 128
0.00.593.832 I llama_init_from_model: n_batch       = 128
0.00.593.833 I llama_init_from_model: n_ubatch      = 128
0.00.593.833 I llama_init_from_model: flash_attn    = 0
0.00.593.835 I llama_init_from_model: freq_base     = 10000.0
0.00.593.836 I llama_init_from_model: freq_scale    = 1
0.00.593.836 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.839 I ggml_metal_init: allocating
0.00.593.906 I ggml_metal_init: found device: Apple M4
0.00.593.920 I ggml_metal_init: picking default device: Apple M4
0.00.595.656 I ggml_metal_init: using embedded metal library
0.00.602.384 I ggml_metal_init: GPU name:   Apple M4
0.00.602.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.389 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.390 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.391 I ggml_metal_init: simdgroup reduction   = true
0.00.602.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.391 I ggml_metal_init: has residency sets    = true
0.00.602.392 I ggml_metal_init: has bfloat            = true
0.00.602.392 I ggml_metal_init: use bfloat            = true
0.00.602.393 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.557 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.029 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.035 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.093 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.257 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.260 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.260 I llama_init_from_model: graph nodes  = 967
0.00.626.260 I llama_init_from_model: graph splits = 2
0.00.626.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.657.476 I 
0.00.657.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.657.572 I perplexity: tokenizing the input ..
0.00.664.915 I perplexity: tokenization took 7.339 ms
0.00.664.921 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.810 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.804.122 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.804.150 I llama_perf_context_print:        load time =     647.47 ms
0.00.804.151 I llama_perf_context_print: prompt eval time =     137.03 ms /   128 tokens (    1.07 ms per token,   934.10 tokens per second)
0.00.804.151 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.804.152 I llama_perf_context_print:       total time =     146.68 ms /   129 tokens
0.00.804.565 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.080s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.687 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.610 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.616 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.618 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.619 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.620 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.389 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.367 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.148 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.150 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.150 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.150 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.151 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.151 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.152 I llama_model_loader: - type  f32:  194 tensors
0.00.025.152 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.153 I print_info: file format = GGUF V3 (latest)
0.00.025.153 I print_info: file type   = Q6_K
0.00.025.154 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.489 I load: special tokens cache size = 25
0.00.039.440 I load: token to piece cache size = 0.2984 MB
0.00.039.444 I print_info: arch             = gptneox
0.00.039.444 I print_info: vocab_only       = 0
0.00.039.445 I print_info: n_ctx_train      = 2048
0.00.039.445 I print_info: n_embd           = 2048
0.00.039.445 I print_info: n_layer          = 24
0.00.039.449 I print_info: n_head           = 16
0.00.039.450 I print_info: n_head_kv        = 16
0.00.039.450 I print_info: n_rot            = 32
0.00.039.450 I print_info: n_swa            = 0
0.00.039.451 I print_info: n_embd_head_k    = 128
0.00.039.451 I print_info: n_embd_head_v    = 128
0.00.039.453 I print_info: n_gqa            = 1
0.00.039.454 I print_info: n_embd_k_gqa     = 2048
0.00.039.456 I print_info: n_embd_v_gqa     = 2048
0.00.039.457 I print_info: f_norm_eps       = 1.0e-05
0.00.039.457 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.458 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.458 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.458 I print_info: f_logit_scale    = 0.0e+00
0.00.039.459 I print_info: n_ff             = 8192
0.00.039.460 I print_info: n_expert         = 0
0.00.039.460 I print_info: n_expert_used    = 0
0.00.039.460 I print_info: causal attn      = 1
0.00.039.463 I print_info: pooling type     = 0
0.00.039.463 I print_info: rope type        = 2
0.00.039.465 I print_info: rope scaling     = linear
0.00.039.465 I print_info: freq_base_train  = 10000.0
0.00.039.466 I print_info: freq_scale_train = 1
0.00.039.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.466 I print_info: rope_finetuned   = unknown
0.00.039.466 I print_info: ssm_d_conv       = 0
0.00.039.466 I print_info: ssm_d_inner      = 0
0.00.039.466 I print_info: ssm_d_state      = 0
0.00.039.467 I print_info: ssm_dt_rank      = 0
0.00.039.467 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.467 I print_info: model type       = 1.4B
0.00.039.467 I print_info: model params     = 1.41 B
0.00.039.468 I print_info: general.name     = 1.4B
0.00.039.468 I print_info: vocab type       = BPE
0.00.039.468 I print_info: n_vocab          = 50304
0.00.039.469 I print_info: n_merges         = 50009
0.00.039.469 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.469 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.470 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.472 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.472 I print_info: LF token         = 187 'Ċ'
0.00.039.473 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.473 I print_info: max token length = 1024
0.00.039.473 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.530 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.536 I load_tensors: offloading output layer to GPU
0.00.528.537 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.570 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.528.572 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.530.220 I llama_init_from_model: n_seq_max     = 1
0.00.530.223 I llama_init_from_model: n_ctx         = 128
0.00.530.224 I llama_init_from_model: n_ctx_per_seq = 128
0.00.530.224 I llama_init_from_model: n_batch       = 128
0.00.530.225 I llama_init_from_model: n_ubatch      = 128
0.00.530.225 I llama_init_from_model: flash_attn    = 0
0.00.530.227 I llama_init_from_model: freq_base     = 10000.0
0.00.530.228 I llama_init_from_model: freq_scale    = 1
0.00.530.228 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.530.231 I ggml_metal_init: allocating
0.00.530.288 I ggml_metal_init: found device: Apple M4
0.00.530.300 I ggml_metal_init: picking default device: Apple M4
0.00.532.046 I ggml_metal_init: using embedded metal library
0.00.535.618 I ggml_metal_init: GPU name:   Apple M4
0.00.535.621 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.625 I ggml_metal_init: simdgroup reduction   = true
0.00.535.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.625 I ggml_metal_init: has residency sets    = true
0.00.535.626 I ggml_metal_init: has bfloat            = true
0.00.535.626 I ggml_metal_init: use bfloat            = true
0.00.535.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.632 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.546.087 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.547.683 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.547.686 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.547.705 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.549.326 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.549.327 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.549.328 I llama_init_from_model: graph nodes  = 967
0.00.549.328 I llama_init_from_model: graph splits = 2
0.00.549.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.549.330 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.474 I 
0.00.583.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.513 I perplexity: tokenizing the input ..
0.00.587.440 I perplexity: tokenization took 3.926 ms
0.00.587.444 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.717.968 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.719.302 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.719.326 I llama_perf_context_print:        load time =     573.78 ms
0.00.719.327 I llama_perf_context_print: prompt eval time =     130.30 ms /   128 tokens (    1.02 ms per token,   982.38 tokens per second)
0.00.719.328 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.719.328 I llama_perf_context_print:       total time =     135.85 ms /   129 tokens
0.00.719.695 I ggml_metal_free: deallocating

real	0m0.734s
user	0m0.066s
sys	0m0.136s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.302 I build: 4756 (cf756d6e) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.852 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.927 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.932 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.934 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.935 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.935 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.935 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.941 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.942 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.942 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.943 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.943 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.944 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.949 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.950 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.950 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.048.480 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.050.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.059.610 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.059.612 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.059.612 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.059.613 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.059.613 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.059.614 I llama_model_loader: - type  f32:  194 tensors
0.00.059.615 I llama_model_loader: - type  f16:   98 tensors
0.00.059.615 I print_info: file format = GGUF V3 (latest)
0.00.059.616 I print_info: file type   = all F32 (guessed)
0.00.059.617 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.072.152 I load: special tokens cache size = 25
0.00.079.552 I load: token to piece cache size = 0.2984 MB
0.00.079.556 I print_info: arch             = gptneox
0.00.079.556 I print_info: vocab_only       = 0
0.00.079.556 I print_info: n_ctx_train      = 2048
0.00.079.556 I print_info: n_embd           = 2048
0.00.079.557 I print_info: n_layer          = 24
0.00.079.560 I print_info: n_head           = 16
0.00.079.561 I print_info: n_head_kv        = 16
0.00.079.561 I print_info: n_rot            = 32
0.00.079.561 I print_info: n_swa            = 0
0.00.079.561 I print_info: n_embd_head_k    = 128
0.00.079.561 I print_info: n_embd_head_v    = 128
0.00.079.562 I print_info: n_gqa            = 1
0.00.079.563 I print_info: n_embd_k_gqa     = 2048
0.00.079.564 I print_info: n_embd_v_gqa     = 2048
0.00.079.564 I print_info: f_norm_eps       = 1.0e-05
0.00.079.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.079.565 I print_info: f_clamp_kqv      = 0.0e+00
0.00.079.565 I print_info: f_max_alibi_bias = 0.0e+00
0.00.079.565 I print_info: f_logit_scale    = 0.0e+00
0.00.079.566 I print_info: n_ff             = 8192
0.00.079.566 I print_info: n_expert         = 0
0.00.079.566 I print_info: n_expert_used    = 0
0.00.079.566 I print_info: causal attn      = 1
0.00.079.566 I print_info: pooling type     = 0
0.00.079.566 I print_info: rope type        = 2
0.00.079.567 I print_info: rope scaling     = linear
0.00.079.567 I print_info: freq_base_train  = 10000.0
0.00.079.567 I print_info: freq_scale_train = 1
0.00.079.568 I print_info: n_ctx_orig_yarn  = 2048
0.00.079.568 I print_info: rope_finetuned   = unknown
0.00.079.568 I print_info: ssm_d_conv       = 0
0.00.079.568 I print_info: ssm_d_inner      = 0
0.00.079.569 I print_info: ssm_d_state      = 0
0.00.079.569 I print_info: ssm_dt_rank      = 0
0.00.079.569 I print_info: ssm_dt_b_c_rms   = 0
0.00.079.569 I print_info: model type       = 1.4B
0.00.079.570 I print_info: model params     = 1.41 B
0.00.079.570 I print_info: general.name     = 1.4B
0.00.079.570 I print_info: vocab type       = BPE
0.00.079.570 I print_info: n_vocab          = 50304
0.00.079.571 I print_info: n_merges         = 50009
0.00.079.571 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.079.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.079.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.079.571 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.079.572 I print_info: LF token         = 187 'Ċ'
0.00.079.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.079.572 I print_info: max token length = 1024
0.00.079.573 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.346.619 I load_tensors: offloading 24 repeating layers to GPU
0.01.346.623 I load_tensors: offloading output layer to GPU
0.01.346.623 I load_tensors: offloaded 25/25 layers to GPU
0.01.346.647 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.346.649 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.347.555 I llama_init_from_model: n_seq_max     = 1
0.01.347.556 I llama_init_from_model: n_ctx         = 128
0.01.347.557 I llama_init_from_model: n_ctx_per_seq = 128
0.01.347.557 I llama_init_from_model: n_batch       = 128
0.01.347.557 I llama_init_from_model: n_ubatch      = 128
0.01.347.557 I llama_init_from_model: flash_attn    = 0
0.01.347.558 I llama_init_from_model: freq_base     = 10000.0
0.01.347.558 I llama_init_from_model: freq_scale    = 1
0.01.347.559 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.347.560 I ggml_metal_init: allocating
0.01.347.630 I ggml_metal_init: found device: Apple M4
0.01.347.638 I ggml_metal_init: picking default device: Apple M4
0.01.348.796 I ggml_metal_init: using embedded metal library
0.01.352.585 I ggml_metal_init: GPU name:   Apple M4
0.01.352.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.352.587 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.352.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.352.588 I ggml_metal_init: simdgroup reduction   = true
0.01.352.588 I ggml_metal_init: simdgroup matrix mul. = true
0.01.352.588 I ggml_metal_init: has residency sets    = true
0.01.352.588 I ggml_metal_init: has bfloat            = true
0.01.352.589 I ggml_metal_init: use bfloat            = true
0.01.352.589 I ggml_metal_init: hasUnifiedMemory      = true
0.01.352.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.363.579 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.365.346 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.365.349 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.365.377 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.367.058 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.367.059 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.367.059 I llama_init_from_model: graph nodes  = 967
0.01.367.060 I llama_init_from_model: graph splits = 2
0.01.367.061 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.367.062 I 
0.01.367.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.367.099 I compute_imatrix: tokenizing the input ..
0.01.371.162 I compute_imatrix: tokenization took 4.062 ms
0.01.371.164 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.585.818 I compute_imatrix: 0.21 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.588.518 I llama_perf_context_print:        load time =    1562.96 ms
0.01.588.519 I llama_perf_context_print: prompt eval time =     212.90 ms /   128 tokens (    1.66 ms per token,   601.21 tokens per second)
0.01.588.520 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.588.521 I llama_perf_context_print:       total time =    1565.65 ms /   129 tokens
0.01.589.100 I ggml_metal_free: deallocating

real	0m1.772s
user	0m0.131s
sys	0m0.244s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4756 (cf756d6e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140306280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1403068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140306d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1403071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140307640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x140307ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140307f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140308390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140308950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140308e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140309350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140309850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14030a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14030ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14030b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14030ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14030c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14030c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14030cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14030d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14030dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14030e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14030ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14030f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14030fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14030ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140310570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1403111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140311720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1403119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140311e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140312140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1403129d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140312f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1403131d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140313670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140313b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140313fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140314450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1403148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140314d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140315230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1403156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140315b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140315e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140316440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140316a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140317370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140317980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140317f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1403185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140318bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1403191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1403197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140319fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14031a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14031a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14031abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14031b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14031b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14031bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14031c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14031c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14031ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14031cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14031d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14031d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14031dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14031e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14031e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14031eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14031ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14031f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14031f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14031fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1403203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x140320940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140320e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1403213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140321930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140321e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1403223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x140322920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140322e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1403233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140323910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140323e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1403243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140324900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140324e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1403253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1403258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140325e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140326390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1403268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140326e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140327380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140317060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1403277f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140327fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1403284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140328a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140328f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1403294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140329a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140329f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14032a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14032aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14032af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14032b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14032ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14032bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14032c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14032c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14032cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14032d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14032d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14032dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14032e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14032e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14032e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14032ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14032f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14032f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14032fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1403300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x140330570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140330a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140330eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x140331350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1403317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140331c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x140332130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1403325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140332a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140332f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1403333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140333850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140333cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140334190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140334630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140334ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140334f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140335410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1403358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140335d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1403361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140336690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140336b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140336fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140337470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140337910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140337db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x140338250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1403386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140338b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140339030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1403394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140339970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140339e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14033a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14033a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14033abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14033b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14033b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14033b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14033be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14033c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14033c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14033cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14033d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14033d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14033da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14033ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14033e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14033e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14033ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14033f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14033f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14033fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14033ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1403403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140340870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140340d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1403411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140341650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140341af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140341f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x140342430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1403428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x140342d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140343210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1403436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140343c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140344150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1403446a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140344bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140344eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1403454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140345ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1403460e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1403468d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140346d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x140347030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140347640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140347c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140348440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1403488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140348d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140349220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1403499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140349f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14034a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14034a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14034af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14034b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14034b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14034bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14034c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14034c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14034cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14034d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14034d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14034dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14034e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14034e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14034eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14034f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14034f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14034fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140350410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140350960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140350eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140351400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140351950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140351ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1403523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140352940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140352e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1403533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140353930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140353e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1403543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140354920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140354e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1403553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140355910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140355e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1403563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140356900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140356e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1403573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1403578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140357e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140358390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1403588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140358e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140359380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1403598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140359e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14035a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14035a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14035ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14035b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14035b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14035be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14035c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14035c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14035cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14035d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14035d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14035da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14035df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14035e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14035e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14035ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14035f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14035f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14035fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14035ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140360410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1403608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140360e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140361520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x140361c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x140362360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140362a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x140362d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x140363530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1403637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140363e00 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.736.135 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.736.139 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14050a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14050a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14050ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14050b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14050b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14050bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14050bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14050c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14050c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14050cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14050d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14050d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14050e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14050eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14050f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14050fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140510170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140510890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140510fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140511780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140511ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1405125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140512ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140513400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140513b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140513de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1405140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140514510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140514980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140514df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1405152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140515800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140515c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140515f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1405163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140516810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140516d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140517270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140517770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140517c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140518170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140518670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140518b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140519070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140519570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1405199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140519e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14051a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14051a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14051aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14051b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14051b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14051b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14051bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14051c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14051c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14051ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14051d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14051d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14051df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14051e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14051e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14051ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14051f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14051f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14051fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10fe058c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10fe05d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10fe061a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10fe06610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10fe06a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10fe06ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10fe07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10fe077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10fe07c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10fe080b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10fe08520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10fe08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10fe08e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10fe09270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10fe096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10fe09b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10fe09fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10fe0a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10fe0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10fe0ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10fe0b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10fe0b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10fe0ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10fe0bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10fe0c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10fe0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10fe0cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10fe0d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10fe0d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10fe0d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10fe0dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10fe0e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10fe0e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10fe0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10fe0efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10fe0f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10fe0f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10fe0fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10fe10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10fe105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10fe10a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10fe10eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10fe11320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10fe11790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10fe11c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10fe12070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10fe124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10fe12950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10fe12dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10fe13230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10fe136a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10fe13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10fe13f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10fe143f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10fe14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10fe14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10fe15140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10fe155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10fe15a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10fe15e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10fe16300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10fe16770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10fe16be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10fe17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10fe174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10fe17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10fe17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10fe18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10fe18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10fe18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10fe18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10fe193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10fe19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10fe19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10fe1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10fe1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10fe1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10fe1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10fe1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10fe1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10fe1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10fe1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10fe1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10fe1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10fe1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10fe1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10fe1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10fe1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10fe1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10fe1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10fe1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10fe1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10fe1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10fe1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10fe1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10fe1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10fe202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10fe20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10fe20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10fe21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10fe21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10fe21f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10fe221e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10fe22650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10fe22ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10fe22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10fe233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10fe23810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10fe23c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10fe240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10fe24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10fe249d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10fe24e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10fe252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10fe25720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10fe25b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10fe26000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10fe26470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10fe268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10fe26d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10fe271c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10fe27630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10fe27aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10fe27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10fe28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10fe287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10fe28c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10fe290d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10fe29540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10fe299b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10fe29e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10fe2a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10fe2a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10fe2ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10fe2afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10fe2b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10fe2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10fe2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10fe2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10fe2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10fe2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10fe2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10fe2d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10fe2dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10fe2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10fe2e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10fe2eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10fe2f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10fe2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10fe2ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10fe305a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10fe30b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10fe31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10fe316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10fe31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10fe32260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10fe32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10fe32de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10fe333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10fe33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10fe33f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10fe344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10fe34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10fe35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10fe35620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10fe35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10fe361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10fe36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10fe36d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10fe372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10fe378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10fe37e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10fe38420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10fe389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10fe38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10fe39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10fe39b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10fe3a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10fe3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10fe3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10fe3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10fe3b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10fe3bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10fe3c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10fe3c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10fe3cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10fe3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10fe3da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10fe3e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10fe3e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10fe3eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10fe3f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10fe3f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10fe3fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10fe402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10fe40860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10fe40e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10fe413e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10fe419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10fe41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10fe42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10fe42ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10fe42fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10fe434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10fe439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10fe43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10fe443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10fe448e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10fe44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10fe452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10fe457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10fe45ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10fe461e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10fe466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10fe46be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10fe470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10fe475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10fe47ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10fe48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10fe48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10fe49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10fe49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10fe4a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10fe4a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10fe4a8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14050db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140509fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14051d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14051c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14050d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14051fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140520040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140520300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1405205c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140520880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140520b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x140520e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1405213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1405219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x140521fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x140522290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x140522550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x140522810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x140522d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140523520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140523a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140523fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1405244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140524a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140524f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1405254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140525760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140525a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140525ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140525fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140526260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140526520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1405267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140526aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140526d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x140527020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1405272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1405275a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140527860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140527b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140527de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1405280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140528360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140528620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1405288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140528ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140528e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140529120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1405293e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1405296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140529960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140529c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140529ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14052a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14052a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14052a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14052a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14052aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14052af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14052b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14052b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14052bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14052c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14052c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14052ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14052ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14052d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14052d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14052dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14052e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14052e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14052e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14052ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14052f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14052f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14052fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14052ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1405303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140530820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x140530c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140531100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140531570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1405319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140531e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1405322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140532730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140532ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140533010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140533480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1405338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140533d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1405341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140534640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140534ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140534f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140535390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140535800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140535c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1405360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140536550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1405369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140536e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1405372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140537710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140537b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140537ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140538460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1405388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140538d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1405391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140539620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140539a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140539f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14053a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14053a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14053ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14053b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14053b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14053b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14053be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14053c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14053c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14053cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14053cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14053d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14053d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14053dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14053e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14053e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14053ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14053eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14053f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14053f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14053fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1405400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140540510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140540980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140540df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140541260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1405416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140541b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140541fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140542420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140542890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140542d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140543170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1405435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140543a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140543ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140544330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1405447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140544c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140545080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1405454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140545960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x140545dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140546240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1405466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140546b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140546f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140547400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140547870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x140547ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140548150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1405485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140548a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140548ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140549310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x140549780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140549bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14054a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14054a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14054a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14054adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14054b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14054b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14054bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14054bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14054c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14054c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14054ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14054d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14054d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14054da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14054de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14054e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14054e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14054ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14054f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14054f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14054f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14054fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140550200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140550670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x140550ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140550f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1405513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140551830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140551ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140552110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140552580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1405529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140552e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1405532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140553740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140553bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140554020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140554490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140554900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x140554d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140555950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140555c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140555ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140556340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1405567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x140556c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140557090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140557500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140557970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140557de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140558250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1405586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140558b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x140558fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140559410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140559880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140559cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14055a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14055a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14055aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14055aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14055b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14055b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14055bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14055c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14055c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14055c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14055cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14055d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14055d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14055db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14055df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14055e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14055e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14055ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14055f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14055f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14055fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14055fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140560300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140560770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140560be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140561050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1405614c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140561930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140561da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140562210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140562680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140562af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x140562f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1405633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140563840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140563cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x140564120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140564590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140564a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140564e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1405652e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x140565750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140565bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140566030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1405664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x140566910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x140566d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1405671f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140567660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140567ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140567f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1405683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x140568820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140568c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140569100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140569570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x140569fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14056a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14056ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14056b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14056b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14056bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14056c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14056c880 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.790s
user	0m0.276s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4756 (cf756d6e)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b710950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b711090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b711640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b711bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b7121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b712750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b712d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b7132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b713860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b713d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b714260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b714760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b715280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b715a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b716960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b717080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b7177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b717ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b718690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b718db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b7194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b71a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b71abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b71ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b71b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b71c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b71c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b71c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b71cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b71d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b71d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b71de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b71e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b71e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b71ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b71eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b71f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b71fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b720140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b7205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b720a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b720d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b721350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b721960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b722280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b722890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b722ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b7234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b723ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b7240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b7246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b724ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b725370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b725810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b725ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b7260e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b7268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b726b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b727030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b7274d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b727970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b727e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b7282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b728750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b728bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b729090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b729530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b7299d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b729e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b72a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b72a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b72adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b72b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b72b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b72bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b72c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b72c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b72cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b72d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b72d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b72dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b72e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b72e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b72ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b72f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b72f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b72fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b7302b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b730800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b730d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b7312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b7317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b731d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b732290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b721f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b732700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b732eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b733400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b733950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b733ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b7343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b734940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b734e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b7353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b735930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b735e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b7363d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b736920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b736e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b7373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b737860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b737d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b7381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b738640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b738ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b738f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b739420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b7398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b739d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b73a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b73a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b73ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b73afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b73b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b73b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b73bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b73c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b73c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b73cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b73d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b73d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b73d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b73de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b73e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b73e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b73ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b73f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b73f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b73f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b73fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b740320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b7407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b740c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b741100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b7415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b741a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b741ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b742380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b742820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b742cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b743160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b743600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b743aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b743f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b7443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b744880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b744d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b7451c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b745660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b745b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b745fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b746440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b7468e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b746d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b747220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b7476c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b747b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b748000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b7484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b748940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b748de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b749280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b749720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b749bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b74a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b74a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b74a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b74ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b74b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b74b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b74bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b74c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b74c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b74ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b74cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b74d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b74d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b74dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b74e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b74e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b74eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b74f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b74f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b74fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b74fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b7503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b7509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b750ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b7517e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b751c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b751f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b752550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b752b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b753350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b7537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b753c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b754130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b7548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b754e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b755380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b7558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b755e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b756370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b7568c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b756e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b757360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b7578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b757e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b758350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b7588a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b758df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b759340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b759890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b759de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b75a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b75a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b75add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b75b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b75b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b75bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b75c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b75c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b75cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b75d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b75d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b75dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b75e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b75e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b75ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b75f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b75f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b75fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b7602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b760820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b760d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b7612c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b761810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b761d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b7622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b762800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b762d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b7632a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b7637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b763d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b764290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b7647e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b764d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b765280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b7657d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b765d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b766270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b7667c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b766d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b767260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b767700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b767ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b768040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b7684e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b768980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b768e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b7692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b769760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b769c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b76a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b76a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b76a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b76ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b76b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b76b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b76bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b76c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b76cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b76d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b76d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b76dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b76e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b76e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b76ed10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.095.760 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.765 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b750690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b752200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b76e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b750080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b750ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b723d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b723770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b725d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b752810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b71b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b721c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b722540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b721610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b724390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b723160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b71a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b7329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b76df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b71d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b71d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b752e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b7512b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b71b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b71ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b71bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b76f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b76f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b76f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b76f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b76fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b76ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b7701f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b7704b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b770770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b770a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b770cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b770fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b771270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b771530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b7717f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b771ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b771d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b772030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b7722f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b7725b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b772870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b772b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b772df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b7730b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b773370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b773630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b7738f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b773bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b773e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b774130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b7743f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b7746b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b774970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b774c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b774ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b7751b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b775470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b775730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b7759f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b775cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b775f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b776230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b7764f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b7767b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b776a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b776d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b776ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b7772b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13b777570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13b777830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13b777af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13b777db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13b778070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13b778330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13b7785f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13b7788b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13b778b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13b778e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13b7790f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13b7793b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13b779670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13b779930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13b779bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13b779eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13b77a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13b77a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13b77a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13b77a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13b77ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13b77af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13b77b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13b77b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13b77b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13b77ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13b77bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13b77bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13b77c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13b77c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13b77c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13b77cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13b77cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13b77d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13b77d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13b77d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13b77d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13b77db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13b77ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13b77e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b77e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b77e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b77e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b77ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b77ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b77f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b77f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b77f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b77f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b77fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b77fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b7801b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b780470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b780730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b7809f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b780cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b780f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b781230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b7814f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b7817b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b781a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b781d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b781ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b7822b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b782570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b782830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b782af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b782db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b783070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b783330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b7835f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b7838b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b783b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b783e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b7840f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b7843b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b784670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b784930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b784bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b784eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b785170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b785430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b7856f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b7859b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b785c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b785f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b7861f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b7864b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b786770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b786a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b786cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b786fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b787270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b787530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b7877f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b787ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b787d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b788030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b7882f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b7885b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b788870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b788b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b788df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b7890b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b789370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b789630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b7898f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b789bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b789e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b78a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b78a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b78a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b78a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b78ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b78aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b78b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b78b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b78b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b78b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b78bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b78bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b78c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b78c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b78c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b78ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b78cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b78cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b78d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13b78d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13b78d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b78daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b78ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13b78e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b78e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b78e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b78ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b78f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b78f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b78fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b78fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b7902b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b790720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b790b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b791000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b791470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b7918e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b791d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b7921c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b792630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b792aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b792f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b793380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b7937f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b793c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b7940d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b794540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b7949b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b794e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b795290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b795700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b795b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b795fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b796450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b7968c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b796d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b7971a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b797610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b797a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b797ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b798360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b7987d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b798c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b7990b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b799520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b799990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b799e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b79a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b79a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b79ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b79afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b79b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b79b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b79bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b79c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b79c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b79ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b79ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b79d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b79d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b79dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b79e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b79e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b79e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b79ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13b79f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13b79f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b79fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b79ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b7a0410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b7a0880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b7a0cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b7a1160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b7a15d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b7a1a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b7a1eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b7a2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b7a2790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b7a2c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b7a3070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b7a34e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b7a3f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b7a4670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b7a4d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b7a54b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b7a5770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13b7a5f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b7a6220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b7a6830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13fe046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13fe04b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13fe04fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13fe05430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13fe058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13fe05d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13fe06180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13fe065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13fe06a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13fe06ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13fe07340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13fe07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13fe08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13fe08d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13fe09540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13fe09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13fe0a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13fe0aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13fe0b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13fe0b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13fe0c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13fe0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13fe0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13fe0d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13fe0dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13fe0df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13fe0e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13fe0e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13fe0eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13fe0ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13fe0f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13fe0f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13fe0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13fe10030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13fe104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13fe10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13fe10d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13fe111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13fe11660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13fe11ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13fe11f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13fe123b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13fe12820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13fe12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13fe13100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13fe13570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13fe139e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13fe13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13fe142c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13fe14730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13fe14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13fe15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13fe15480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13fe158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13fe15d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13fe161d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13fe16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13fe16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13fe170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13fe17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13fe17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13fe17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13fe18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13fe186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13fe18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13fe18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13fe19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13fe198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13fe19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13fe1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13fe1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13fe1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13fe1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13fe1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13fe1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13fe1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13fe1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13fe1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13fe1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13fe1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13fe1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13fe1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13fe1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13fe1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13fe1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13fe1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13fe1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13fe1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13fe1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13fe1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13fe1feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13fe20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13fe20790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13fe20c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13fe21070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13fe214e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13fe21950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13fe21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13fe22230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13fe226a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13fe22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13fe22f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13fe233f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13fe23860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13fe241d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13fe24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13fe24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13fe24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13fe251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13fe25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13fe25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13fe25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13fe263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13fe26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13fe26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13fe270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13fe27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13fe279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13fe27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13fe282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13fe28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13fe28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13fe29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13fe29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13fe298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13fe29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13fe2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13fe2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13fe2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13fe2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13fe2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13fe2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13fe2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13fe2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13fe2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13fe2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13fe2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13fe2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13fe2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13fe2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13fe2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13fe2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13fe2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13fe2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13fe2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13fe2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13fe2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13fe2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13fe30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13fe307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13fe30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13fe310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13fe31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13fe31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13fe31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13fe32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13fe326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13fe32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13fe32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13fe33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13fe338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13fe33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13fe34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13fe345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13fe34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13fe34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13fe35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13fe357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13fe35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13fe36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13fe36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13fe36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13fe36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13fe37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13fe376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13fe37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13fe37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13fe38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13fe38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13fe38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13fe39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13fe395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13fe39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13fe39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13fe3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13fe3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13fe3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13fe3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13fe3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13fe3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13fe3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13fe3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13fe3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13fe3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13fe3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13fe3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13fe3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13fe3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13fe3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13fe3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13fe3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13fe3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13fe3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13fe3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13fe3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13fe40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13fe404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13fe40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13fe40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13fe41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13fe41e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13fe42140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13fe42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13fe42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13fe42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13fe43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13fe435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13fe43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13fe43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13fe44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13fe44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13fe44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13fe45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13fe454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13fe45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13fe45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13fe46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13fe46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13fe46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13fe46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13fe473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13fe47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13fe47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13fe48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13fe485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13fe48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13fe48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13fe492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13fe49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13fe49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13fe4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13fe4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13fe4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13fe4ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13fe4b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13fe4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13fe4bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13fe4bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13fe4c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13fe4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13fe4cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13fe4d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13fe4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13fe4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13fe4de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13fe4e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13fe4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13fe4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13fe4f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13fe4f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13fe4f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13fe4fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13fe501e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13fe50650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13fe50ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13fe50f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13fe513a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13fe51810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13fe51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13fe520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13fe52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13fe529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13fe52e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13fe532b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13fe53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13fe53b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13fe54000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13fe54470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13fe548e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13fe54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13fe551c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13fe55630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13fe55aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13fe56510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13fe56c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13fe57350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13fe57a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13fe57d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13fe581a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13fe587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13fe58db0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.952s
user	0m0.230s
sys	0m0.184s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
