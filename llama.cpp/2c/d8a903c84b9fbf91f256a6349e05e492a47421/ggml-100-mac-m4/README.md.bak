### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.12 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.45 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.67 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.62 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.19 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.54 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.11 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.96 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.96 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.30 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.91 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.36 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.34 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 253.98 sec*proc (29 tests)

Total Test time (real) = 253.99 sec

real	4m14.068s
user	8m31.309s
sys	0m7.135s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.82 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.30 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.22 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.45 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.39 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.80 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.15 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.85 sec*proc (29 tests)

Total Test time (real) =  54.86 sec

real	0m54.874s
user	1m16.637s
sys	0m6.426s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.137 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.139 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.628 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.017.633 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.635 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.017.637 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.637 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.017.637 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.017.637 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.017.638 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.017.638 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.017.639 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.017.639 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.017.639 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.017.641 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.017.642 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.017.642 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.017.644 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.017.644 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.017.645 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.017.645 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.019.868 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.020.485 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.020.486 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.020.486 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.020.487 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.020.487 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.020.487 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.020.488 I llama_model_loader: - type  f32:  124 tensors
0.00.020.488 I llama_model_loader: - type  f16:   73 tensors
0.00.020.489 I print_info: file format = GGUF V3 (latest)
0.00.020.489 I print_info: file type   = F16
0.00.020.490 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.022.969 I load: special tokens cache size = 5
0.00.024.238 I load: token to piece cache size = 0.2032 MB
0.00.024.252 I print_info: arch             = bert
0.00.024.253 I print_info: vocab_only       = 0
0.00.024.253 I print_info: n_ctx_train      = 512
0.00.024.253 I print_info: n_embd           = 384
0.00.024.253 I print_info: n_layer          = 12
0.00.024.256 I print_info: n_head           = 12
0.00.024.256 I print_info: n_head_kv        = 12
0.00.024.276 I print_info: n_rot            = 32
0.00.024.278 I print_info: n_swa            = 0
0.00.024.278 I print_info: n_embd_head_k    = 32
0.00.024.278 I print_info: n_embd_head_v    = 32
0.00.024.282 I print_info: n_gqa            = 1
0.00.024.284 I print_info: n_embd_k_gqa     = 384
0.00.024.285 I print_info: n_embd_v_gqa     = 384
0.00.024.285 I print_info: f_norm_eps       = 1.0e-12
0.00.024.286 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.024.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.024.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.024.286 I print_info: f_logit_scale    = 0.0e+00
0.00.024.287 I print_info: n_ff             = 1536
0.00.024.287 I print_info: n_expert         = 0
0.00.024.287 I print_info: n_expert_used    = 0
0.00.024.287 I print_info: causal attn      = 0
0.00.024.287 I print_info: pooling type     = 2
0.00.024.287 I print_info: rope type        = 2
0.00.024.288 I print_info: rope scaling     = linear
0.00.024.288 I print_info: freq_base_train  = 10000.0
0.00.024.288 I print_info: freq_scale_train = 1
0.00.024.288 I print_info: n_ctx_orig_yarn  = 512
0.00.024.289 I print_info: rope_finetuned   = unknown
0.00.024.289 I print_info: ssm_d_conv       = 0
0.00.024.289 I print_info: ssm_d_inner      = 0
0.00.024.290 I print_info: ssm_d_state      = 0
0.00.024.290 I print_info: ssm_dt_rank      = 0
0.00.024.290 I print_info: ssm_dt_b_c_rms   = 0
0.00.024.290 I print_info: model type       = 33M
0.00.024.291 I print_info: model params     = 33.21 M
0.00.024.291 I print_info: general.name     = Bge Small
0.00.024.291 I print_info: vocab type       = WPM
0.00.024.292 I print_info: n_vocab          = 30522
0.00.024.292 I print_info: n_merges         = 0
0.00.024.292 I print_info: BOS token        = 101 '[CLS]'
0.00.024.292 I print_info: UNK token        = 100 '[UNK]'
0.00.024.292 I print_info: SEP token        = 102 '[SEP]'
0.00.024.293 I print_info: PAD token        = 0 '[PAD]'
0.00.024.293 I print_info: MASK token       = 103 '[MASK]'
0.00.024.293 I print_info: LF token         = 0 '[PAD]'
0.00.024.293 I print_info: max token length = 21
0.00.024.294 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.026.272 I load_tensors: offloading 12 repeating layers to GPU
0.00.026.273 I load_tensors: offloading output layer to GPU
0.00.026.273 I load_tensors: offloaded 13/13 layers to GPU
0.00.026.293 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.026.294 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.026.490 I llama_context: n_seq_max     = 1
0.00.026.490 I llama_context: n_ctx         = 512
0.00.026.491 I llama_context: n_ctx_per_seq = 512
0.00.026.491 I llama_context: n_batch       = 2048
0.00.026.491 I llama_context: n_ubatch      = 2048
0.00.026.491 I llama_context: flash_attn    = 0
0.00.026.492 I llama_context: freq_base     = 10000.0
0.00.026.492 I llama_context: freq_scale    = 1
0.00.026.492 I ggml_metal_init: allocating
0.00.026.496 I ggml_metal_init: found device: Apple M4
0.00.026.501 I ggml_metal_init: picking default device: Apple M4
0.00.027.019 I ggml_metal_init: using embedded metal library
0.00.029.536 I ggml_metal_init: GPU name:   Apple M4
0.00.029.537 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.029.538 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.029.538 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.029.539 I ggml_metal_init: simdgroup reduction   = true
0.00.029.539 I ggml_metal_init: simdgroup matrix mul. = true
0.00.029.539 I ggml_metal_init: has residency sets    = true
0.00.029.539 I ggml_metal_init: has bfloat            = true
0.00.029.539 I ggml_metal_init: use bfloat            = true
0.00.029.540 I ggml_metal_init: hasUnifiedMemory      = true
0.00.029.540 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.039.699 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.040.311 I init:      Metal KV buffer size =     9.00 MiB
0.00.040.313 I llama_context: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.040.333 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.041.419 I llama_context:      Metal compute buffer size =    16.00 MiB
0.00.041.421 I llama_context:        CPU compute buffer size =     2.51 MiB
0.00.041.421 I llama_context: graph nodes  = 429
0.00.041.421 I llama_context: graph splits = 2
0.00.041.422 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.041.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.045.741 I 
0.00.045.771 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.046.316 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.050.548 I llama_perf_context_print:        load time =      30.60 ms
0.00.050.550 I llama_perf_context_print: prompt eval time =       4.12 ms /     9 tokens (    0.46 ms per token,  2181.82 tokens per second)
0.00.050.551 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.050.551 I llama_perf_context_print:       total time =       4.81 ms /    10 tokens
0.00.050.724 I ggml_metal_free: deallocating

real	0m0.224s
user	0m0.034s
sys	0m0.025s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.042 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.040 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.439 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.442 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.444 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.444 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.445 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.445 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.445 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.446 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.447 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.447 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.448 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.448 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.450 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.451 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.451 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.451 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.452 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.452 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.719 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.349 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.350 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.351 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.351 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.351 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.351 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.352 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.352 I llama_model_loader: - type  f32:  124 tensors
0.00.014.353 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.353 I print_info: file format = GGUF V3 (latest)
0.00.014.354 I print_info: file type   = Q8_0
0.00.014.355 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.709 I load: special tokens cache size = 5
0.00.017.951 I load: token to piece cache size = 0.2032 MB
0.00.017.960 I print_info: arch             = bert
0.00.017.961 I print_info: vocab_only       = 0
0.00.017.961 I print_info: n_ctx_train      = 512
0.00.017.961 I print_info: n_embd           = 384
0.00.017.961 I print_info: n_layer          = 12
0.00.017.964 I print_info: n_head           = 12
0.00.017.965 I print_info: n_head_kv        = 12
0.00.017.965 I print_info: n_rot            = 32
0.00.017.965 I print_info: n_swa            = 0
0.00.017.965 I print_info: n_embd_head_k    = 32
0.00.017.965 I print_info: n_embd_head_v    = 32
0.00.017.967 I print_info: n_gqa            = 1
0.00.017.968 I print_info: n_embd_k_gqa     = 384
0.00.017.969 I print_info: n_embd_v_gqa     = 384
0.00.017.969 I print_info: f_norm_eps       = 1.0e-12
0.00.017.970 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.970 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.972 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.972 I print_info: f_logit_scale    = 0.0e+00
0.00.017.973 I print_info: n_ff             = 1536
0.00.017.973 I print_info: n_expert         = 0
0.00.017.973 I print_info: n_expert_used    = 0
0.00.017.973 I print_info: causal attn      = 0
0.00.017.973 I print_info: pooling type     = 2
0.00.017.974 I print_info: rope type        = 2
0.00.017.974 I print_info: rope scaling     = linear
0.00.017.974 I print_info: freq_base_train  = 10000.0
0.00.017.974 I print_info: freq_scale_train = 1
0.00.017.975 I print_info: n_ctx_orig_yarn  = 512
0.00.017.975 I print_info: rope_finetuned   = unknown
0.00.017.975 I print_info: ssm_d_conv       = 0
0.00.017.975 I print_info: ssm_d_inner      = 0
0.00.017.975 I print_info: ssm_d_state      = 0
0.00.017.975 I print_info: ssm_dt_rank      = 0
0.00.017.975 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.976 I print_info: model type       = 33M
0.00.017.976 I print_info: model params     = 33.21 M
0.00.017.976 I print_info: general.name     = Bge Small
0.00.017.977 I print_info: vocab type       = WPM
0.00.017.977 I print_info: n_vocab          = 30522
0.00.017.977 I print_info: n_merges         = 0
0.00.017.978 I print_info: BOS token        = 101 '[CLS]'
0.00.017.978 I print_info: UNK token        = 100 '[UNK]'
0.00.017.978 I print_info: SEP token        = 102 '[SEP]'
0.00.017.978 I print_info: PAD token        = 0 '[PAD]'
0.00.017.978 I print_info: MASK token       = 103 '[MASK]'
0.00.017.979 I print_info: LF token         = 0 '[PAD]'
0.00.017.979 I print_info: max token length = 21
0.00.017.979 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.019.625 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.626 I load_tensors: offloading output layer to GPU
0.00.019.627 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.632 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.633 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.805 I llama_context: n_seq_max     = 1
0.00.019.806 I llama_context: n_ctx         = 512
0.00.019.806 I llama_context: n_ctx_per_seq = 512
0.00.019.806 I llama_context: n_batch       = 2048
0.00.019.807 I llama_context: n_ubatch      = 2048
0.00.019.807 I llama_context: flash_attn    = 0
0.00.019.807 I llama_context: freq_base     = 10000.0
0.00.019.807 I llama_context: freq_scale    = 1
0.00.019.808 I ggml_metal_init: allocating
0.00.019.814 I ggml_metal_init: found device: Apple M4
0.00.019.817 I ggml_metal_init: picking default device: Apple M4
0.00.020.309 I ggml_metal_init: using embedded metal library
0.00.022.676 I ggml_metal_init: GPU name:   Apple M4
0.00.022.677 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.678 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.678 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.679 I ggml_metal_init: simdgroup reduction   = true
0.00.022.679 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.679 I ggml_metal_init: has residency sets    = true
0.00.022.679 I ggml_metal_init: has bfloat            = true
0.00.022.679 I ggml_metal_init: use bfloat            = true
0.00.022.680 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.680 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.032.871 I init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.474 I init:      Metal KV buffer size =     9.00 MiB
0.00.033.476 I llama_context: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.488 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.034.486 I llama_context:      Metal compute buffer size =    16.00 MiB
0.00.034.487 I llama_context:        CPU compute buffer size =     2.51 MiB
0.00.034.487 I llama_context: graph nodes  = 429
0.00.034.487 I llama_context: graph splits = 2
0.00.034.489 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.489 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.464 I 
0.00.038.487 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.018 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.486 I llama_perf_context_print:        load time =      29.42 ms
0.00.043.487 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2073.26 tokens per second)
0.00.043.488 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.488 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.043.774 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.300 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.133 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.034.850 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.856 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.857 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.858 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.859 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.860 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.860 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.862 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.863 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.863 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.864 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.864 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.867 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.868 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.869 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.869 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.870 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.396 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.782 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.784 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.784 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.785 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.785 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.786 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.786 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.786 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.787 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.787 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.787 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.788 I llama_model_loader: - type  f32:   40 tensors
0.00.048.788 I llama_model_loader: - type  f16:   30 tensors
0.00.048.789 I print_info: file format = GGUF V3 (latest)
0.00.048.790 I print_info: file type   = F16
0.00.048.791 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.961 W load: empty token at index 5
0.00.058.054 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.571 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.608 I load: special tokens cache size = 5
0.00.325.298 I load: token to piece cache size = 1.5060 MB
0.00.325.304 I print_info: arch             = jina-bert-v2
0.00.325.304 I print_info: vocab_only       = 0
0.00.325.305 I print_info: n_ctx_train      = 8192
0.00.325.305 I print_info: n_embd           = 384
0.00.325.305 I print_info: n_layer          = 4
0.00.325.309 I print_info: n_head           = 12
0.00.325.310 I print_info: n_head_kv        = 12
0.00.325.311 I print_info: n_rot            = 32
0.00.325.311 I print_info: n_swa            = 0
0.00.325.311 I print_info: n_embd_head_k    = 32
0.00.325.311 I print_info: n_embd_head_v    = 32
0.00.325.312 I print_info: n_gqa            = 1
0.00.325.313 I print_info: n_embd_k_gqa     = 384
0.00.325.313 I print_info: n_embd_v_gqa     = 384
0.00.325.314 I print_info: f_norm_eps       = 1.0e-12
0.00.325.315 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.325.315 I print_info: f_clamp_kqv      = 0.0e+00
0.00.325.316 I print_info: f_max_alibi_bias = 8.0e+00
0.00.325.316 I print_info: f_logit_scale    = 0.0e+00
0.00.325.316 I print_info: n_ff             = 1536
0.00.325.316 I print_info: n_expert         = 0
0.00.325.316 I print_info: n_expert_used    = 0
0.00.325.317 I print_info: causal attn      = 0
0.00.325.317 I print_info: pooling type     = -1
0.00.325.317 I print_info: rope type        = -1
0.00.325.317 I print_info: rope scaling     = linear
0.00.325.318 I print_info: freq_base_train  = 10000.0
0.00.325.318 I print_info: freq_scale_train = 1
0.00.325.318 I print_info: n_ctx_orig_yarn  = 8192
0.00.325.318 I print_info: rope_finetuned   = unknown
0.00.325.319 I print_info: ssm_d_conv       = 0
0.00.325.319 I print_info: ssm_d_inner      = 0
0.00.325.319 I print_info: ssm_d_state      = 0
0.00.325.319 I print_info: ssm_dt_rank      = 0
0.00.325.319 I print_info: ssm_dt_b_c_rms   = 0
0.00.325.319 I print_info: model type       = 33M
0.00.325.320 I print_info: model params     = 32.90 M
0.00.325.320 I print_info: general.name     = Jina Bert Implementation
0.00.325.321 I print_info: vocab type       = BPE
0.00.325.322 I print_info: n_vocab          = 61056
0.00.325.326 I print_info: n_merges         = 39382
0.00.325.326 I print_info: BOS token        = 0 '<s>'
0.00.325.326 I print_info: EOS token        = 2 '</s>'
0.00.325.327 I print_info: UNK token        = 3 '<unk>'
0.00.325.327 I print_info: SEP token        = 2 '</s>'
0.00.325.327 I print_info: PAD token        = 1 '<pad>'
0.00.325.327 I print_info: MASK token       = 4 '<mask>'
0.00.325.328 I print_info: EOG token        = 2 '</s>'
0.00.325.328 I print_info: max token length = 45
0.00.325.328 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.327.561 I load_tensors: offloading 4 repeating layers to GPU
0.00.327.562 I load_tensors: offloading output layer to GPU
0.00.327.562 I load_tensors: offloaded 5/5 layers to GPU
0.00.327.586 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.327.587 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.328.091 I llama_context: n_seq_max     = 1
0.00.328.092 I llama_context: n_ctx         = 8192
0.00.328.092 I llama_context: n_ctx_per_seq = 8192
0.00.328.093 I llama_context: n_batch       = 2048
0.00.328.093 I llama_context: n_ubatch      = 2048
0.00.328.093 I llama_context: flash_attn    = 0
0.00.328.094 I llama_context: freq_base     = 10000.0
0.00.328.094 I llama_context: freq_scale    = 1
0.00.328.095 I ggml_metal_init: allocating
0.00.328.103 I ggml_metal_init: found device: Apple M4
0.00.328.108 I ggml_metal_init: picking default device: Apple M4
0.00.329.057 I ggml_metal_init: using embedded metal library
0.00.332.311 I ggml_metal_init: GPU name:   Apple M4
0.00.332.313 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.313 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.314 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.314 I ggml_metal_init: simdgroup reduction   = true
0.00.332.314 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.314 I ggml_metal_init: has residency sets    = true
0.00.332.314 I ggml_metal_init: has bfloat            = true
0.00.332.314 I ggml_metal_init: use bfloat            = true
0.00.332.315 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.772 I init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.855 I init:      Metal KV buffer size =    48.00 MiB
0.00.344.858 I llama_context: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.344.879 I llama_context:        CPU  output buffer size =     0.00 MiB
0.00.352.319 I llama_context:      Metal compute buffer size =   220.01 MiB
0.00.352.320 I llama_context:        CPU compute buffer size =    22.02 MiB
0.00.352.320 I llama_context: graph nodes  = 154
0.00.352.321 I llama_context: graph splits = 2
0.00.352.322 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.352.322 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.359.454 I 
0.00.359.486 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.359.867 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.359.868 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.359.883 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.359.883 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.359.891 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.359.891 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.360.412 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.160 I llama_perf_context_print:        load time =     337.31 ms
0.00.364.162 I llama_perf_context_print: prompt eval time =       3.74 ms /    62 tokens (    0.06 ms per token, 16586.41 tokens per second)
0.00.364.163 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.164 I llama_perf_context_print:       total time =       4.70 ms /    63 tokens
0.00.364.618 I ggml_metal_free: deallocating

real	0m1.089s
user	0m0.333s
sys	0m0.050s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.231 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.389 I main: llama backend init
0.00.000.395 I main: load the model and apply lora adapter, if any
0.00.049.207 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.061.493 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.061.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.061.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.061.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.061.518 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.061.521 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.061.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.061.524 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.061.524 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.061.525 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.061.526 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.061.526 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.061.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.061.528 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.061.533 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.061.534 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.061.534 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.068.405 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.071.071 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.800 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.801 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.801 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.802 I llama_model_loader: - type  f32:  194 tensors
0.00.079.803 I llama_model_loader: - type  f16:   98 tensors
0.00.079.804 I print_info: file format = GGUF V3 (latest)
0.00.079.810 I print_info: file type   = all F32 (guessed)
0.00.079.812 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.739 I load: special tokens cache size = 25
0.00.103.939 I load: token to piece cache size = 0.2984 MB
0.00.103.944 I print_info: arch             = gptneox
0.00.103.944 I print_info: vocab_only       = 0
0.00.103.944 I print_info: n_ctx_train      = 2048
0.00.103.944 I print_info: n_embd           = 2048
0.00.103.945 I print_info: n_layer          = 24
0.00.103.949 I print_info: n_head           = 16
0.00.103.950 I print_info: n_head_kv        = 16
0.00.103.953 I print_info: n_rot            = 32
0.00.103.953 I print_info: n_swa            = 0
0.00.103.953 I print_info: n_embd_head_k    = 128
0.00.103.953 I print_info: n_embd_head_v    = 128
0.00.103.954 I print_info: n_gqa            = 1
0.00.103.956 I print_info: n_embd_k_gqa     = 2048
0.00.103.957 I print_info: n_embd_v_gqa     = 2048
0.00.103.958 I print_info: f_norm_eps       = 1.0e-05
0.00.103.958 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.958 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.959 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.959 I print_info: f_logit_scale    = 0.0e+00
0.00.103.960 I print_info: n_ff             = 8192
0.00.103.960 I print_info: n_expert         = 0
0.00.103.960 I print_info: n_expert_used    = 0
0.00.103.960 I print_info: causal attn      = 1
0.00.103.962 I print_info: pooling type     = 0
0.00.103.962 I print_info: rope type        = 2
0.00.103.962 I print_info: rope scaling     = linear
0.00.103.963 I print_info: freq_base_train  = 10000.0
0.00.103.963 I print_info: freq_scale_train = 1
0.00.103.963 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.964 I print_info: rope_finetuned   = unknown
0.00.103.964 I print_info: ssm_d_conv       = 0
0.00.103.964 I print_info: ssm_d_inner      = 0
0.00.103.964 I print_info: ssm_d_state      = 0
0.00.103.964 I print_info: ssm_dt_rank      = 0
0.00.103.964 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.965 I print_info: model type       = 1.4B
0.00.103.970 I print_info: model params     = 1.41 B
0.00.103.970 I print_info: general.name     = 1.4B
0.00.103.971 I print_info: vocab type       = BPE
0.00.103.972 I print_info: n_vocab          = 50304
0.00.103.972 I print_info: n_merges         = 50009
0.00.103.973 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.973 I print_info: LF token         = 187 'Ċ'
0.00.103.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.974 I print_info: max token length = 1024
0.00.103.975 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.141.821 I load_tensors: offloading 24 repeating layers to GPU
0.00.141.826 I load_tensors: offloading output layer to GPU
0.00.141.826 I load_tensors: offloaded 25/25 layers to GPU
0.00.141.850 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.141.851 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.142.235 I llama_context: n_seq_max     = 1
0.00.142.236 I llama_context: n_ctx         = 2048
0.00.142.236 I llama_context: n_ctx_per_seq = 2048
0.00.142.236 I llama_context: n_batch       = 2048
0.00.142.237 I llama_context: n_ubatch      = 512
0.00.142.237 I llama_context: flash_attn    = 0
0.00.142.237 I llama_context: freq_base     = 10000.0
0.00.142.238 I llama_context: freq_scale    = 1
0.00.142.239 I ggml_metal_init: allocating
0.00.142.258 I ggml_metal_init: found device: Apple M4
0.00.142.263 I ggml_metal_init: picking default device: Apple M4
0.00.142.866 I ggml_metal_init: using embedded metal library
0.00.351.465 I ggml_metal_init: GPU name:   Apple M4
0.00.351.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.351.479 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.351.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.351.480 I ggml_metal_init: simdgroup reduction   = true
0.00.351.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.351.481 I ggml_metal_init: has residency sets    = true
0.00.351.481 I ggml_metal_init: has bfloat            = true
0.00.351.482 I ggml_metal_init: use bfloat            = true
0.00.351.484 I ggml_metal_init: hasUnifiedMemory      = true
0.00.351.489 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.314 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.425.965 I init:      Metal KV buffer size =   384.00 MiB
0.00.425.977 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.426.035 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.429.699 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.429.703 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.429.703 I llama_context: graph nodes  = 967
0.00.429.703 I llama_context: graph splits = 2
0.00.429.707 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.429.835 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.429.836 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.497.900 I main: llama threadpool init, n_threads = 4
0.00.497.940 I 
0.00.497.974 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.497.975 I 
0.00.498.017 I sampler seed: 1234
0.00.498.021 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.498.045 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.498.047 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.498.047 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.326.626 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56982.34 tokens per second)
0.02.326.627 I llama_perf_context_print:        load time =     447.79 ms
0.02.326.628 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.42 tokens per second)
0.02.326.629 I llama_perf_context_print:        eval time =    1781.97 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.326.630 I llama_perf_context_print:       total time =    1829.62 ms /    70 tokens
0.02.330.493 I ggml_metal_free: deallocating

real	0m2.620s
user	0m0.142s
sys	0m0.148s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.605 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.028.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.043.588 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.606 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.608 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.609 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.611 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.612 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.612 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.613 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.613 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.614 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.615 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.617 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.618 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.444 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.061.448 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.449 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.449 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.450 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.450 I llama_model_loader: - type  f32:  194 tensors
0.00.061.451 I llama_model_loader: - type  f16:   98 tensors
0.00.061.451 I print_info: file format = GGUF V3 (latest)
0.00.061.452 I print_info: file type   = all F32 (guessed)
0.00.061.454 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.073.850 I load: special tokens cache size = 25
0.00.081.908 I load: token to piece cache size = 0.2984 MB
0.00.081.911 I print_info: arch             = gptneox
0.00.081.911 I print_info: vocab_only       = 0
0.00.081.912 I print_info: n_ctx_train      = 2048
0.00.081.912 I print_info: n_embd           = 2048
0.00.081.912 I print_info: n_layer          = 24
0.00.081.915 I print_info: n_head           = 16
0.00.081.916 I print_info: n_head_kv        = 16
0.00.081.916 I print_info: n_rot            = 32
0.00.081.916 I print_info: n_swa            = 0
0.00.081.916 I print_info: n_embd_head_k    = 128
0.00.081.917 I print_info: n_embd_head_v    = 128
0.00.081.917 I print_info: n_gqa            = 1
0.00.081.918 I print_info: n_embd_k_gqa     = 2048
0.00.081.919 I print_info: n_embd_v_gqa     = 2048
0.00.081.919 I print_info: f_norm_eps       = 1.0e-05
0.00.081.920 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.081.920 I print_info: f_clamp_kqv      = 0.0e+00
0.00.081.920 I print_info: f_max_alibi_bias = 0.0e+00
0.00.081.920 I print_info: f_logit_scale    = 0.0e+00
0.00.081.921 I print_info: n_ff             = 8192
0.00.081.921 I print_info: n_expert         = 0
0.00.081.922 I print_info: n_expert_used    = 0
0.00.081.922 I print_info: causal attn      = 1
0.00.081.922 I print_info: pooling type     = 0
0.00.081.922 I print_info: rope type        = 2
0.00.081.922 I print_info: rope scaling     = linear
0.00.081.923 I print_info: freq_base_train  = 10000.0
0.00.081.923 I print_info: freq_scale_train = 1
0.00.081.923 I print_info: n_ctx_orig_yarn  = 2048
0.00.081.923 I print_info: rope_finetuned   = unknown
0.00.081.924 I print_info: ssm_d_conv       = 0
0.00.081.924 I print_info: ssm_d_inner      = 0
0.00.081.925 I print_info: ssm_d_state      = 0
0.00.081.926 I print_info: ssm_dt_rank      = 0
0.00.081.926 I print_info: ssm_dt_b_c_rms   = 0
0.00.081.926 I print_info: model type       = 1.4B
0.00.081.927 I print_info: model params     = 1.41 B
0.00.081.927 I print_info: general.name     = 1.4B
0.00.081.927 I print_info: vocab type       = BPE
0.00.081.928 I print_info: n_vocab          = 50304
0.00.081.928 I print_info: n_merges         = 50009
0.00.081.928 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.081.928 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.081.928 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.081.929 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.081.929 I print_info: LF token         = 187 'Ċ'
0.00.081.929 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.081.929 I print_info: max token length = 1024
0.00.081.930 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.036.078 I load_tensors: offloading 24 repeating layers to GPU
0.01.036.083 I load_tensors: offloading output layer to GPU
0.01.036.084 I load_tensors: offloaded 25/25 layers to GPU
0.01.036.112 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.036.114 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.036.994 I llama_context: n_seq_max     = 1
0.01.036.995 I llama_context: n_ctx         = 128
0.01.036.995 I llama_context: n_ctx_per_seq = 128
0.01.036.996 I llama_context: n_batch       = 128
0.01.036.996 I llama_context: n_ubatch      = 128
0.01.036.996 I llama_context: flash_attn    = 0
0.01.036.996 I llama_context: freq_base     = 10000.0
0.01.036.997 I llama_context: freq_scale    = 1
0.01.036.997 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.036.998 I ggml_metal_init: allocating
0.01.037.044 I ggml_metal_init: found device: Apple M4
0.01.037.051 I ggml_metal_init: picking default device: Apple M4
0.01.038.109 I ggml_metal_init: using embedded metal library
0.01.042.031 I ggml_metal_init: GPU name:   Apple M4
0.01.042.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.042.034 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.042.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.042.035 I ggml_metal_init: simdgroup reduction   = true
0.01.042.035 I ggml_metal_init: simdgroup matrix mul. = true
0.01.042.035 I ggml_metal_init: has residency sets    = true
0.01.042.036 I ggml_metal_init: has bfloat            = true
0.01.042.036 I ggml_metal_init: use bfloat            = true
0.01.042.036 I ggml_metal_init: hasUnifiedMemory      = true
0.01.042.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.053.255 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.055.039 I init:      Metal KV buffer size =    24.00 MiB
0.01.055.042 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.055.068 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.056.685 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.056.686 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.056.687 I llama_context: graph nodes  = 967
0.01.056.687 I llama_context: graph splits = 2
0.01.056.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.056.688 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.092.393 I 
0.01.092.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.092.453 I perplexity: tokenizing the input ..
0.01.097.982 I perplexity: tokenization took 5.526 ms
0.01.098.006 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.216.709 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.218.045 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.218.063 I llama_perf_context_print:        load time =    1063.71 ms
0.01.218.064 I llama_perf_context_print: prompt eval time =     118.31 ms /   128 tokens (    0.92 ms per token,  1081.86 tokens per second)
0.01.218.065 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.218.069 I llama_perf_context_print:       total time =     125.67 ms /   129 tokens
0.01.218.681 I ggml_metal_free: deallocating

real	0m1.404s
user	0m0.101s
sys	0m0.213s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.009.995 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.286 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.291 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.178 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.179 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.180 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.180 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.181 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.181 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.182 I llama_model_loader: - type  f32:  194 tensors
0.00.027.182 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.183 I print_info: file format = GGUF V3 (latest)
0.00.027.183 I print_info: file type   = Q8_0
0.00.027.184 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.403 I load: special tokens cache size = 25
0.00.041.494 I load: token to piece cache size = 0.2984 MB
0.00.041.499 I print_info: arch             = gptneox
0.00.041.499 I print_info: vocab_only       = 0
0.00.041.505 I print_info: n_ctx_train      = 2048
0.00.041.507 I print_info: n_embd           = 2048
0.00.041.507 I print_info: n_layer          = 24
0.00.041.514 I print_info: n_head           = 16
0.00.041.515 I print_info: n_head_kv        = 16
0.00.041.515 I print_info: n_rot            = 32
0.00.041.515 I print_info: n_swa            = 0
0.00.041.515 I print_info: n_embd_head_k    = 128
0.00.041.516 I print_info: n_embd_head_v    = 128
0.00.041.516 I print_info: n_gqa            = 1
0.00.041.517 I print_info: n_embd_k_gqa     = 2048
0.00.041.518 I print_info: n_embd_v_gqa     = 2048
0.00.041.518 I print_info: f_norm_eps       = 1.0e-05
0.00.041.519 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.519 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.519 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.519 I print_info: f_logit_scale    = 0.0e+00
0.00.041.520 I print_info: n_ff             = 8192
0.00.041.520 I print_info: n_expert         = 0
0.00.041.520 I print_info: n_expert_used    = 0
0.00.041.520 I print_info: causal attn      = 1
0.00.041.520 I print_info: pooling type     = 0
0.00.041.521 I print_info: rope type        = 2
0.00.041.521 I print_info: rope scaling     = linear
0.00.041.521 I print_info: freq_base_train  = 10000.0
0.00.041.521 I print_info: freq_scale_train = 1
0.00.041.521 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.522 I print_info: rope_finetuned   = unknown
0.00.041.522 I print_info: ssm_d_conv       = 0
0.00.041.522 I print_info: ssm_d_inner      = 0
0.00.041.522 I print_info: ssm_d_state      = 0
0.00.041.522 I print_info: ssm_dt_rank      = 0
0.00.041.522 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.522 I print_info: model type       = 1.4B
0.00.041.523 I print_info: model params     = 1.41 B
0.00.041.523 I print_info: general.name     = 1.4B
0.00.041.524 I print_info: vocab type       = BPE
0.00.041.524 I print_info: n_vocab          = 50304
0.00.041.524 I print_info: n_merges         = 50009
0.00.041.524 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.524 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.525 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.525 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.526 I print_info: LF token         = 187 'Ċ'
0.00.041.526 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.526 I print_info: max token length = 1024
0.00.041.527 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.959.366 I load_tensors: offloading 24 repeating layers to GPU
0.00.959.371 I load_tensors: offloading output layer to GPU
0.00.959.373 I load_tensors: offloaded 25/25 layers to GPU
0.00.959.396 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.959.397 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.960.184 I llama_context: n_seq_max     = 1
0.00.960.186 I llama_context: n_ctx         = 2048
0.00.960.186 I llama_context: n_ctx_per_seq = 2048
0.00.960.186 I llama_context: n_batch       = 2048
0.00.960.187 I llama_context: n_ubatch      = 512
0.00.960.187 I llama_context: flash_attn    = 0
0.00.960.188 I llama_context: freq_base     = 10000.0
0.00.960.189 I llama_context: freq_scale    = 1
0.00.960.190 I ggml_metal_init: allocating
0.00.960.205 I ggml_metal_init: found device: Apple M4
0.00.960.216 I ggml_metal_init: picking default device: Apple M4
0.00.961.530 I ggml_metal_init: using embedded metal library
0.00.967.041 I ggml_metal_init: GPU name:   Apple M4
0.00.967.045 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.967.045 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.967.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.967.046 I ggml_metal_init: simdgroup reduction   = true
0.00.967.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.967.047 I ggml_metal_init: has residency sets    = true
0.00.967.047 I ggml_metal_init: has bfloat            = true
0.00.967.047 I ggml_metal_init: use bfloat            = true
0.00.967.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.967.049 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.982.128 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.034.695 I init:      Metal KV buffer size =   384.00 MiB
0.01.034.703 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.034.739 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.039.743 I llama_context:      Metal compute buffer size =   102.25 MiB
0.01.039.745 I llama_context:        CPU compute buffer size =     8.01 MiB
0.01.039.745 I llama_context: graph nodes  = 967
0.01.039.745 I llama_context: graph splits = 2
0.01.039.748 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.039.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.039.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.095.300 I main: llama threadpool init, n_threads = 4
0.01.095.343 I 
0.01.095.377 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.095.379 I 
0.01.095.624 I sampler seed: 1234
0.01.095.633 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.095.650 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.095.651 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.095.652 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.185.374 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.02.185.375 I llama_perf_context_print:        load time =    1084.57 ms
0.02.185.375 I llama_perf_context_print: prompt eval time =      49.13 ms /     7 tokens (    7.02 ms per token,   142.49 tokens per second)
0.02.185.376 I llama_perf_context_print:        eval time =    1037.68 ms /    63 runs   (   16.47 ms per token,    60.71 tokens per second)
0.02.185.377 I llama_perf_context_print:       total time =    1090.81 ms /    70 tokens
0.02.189.122 I ggml_metal_free: deallocating

real	0m2.208s
user	0m0.106s
sys	0m0.257s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.254 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.409 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.415 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.422 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.423 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.425 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.425 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.425 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.426 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.426 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.426 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.428 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.429 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.429 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.292 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.112 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.113 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.114 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.115 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.115 I llama_model_loader: - type  f32:  194 tensors
0.00.025.116 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.116 I print_info: file format = GGUF V3 (latest)
0.00.025.117 I print_info: file type   = Q8_0
0.00.025.118 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.552 I load: special tokens cache size = 25
0.00.039.880 I load: token to piece cache size = 0.2984 MB
0.00.039.885 I print_info: arch             = gptneox
0.00.039.885 I print_info: vocab_only       = 0
0.00.039.885 I print_info: n_ctx_train      = 2048
0.00.039.885 I print_info: n_embd           = 2048
0.00.039.885 I print_info: n_layer          = 24
0.00.039.890 I print_info: n_head           = 16
0.00.039.890 I print_info: n_head_kv        = 16
0.00.039.891 I print_info: n_rot            = 32
0.00.039.891 I print_info: n_swa            = 0
0.00.039.891 I print_info: n_embd_head_k    = 128
0.00.039.891 I print_info: n_embd_head_v    = 128
0.00.039.892 I print_info: n_gqa            = 1
0.00.039.893 I print_info: n_embd_k_gqa     = 2048
0.00.039.894 I print_info: n_embd_v_gqa     = 2048
0.00.039.895 I print_info: f_norm_eps       = 1.0e-05
0.00.039.896 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.896 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.896 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.896 I print_info: f_logit_scale    = 0.0e+00
0.00.039.897 I print_info: n_ff             = 8192
0.00.039.897 I print_info: n_expert         = 0
0.00.039.897 I print_info: n_expert_used    = 0
0.00.039.897 I print_info: causal attn      = 1
0.00.039.897 I print_info: pooling type     = 0
0.00.039.897 I print_info: rope type        = 2
0.00.039.898 I print_info: rope scaling     = linear
0.00.039.898 I print_info: freq_base_train  = 10000.0
0.00.039.898 I print_info: freq_scale_train = 1
0.00.039.899 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.899 I print_info: rope_finetuned   = unknown
0.00.039.899 I print_info: ssm_d_conv       = 0
0.00.039.899 I print_info: ssm_d_inner      = 0
0.00.039.899 I print_info: ssm_d_state      = 0
0.00.039.899 I print_info: ssm_dt_rank      = 0
0.00.039.899 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.900 I print_info: model type       = 1.4B
0.00.039.902 I print_info: model params     = 1.41 B
0.00.039.902 I print_info: general.name     = 1.4B
0.00.039.902 I print_info: vocab type       = BPE
0.00.039.903 I print_info: n_vocab          = 50304
0.00.039.903 I print_info: n_merges         = 50009
0.00.039.903 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.903 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.904 I print_info: LF token         = 187 'Ċ'
0.00.039.904 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.904 I print_info: max token length = 1024
0.00.039.905 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.799.959 I load_tensors: offloading 24 repeating layers to GPU
0.00.799.965 I load_tensors: offloading output layer to GPU
0.00.799.966 I load_tensors: offloaded 25/25 layers to GPU
0.00.799.994 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.799.998 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.801.454 I llama_context: n_seq_max     = 1
0.00.801.455 I llama_context: n_ctx         = 128
0.00.801.456 I llama_context: n_ctx_per_seq = 128
0.00.801.456 I llama_context: n_batch       = 128
0.00.801.456 I llama_context: n_ubatch      = 128
0.00.801.456 I llama_context: flash_attn    = 0
0.00.801.457 I llama_context: freq_base     = 10000.0
0.00.801.458 I llama_context: freq_scale    = 1
0.00.801.458 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.801.459 I ggml_metal_init: allocating
0.00.801.554 I ggml_metal_init: found device: Apple M4
0.00.801.564 I ggml_metal_init: picking default device: Apple M4
0.00.802.928 I ggml_metal_init: using embedded metal library
0.00.808.028 I ggml_metal_init: GPU name:   Apple M4
0.00.808.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.808.032 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.808.033 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.808.033 I ggml_metal_init: simdgroup reduction   = true
0.00.808.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.808.033 I ggml_metal_init: has residency sets    = true
0.00.808.034 I ggml_metal_init: has bfloat            = true
0.00.808.034 I ggml_metal_init: use bfloat            = true
0.00.808.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.808.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.822.727 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.825.954 I init:      Metal KV buffer size =    24.00 MiB
0.00.825.962 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.826.020 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.829.020 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.829.022 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.829.022 I llama_context: graph nodes  = 967
0.00.829.023 I llama_context: graph splits = 2
0.00.829.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.829.026 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.855.641 I 
0.00.855.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.855.751 I perplexity: tokenizing the input ..
0.00.862.628 I perplexity: tokenization took 6.875 ms
0.00.862.640 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.999.537 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.000.942 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.000.954 I llama_perf_context_print:        load time =     846.37 ms
0.01.000.955 I llama_perf_context_print: prompt eval time =     136.67 ms /   128 tokens (    1.07 ms per token,   936.58 tokens per second)
0.01.000.956 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.000.956 I llama_perf_context_print:       total time =     145.32 ms /   129 tokens
0.01.001.552 I ggml_metal_free: deallocating

real	0m1.017s
user	0m0.075s
sys	0m0.158s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.011.690 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.734 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.739 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.741 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.742 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.742 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.742 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.743 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.744 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.745 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.746 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.746 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.747 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.747 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.749 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.751 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.752 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.752 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.566 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.343 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.344 I llama_model_loader: - type  f32:  194 tensors
0.00.028.344 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.345 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.345 I print_info: file format = GGUF V3 (latest)
0.00.028.346 I print_info: file type   = Q4_0
0.00.028.347 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.677 I load: special tokens cache size = 25
0.00.042.722 I load: token to piece cache size = 0.2984 MB
0.00.042.726 I print_info: arch             = gptneox
0.00.042.726 I print_info: vocab_only       = 0
0.00.042.727 I print_info: n_ctx_train      = 2048
0.00.042.727 I print_info: n_embd           = 2048
0.00.042.727 I print_info: n_layer          = 24
0.00.042.731 I print_info: n_head           = 16
0.00.042.732 I print_info: n_head_kv        = 16
0.00.042.733 I print_info: n_rot            = 32
0.00.042.734 I print_info: n_swa            = 0
0.00.042.735 I print_info: n_embd_head_k    = 128
0.00.042.735 I print_info: n_embd_head_v    = 128
0.00.042.736 I print_info: n_gqa            = 1
0.00.042.737 I print_info: n_embd_k_gqa     = 2048
0.00.042.738 I print_info: n_embd_v_gqa     = 2048
0.00.042.739 I print_info: f_norm_eps       = 1.0e-05
0.00.042.739 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.739 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.739 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.740 I print_info: f_logit_scale    = 0.0e+00
0.00.042.741 I print_info: n_ff             = 8192
0.00.042.741 I print_info: n_expert         = 0
0.00.042.741 I print_info: n_expert_used    = 0
0.00.042.741 I print_info: causal attn      = 1
0.00.042.741 I print_info: pooling type     = 0
0.00.042.742 I print_info: rope type        = 2
0.00.042.742 I print_info: rope scaling     = linear
0.00.042.742 I print_info: freq_base_train  = 10000.0
0.00.042.743 I print_info: freq_scale_train = 1
0.00.042.743 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.743 I print_info: rope_finetuned   = unknown
0.00.042.744 I print_info: ssm_d_conv       = 0
0.00.042.745 I print_info: ssm_d_inner      = 0
0.00.042.745 I print_info: ssm_d_state      = 0
0.00.042.745 I print_info: ssm_dt_rank      = 0
0.00.042.745 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.745 I print_info: model type       = 1.4B
0.00.042.746 I print_info: model params     = 1.41 B
0.00.042.746 I print_info: general.name     = 1.4B
0.00.042.746 I print_info: vocab type       = BPE
0.00.042.747 I print_info: n_vocab          = 50304
0.00.042.747 I print_info: n_merges         = 50009
0.00.042.747 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.747 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.748 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.748 I print_info: LF token         = 187 'Ċ'
0.00.042.748 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.748 I print_info: max token length = 1024
0.00.042.749 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.724 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.739 I load_tensors: offloading output layer to GPU
0.00.596.740 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.772 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.596.773 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.598.471 I llama_context: n_seq_max     = 1
0.00.598.473 I llama_context: n_ctx         = 2048
0.00.598.473 I llama_context: n_ctx_per_seq = 2048
0.00.598.474 I llama_context: n_batch       = 2048
0.00.598.475 I llama_context: n_ubatch      = 512
0.00.598.475 I llama_context: flash_attn    = 0
0.00.598.477 I llama_context: freq_base     = 10000.0
0.00.598.478 I llama_context: freq_scale    = 1
0.00.598.480 I ggml_metal_init: allocating
0.00.598.554 I ggml_metal_init: found device: Apple M4
0.00.598.568 I ggml_metal_init: picking default device: Apple M4
0.00.600.403 I ggml_metal_init: using embedded metal library
0.00.605.981 I ggml_metal_init: GPU name:   Apple M4
0.00.605.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.986 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.987 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.988 I ggml_metal_init: simdgroup reduction   = true
0.00.605.988 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.989 I ggml_metal_init: has residency sets    = true
0.00.605.989 I ggml_metal_init: has bfloat            = true
0.00.605.989 I ggml_metal_init: use bfloat            = true
0.00.605.990 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.992 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.596 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.683.849 I init:      Metal KV buffer size =   384.00 MiB
0.00.683.856 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.683.894 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.688.226 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.688.228 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.688.228 I llama_context: graph nodes  = 967
0.00.688.228 I llama_context: graph splits = 2
0.00.688.235 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.688.366 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.688.366 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.050 I main: llama threadpool init, n_threads = 4
0.00.746.096 I 
0.00.746.118 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.118 I 
0.00.746.291 I sampler seed: 1234
0.00.746.295 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.317 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.317 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.317 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.424.784 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.424.784 I llama_perf_context_print:        load time =     733.64 ms
0.01.424.785 I llama_perf_context_print: prompt eval time =      48.74 ms /     7 tokens (    6.96 ms per token,   143.62 tokens per second)
0.01.424.786 I llama_perf_context_print:        eval time =     626.90 ms /    63 runs   (    9.95 ms per token,   100.49 tokens per second)
0.01.424.786 I llama_perf_context_print:       total time =     679.45 ms /    70 tokens
0.01.427.764 I ggml_metal_free: deallocating

real	0m1.444s
user	0m0.111s
sys	0m0.208s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.211 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.619 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.630 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.631 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.631 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.631 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.632 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.633 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.633 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.633 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.634 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.634 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.634 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.635 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.636 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.636 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.626 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.598 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.599 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.600 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.600 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.600 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.601 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.601 I llama_model_loader: - type  f32:  194 tensors
0.00.026.602 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.602 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.603 I print_info: file format = GGUF V3 (latest)
0.00.026.603 I print_info: file type   = Q4_0
0.00.026.604 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.345 I load: special tokens cache size = 25
0.00.041.524 I load: token to piece cache size = 0.2984 MB
0.00.041.528 I print_info: arch             = gptneox
0.00.041.528 I print_info: vocab_only       = 0
0.00.041.528 I print_info: n_ctx_train      = 2048
0.00.041.528 I print_info: n_embd           = 2048
0.00.041.528 I print_info: n_layer          = 24
0.00.041.533 I print_info: n_head           = 16
0.00.041.533 I print_info: n_head_kv        = 16
0.00.041.534 I print_info: n_rot            = 32
0.00.041.534 I print_info: n_swa            = 0
0.00.041.534 I print_info: n_embd_head_k    = 128
0.00.041.534 I print_info: n_embd_head_v    = 128
0.00.041.535 I print_info: n_gqa            = 1
0.00.041.535 I print_info: n_embd_k_gqa     = 2048
0.00.041.536 I print_info: n_embd_v_gqa     = 2048
0.00.041.537 I print_info: f_norm_eps       = 1.0e-05
0.00.041.537 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.537 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.537 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.538 I print_info: f_logit_scale    = 0.0e+00
0.00.041.538 I print_info: n_ff             = 8192
0.00.041.538 I print_info: n_expert         = 0
0.00.041.539 I print_info: n_expert_used    = 0
0.00.041.539 I print_info: causal attn      = 1
0.00.041.539 I print_info: pooling type     = 0
0.00.041.539 I print_info: rope type        = 2
0.00.041.540 I print_info: rope scaling     = linear
0.00.041.540 I print_info: freq_base_train  = 10000.0
0.00.041.540 I print_info: freq_scale_train = 1
0.00.041.540 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.541 I print_info: rope_finetuned   = unknown
0.00.041.541 I print_info: ssm_d_conv       = 0
0.00.041.541 I print_info: ssm_d_inner      = 0
0.00.041.541 I print_info: ssm_d_state      = 0
0.00.041.541 I print_info: ssm_dt_rank      = 0
0.00.041.541 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.542 I print_info: model type       = 1.4B
0.00.041.542 I print_info: model params     = 1.41 B
0.00.041.542 I print_info: general.name     = 1.4B
0.00.041.543 I print_info: vocab type       = BPE
0.00.041.543 I print_info: n_vocab          = 50304
0.00.041.543 I print_info: n_merges         = 50009
0.00.041.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.544 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.544 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.544 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.544 I print_info: LF token         = 187 'Ċ'
0.00.041.545 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.545 I print_info: max token length = 1024
0.00.041.545 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.599.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.599.390 I load_tensors: offloading output layer to GPU
0.00.599.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.599.424 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.599.425 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.601.184 I llama_context: n_seq_max     = 1
0.00.601.186 I llama_context: n_ctx         = 128
0.00.601.187 I llama_context: n_ctx_per_seq = 128
0.00.601.187 I llama_context: n_batch       = 128
0.00.601.188 I llama_context: n_ubatch      = 128
0.00.601.188 I llama_context: flash_attn    = 0
0.00.601.191 I llama_context: freq_base     = 10000.0
0.00.601.191 I llama_context: freq_scale    = 1
0.00.601.192 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.198 I ggml_metal_init: allocating
0.00.601.320 I ggml_metal_init: found device: Apple M4
0.00.601.332 I ggml_metal_init: picking default device: Apple M4
0.00.603.317 I ggml_metal_init: using embedded metal library
0.00.608.716 I ggml_metal_init: GPU name:   Apple M4
0.00.608.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.723 I ggml_metal_init: simdgroup reduction   = true
0.00.608.723 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.724 I ggml_metal_init: has residency sets    = true
0.00.608.724 I ggml_metal_init: has bfloat            = true
0.00.608.724 I ggml_metal_init: use bfloat            = true
0.00.608.725 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.729 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.628.386 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.631.959 I init:      Metal KV buffer size =    24.00 MiB
0.00.631.962 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.632.006 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.635.299 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.635.301 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.635.302 I llama_context: graph nodes  = 967
0.00.635.302 I llama_context: graph splits = 2
0.00.635.306 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.635.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.665.577 I 
0.00.665.654 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.665.673 I perplexity: tokenizing the input ..
0.00.670.960 I perplexity: tokenization took 5.285 ms
0.00.670.973 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.594 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.795.932 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.795.947 I llama_perf_context_print:        load time =     655.36 ms
0.00.795.948 I llama_perf_context_print: prompt eval time =     123.39 ms /   128 tokens (    0.96 ms per token,  1037.34 tokens per second)
0.00.795.949 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.950 I llama_perf_context_print:       total time =     130.38 ms /   129 tokens
0.00.796.515 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.079s
sys	0m0.138s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.388 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.126 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.132 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.133 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.133 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.134 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.134 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.135 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.136 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.136 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.136 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.138 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.139 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.139 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.141 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.018 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.024 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.770 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.771 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.771 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.772 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.772 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.773 I llama_model_loader: - type  f32:  194 tensors
0.00.025.773 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.773 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.774 I print_info: file format = GGUF V3 (latest)
0.00.025.774 I print_info: file type   = Q4_1
0.00.025.775 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.028 I load: special tokens cache size = 25
0.00.040.151 I load: token to piece cache size = 0.2984 MB
0.00.040.154 I print_info: arch             = gptneox
0.00.040.154 I print_info: vocab_only       = 0
0.00.040.154 I print_info: n_ctx_train      = 2048
0.00.040.154 I print_info: n_embd           = 2048
0.00.040.155 I print_info: n_layer          = 24
0.00.040.157 I print_info: n_head           = 16
0.00.040.158 I print_info: n_head_kv        = 16
0.00.040.158 I print_info: n_rot            = 32
0.00.040.159 I print_info: n_swa            = 0
0.00.040.159 I print_info: n_embd_head_k    = 128
0.00.040.159 I print_info: n_embd_head_v    = 128
0.00.040.160 I print_info: n_gqa            = 1
0.00.040.160 I print_info: n_embd_k_gqa     = 2048
0.00.040.161 I print_info: n_embd_v_gqa     = 2048
0.00.040.162 I print_info: f_norm_eps       = 1.0e-05
0.00.040.162 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.162 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.162 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.163 I print_info: f_logit_scale    = 0.0e+00
0.00.040.164 I print_info: n_ff             = 8192
0.00.040.164 I print_info: n_expert         = 0
0.00.040.164 I print_info: n_expert_used    = 0
0.00.040.165 I print_info: causal attn      = 1
0.00.040.165 I print_info: pooling type     = 0
0.00.040.165 I print_info: rope type        = 2
0.00.040.165 I print_info: rope scaling     = linear
0.00.040.166 I print_info: freq_base_train  = 10000.0
0.00.040.166 I print_info: freq_scale_train = 1
0.00.040.166 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.166 I print_info: rope_finetuned   = unknown
0.00.040.167 I print_info: ssm_d_conv       = 0
0.00.040.167 I print_info: ssm_d_inner      = 0
0.00.040.167 I print_info: ssm_d_state      = 0
0.00.040.167 I print_info: ssm_dt_rank      = 0
0.00.040.167 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.167 I print_info: model type       = 1.4B
0.00.040.168 I print_info: model params     = 1.41 B
0.00.040.168 I print_info: general.name     = 1.4B
0.00.040.168 I print_info: vocab type       = BPE
0.00.040.169 I print_info: n_vocab          = 50304
0.00.040.169 I print_info: n_merges         = 50009
0.00.040.169 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.169 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.170 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.170 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.170 I print_info: LF token         = 187 'Ċ'
0.00.040.170 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.171 I print_info: max token length = 1024
0.00.040.171 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.626.320 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.333 I load_tensors: offloading output layer to GPU
0.00.626.334 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.368 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.626.369 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.627.786 I llama_context: n_seq_max     = 1
0.00.627.789 I llama_context: n_ctx         = 2048
0.00.627.790 I llama_context: n_ctx_per_seq = 2048
0.00.627.790 I llama_context: n_batch       = 2048
0.00.627.791 I llama_context: n_ubatch      = 512
0.00.627.791 I llama_context: flash_attn    = 0
0.00.627.794 I llama_context: freq_base     = 10000.0
0.00.627.794 I llama_context: freq_scale    = 1
0.00.627.796 I ggml_metal_init: allocating
0.00.627.876 I ggml_metal_init: found device: Apple M4
0.00.627.890 I ggml_metal_init: picking default device: Apple M4
0.00.629.705 I ggml_metal_init: using embedded metal library
0.00.636.269 I ggml_metal_init: GPU name:   Apple M4
0.00.636.274 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.636.275 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.636.276 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.636.277 I ggml_metal_init: simdgroup reduction   = true
0.00.636.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.636.278 I ggml_metal_init: has residency sets    = true
0.00.636.278 I ggml_metal_init: has bfloat            = true
0.00.636.278 I ggml_metal_init: use bfloat            = true
0.00.636.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.636.281 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.654.900 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.761 I init:      Metal KV buffer size =   384.00 MiB
0.00.710.768 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.806 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.715.930 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.715.932 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.715.932 I llama_context: graph nodes  = 967
0.00.715.932 I llama_context: graph splits = 2
0.00.715.939 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.063 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.063 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.455 I main: llama threadpool init, n_threads = 4
0.00.772.497 I 
0.00.772.518 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.518 I 
0.00.772.674 I sampler seed: 1234
0.00.772.679 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.698 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.698 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.497.023 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56304.52 tokens per second)
0.01.497.024 I llama_perf_context_print:        load time =     762.33 ms
0.01.497.025 I llama_perf_context_print: prompt eval time =      48.01 ms /     7 tokens (    6.86 ms per token,   145.79 tokens per second)
0.01.497.025 I llama_perf_context_print:        eval time =     673.58 ms /    63 runs   (   10.69 ms per token,    93.53 tokens per second)
0.01.497.026 I llama_perf_context_print:       total time =     725.31 ms /    70 tokens
0.01.500.787 I ggml_metal_free: deallocating

real	0m1.517s
user	0m0.109s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.859 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.949 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.954 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.955 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.961 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.961 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.961 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.962 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.963 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.963 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.964 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.964 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.964 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.965 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.967 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.967 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.967 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.889 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.901 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.755 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.757 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.757 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.758 I llama_model_loader: - type  f32:  194 tensors
0.00.024.758 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.759 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.760 I print_info: file format = GGUF V3 (latest)
0.00.024.760 I print_info: file type   = Q4_1
0.00.024.761 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.248 I load: special tokens cache size = 25
0.00.039.522 I load: token to piece cache size = 0.2984 MB
0.00.039.527 I print_info: arch             = gptneox
0.00.039.527 I print_info: vocab_only       = 0
0.00.039.527 I print_info: n_ctx_train      = 2048
0.00.039.528 I print_info: n_embd           = 2048
0.00.039.528 I print_info: n_layer          = 24
0.00.039.532 I print_info: n_head           = 16
0.00.039.532 I print_info: n_head_kv        = 16
0.00.039.532 I print_info: n_rot            = 32
0.00.039.533 I print_info: n_swa            = 0
0.00.039.533 I print_info: n_embd_head_k    = 128
0.00.039.533 I print_info: n_embd_head_v    = 128
0.00.039.534 I print_info: n_gqa            = 1
0.00.039.534 I print_info: n_embd_k_gqa     = 2048
0.00.039.535 I print_info: n_embd_v_gqa     = 2048
0.00.039.536 I print_info: f_norm_eps       = 1.0e-05
0.00.039.536 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.537 I print_info: f_logit_scale    = 0.0e+00
0.00.039.537 I print_info: n_ff             = 8192
0.00.039.537 I print_info: n_expert         = 0
0.00.039.537 I print_info: n_expert_used    = 0
0.00.039.537 I print_info: causal attn      = 1
0.00.039.538 I print_info: pooling type     = 0
0.00.039.538 I print_info: rope type        = 2
0.00.039.538 I print_info: rope scaling     = linear
0.00.039.538 I print_info: freq_base_train  = 10000.0
0.00.039.539 I print_info: freq_scale_train = 1
0.00.039.539 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.539 I print_info: rope_finetuned   = unknown
0.00.039.539 I print_info: ssm_d_conv       = 0
0.00.039.539 I print_info: ssm_d_inner      = 0
0.00.039.539 I print_info: ssm_d_state      = 0
0.00.039.539 I print_info: ssm_dt_rank      = 0
0.00.039.540 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.540 I print_info: model type       = 1.4B
0.00.039.540 I print_info: model params     = 1.41 B
0.00.039.540 I print_info: general.name     = 1.4B
0.00.039.541 I print_info: vocab type       = BPE
0.00.039.541 I print_info: n_vocab          = 50304
0.00.039.541 I print_info: n_merges         = 50009
0.00.039.541 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.542 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.542 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.542 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.542 I print_info: LF token         = 187 'Ċ'
0.00.039.542 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.542 I print_info: max token length = 1024
0.00.039.543 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.603.185 I load_tensors: offloading 24 repeating layers to GPU
0.00.603.201 I load_tensors: offloading output layer to GPU
0.00.603.202 I load_tensors: offloaded 25/25 layers to GPU
0.00.603.236 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.603.238 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.604.982 I llama_context: n_seq_max     = 1
0.00.604.985 I llama_context: n_ctx         = 128
0.00.604.986 I llama_context: n_ctx_per_seq = 128
0.00.604.986 I llama_context: n_batch       = 128
0.00.604.987 I llama_context: n_ubatch      = 128
0.00.604.987 I llama_context: flash_attn    = 0
0.00.604.989 I llama_context: freq_base     = 10000.0
0.00.604.990 I llama_context: freq_scale    = 1
0.00.604.990 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.604.992 I ggml_metal_init: allocating
0.00.605.072 I ggml_metal_init: found device: Apple M4
0.00.605.086 I ggml_metal_init: picking default device: Apple M4
0.00.606.832 I ggml_metal_init: using embedded metal library
0.00.613.466 I ggml_metal_init: GPU name:   Apple M4
0.00.613.471 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.613.472 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.613.473 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.613.474 I ggml_metal_init: simdgroup reduction   = true
0.00.613.474 I ggml_metal_init: simdgroup matrix mul. = true
0.00.613.474 I ggml_metal_init: has residency sets    = true
0.00.613.475 I ggml_metal_init: has bfloat            = true
0.00.613.475 I ggml_metal_init: use bfloat            = true
0.00.613.476 I ggml_metal_init: hasUnifiedMemory      = true
0.00.613.485 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.903 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.634.370 I init:      Metal KV buffer size =    24.00 MiB
0.00.634.373 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.634.414 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.637.460 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.637.462 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.637.462 I llama_context: graph nodes  = 967
0.00.637.462 I llama_context: graph splits = 2
0.00.637.466 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.637.466 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.847 I 
0.00.661.925 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.944 I perplexity: tokenizing the input ..
0.00.669.256 I perplexity: tokenization took 7.309 ms
0.00.669.277 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.806.148 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.807.486 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.807.500 I llama_perf_context_print:        load time =     652.98 ms
0.00.807.501 I llama_perf_context_print: prompt eval time =     135.93 ms /   128 tokens (    1.06 ms per token,   941.68 tokens per second)
0.00.807.502 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.807.502 I llama_perf_context_print:       total time =     145.66 ms /   129 tokens
0.00.808.057 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.081s
sys	0m0.128s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.620 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.337 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.347 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.348 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.348 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.348 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.349 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.350 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.351 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.352 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.352 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.353 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.355 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.355 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.002 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.003 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.004 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.004 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.005 I llama_model_loader: - type  f32:  194 tensors
0.00.025.005 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.005 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.006 I print_info: file format = GGUF V3 (latest)
0.00.025.006 I print_info: file type   = Q5_0
0.00.025.007 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.904 I load: special tokens cache size = 25
0.00.038.983 I load: token to piece cache size = 0.2984 MB
0.00.038.986 I print_info: arch             = gptneox
0.00.038.986 I print_info: vocab_only       = 0
0.00.038.986 I print_info: n_ctx_train      = 2048
0.00.038.987 I print_info: n_embd           = 2048
0.00.038.987 I print_info: n_layer          = 24
0.00.038.989 I print_info: n_head           = 16
0.00.038.990 I print_info: n_head_kv        = 16
0.00.038.990 I print_info: n_rot            = 32
0.00.038.991 I print_info: n_swa            = 0
0.00.038.991 I print_info: n_embd_head_k    = 128
0.00.038.991 I print_info: n_embd_head_v    = 128
0.00.038.992 I print_info: n_gqa            = 1
0.00.038.993 I print_info: n_embd_k_gqa     = 2048
0.00.038.994 I print_info: n_embd_v_gqa     = 2048
0.00.038.994 I print_info: f_norm_eps       = 1.0e-05
0.00.038.995 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.997 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.997 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.997 I print_info: f_logit_scale    = 0.0e+00
0.00.038.997 I print_info: n_ff             = 8192
0.00.038.998 I print_info: n_expert         = 0
0.00.038.998 I print_info: n_expert_used    = 0
0.00.038.998 I print_info: causal attn      = 1
0.00.038.998 I print_info: pooling type     = 0
0.00.038.998 I print_info: rope type        = 2
0.00.038.999 I print_info: rope scaling     = linear
0.00.038.999 I print_info: freq_base_train  = 10000.0
0.00.038.999 I print_info: freq_scale_train = 1
0.00.038.999 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.001 I print_info: rope_finetuned   = unknown
0.00.039.001 I print_info: ssm_d_conv       = 0
0.00.039.002 I print_info: ssm_d_inner      = 0
0.00.039.002 I print_info: ssm_d_state      = 0
0.00.039.002 I print_info: ssm_dt_rank      = 0
0.00.039.002 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.002 I print_info: model type       = 1.4B
0.00.039.003 I print_info: model params     = 1.41 B
0.00.039.003 I print_info: general.name     = 1.4B
0.00.039.003 I print_info: vocab type       = BPE
0.00.039.004 I print_info: n_vocab          = 50304
0.00.039.004 I print_info: n_merges         = 50009
0.00.039.004 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.005 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.005 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.010 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.011 I print_info: LF token         = 187 'Ċ'
0.00.039.011 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.011 I print_info: max token length = 1024
0.00.039.012 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.655.259 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.272 I load_tensors: offloading output layer to GPU
0.00.655.273 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.301 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.655.302 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.656.611 I llama_context: n_seq_max     = 1
0.00.656.616 I llama_context: n_ctx         = 2048
0.00.656.617 I llama_context: n_ctx_per_seq = 2048
0.00.656.618 I llama_context: n_batch       = 2048
0.00.656.618 I llama_context: n_ubatch      = 512
0.00.656.618 I llama_context: flash_attn    = 0
0.00.656.620 I llama_context: freq_base     = 10000.0
0.00.656.620 I llama_context: freq_scale    = 1
0.00.656.625 I ggml_metal_init: allocating
0.00.656.678 I ggml_metal_init: found device: Apple M4
0.00.656.690 I ggml_metal_init: picking default device: Apple M4
0.00.658.448 I ggml_metal_init: using embedded metal library
0.00.664.331 I ggml_metal_init: GPU name:   Apple M4
0.00.664.336 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.337 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.338 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.339 I ggml_metal_init: simdgroup reduction   = true
0.00.664.339 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.339 I ggml_metal_init: has residency sets    = true
0.00.664.340 I ggml_metal_init: has bfloat            = true
0.00.664.340 I ggml_metal_init: use bfloat            = true
0.00.664.341 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.129 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.747.230 I init:      Metal KV buffer size =   384.00 MiB
0.00.747.236 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.747.266 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.751.780 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.751.782 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.751.782 I llama_context: graph nodes  = 967
0.00.751.782 I llama_context: graph splits = 2
0.00.751.788 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.751.910 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.751.910 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.989 I main: llama threadpool init, n_threads = 4
0.00.810.034 I 
0.00.810.057 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.810.057 I 
0.00.810.207 I sampler seed: 1234
0.00.810.211 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.810.229 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.810.230 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.810.230 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.601.250 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.601.251 I llama_perf_context_print:        load time =     800.64 ms
0.01.601.253 I llama_perf_context_print: prompt eval time =      51.86 ms /     7 tokens (    7.41 ms per token,   134.99 tokens per second)
0.01.601.254 I llama_perf_context_print:        eval time =     736.38 ms /    63 runs   (   11.69 ms per token,    85.55 tokens per second)
0.01.601.254 I llama_perf_context_print:       total time =     791.98 ms /    70 tokens
0.01.604.466 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.110s
sys	0m0.222s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.867 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.254 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.262 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.262 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.265 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.266 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.266 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.269 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.271 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.162 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.193 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.114 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.115 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.116 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.116 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.116 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.117 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.117 I llama_model_loader: - type  f32:  194 tensors
0.00.026.118 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.118 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.119 I print_info: file format = GGUF V3 (latest)
0.00.026.119 I print_info: file type   = Q5_0
0.00.026.121 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.262 I load: special tokens cache size = 25
0.00.040.347 I load: token to piece cache size = 0.2984 MB
0.00.040.351 I print_info: arch             = gptneox
0.00.040.351 I print_info: vocab_only       = 0
0.00.040.351 I print_info: n_ctx_train      = 2048
0.00.040.351 I print_info: n_embd           = 2048
0.00.040.352 I print_info: n_layer          = 24
0.00.040.356 I print_info: n_head           = 16
0.00.040.356 I print_info: n_head_kv        = 16
0.00.040.357 I print_info: n_rot            = 32
0.00.040.357 I print_info: n_swa            = 0
0.00.040.359 I print_info: n_embd_head_k    = 128
0.00.040.359 I print_info: n_embd_head_v    = 128
0.00.040.360 I print_info: n_gqa            = 1
0.00.040.360 I print_info: n_embd_k_gqa     = 2048
0.00.040.361 I print_info: n_embd_v_gqa     = 2048
0.00.040.362 I print_info: f_norm_eps       = 1.0e-05
0.00.040.362 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.362 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.362 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.362 I print_info: f_logit_scale    = 0.0e+00
0.00.040.363 I print_info: n_ff             = 8192
0.00.040.363 I print_info: n_expert         = 0
0.00.040.364 I print_info: n_expert_used    = 0
0.00.040.364 I print_info: causal attn      = 1
0.00.040.364 I print_info: pooling type     = 0
0.00.040.364 I print_info: rope type        = 2
0.00.040.364 I print_info: rope scaling     = linear
0.00.040.365 I print_info: freq_base_train  = 10000.0
0.00.040.365 I print_info: freq_scale_train = 1
0.00.040.365 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.366 I print_info: rope_finetuned   = unknown
0.00.040.366 I print_info: ssm_d_conv       = 0
0.00.040.366 I print_info: ssm_d_inner      = 0
0.00.040.368 I print_info: ssm_d_state      = 0
0.00.040.368 I print_info: ssm_dt_rank      = 0
0.00.040.368 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.369 I print_info: model type       = 1.4B
0.00.040.369 I print_info: model params     = 1.41 B
0.00.040.369 I print_info: general.name     = 1.4B
0.00.040.370 I print_info: vocab type       = BPE
0.00.040.370 I print_info: n_vocab          = 50304
0.00.040.370 I print_info: n_merges         = 50009
0.00.040.371 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.371 I print_info: LF token         = 187 'Ċ'
0.00.040.372 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.372 I print_info: max token length = 1024
0.00.040.372 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.796 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.806 I load_tensors: offloading output layer to GPU
0.00.641.807 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.834 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.641.836 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.643.481 I llama_context: n_seq_max     = 1
0.00.643.486 I llama_context: n_ctx         = 128
0.00.643.487 I llama_context: n_ctx_per_seq = 128
0.00.643.487 I llama_context: n_batch       = 128
0.00.643.487 I llama_context: n_ubatch      = 128
0.00.643.488 I llama_context: flash_attn    = 0
0.00.643.489 I llama_context: freq_base     = 10000.0
0.00.643.489 I llama_context: freq_scale    = 1
0.00.643.490 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.643.492 I ggml_metal_init: allocating
0.00.643.559 I ggml_metal_init: found device: Apple M4
0.00.643.572 I ggml_metal_init: picking default device: Apple M4
0.00.645.682 I ggml_metal_init: using embedded metal library
0.00.652.527 I ggml_metal_init: GPU name:   Apple M4
0.00.652.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.532 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.533 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.538 I ggml_metal_init: simdgroup reduction   = true
0.00.652.538 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.539 I ggml_metal_init: has residency sets    = true
0.00.652.539 I ggml_metal_init: has bfloat            = true
0.00.652.539 I ggml_metal_init: use bfloat            = true
0.00.652.540 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.542 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.943 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.485 I init:      Metal KV buffer size =    24.00 MiB
0.00.673.488 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.536 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.676.669 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.676.671 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.676.672 I llama_context: graph nodes  = 967
0.00.676.672 I llama_context: graph splits = 2
0.00.676.675 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.676 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.706.209 I 
0.00.706.283 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.706.304 I perplexity: tokenizing the input ..
0.00.713.349 I perplexity: tokenization took 7.042 ms
0.00.713.368 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.849.929 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.851.416 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.851.436 I llama_perf_context_print:        load time =     696.33 ms
0.00.851.437 I llama_perf_context_print: prompt eval time =     135.59 ms /   128 tokens (    1.06 ms per token,   944.04 tokens per second)
0.00.851.438 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.851.438 I llama_perf_context_print:       total time =     145.23 ms /   129 tokens
0.00.852.060 I ggml_metal_free: deallocating

real	0m0.868s
user	0m0.079s
sys	0m0.131s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.009.728 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.280 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.286 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.288 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.288 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.288 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.289 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.292 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.293 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.293 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.294 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.294 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.297 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.154 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.941 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.942 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.943 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.943 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.943 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.944 I llama_model_loader: - type  f32:  194 tensors
0.00.025.944 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.944 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.945 I print_info: file format = GGUF V3 (latest)
0.00.025.946 I print_info: file type   = Q5_1
0.00.025.946 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.190 I load: special tokens cache size = 25
0.00.040.324 I load: token to piece cache size = 0.2984 MB
0.00.040.327 I print_info: arch             = gptneox
0.00.040.328 I print_info: vocab_only       = 0
0.00.040.328 I print_info: n_ctx_train      = 2048
0.00.040.328 I print_info: n_embd           = 2048
0.00.040.328 I print_info: n_layer          = 24
0.00.040.331 I print_info: n_head           = 16
0.00.040.331 I print_info: n_head_kv        = 16
0.00.040.332 I print_info: n_rot            = 32
0.00.040.332 I print_info: n_swa            = 0
0.00.040.333 I print_info: n_embd_head_k    = 128
0.00.040.335 I print_info: n_embd_head_v    = 128
0.00.040.335 I print_info: n_gqa            = 1
0.00.040.336 I print_info: n_embd_k_gqa     = 2048
0.00.040.337 I print_info: n_embd_v_gqa     = 2048
0.00.040.342 I print_info: f_norm_eps       = 1.0e-05
0.00.040.342 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.342 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.342 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.343 I print_info: f_logit_scale    = 0.0e+00
0.00.040.343 I print_info: n_ff             = 8192
0.00.040.344 I print_info: n_expert         = 0
0.00.040.344 I print_info: n_expert_used    = 0
0.00.040.345 I print_info: causal attn      = 1
0.00.040.346 I print_info: pooling type     = 0
0.00.040.346 I print_info: rope type        = 2
0.00.040.346 I print_info: rope scaling     = linear
0.00.040.346 I print_info: freq_base_train  = 10000.0
0.00.040.347 I print_info: freq_scale_train = 1
0.00.040.347 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.347 I print_info: rope_finetuned   = unknown
0.00.040.349 I print_info: ssm_d_conv       = 0
0.00.040.349 I print_info: ssm_d_inner      = 0
0.00.040.349 I print_info: ssm_d_state      = 0
0.00.040.349 I print_info: ssm_dt_rank      = 0
0.00.040.349 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.349 I print_info: model type       = 1.4B
0.00.040.350 I print_info: model params     = 1.41 B
0.00.040.350 I print_info: general.name     = 1.4B
0.00.040.350 I print_info: vocab type       = BPE
0.00.040.351 I print_info: n_vocab          = 50304
0.00.040.351 I print_info: n_merges         = 50009
0.00.040.351 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.351 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.351 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.351 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.352 I print_info: LF token         = 187 'Ċ'
0.00.040.352 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.352 I print_info: max token length = 1024
0.00.040.352 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.677.172 I load_tensors: offloading 24 repeating layers to GPU
0.00.677.188 I load_tensors: offloading output layer to GPU
0.00.677.188 I load_tensors: offloaded 25/25 layers to GPU
0.00.677.221 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.677.222 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.678.564 I llama_context: n_seq_max     = 1
0.00.678.567 I llama_context: n_ctx         = 2048
0.00.678.567 I llama_context: n_ctx_per_seq = 2048
0.00.678.568 I llama_context: n_batch       = 2048
0.00.678.568 I llama_context: n_ubatch      = 512
0.00.678.569 I llama_context: flash_attn    = 0
0.00.678.571 I llama_context: freq_base     = 10000.0
0.00.678.571 I llama_context: freq_scale    = 1
0.00.678.573 I ggml_metal_init: allocating
0.00.678.637 I ggml_metal_init: found device: Apple M4
0.00.678.671 I ggml_metal_init: picking default device: Apple M4
0.00.679.999 I ggml_metal_init: using embedded metal library
0.00.686.378 I ggml_metal_init: GPU name:   Apple M4
0.00.686.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.382 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.384 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.384 I ggml_metal_init: simdgroup reduction   = true
0.00.686.384 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.385 I ggml_metal_init: has residency sets    = true
0.00.686.385 I ggml_metal_init: has bfloat            = true
0.00.686.385 I ggml_metal_init: use bfloat            = true
0.00.686.386 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.387 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.703.804 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.760.690 I init:      Metal KV buffer size =   384.00 MiB
0.00.760.696 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.760.739 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.765.263 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.765.265 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.765.265 I llama_context: graph nodes  = 967
0.00.765.265 I llama_context: graph splits = 2
0.00.765.271 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.765.413 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.765.414 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.823.396 I main: llama threadpool init, n_threads = 4
0.00.823.439 I 
0.00.823.460 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.823.460 I 
0.00.823.615 I sampler seed: 1234
0.00.823.620 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.823.631 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.823.631 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.823.631 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.657.311 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52244.30 tokens per second)
0.01.657.311 I llama_perf_context_print:        load time =     812.97 ms
0.01.657.312 I llama_perf_context_print: prompt eval time =      41.97 ms /     7 tokens (    6.00 ms per token,   166.80 tokens per second)
0.01.657.313 I llama_perf_context_print:        eval time =     788.80 ms /    63 runs   (   12.52 ms per token,    79.87 tokens per second)
0.01.657.313 I llama_perf_context_print:       total time =     834.61 ms /    70 tokens
0.01.661.255 I ggml_metal_free: deallocating

real	0m1.679s
user	0m0.109s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.062 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.911 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.917 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.918 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.919 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.924 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.924 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.925 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.926 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.926 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.927 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.927 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.928 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.930 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.830 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.841 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.778 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.780 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.780 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.781 I llama_model_loader: - type  f32:  194 tensors
0.00.024.782 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.782 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.783 I print_info: file format = GGUF V3 (latest)
0.00.024.785 I print_info: file type   = Q5_1
0.00.024.786 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.834 I load: special tokens cache size = 25
0.00.038.958 I load: token to piece cache size = 0.2984 MB
0.00.038.963 I print_info: arch             = gptneox
0.00.038.963 I print_info: vocab_only       = 0
0.00.038.963 I print_info: n_ctx_train      = 2048
0.00.038.963 I print_info: n_embd           = 2048
0.00.038.964 I print_info: n_layer          = 24
0.00.038.968 I print_info: n_head           = 16
0.00.038.968 I print_info: n_head_kv        = 16
0.00.038.969 I print_info: n_rot            = 32
0.00.038.969 I print_info: n_swa            = 0
0.00.038.969 I print_info: n_embd_head_k    = 128
0.00.038.969 I print_info: n_embd_head_v    = 128
0.00.038.970 I print_info: n_gqa            = 1
0.00.038.971 I print_info: n_embd_k_gqa     = 2048
0.00.038.973 I print_info: n_embd_v_gqa     = 2048
0.00.038.973 I print_info: f_norm_eps       = 1.0e-05
0.00.038.974 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.974 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.974 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.976 I print_info: f_logit_scale    = 0.0e+00
0.00.038.977 I print_info: n_ff             = 8192
0.00.038.977 I print_info: n_expert         = 0
0.00.038.977 I print_info: n_expert_used    = 0
0.00.038.978 I print_info: causal attn      = 1
0.00.038.978 I print_info: pooling type     = 0
0.00.038.978 I print_info: rope type        = 2
0.00.038.978 I print_info: rope scaling     = linear
0.00.038.979 I print_info: freq_base_train  = 10000.0
0.00.038.979 I print_info: freq_scale_train = 1
0.00.038.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.980 I print_info: rope_finetuned   = unknown
0.00.038.980 I print_info: ssm_d_conv       = 0
0.00.038.980 I print_info: ssm_d_inner      = 0
0.00.038.980 I print_info: ssm_d_state      = 0
0.00.038.980 I print_info: ssm_dt_rank      = 0
0.00.038.980 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.981 I print_info: model type       = 1.4B
0.00.038.981 I print_info: model params     = 1.41 B
0.00.038.981 I print_info: general.name     = 1.4B
0.00.038.982 I print_info: vocab type       = BPE
0.00.038.982 I print_info: n_vocab          = 50304
0.00.038.982 I print_info: n_merges         = 50009
0.00.038.982 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.983 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.983 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.983 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.983 I print_info: LF token         = 187 'Ċ'
0.00.038.983 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.983 I print_info: max token length = 1024
0.00.038.984 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.668.641 I load_tensors: offloading 24 repeating layers to GPU
0.00.668.661 I load_tensors: offloading output layer to GPU
0.00.668.661 I load_tensors: offloaded 25/25 layers to GPU
0.00.668.697 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.668.699 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.670.326 I llama_context: n_seq_max     = 1
0.00.670.329 I llama_context: n_ctx         = 128
0.00.670.329 I llama_context: n_ctx_per_seq = 128
0.00.670.329 I llama_context: n_batch       = 128
0.00.670.330 I llama_context: n_ubatch      = 128
0.00.670.330 I llama_context: flash_attn    = 0
0.00.670.331 I llama_context: freq_base     = 10000.0
0.00.670.331 I llama_context: freq_scale    = 1
0.00.670.332 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.670.333 I ggml_metal_init: allocating
0.00.670.352 I ggml_metal_init: found device: Apple M4
0.00.670.360 I ggml_metal_init: picking default device: Apple M4
0.00.671.736 I ggml_metal_init: using embedded metal library
0.00.678.006 I ggml_metal_init: GPU name:   Apple M4
0.00.678.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.678.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.678.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.678.012 I ggml_metal_init: simdgroup reduction   = true
0.00.678.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.678.013 I ggml_metal_init: has residency sets    = true
0.00.678.013 I ggml_metal_init: has bfloat            = true
0.00.678.013 I ggml_metal_init: use bfloat            = true
0.00.678.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.678.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.694.559 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.697.991 I init:      Metal KV buffer size =    24.00 MiB
0.00.697.994 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.698.037 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.701.356 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.701.357 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.701.358 I llama_context: graph nodes  = 967
0.00.701.358 I llama_context: graph splits = 2
0.00.701.362 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.701.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.877 I 
0.00.732.964 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.985 I perplexity: tokenizing the input ..
0.00.740.308 I perplexity: tokenization took 7.319 ms
0.00.740.329 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.882.117 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.883.460 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.883.473 I llama_perf_context_print:        load time =     723.81 ms
0.00.883.475 I llama_perf_context_print: prompt eval time =     140.91 ms /   128 tokens (    1.10 ms per token,   908.41 tokens per second)
0.00.883.475 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.883.476 I llama_perf_context_print:       total time =     150.60 ms /   129 tokens
0.00.884.014 I ggml_metal_free: deallocating

real	0m0.899s
user	0m0.079s
sys	0m0.137s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.153 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.700 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.705 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.707 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.707 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.708 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.708 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.709 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.710 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.710 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.711 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.712 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.713 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.714 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.714 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.588 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.383 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.385 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.385 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.385 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.386 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.386 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.386 I llama_model_loader: - type  f32:  194 tensors
0.00.024.387 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.387 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.387 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.388 I print_info: file format = GGUF V3 (latest)
0.00.024.388 I print_info: file type   = Q2_K - Medium
0.00.024.389 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.287 I load: special tokens cache size = 25
0.00.038.312 I load: token to piece cache size = 0.2984 MB
0.00.038.315 I print_info: arch             = gptneox
0.00.038.316 I print_info: vocab_only       = 0
0.00.038.316 I print_info: n_ctx_train      = 2048
0.00.038.316 I print_info: n_embd           = 2048
0.00.038.316 I print_info: n_layer          = 24
0.00.038.319 I print_info: n_head           = 16
0.00.038.320 I print_info: n_head_kv        = 16
0.00.038.320 I print_info: n_rot            = 32
0.00.038.320 I print_info: n_swa            = 0
0.00.038.320 I print_info: n_embd_head_k    = 128
0.00.038.320 I print_info: n_embd_head_v    = 128
0.00.038.321 I print_info: n_gqa            = 1
0.00.038.322 I print_info: n_embd_k_gqa     = 2048
0.00.038.323 I print_info: n_embd_v_gqa     = 2048
0.00.038.323 I print_info: f_norm_eps       = 1.0e-05
0.00.038.324 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.324 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.326 I print_info: f_logit_scale    = 0.0e+00
0.00.038.327 I print_info: n_ff             = 8192
0.00.038.327 I print_info: n_expert         = 0
0.00.038.327 I print_info: n_expert_used    = 0
0.00.038.328 I print_info: causal attn      = 1
0.00.038.328 I print_info: pooling type     = 0
0.00.038.328 I print_info: rope type        = 2
0.00.038.328 I print_info: rope scaling     = linear
0.00.038.329 I print_info: freq_base_train  = 10000.0
0.00.038.329 I print_info: freq_scale_train = 1
0.00.038.329 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.329 I print_info: rope_finetuned   = unknown
0.00.038.330 I print_info: ssm_d_conv       = 0
0.00.038.330 I print_info: ssm_d_inner      = 0
0.00.038.330 I print_info: ssm_d_state      = 0
0.00.038.330 I print_info: ssm_dt_rank      = 0
0.00.038.330 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.332 I print_info: model type       = 1.4B
0.00.038.332 I print_info: model params     = 1.41 B
0.00.038.332 I print_info: general.name     = 1.4B
0.00.038.333 I print_info: vocab type       = BPE
0.00.038.333 I print_info: n_vocab          = 50304
0.00.038.333 I print_info: n_merges         = 50009
0.00.038.333 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.334 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.334 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.334 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.334 I print_info: LF token         = 187 'Ċ'
0.00.038.338 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.339 I print_info: max token length = 1024
0.00.038.339 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.357.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.357.178 I load_tensors: offloading output layer to GPU
0.00.357.178 I load_tensors: offloaded 25/25 layers to GPU
0.00.357.211 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.357.213 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.358.878 I llama_context: n_seq_max     = 1
0.00.358.884 I llama_context: n_ctx         = 2048
0.00.358.885 I llama_context: n_ctx_per_seq = 2048
0.00.358.886 I llama_context: n_batch       = 2048
0.00.358.886 I llama_context: n_ubatch      = 512
0.00.358.887 I llama_context: flash_attn    = 0
0.00.358.888 I llama_context: freq_base     = 10000.0
0.00.358.889 I llama_context: freq_scale    = 1
0.00.358.892 I ggml_metal_init: allocating
0.00.358.997 I ggml_metal_init: found device: Apple M4
0.00.359.010 I ggml_metal_init: picking default device: Apple M4
0.00.360.908 I ggml_metal_init: using embedded metal library
0.00.366.302 I ggml_metal_init: GPU name:   Apple M4
0.00.366.315 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.366.315 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.366.317 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.366.317 I ggml_metal_init: simdgroup reduction   = true
0.00.366.317 I ggml_metal_init: simdgroup matrix mul. = true
0.00.366.318 I ggml_metal_init: has residency sets    = true
0.00.366.318 I ggml_metal_init: has bfloat            = true
0.00.366.318 I ggml_metal_init: use bfloat            = true
0.00.366.320 I ggml_metal_init: hasUnifiedMemory      = true
0.00.366.325 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.387.455 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.447.781 I init:      Metal KV buffer size =   384.00 MiB
0.00.447.790 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.447.828 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.452.038 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.452.040 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.452.041 I llama_context: graph nodes  = 967
0.00.452.041 I llama_context: graph splits = 2
0.00.452.047 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.452.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.452.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.510.226 I main: llama threadpool init, n_threads = 4
0.00.510.271 I 
0.00.510.292 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.510.292 I 
0.00.510.470 I sampler seed: 1234
0.00.510.475 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.510.496 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.510.496 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.510.496 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.184.769 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.184.770 I llama_perf_context_print:        load time =     500.34 ms
0.01.184.771 I llama_perf_context_print: prompt eval time =      35.44 ms /     7 tokens (    5.06 ms per token,   197.52 tokens per second)
0.01.184.772 I llama_perf_context_print:        eval time =     636.00 ms /    63 runs   (   10.10 ms per token,    99.06 tokens per second)
0.01.184.773 I llama_perf_context_print:       total time =     675.27 ms /    70 tokens
0.01.188.512 I ggml_metal_free: deallocating

real	0m1.206s
user	0m0.111s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.965 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.793 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.800 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.801 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.801 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.802 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.803 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.804 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.804 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.804 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.805 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.805 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.807 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.807 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.807 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.771 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.615 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.617 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.617 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.617 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.618 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.618 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.619 I llama_model_loader: - type  f32:  194 tensors
0.00.026.619 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.619 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.620 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.620 I print_info: file format = GGUF V3 (latest)
0.00.026.621 I print_info: file type   = Q2_K - Medium
0.00.026.623 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.957 I load: special tokens cache size = 25
0.00.041.071 I load: token to piece cache size = 0.2984 MB
0.00.041.075 I print_info: arch             = gptneox
0.00.041.075 I print_info: vocab_only       = 0
0.00.041.075 I print_info: n_ctx_train      = 2048
0.00.041.075 I print_info: n_embd           = 2048
0.00.041.075 I print_info: n_layer          = 24
0.00.041.079 I print_info: n_head           = 16
0.00.041.080 I print_info: n_head_kv        = 16
0.00.041.080 I print_info: n_rot            = 32
0.00.041.081 I print_info: n_swa            = 0
0.00.041.081 I print_info: n_embd_head_k    = 128
0.00.041.081 I print_info: n_embd_head_v    = 128
0.00.041.082 I print_info: n_gqa            = 1
0.00.041.082 I print_info: n_embd_k_gqa     = 2048
0.00.041.083 I print_info: n_embd_v_gqa     = 2048
0.00.041.084 I print_info: f_norm_eps       = 1.0e-05
0.00.041.084 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.084 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.085 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.085 I print_info: f_logit_scale    = 0.0e+00
0.00.041.085 I print_info: n_ff             = 8192
0.00.041.086 I print_info: n_expert         = 0
0.00.041.086 I print_info: n_expert_used    = 0
0.00.041.086 I print_info: causal attn      = 1
0.00.041.086 I print_info: pooling type     = 0
0.00.041.086 I print_info: rope type        = 2
0.00.041.086 I print_info: rope scaling     = linear
0.00.041.087 I print_info: freq_base_train  = 10000.0
0.00.041.087 I print_info: freq_scale_train = 1
0.00.041.087 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.087 I print_info: rope_finetuned   = unknown
0.00.041.088 I print_info: ssm_d_conv       = 0
0.00.041.088 I print_info: ssm_d_inner      = 0
0.00.041.088 I print_info: ssm_d_state      = 0
0.00.041.088 I print_info: ssm_dt_rank      = 0
0.00.041.088 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.088 I print_info: model type       = 1.4B
0.00.041.089 I print_info: model params     = 1.41 B
0.00.041.089 I print_info: general.name     = 1.4B
0.00.041.090 I print_info: vocab type       = BPE
0.00.041.090 I print_info: n_vocab          = 50304
0.00.041.090 I print_info: n_merges         = 50009
0.00.041.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.091 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.091 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.091 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.091 I print_info: LF token         = 187 'Ċ'
0.00.041.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.092 I print_info: max token length = 1024
0.00.041.092 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.360.460 I load_tensors: offloading 24 repeating layers to GPU
0.00.360.475 I load_tensors: offloading output layer to GPU
0.00.360.476 I load_tensors: offloaded 25/25 layers to GPU
0.00.360.505 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.360.507 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.362.198 I llama_context: n_seq_max     = 1
0.00.362.206 I llama_context: n_ctx         = 128
0.00.362.207 I llama_context: n_ctx_per_seq = 128
0.00.362.207 I llama_context: n_batch       = 128
0.00.362.208 I llama_context: n_ubatch      = 128
0.00.362.208 I llama_context: flash_attn    = 0
0.00.362.210 I llama_context: freq_base     = 10000.0
0.00.362.211 I llama_context: freq_scale    = 1
0.00.362.211 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.362.213 I ggml_metal_init: allocating
0.00.362.289 I ggml_metal_init: found device: Apple M4
0.00.362.302 I ggml_metal_init: picking default device: Apple M4
0.00.364.013 I ggml_metal_init: using embedded metal library
0.00.369.370 I ggml_metal_init: GPU name:   Apple M4
0.00.369.386 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.369.387 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.369.387 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.369.388 I ggml_metal_init: simdgroup reduction   = true
0.00.369.388 I ggml_metal_init: simdgroup matrix mul. = true
0.00.369.388 I ggml_metal_init: has residency sets    = true
0.00.369.389 I ggml_metal_init: has bfloat            = true
0.00.369.389 I ggml_metal_init: use bfloat            = true
0.00.369.391 I ggml_metal_init: hasUnifiedMemory      = true
0.00.369.396 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.390.147 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.393.742 I init:      Metal KV buffer size =    24.00 MiB
0.00.393.749 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.393.798 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.396.940 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.396.942 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.396.943 I llama_context: graph nodes  = 967
0.00.396.943 I llama_context: graph splits = 2
0.00.396.947 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.396.947 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.430.221 I 
0.00.430.301 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.430.321 I perplexity: tokenizing the input ..
0.00.435.410 I perplexity: tokenization took 5.088 ms
0.00.435.421 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.568.804 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.570.150 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.570.165 I llama_perf_context_print:        load time =     419.25 ms
0.00.570.166 I llama_perf_context_print: prompt eval time =     133.15 ms /   128 tokens (    1.04 ms per token,   961.29 tokens per second)
0.00.570.167 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.570.167 I llama_perf_context_print:       total time =     139.95 ms /   129 tokens
0.00.570.739 I ggml_metal_free: deallocating

real	0m0.586s
user	0m0.079s
sys	0m0.094s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.873 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.555 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.561 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.563 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.564 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.564 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.564 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.565 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.565 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.566 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.569 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.573 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.574 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.574 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.409 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.448 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.251 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.252 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.252 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.253 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.253 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.253 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.254 I llama_model_loader: - type  f32:  194 tensors
0.00.025.254 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.254 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.255 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.255 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.255 I print_info: file format = GGUF V3 (latest)
0.00.025.256 I print_info: file type   = Q3_K - Medium
0.00.025.257 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.147 I load: special tokens cache size = 25
0.00.039.159 I load: token to piece cache size = 0.2984 MB
0.00.039.162 I print_info: arch             = gptneox
0.00.039.162 I print_info: vocab_only       = 0
0.00.039.162 I print_info: n_ctx_train      = 2048
0.00.039.163 I print_info: n_embd           = 2048
0.00.039.163 I print_info: n_layer          = 24
0.00.039.165 I print_info: n_head           = 16
0.00.039.166 I print_info: n_head_kv        = 16
0.00.039.166 I print_info: n_rot            = 32
0.00.039.167 I print_info: n_swa            = 0
0.00.039.167 I print_info: n_embd_head_k    = 128
0.00.039.167 I print_info: n_embd_head_v    = 128
0.00.039.168 I print_info: n_gqa            = 1
0.00.039.169 I print_info: n_embd_k_gqa     = 2048
0.00.039.169 I print_info: n_embd_v_gqa     = 2048
0.00.039.170 I print_info: f_norm_eps       = 1.0e-05
0.00.039.170 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.170 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.171 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.171 I print_info: f_logit_scale    = 0.0e+00
0.00.039.171 I print_info: n_ff             = 8192
0.00.039.172 I print_info: n_expert         = 0
0.00.039.172 I print_info: n_expert_used    = 0
0.00.039.173 I print_info: causal attn      = 1
0.00.039.175 I print_info: pooling type     = 0
0.00.039.175 I print_info: rope type        = 2
0.00.039.175 I print_info: rope scaling     = linear
0.00.039.176 I print_info: freq_base_train  = 10000.0
0.00.039.176 I print_info: freq_scale_train = 1
0.00.039.176 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.176 I print_info: rope_finetuned   = unknown
0.00.039.177 I print_info: ssm_d_conv       = 0
0.00.039.177 I print_info: ssm_d_inner      = 0
0.00.039.178 I print_info: ssm_d_state      = 0
0.00.039.178 I print_info: ssm_dt_rank      = 0
0.00.039.178 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.178 I print_info: model type       = 1.4B
0.00.039.179 I print_info: model params     = 1.41 B
0.00.039.179 I print_info: general.name     = 1.4B
0.00.039.179 I print_info: vocab type       = BPE
0.00.039.180 I print_info: n_vocab          = 50304
0.00.039.180 I print_info: n_merges         = 50009
0.00.039.181 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.181 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.181 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: LF token         = 187 'Ċ'
0.00.039.182 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.182 I print_info: max token length = 1024
0.00.039.183 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.437.526 I load_tensors: offloading 24 repeating layers to GPU
0.00.437.542 I load_tensors: offloading output layer to GPU
0.00.437.542 I load_tensors: offloaded 25/25 layers to GPU
0.00.437.572 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.437.573 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.439.242 I llama_context: n_seq_max     = 1
0.00.439.250 I llama_context: n_ctx         = 2048
0.00.439.250 I llama_context: n_ctx_per_seq = 2048
0.00.439.251 I llama_context: n_batch       = 2048
0.00.439.251 I llama_context: n_ubatch      = 512
0.00.439.252 I llama_context: flash_attn    = 0
0.00.439.254 I llama_context: freq_base     = 10000.0
0.00.439.254 I llama_context: freq_scale    = 1
0.00.439.258 I ggml_metal_init: allocating
0.00.439.333 I ggml_metal_init: found device: Apple M4
0.00.439.346 I ggml_metal_init: picking default device: Apple M4
0.00.441.203 I ggml_metal_init: using embedded metal library
0.00.446.776 I ggml_metal_init: GPU name:   Apple M4
0.00.446.782 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.446.783 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.446.784 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.446.784 I ggml_metal_init: simdgroup reduction   = true
0.00.446.785 I ggml_metal_init: simdgroup matrix mul. = true
0.00.446.785 I ggml_metal_init: has residency sets    = true
0.00.446.785 I ggml_metal_init: has bfloat            = true
0.00.446.786 I ggml_metal_init: use bfloat            = true
0.00.446.786 I ggml_metal_init: hasUnifiedMemory      = true
0.00.446.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.466.651 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.520.472 I init:      Metal KV buffer size =   384.00 MiB
0.00.520.479 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.520.516 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.525.477 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.525.480 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.525.480 I llama_context: graph nodes  = 967
0.00.525.480 I llama_context: graph splits = 2
0.00.525.485 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.525.609 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.525.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.250 I main: llama threadpool init, n_threads = 4
0.00.583.292 I 
0.00.583.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.315 I 
0.00.583.483 I sampler seed: 1234
0.00.583.488 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.583.539 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.583.542 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.583.542 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.337.727 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.01.337.729 I llama_perf_context_print:        load time =     573.65 ms
0.01.337.729 I llama_perf_context_print: prompt eval time =      50.16 ms /     7 tokens (    7.17 ms per token,   139.55 tokens per second)
0.01.337.730 I llama_perf_context_print:        eval time =     701.09 ms /    63 runs   (   11.13 ms per token,    89.86 tokens per second)
0.01.337.731 I llama_perf_context_print:       total time =     755.20 ms /    70 tokens
0.01.341.654 I ggml_metal_free: deallocating

real	0m1.357s
user	0m0.110s
sys	0m0.179s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.919 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.263 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.267 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.270 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.273 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.126 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.111 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.955 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.957 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.957 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.958 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.958 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.959 I llama_model_loader: - type  f32:  194 tensors
0.00.024.959 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.960 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.960 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.960 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.961 I print_info: file format = GGUF V3 (latest)
0.00.024.961 I print_info: file type   = Q3_K - Medium
0.00.024.962 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.170 I load: special tokens cache size = 25
0.00.039.450 I load: token to piece cache size = 0.2984 MB
0.00.039.452 I print_info: arch             = gptneox
0.00.039.453 I print_info: vocab_only       = 0
0.00.039.453 I print_info: n_ctx_train      = 2048
0.00.039.453 I print_info: n_embd           = 2048
0.00.039.453 I print_info: n_layer          = 24
0.00.039.458 I print_info: n_head           = 16
0.00.039.459 I print_info: n_head_kv        = 16
0.00.039.459 I print_info: n_rot            = 32
0.00.039.459 I print_info: n_swa            = 0
0.00.039.459 I print_info: n_embd_head_k    = 128
0.00.039.459 I print_info: n_embd_head_v    = 128
0.00.039.460 I print_info: n_gqa            = 1
0.00.039.461 I print_info: n_embd_k_gqa     = 2048
0.00.039.462 I print_info: n_embd_v_gqa     = 2048
0.00.039.462 I print_info: f_norm_eps       = 1.0e-05
0.00.039.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.463 I print_info: f_logit_scale    = 0.0e+00
0.00.039.467 I print_info: n_ff             = 8192
0.00.039.467 I print_info: n_expert         = 0
0.00.039.467 I print_info: n_expert_used    = 0
0.00.039.468 I print_info: causal attn      = 1
0.00.039.468 I print_info: pooling type     = 0
0.00.039.468 I print_info: rope type        = 2
0.00.039.468 I print_info: rope scaling     = linear
0.00.039.469 I print_info: freq_base_train  = 10000.0
0.00.039.469 I print_info: freq_scale_train = 1
0.00.039.469 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.469 I print_info: rope_finetuned   = unknown
0.00.039.469 I print_info: ssm_d_conv       = 0
0.00.039.469 I print_info: ssm_d_inner      = 0
0.00.039.469 I print_info: ssm_d_state      = 0
0.00.039.470 I print_info: ssm_dt_rank      = 0
0.00.039.470 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.470 I print_info: model type       = 1.4B
0.00.039.470 I print_info: model params     = 1.41 B
0.00.039.474 I print_info: general.name     = 1.4B
0.00.039.475 I print_info: vocab type       = BPE
0.00.039.475 I print_info: n_vocab          = 50304
0.00.039.475 I print_info: n_merges         = 50009
0.00.039.475 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.476 I print_info: LF token         = 187 'Ċ'
0.00.039.476 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.477 I print_info: max token length = 1024
0.00.039.477 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.443.520 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.531 I load_tensors: offloading output layer to GPU
0.00.443.532 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.566 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.568 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.445.186 I llama_context: n_seq_max     = 1
0.00.445.192 I llama_context: n_ctx         = 128
0.00.445.192 I llama_context: n_ctx_per_seq = 128
0.00.445.193 I llama_context: n_batch       = 128
0.00.445.193 I llama_context: n_ubatch      = 128
0.00.445.194 I llama_context: flash_attn    = 0
0.00.445.195 I llama_context: freq_base     = 10000.0
0.00.445.196 I llama_context: freq_scale    = 1
0.00.445.196 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.445.201 I ggml_metal_init: allocating
0.00.445.249 I ggml_metal_init: found device: Apple M4
0.00.445.262 I ggml_metal_init: picking default device: Apple M4
0.00.447.401 I ggml_metal_init: using embedded metal library
0.00.453.608 I ggml_metal_init: GPU name:   Apple M4
0.00.453.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.620 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.622 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.622 I ggml_metal_init: simdgroup reduction   = true
0.00.453.623 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.623 I ggml_metal_init: has residency sets    = true
0.00.453.624 I ggml_metal_init: has bfloat            = true
0.00.453.624 I ggml_metal_init: use bfloat            = true
0.00.453.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.720 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.394 I init:      Metal KV buffer size =    24.00 MiB
0.00.478.400 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.478.454 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.481.734 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.481.736 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.481.736 I llama_context: graph nodes  = 967
0.00.481.736 I llama_context: graph splits = 2
0.00.481.747 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.481.747 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.274 I 
0.00.513.368 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.513.394 I perplexity: tokenizing the input ..
0.00.520.421 I perplexity: tokenization took 7.025 ms
0.00.520.442 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.664.431 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.665.851 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.665.864 I llama_perf_context_print:        load time =     504.34 ms
0.00.665.865 I llama_perf_context_print: prompt eval time =     143.05 ms /   128 tokens (    1.12 ms per token,   894.80 tokens per second)
0.00.665.865 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.665.866 I llama_perf_context_print:       total time =     152.60 ms /   129 tokens
0.00.666.409 I ggml_metal_free: deallocating

real	0m0.680s
user	0m0.081s
sys	0m0.111s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.009.647 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.549 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.554 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.565 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.566 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.566 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.567 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.505 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.296 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.297 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.298 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.298 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.299 I llama_model_loader: - type  f32:  194 tensors
0.00.026.299 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.299 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.300 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.300 I print_info: file format = GGUF V3 (latest)
0.00.026.301 I print_info: file type   = Q4_K - Medium
0.00.026.302 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.203 I load: special tokens cache size = 25
0.00.040.236 I load: token to piece cache size = 0.2984 MB
0.00.040.239 I print_info: arch             = gptneox
0.00.040.239 I print_info: vocab_only       = 0
0.00.040.239 I print_info: n_ctx_train      = 2048
0.00.040.240 I print_info: n_embd           = 2048
0.00.040.240 I print_info: n_layer          = 24
0.00.040.243 I print_info: n_head           = 16
0.00.040.243 I print_info: n_head_kv        = 16
0.00.040.245 I print_info: n_rot            = 32
0.00.040.246 I print_info: n_swa            = 0
0.00.040.246 I print_info: n_embd_head_k    = 128
0.00.040.246 I print_info: n_embd_head_v    = 128
0.00.040.247 I print_info: n_gqa            = 1
0.00.040.247 I print_info: n_embd_k_gqa     = 2048
0.00.040.248 I print_info: n_embd_v_gqa     = 2048
0.00.040.249 I print_info: f_norm_eps       = 1.0e-05
0.00.040.249 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.249 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.249 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.250 I print_info: f_logit_scale    = 0.0e+00
0.00.040.250 I print_info: n_ff             = 8192
0.00.040.250 I print_info: n_expert         = 0
0.00.040.251 I print_info: n_expert_used    = 0
0.00.040.251 I print_info: causal attn      = 1
0.00.040.252 I print_info: pooling type     = 0
0.00.040.252 I print_info: rope type        = 2
0.00.040.252 I print_info: rope scaling     = linear
0.00.040.254 I print_info: freq_base_train  = 10000.0
0.00.040.254 I print_info: freq_scale_train = 1
0.00.040.254 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.254 I print_info: rope_finetuned   = unknown
0.00.040.254 I print_info: ssm_d_conv       = 0
0.00.040.254 I print_info: ssm_d_inner      = 0
0.00.040.255 I print_info: ssm_d_state      = 0
0.00.040.255 I print_info: ssm_dt_rank      = 0
0.00.040.255 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.255 I print_info: model type       = 1.4B
0.00.040.256 I print_info: model params     = 1.41 B
0.00.040.256 I print_info: general.name     = 1.4B
0.00.040.256 I print_info: vocab type       = BPE
0.00.040.256 I print_info: n_vocab          = 50304
0.00.040.257 I print_info: n_merges         = 50009
0.00.040.257 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.257 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.258 I print_info: LF token         = 187 'Ċ'
0.00.040.258 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.259 I print_info: max token length = 1024
0.00.040.259 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.526.295 I load_tensors: offloading 24 repeating layers to GPU
0.00.526.312 I load_tensors: offloading output layer to GPU
0.00.526.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.526.346 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.526.347 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.527.807 I llama_context: n_seq_max     = 1
0.00.527.811 I llama_context: n_ctx         = 2048
0.00.527.811 I llama_context: n_ctx_per_seq = 2048
0.00.527.812 I llama_context: n_batch       = 2048
0.00.527.812 I llama_context: n_ubatch      = 512
0.00.527.812 I llama_context: flash_attn    = 0
0.00.527.815 I llama_context: freq_base     = 10000.0
0.00.527.815 I llama_context: freq_scale    = 1
0.00.527.818 I ggml_metal_init: allocating
0.00.527.896 I ggml_metal_init: found device: Apple M4
0.00.527.909 I ggml_metal_init: picking default device: Apple M4
0.00.529.754 I ggml_metal_init: using embedded metal library
0.00.535.976 I ggml_metal_init: GPU name:   Apple M4
0.00.535.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.982 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.983 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.984 I ggml_metal_init: simdgroup reduction   = true
0.00.535.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.985 I ggml_metal_init: has residency sets    = true
0.00.535.985 I ggml_metal_init: has bfloat            = true
0.00.535.985 I ggml_metal_init: use bfloat            = true
0.00.535.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.554.388 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.598 I init:      Metal KV buffer size =   384.00 MiB
0.00.611.604 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.611.639 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.615.816 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.615.818 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.615.818 I llama_context: graph nodes  = 967
0.00.615.819 I llama_context: graph splits = 2
0.00.615.826 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.615.954 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.615.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.211 I main: llama threadpool init, n_threads = 4
0.00.674.258 I 
0.00.674.280 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.280 I 
0.00.674.433 I sampler seed: 1234
0.00.674.438 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.674.459 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.674.460 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.674.460 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.437.392 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53869.50 tokens per second)
0.01.437.392 I llama_perf_context_print:        load time =     663.85 ms
0.01.437.394 I llama_perf_context_print: prompt eval time =      53.04 ms /     7 tokens (    7.58 ms per token,   131.99 tokens per second)
0.01.437.394 I llama_perf_context_print:        eval time =     707.04 ms /    63 runs   (   11.22 ms per token,    89.10 tokens per second)
0.01.437.395 I llama_perf_context_print:       total time =     763.89 ms /    70 tokens
0.01.441.325 I ggml_metal_free: deallocating

real	0m1.460s
user	0m0.110s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.076 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.172 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.180 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.181 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.181 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.181 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.182 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.182 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.183 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.183 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.183 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.184 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.184 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.187 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.187 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.187 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.049 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.968 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.969 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.970 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.970 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.970 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.971 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.971 I llama_model_loader: - type  f32:  194 tensors
0.00.027.972 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.972 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.972 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.973 I print_info: file format = GGUF V3 (latest)
0.00.027.973 I print_info: file type   = Q4_K - Medium
0.00.027.974 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.173 I load: special tokens cache size = 25
0.00.041.977 I load: token to piece cache size = 0.2984 MB
0.00.041.980 I print_info: arch             = gptneox
0.00.041.981 I print_info: vocab_only       = 0
0.00.041.981 I print_info: n_ctx_train      = 2048
0.00.041.981 I print_info: n_embd           = 2048
0.00.041.981 I print_info: n_layer          = 24
0.00.041.985 I print_info: n_head           = 16
0.00.041.985 I print_info: n_head_kv        = 16
0.00.041.986 I print_info: n_rot            = 32
0.00.041.986 I print_info: n_swa            = 0
0.00.041.986 I print_info: n_embd_head_k    = 128
0.00.041.986 I print_info: n_embd_head_v    = 128
0.00.041.987 I print_info: n_gqa            = 1
0.00.041.988 I print_info: n_embd_k_gqa     = 2048
0.00.041.989 I print_info: n_embd_v_gqa     = 2048
0.00.041.989 I print_info: f_norm_eps       = 1.0e-05
0.00.041.990 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.990 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.990 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.990 I print_info: f_logit_scale    = 0.0e+00
0.00.041.991 I print_info: n_ff             = 8192
0.00.041.991 I print_info: n_expert         = 0
0.00.041.991 I print_info: n_expert_used    = 0
0.00.041.991 I print_info: causal attn      = 1
0.00.041.991 I print_info: pooling type     = 0
0.00.041.991 I print_info: rope type        = 2
0.00.041.992 I print_info: rope scaling     = linear
0.00.041.992 I print_info: freq_base_train  = 10000.0
0.00.041.992 I print_info: freq_scale_train = 1
0.00.041.993 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.993 I print_info: rope_finetuned   = unknown
0.00.041.993 I print_info: ssm_d_conv       = 0
0.00.041.993 I print_info: ssm_d_inner      = 0
0.00.041.996 I print_info: ssm_d_state      = 0
0.00.041.996 I print_info: ssm_dt_rank      = 0
0.00.041.996 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.996 I print_info: model type       = 1.4B
0.00.041.997 I print_info: model params     = 1.41 B
0.00.041.997 I print_info: general.name     = 1.4B
0.00.041.997 I print_info: vocab type       = BPE
0.00.041.998 I print_info: n_vocab          = 50304
0.00.041.998 I print_info: n_merges         = 50009
0.00.041.998 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.998 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.002 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.002 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.002 I print_info: LF token         = 187 'Ċ'
0.00.042.004 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.004 I print_info: max token length = 1024
0.00.042.004 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.921 I load_tensors: offloading output layer to GPU
0.00.528.922 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.955 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.956 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.530.644 I llama_context: n_seq_max     = 1
0.00.530.646 I llama_context: n_ctx         = 128
0.00.530.647 I llama_context: n_ctx_per_seq = 128
0.00.530.647 I llama_context: n_batch       = 128
0.00.530.648 I llama_context: n_ubatch      = 128
0.00.530.648 I llama_context: flash_attn    = 0
0.00.530.651 I llama_context: freq_base     = 10000.0
0.00.530.651 I llama_context: freq_scale    = 1
0.00.530.652 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.530.654 I ggml_metal_init: allocating
0.00.530.728 I ggml_metal_init: found device: Apple M4
0.00.530.742 I ggml_metal_init: picking default device: Apple M4
0.00.532.428 I ggml_metal_init: using embedded metal library
0.00.539.027 I ggml_metal_init: GPU name:   Apple M4
0.00.539.031 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.539.031 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.539.032 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.539.033 I ggml_metal_init: simdgroup reduction   = true
0.00.539.033 I ggml_metal_init: simdgroup matrix mul. = true
0.00.539.033 I ggml_metal_init: has residency sets    = true
0.00.539.034 I ggml_metal_init: has bfloat            = true
0.00.539.034 I ggml_metal_init: use bfloat            = true
0.00.539.035 I ggml_metal_init: hasUnifiedMemory      = true
0.00.539.036 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.733 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.287 I init:      Metal KV buffer size =    24.00 MiB
0.00.560.291 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.332 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.563.464 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.563.466 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.563.466 I llama_context: graph nodes  = 967
0.00.563.467 I llama_context: graph splits = 2
0.00.563.471 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.471 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.300 I 
0.00.594.381 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.401 I perplexity: tokenizing the input ..
0.00.599.766 I perplexity: tokenization took 5.363 ms
0.00.599.779 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.735.727 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.737.063 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.737.077 I llama_perf_context_print:        load time =     584.22 ms
0.00.737.078 I llama_perf_context_print: prompt eval time =     135.66 ms /   128 tokens (    1.06 ms per token,   943.54 tokens per second)
0.00.737.079 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.737.080 I llama_perf_context_print:       total time =     142.78 ms /   129 tokens
0.00.737.654 I ggml_metal_free: deallocating

real	0m0.753s
user	0m0.077s
sys	0m0.131s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.008.795 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.899 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.904 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.906 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.907 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.907 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.908 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.909 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.911 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.913 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.913 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.914 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.914 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.917 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.917 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.776 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.566 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.567 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.568 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.569 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.569 I llama_model_loader: - type  f32:  194 tensors
0.00.025.569 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.570 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.570 I print_info: file format = GGUF V3 (latest)
0.00.025.571 I print_info: file type   = Q5_K - Medium
0.00.025.571 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.841 I load: special tokens cache size = 25
0.00.039.895 I load: token to piece cache size = 0.2984 MB
0.00.039.898 I print_info: arch             = gptneox
0.00.039.898 I print_info: vocab_only       = 0
0.00.039.898 I print_info: n_ctx_train      = 2048
0.00.039.898 I print_info: n_embd           = 2048
0.00.039.899 I print_info: n_layer          = 24
0.00.039.902 I print_info: n_head           = 16
0.00.039.902 I print_info: n_head_kv        = 16
0.00.039.903 I print_info: n_rot            = 32
0.00.039.903 I print_info: n_swa            = 0
0.00.039.903 I print_info: n_embd_head_k    = 128
0.00.039.903 I print_info: n_embd_head_v    = 128
0.00.039.904 I print_info: n_gqa            = 1
0.00.039.905 I print_info: n_embd_k_gqa     = 2048
0.00.039.906 I print_info: n_embd_v_gqa     = 2048
0.00.039.906 I print_info: f_norm_eps       = 1.0e-05
0.00.039.906 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.909 I print_info: f_logit_scale    = 0.0e+00
0.00.039.909 I print_info: n_ff             = 8192
0.00.039.909 I print_info: n_expert         = 0
0.00.039.910 I print_info: n_expert_used    = 0
0.00.039.910 I print_info: causal attn      = 1
0.00.039.910 I print_info: pooling type     = 0
0.00.039.911 I print_info: rope type        = 2
0.00.039.913 I print_info: rope scaling     = linear
0.00.039.914 I print_info: freq_base_train  = 10000.0
0.00.039.914 I print_info: freq_scale_train = 1
0.00.039.914 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.914 I print_info: rope_finetuned   = unknown
0.00.039.915 I print_info: ssm_d_conv       = 0
0.00.039.915 I print_info: ssm_d_inner      = 0
0.00.039.915 I print_info: ssm_d_state      = 0
0.00.039.915 I print_info: ssm_dt_rank      = 0
0.00.039.915 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.915 I print_info: model type       = 1.4B
0.00.039.916 I print_info: model params     = 1.41 B
0.00.039.916 I print_info: general.name     = 1.4B
0.00.039.916 I print_info: vocab type       = BPE
0.00.039.916 I print_info: n_vocab          = 50304
0.00.039.917 I print_info: n_merges         = 50009
0.00.039.921 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.921 I print_info: LF token         = 187 'Ċ'
0.00.039.923 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.923 I print_info: max token length = 1024
0.00.039.923 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.081 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.085 I load_tensors: offloading output layer to GPU
0.00.589.086 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.110 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.112 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.590.501 I llama_context: n_seq_max     = 1
0.00.590.503 I llama_context: n_ctx         = 2048
0.00.590.503 I llama_context: n_ctx_per_seq = 2048
0.00.590.504 I llama_context: n_batch       = 2048
0.00.590.505 I llama_context: n_ubatch      = 512
0.00.590.505 I llama_context: flash_attn    = 0
0.00.590.506 I llama_context: freq_base     = 10000.0
0.00.590.506 I llama_context: freq_scale    = 1
0.00.590.507 I ggml_metal_init: allocating
0.00.590.524 I ggml_metal_init: found device: Apple M4
0.00.590.536 I ggml_metal_init: picking default device: Apple M4
0.00.592.006 I ggml_metal_init: using embedded metal library
0.00.598.164 I ggml_metal_init: GPU name:   Apple M4
0.00.598.167 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.168 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.169 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.169 I ggml_metal_init: simdgroup reduction   = true
0.00.598.170 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.170 I ggml_metal_init: has residency sets    = true
0.00.598.170 I ggml_metal_init: has bfloat            = true
0.00.598.170 I ggml_metal_init: use bfloat            = true
0.00.598.171 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.172 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.828 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.670.591 I init:      Metal KV buffer size =   384.00 MiB
0.00.670.597 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.670.632 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.675.119 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.675.120 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.675.121 I llama_context: graph nodes  = 967
0.00.675.121 I llama_context: graph splits = 2
0.00.675.128 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.675.256 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.675.257 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.569 I main: llama threadpool init, n_threads = 4
0.00.735.612 I 
0.00.735.633 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.633 I 
0.00.735.811 I sampler seed: 1234
0.00.735.816 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.826 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.827 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.827 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.582.503 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53706.51 tokens per second)
0.01.582.504 I llama_perf_context_print:        load time =     726.03 ms
0.01.582.504 I llama_perf_context_print: prompt eval time =      55.23 ms /     7 tokens (    7.89 ms per token,   126.75 tokens per second)
0.01.582.505 I llama_perf_context_print:        eval time =     788.54 ms /    63 runs   (   12.52 ms per token,    79.89 tokens per second)
0.01.582.505 I llama_perf_context_print:       total time =     847.67 ms /    70 tokens
0.01.586.692 I ggml_metal_free: deallocating

real	0m1.604s
user	0m0.108s
sys	0m0.212s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.945 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.259 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.265 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.267 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.268 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.273 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.273 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.273 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.274 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.274 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.276 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.276 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.277 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.261 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.154 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.155 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.156 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.156 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.157 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.157 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.158 I llama_model_loader: - type  f32:  194 tensors
0.00.025.158 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.158 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.159 I print_info: file format = GGUF V3 (latest)
0.00.025.159 I print_info: file type   = Q5_K - Medium
0.00.025.160 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.092 I load: special tokens cache size = 25
0.00.039.104 I load: token to piece cache size = 0.2984 MB
0.00.039.107 I print_info: arch             = gptneox
0.00.039.107 I print_info: vocab_only       = 0
0.00.039.108 I print_info: n_ctx_train      = 2048
0.00.039.108 I print_info: n_embd           = 2048
0.00.039.108 I print_info: n_layer          = 24
0.00.039.111 I print_info: n_head           = 16
0.00.039.112 I print_info: n_head_kv        = 16
0.00.039.112 I print_info: n_rot            = 32
0.00.039.112 I print_info: n_swa            = 0
0.00.039.113 I print_info: n_embd_head_k    = 128
0.00.039.115 I print_info: n_embd_head_v    = 128
0.00.039.116 I print_info: n_gqa            = 1
0.00.039.117 I print_info: n_embd_k_gqa     = 2048
0.00.039.117 I print_info: n_embd_v_gqa     = 2048
0.00.039.118 I print_info: f_norm_eps       = 1.0e-05
0.00.039.119 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.119 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.119 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.119 I print_info: f_logit_scale    = 0.0e+00
0.00.039.120 I print_info: n_ff             = 8192
0.00.039.120 I print_info: n_expert         = 0
0.00.039.120 I print_info: n_expert_used    = 0
0.00.039.120 I print_info: causal attn      = 1
0.00.039.121 I print_info: pooling type     = 0
0.00.039.121 I print_info: rope type        = 2
0.00.039.121 I print_info: rope scaling     = linear
0.00.039.121 I print_info: freq_base_train  = 10000.0
0.00.039.122 I print_info: freq_scale_train = 1
0.00.039.122 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.122 I print_info: rope_finetuned   = unknown
0.00.039.122 I print_info: ssm_d_conv       = 0
0.00.039.124 I print_info: ssm_d_inner      = 0
0.00.039.124 I print_info: ssm_d_state      = 0
0.00.039.124 I print_info: ssm_dt_rank      = 0
0.00.039.124 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.124 I print_info: model type       = 1.4B
0.00.039.125 I print_info: model params     = 1.41 B
0.00.039.125 I print_info: general.name     = 1.4B
0.00.039.126 I print_info: vocab type       = BPE
0.00.039.126 I print_info: n_vocab          = 50304
0.00.039.126 I print_info: n_merges         = 50009
0.00.039.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.126 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.127 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.130 I print_info: LF token         = 187 'Ċ'
0.00.039.131 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.131 I print_info: max token length = 1024
0.00.039.131 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.123 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.127 I load_tensors: offloading output layer to GPU
0.00.586.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.151 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.586.152 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.587.502 I llama_context: n_seq_max     = 1
0.00.587.504 I llama_context: n_ctx         = 128
0.00.587.504 I llama_context: n_ctx_per_seq = 128
0.00.587.504 I llama_context: n_batch       = 128
0.00.587.505 I llama_context: n_ubatch      = 128
0.00.587.506 I llama_context: flash_attn    = 0
0.00.587.507 I llama_context: freq_base     = 10000.0
0.00.587.507 I llama_context: freq_scale    = 1
0.00.587.508 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.509 I ggml_metal_init: allocating
0.00.587.569 I ggml_metal_init: found device: Apple M4
0.00.587.582 I ggml_metal_init: picking default device: Apple M4
0.00.589.083 I ggml_metal_init: using embedded metal library
0.00.595.165 I ggml_metal_init: GPU name:   Apple M4
0.00.595.169 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.169 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.170 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.171 I ggml_metal_init: simdgroup reduction   = true
0.00.595.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.172 I ggml_metal_init: has residency sets    = true
0.00.595.172 I ggml_metal_init: has bfloat            = true
0.00.595.172 I ggml_metal_init: use bfloat            = true
0.00.595.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.175 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.612.017 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.615.500 I init:      Metal KV buffer size =    24.00 MiB
0.00.615.506 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.615.555 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.618.637 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.618.639 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.618.639 I llama_context: graph nodes  = 967
0.00.618.640 I llama_context: graph splits = 2
0.00.618.643 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.618.643 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.651.426 I 
0.00.651.509 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.651.528 I perplexity: tokenizing the input ..
0.00.656.457 I perplexity: tokenization took 4.927 ms
0.00.656.469 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.796.079 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.797.429 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.797.442 I llama_perf_context_print:        load time =     642.47 ms
0.00.797.443 I llama_perf_context_print: prompt eval time =     139.38 ms /   128 tokens (    1.09 ms per token,   918.35 tokens per second)
0.00.797.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.444 I llama_perf_context_print:       total time =     146.02 ms /   129 tokens
0.00.798.004 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.075s
sys	0m0.131s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.584 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.588 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.600 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.601 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.601 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.601 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.603 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.604 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.604 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.565 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.403 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.404 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.404 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.405 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.405 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.405 I llama_model_loader: - type  f32:  194 tensors
0.00.026.406 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.407 I print_info: file format = GGUF V3 (latest)
0.00.026.407 I print_info: file type   = Q6_K
0.00.026.408 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.304 I load: special tokens cache size = 25
0.00.040.347 I load: token to piece cache size = 0.2984 MB
0.00.040.350 I print_info: arch             = gptneox
0.00.040.350 I print_info: vocab_only       = 0
0.00.040.351 I print_info: n_ctx_train      = 2048
0.00.040.351 I print_info: n_embd           = 2048
0.00.040.351 I print_info: n_layer          = 24
0.00.040.353 I print_info: n_head           = 16
0.00.040.354 I print_info: n_head_kv        = 16
0.00.040.354 I print_info: n_rot            = 32
0.00.040.354 I print_info: n_swa            = 0
0.00.040.355 I print_info: n_embd_head_k    = 128
0.00.040.356 I print_info: n_embd_head_v    = 128
0.00.040.357 I print_info: n_gqa            = 1
0.00.040.357 I print_info: n_embd_k_gqa     = 2048
0.00.040.358 I print_info: n_embd_v_gqa     = 2048
0.00.040.359 I print_info: f_norm_eps       = 1.0e-05
0.00.040.363 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.364 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.364 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.364 I print_info: f_logit_scale    = 0.0e+00
0.00.040.365 I print_info: n_ff             = 8192
0.00.040.366 I print_info: n_expert         = 0
0.00.040.366 I print_info: n_expert_used    = 0
0.00.040.367 I print_info: causal attn      = 1
0.00.040.367 I print_info: pooling type     = 0
0.00.040.367 I print_info: rope type        = 2
0.00.040.367 I print_info: rope scaling     = linear
0.00.040.367 I print_info: freq_base_train  = 10000.0
0.00.040.368 I print_info: freq_scale_train = 1
0.00.040.368 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.368 I print_info: rope_finetuned   = unknown
0.00.040.368 I print_info: ssm_d_conv       = 0
0.00.040.368 I print_info: ssm_d_inner      = 0
0.00.040.377 I print_info: ssm_d_state      = 0
0.00.040.377 I print_info: ssm_dt_rank      = 0
0.00.040.378 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.378 I print_info: model type       = 1.4B
0.00.040.379 I print_info: model params     = 1.41 B
0.00.040.380 I print_info: general.name     = 1.4B
0.00.040.380 I print_info: vocab type       = BPE
0.00.040.380 I print_info: n_vocab          = 50304
0.00.040.380 I print_info: n_merges         = 50009
0.00.040.381 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.382 I print_info: LF token         = 187 'Ċ'
0.00.040.383 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.383 I print_info: max token length = 1024
0.00.040.383 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.656.453 I load_tensors: offloading 24 repeating layers to GPU
0.00.656.463 I load_tensors: offloading output layer to GPU
0.00.656.464 I load_tensors: offloaded 25/25 layers to GPU
0.00.656.497 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.656.502 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.657.905 I llama_context: n_seq_max     = 1
0.00.657.908 I llama_context: n_ctx         = 2048
0.00.657.908 I llama_context: n_ctx_per_seq = 2048
0.00.657.909 I llama_context: n_batch       = 2048
0.00.657.909 I llama_context: n_ubatch      = 512
0.00.657.910 I llama_context: flash_attn    = 0
0.00.657.911 I llama_context: freq_base     = 10000.0
0.00.657.911 I llama_context: freq_scale    = 1
0.00.657.912 I ggml_metal_init: allocating
0.00.657.936 I ggml_metal_init: found device: Apple M4
0.00.657.947 I ggml_metal_init: picking default device: Apple M4
0.00.659.361 I ggml_metal_init: using embedded metal library
0.00.666.757 I ggml_metal_init: GPU name:   Apple M4
0.00.666.761 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.764 I ggml_metal_init: simdgroup reduction   = true
0.00.666.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.765 I ggml_metal_init: has residency sets    = true
0.00.666.765 I ggml_metal_init: has bfloat            = true
0.00.666.766 I ggml_metal_init: use bfloat            = true
0.00.666.776 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.669 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.744.061 I init:      Metal KV buffer size =   384.00 MiB
0.00.744.074 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.744.123 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.748.209 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.748.211 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.748.211 I llama_context: graph nodes  = 967
0.00.748.211 I llama_context: graph splits = 2
0.00.748.216 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.339 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.340 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.642 I main: llama threadpool init, n_threads = 4
0.00.816.684 I 
0.00.816.704 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.704 I 
0.00.816.858 I sampler seed: 1234
0.00.816.862 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.881 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.881 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.881 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.697.842 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47619.05 tokens per second)
0.01.697.843 I llama_perf_context_print:        load time =     805.97 ms
0.01.697.844 I llama_perf_context_print: prompt eval time =      54.17 ms /     7 tokens (    7.74 ms per token,   129.22 tokens per second)
0.01.697.845 I llama_perf_context_print:        eval time =     824.23 ms /    63 runs   (   13.08 ms per token,    76.44 tokens per second)
0.01.697.845 I llama_perf_context_print:       total time =     881.91 ms /    70 tokens
0.01.701.181 I ggml_metal_free: deallocating

real	0m1.717s
user	0m0.112s
sys	0m0.227s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4716 (2cd8a903) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.840 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.996 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.006 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.007 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.007 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.012 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.012 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.013 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.013 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.016 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.016 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.016 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.956 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.975 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.841 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.842 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.842 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.843 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.843 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.844 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.844 I llama_model_loader: - type  f32:  194 tensors
0.00.025.844 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.845 I print_info: file format = GGUF V3 (latest)
0.00.025.846 I print_info: file type   = Q6_K
0.00.025.846 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.964 I load: special tokens cache size = 25
0.00.039.928 I load: token to piece cache size = 0.2984 MB
0.00.039.931 I print_info: arch             = gptneox
0.00.039.931 I print_info: vocab_only       = 0
0.00.039.931 I print_info: n_ctx_train      = 2048
0.00.039.931 I print_info: n_embd           = 2048
0.00.039.932 I print_info: n_layer          = 24
0.00.039.936 I print_info: n_head           = 16
0.00.039.936 I print_info: n_head_kv        = 16
0.00.039.937 I print_info: n_rot            = 32
0.00.039.937 I print_info: n_swa            = 0
0.00.039.937 I print_info: n_embd_head_k    = 128
0.00.039.937 I print_info: n_embd_head_v    = 128
0.00.039.938 I print_info: n_gqa            = 1
0.00.039.939 I print_info: n_embd_k_gqa     = 2048
0.00.039.940 I print_info: n_embd_v_gqa     = 2048
0.00.039.940 I print_info: f_norm_eps       = 1.0e-05
0.00.039.940 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.942 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.942 I print_info: f_logit_scale    = 0.0e+00
0.00.039.943 I print_info: n_ff             = 8192
0.00.039.943 I print_info: n_expert         = 0
0.00.039.943 I print_info: n_expert_used    = 0
0.00.039.945 I print_info: causal attn      = 1
0.00.039.945 I print_info: pooling type     = 0
0.00.039.945 I print_info: rope type        = 2
0.00.039.945 I print_info: rope scaling     = linear
0.00.039.946 I print_info: freq_base_train  = 10000.0
0.00.039.946 I print_info: freq_scale_train = 1
0.00.039.946 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.947 I print_info: rope_finetuned   = unknown
0.00.039.947 I print_info: ssm_d_conv       = 0
0.00.039.947 I print_info: ssm_d_inner      = 0
0.00.039.947 I print_info: ssm_d_state      = 0
0.00.039.947 I print_info: ssm_dt_rank      = 0
0.00.039.947 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.947 I print_info: model type       = 1.4B
0.00.039.948 I print_info: model params     = 1.41 B
0.00.039.948 I print_info: general.name     = 1.4B
0.00.039.952 I print_info: vocab type       = BPE
0.00.039.953 I print_info: n_vocab          = 50304
0.00.039.953 I print_info: n_merges         = 50009
0.00.039.953 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.953 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.953 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.953 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.954 I print_info: LF token         = 187 'Ċ'
0.00.039.954 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.954 I print_info: max token length = 1024
0.00.039.954 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.276.606 I load_tensors: offloading 24 repeating layers to GPU
0.00.276.613 I load_tensors: offloading output layer to GPU
0.00.276.614 I load_tensors: offloaded 25/25 layers to GPU
0.00.276.638 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.276.640 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.278.130 I llama_context: n_seq_max     = 1
0.00.278.132 I llama_context: n_ctx         = 128
0.00.278.132 I llama_context: n_ctx_per_seq = 128
0.00.278.133 I llama_context: n_batch       = 128
0.00.278.133 I llama_context: n_ubatch      = 128
0.00.278.134 I llama_context: flash_attn    = 0
0.00.278.134 I llama_context: freq_base     = 10000.0
0.00.278.135 I llama_context: freq_scale    = 1
0.00.278.136 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.278.138 I ggml_metal_init: allocating
0.00.278.164 I ggml_metal_init: found device: Apple M4
0.00.278.173 I ggml_metal_init: picking default device: Apple M4
0.00.279.530 I ggml_metal_init: using embedded metal library
0.00.285.508 I ggml_metal_init: GPU name:   Apple M4
0.00.285.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.285.512 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.285.513 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.285.514 I ggml_metal_init: simdgroup reduction   = true
0.00.285.514 I ggml_metal_init: simdgroup matrix mul. = true
0.00.285.514 I ggml_metal_init: has residency sets    = true
0.00.285.514 I ggml_metal_init: has bfloat            = true
0.00.285.515 I ggml_metal_init: use bfloat            = true
0.00.285.515 I ggml_metal_init: hasUnifiedMemory      = true
0.00.285.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.301.821 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.305.392 I init:      Metal KV buffer size =    24.00 MiB
0.00.305.396 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.305.438 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.308.606 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.308.608 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.308.608 I llama_context: graph nodes  = 967
0.00.308.609 I llama_context: graph splits = 2
0.00.308.612 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.308.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.350.726 I 
0.00.350.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.350.822 I perplexity: tokenizing the input ..
0.00.357.564 I perplexity: tokenization took 6.739 ms
0.00.357.582 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.497.749 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.499.204 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.499.223 I llama_perf_context_print:        load time =     340.88 ms
0.00.499.224 I llama_perf_context_print: prompt eval time =     139.22 ms /   128 tokens (    1.09 ms per token,   919.38 tokens per second)
0.00.499.227 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.499.227 I llama_perf_context_print:       total time =     148.50 ms /   129 tokens
0.00.499.763 I ggml_metal_free: deallocating

real	0m0.516s
user	0m0.077s
sys	0m0.094s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4716 (2cd8a903)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134408ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1344095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134409b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13440a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13440a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13440ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13440b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13440b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13440bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13440c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13440c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13440cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13440d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13440df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13440e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13440ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13440f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13440fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1344103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134410bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1344112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1344119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134412110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1344129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1344130d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134413390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1344139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134414610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134414b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134414e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1344152b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134415570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134415e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134416340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134416600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134416aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134416f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1344173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134417880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134417d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1344181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134418660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134418b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134418fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134419260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134419870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134419e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13441a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13441adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13441b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13441b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13441bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13441c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13441cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13441d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13441d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13441dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13441dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13441e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13441edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13441f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13441f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13441f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13441fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134420330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1344207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134420c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134421110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1344215b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134421a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134421ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134422390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134422830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134422d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1344232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134423820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134423d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1344242c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134424810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134424d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1344252b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134425800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134425d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1344262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1344267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134426d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134427290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1344277e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134427d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134428280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1344287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134428d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134429270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1344297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134429d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13442a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13442a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13441a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13442ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13442b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13442b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13442be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13442c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13442c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13442ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13442d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13442d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13442de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13442e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13442e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13442ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13442f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13442f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13442fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134430220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1344306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134430b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134431000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1344314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134431940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134431de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134432280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134432720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134432bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134433060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134433500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1344339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134433e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1344342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134434780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134434c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1344350c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134435560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134435a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134435ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134436340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1344367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134436c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134437120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1344375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134437a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134437f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1344383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134438840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134438ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134439180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134439620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134439ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134439f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13443a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13443a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13443ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13443b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13443b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13443bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13443bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13443c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13443c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13443cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13443d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13443d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13443db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13443e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13443e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13443e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13443ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13443f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13443f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13443fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134440080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134440520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1344409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134440e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134441300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1344417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134441c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1344420e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134442580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134442a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134442ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134443360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134443800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134443ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134444140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1344445e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134444a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134444f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1344453c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134445860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134445d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1344461a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134446640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134446ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134447030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134447580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134447ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134448020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1344482e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1344488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134448f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134449510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134449d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13444a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13444a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13444aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13444b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13444b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13444bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13444c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13444c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13444ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13444d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13444d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13444ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13444e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13444e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13444ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13444f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13444f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13444fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134450320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134450870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134450dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134451310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134451860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134451db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134452300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134452850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134452da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1344532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134453840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134453d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1344542e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134454830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134454d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1344552d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134455820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134455d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1344562c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134456810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134456d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1344572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134457800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134457d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1344582a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1344587f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134458d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134459290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1344597e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134459d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13445a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13445a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13445ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13445b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13445b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13445bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13445c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13445c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13445cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13445d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13445d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13445dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13445e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13445e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13445ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13445f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13445f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13445fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1344600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134460560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134460a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134460ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134461340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1344617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134461c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134462120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1344625c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134462a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134462f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1344633a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134463840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134463ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134464230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134464950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134465070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134465790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134465eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134466170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134466960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134466c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134467230 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.717.534 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.539 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x116404b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x116404f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x116405400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116405870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116405ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116406150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1164065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116406a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x116406ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116407310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116407780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116407e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116408990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x116409140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x116409950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11640a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11640a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11640aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11640b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11640bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11640c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11640cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11640d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11640d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11640e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11640e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11640e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11640ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11640ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11640f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11640f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11640fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x116410180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x116410440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1164108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x116410d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116411190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x116411600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116411a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116411ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116412350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1164127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116412c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1164130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116413510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116413980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116413df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x116414260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1164146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116414b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x116414fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x116415420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x116415890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x116415d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x116416170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1164165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x116416b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x116417050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1164174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116417930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116417da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x116418210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116418680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116418af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116418f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1164193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116419840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116419cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11641a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11641a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11641aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11641ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11641b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11641b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11641bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11641c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11641c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11641c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11641cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11641d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11641d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11641dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11641df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11641e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11641e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11641ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11641f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11641f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11641f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11641fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1164202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116420730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116420ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116421010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116421480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1164218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116421d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1164221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116422640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116422ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116422f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x116423390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116423800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116423c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1164240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x116424550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1164249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116424e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1164252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116425710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116425b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116425ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116426460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1164268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116426d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1164271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116427620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x116427a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x116427f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x116428370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1164287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x116428c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1164290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x116429530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1164299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x116429e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11642a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11642a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11642ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11642afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11642b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11642b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11642bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11642c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11642c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11642ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11642cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11642d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11642d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11642dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11642e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11642e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11642e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11642edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11642f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11642f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11642fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11642ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x116430420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x116430890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x116430d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x116431170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1164315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x116431a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116431ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116432330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1164327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116432c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116433080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1164334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116433960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116433dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116434240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1164346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116434b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x116434f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116435bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116435e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116436140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1164365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116436a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116436e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116437300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116437770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116437be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116438050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1164384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x116438930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116438da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116439210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116439680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x116439af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116439f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11643a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11643a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11643acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11643b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11643b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11643ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11643be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11643c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11643c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11643cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11643d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11643d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11643d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11643dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11643e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11643e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11643ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11643ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11643f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11643f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11643fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116440290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x116440700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116440b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116440fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116441500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116441a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x116442580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x116442840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x116442e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1164433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x116443980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x116443f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x116444500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x116444ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x116445080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x116445640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x116445c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1164461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x116446780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x116446d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x116447300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1164478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x116447e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x116448440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x116448a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x116448fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x116449580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x116449b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11644a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11644a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11644ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11644b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11644b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11644bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11644c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11644c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11644cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11644d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11644da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11644e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11644e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11644ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11644f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11644f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11644fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1164502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116450880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116450e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116451400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1164519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116451f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116452540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116452b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1164530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116453680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116453c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x116454200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1164547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x116454d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x116455340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x116455900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x116455ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x116456480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x116456a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x116456f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x116457440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x116457940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x116457e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x116458340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x116458840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x116458d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x116459240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x116459740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x116459c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11645a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11645a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11645ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11645b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11645b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11645bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11645c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11645cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11645d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11645d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11645df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11645e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11645e830 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11645b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11644c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11644b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x116448140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x116445900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x116455040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x116452800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x116450580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11644e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x116446480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x116443c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x116448cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x116449e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11644f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11644c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x116453f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x116447b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x116451100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11644a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11644cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1164475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x116455600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1164447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1164430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x116445340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x116455bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11644af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x116453380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x116449280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11644bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11644fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x116447000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11644ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1164516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x116445ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1164544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x116451c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11644d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x116456740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x116444d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x116456180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x116444200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x116454a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11644e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x116450b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x116453940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x116452240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11644a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x116441cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x116404680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11645da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11640b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11645ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11645f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11645f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11645f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11645fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11645fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11645ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x116460250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x116460510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1164607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x116460a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x116460d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x116461010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1164612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x116461590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x116461850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x116461b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x116461dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x116462090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x116462350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x116462610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1164628d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x116462b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x116462e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x116463110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1164633d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x116463690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x116463950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x116463c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x116463ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x116464190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x116464450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x116464710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1164649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x116464c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x116464f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x116465210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1164654d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x116465790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x116465a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x116465d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x116465fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x116466290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x116466550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x116466810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x116466ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x116466d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x116467050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x116467310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1164675d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x116467890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x116467b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x116467e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1164680d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x116468390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x116468650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x116468910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x116468bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x116468e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x116469150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x116469410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1164696d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x116469990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x116469c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x116469f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11646a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11646a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11646a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11646aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11646acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11646af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11646b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11646b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11646b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11646ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11646bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11646c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11646c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11646c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11646c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11646cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11646cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11646d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11646d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11646d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11646d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11646db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11646de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11646e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11646e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11646e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11646e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11646ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11646eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11646f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11646f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11646f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11646f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11646fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11646ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x116470210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1164704d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x116470790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x116470a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x116470d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x116470fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x116471290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x116471550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x116471810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x116471ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x116471d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x116472050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x116472310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1164725d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x116472890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x116472b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x116472e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1164730d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x116473390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x116473650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x116473910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x116473bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x116473e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x116474150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x116474410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1164746d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x116474990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x116474c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x116474f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1164751d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x116475490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x116475750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x116475a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x116475cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x116475f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x116476250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x116476510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1164767d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x116476a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x116476d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x116477010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1164772d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x116477590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x116477850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x116477b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x116477dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x116478090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x116478350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x116478610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1164788d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x116478b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x116478e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x116479110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1164793d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x116479690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x116479950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x116479c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x116479ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11647a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11647a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11647aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11647ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11647afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11647b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11647ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11647bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11647c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11647ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11647cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11647d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11647da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11647df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11647e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11647ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11647ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11647f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11647fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11647ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1164804a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1164809f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x116480f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x116481490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1164819e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x116481f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x116482480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1164829d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x116482f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x116483470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1164839c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x116483f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x116484460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1164849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x116484f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x116485450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1164859a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x116485ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x116486440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x116486990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x116486ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x116487430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x116487980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x116487ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x116488420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x116488970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x116488ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x116489410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x116489960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x116489eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11648a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11648a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11648aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11648b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11648b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11648be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11648c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11648c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11648c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11648cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11648d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11648d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11648d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11648dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11648e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11648e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11648eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11648efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11648f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11648f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11648fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x116490160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1164905d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x116490a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x116491730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x116491e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x116492570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x116492830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x116492ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1164932a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1164938b0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.779s
user	0m0.286s
sys	0m0.332s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4716 (2cd8a903)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x148e0bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x148e0c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x148e0ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x148e0d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x148e0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x148e0dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x148e0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x148e0e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x148e0ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x148e0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x148e0f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x148e0fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x148e106d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x148e10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x148e11690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x148e11db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x148e124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x148e12bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x148e13310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x148e13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x148e14200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x148e14920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x148e15040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x148e158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x148e16000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x148e162c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x148e168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x148e17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x148e17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x148e17d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x148e181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x148e184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x148e18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x148e19270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x148e19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x148e199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x148e19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x148e1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x148e1a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x148e1ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x148e1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x148e1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x148e1ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x148e1bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x148e1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x148e1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x148e1cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x148e1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x148e1dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x148e1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x148e1e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x148e1ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x148e1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x148e1fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x148e20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x148e207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x148e20c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x148e20f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x148e21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x148e21d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x148e21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x148e22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x148e22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x148e22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x148e23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x148e23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x148e23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x148e24040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x148e244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x148e24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x148e24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x148e252c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x148e25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x148e25cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x148e26200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x148e26750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x148e26ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x148e271f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x148e27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x148e27c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x148e281e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x148e28730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x148e28c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x148e291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x148e29720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x148e29c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x148e2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x148e2a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x148e2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x148e2b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x148e2b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x148e2bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x148e2c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x148e2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x148e2cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x148e2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x148e2d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x148e1d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x148e2db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x148e2e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x148e2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x148e2eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x148e2f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x148e2f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x148e2fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x148e302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x148e30830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x148e30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x148e312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x148e31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x148e31d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x148e322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x148e32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x148e32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x148e33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x148e335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x148e33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x148e33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x148e343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x148e34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x148e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x148e351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x148e35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x148e35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x148e35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x148e36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x148e368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x148e36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x148e37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x148e376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x148e37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x148e37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x148e38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x148e38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x148e38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x148e39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x148e39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x148e39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x148e3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x148e3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x148e3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x148e3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x148e3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x148e3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x148e3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x148e3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x148e3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x148e3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x148e3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x148e3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x148e3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x148e3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x148e3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x148e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x148e3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x148e3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x148e3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x148e3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x148e3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x148e40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x148e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x148e40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x148e40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x148e413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x148e41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x148e41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x148e421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x148e42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x148e42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x148e42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x148e43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x148e438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x148e43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x148e44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x148e446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x148e44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x148e45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x148e454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x148e45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x148e45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x148e46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x148e46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x148e46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x148e47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x148e47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x148e479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x148e47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x148e482f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x148e48790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x148e48c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x148e490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x148e49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x148e49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x148e49f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x148e4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x148e4aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x148e4af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x148e4b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x148e4b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x148e4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x148e4c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x148e4cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x148e4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x148e4d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x148e4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x148e4dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x148e4e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x148e4ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x148e4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x148e4f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x148e4fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x148e50280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x148e507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x148e50d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x148e51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x148e517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x148e51d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x148e52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x148e527b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x148e52d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x148e53250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x148e537a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x148e53cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x148e54240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x148e54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x148e54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x148e55230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x148e55780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x148e55cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x148e56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x148e56770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x148e56cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x148e57210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x148e57760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x148e57cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x148e58200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x148e58750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x148e58ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x148e591f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x148e59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x148e59c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x148e5a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x148e5a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x148e5ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x148e5b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x148e5b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x148e5bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x148e5c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x148e5c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x148e5cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x148e5d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x148e5d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x148e5dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x148e5e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x148e5e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x148e5ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x148e5f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x148e5f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x148e5fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x148e60180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x148e606d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x148e60c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x148e61170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x148e616c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x148e61c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x148e62160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x148e626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x148e62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x148e62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x148e63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x148e63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x148e63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x148e64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x148e64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x148e64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x148e65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x148e654f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x148e65990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x148e65e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x148e662d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x148e66770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x148e66c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x148e67160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x148e67880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x148e67fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x148e686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x148e68de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x148e690a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x148e69890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x148e69b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x148e6a160 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.118.659 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a804ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a805150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a8055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a805a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a805ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a806310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a806780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a806bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a807060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a8074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a807940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a808020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a8092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a809b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a80a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a80a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a80b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a80b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a80bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a80c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a80cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a80d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a80dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a80e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a80e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a80e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a80ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a80f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a80f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a80fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a80ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a8103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a810690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a810b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a810f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a8113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a811850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a811cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a812130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a8125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a812a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a8132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a813760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a813bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a814040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a8144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a814920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a814d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a815200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a815670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a815ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a815f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a8163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a816830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a816da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a8172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a817710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a817b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a817ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a818460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a8188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a818d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a8191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a819a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a819f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a81a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a81a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a81ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a81b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a81b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a81b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a81be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a81c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a81c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a81cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a81cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a81d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a81d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a81dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a81e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a81e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a81ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a81eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a81f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a81f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a81fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a8200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a820510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a820980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a820df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a821260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a8216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a821b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a821fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a822420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a822890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a822d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a8235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a823ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a824330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a8247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a824c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a825080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a8254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a825960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a825dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a826240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a8266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a826b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a826f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a827400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a827870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a827ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a828150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a8285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a828a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a828ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a829310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a829780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a829bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a82a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a82a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a82a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a82adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a82b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a82b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a82bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a82bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a82c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a82c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a82ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a82d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a82d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a82da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a82de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a82e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a82e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a82ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a82f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a82f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a82f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a82fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a830200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a830670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a830ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a830f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a8313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a831ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a832110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a832580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a8329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a832e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a8332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a833740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a833bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a834020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a834490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a834900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a834d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a8351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a835e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a8360d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a836390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a836800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a836c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a8370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a837550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a8379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a837e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a8382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a838710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a838b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a838ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a839460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a8398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a839d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a83a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a83a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a83aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a83af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a83b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a83b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a83bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a83c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a83c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a83c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a83ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a83d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a83d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a83db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a83dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a83e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a83e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a83ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a83f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a83f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a83fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a840070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a8404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a840950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a840dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a841230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a841750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a841c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a8427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a842a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a843050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a843610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a843bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a844190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a844750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a844d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a8452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a845890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a845e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a846410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a8469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a846f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a847550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a847b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a8480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a848690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a848c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a849210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a8497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a849d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a84a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a84a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a84aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a84b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a84ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a84c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a84c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a84cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a84d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a84d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a84dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a84e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a84e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a84ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a84f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a84f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a84ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a850510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a850ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a851090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a851650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a851c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a8521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a852790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a852d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a853310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a8538d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a853e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a854450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a854a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a854fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a855590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a855b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a856110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a8566d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a856c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a857190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a857690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a857b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a858090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14a858590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14a858a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14a858f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14a859490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14a859990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14a859e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14a85a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14a85a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14a85ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14a85b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14a85b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14a85c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14a85c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14a85cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14a85d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14a85d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14a85e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14a85e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14a85ea80 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14a85ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14a84c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14a84b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14a848390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14a845b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14a855290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14a852a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14a8507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14a84e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14a8466d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14a843e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14a848f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14a84a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14a84f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14a84c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14a854150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14a846c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14a84f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14a849a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14a842d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14a84d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14a848950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14a853010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14a84df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14a8438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14a845590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14a855e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14a84b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14a8535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14a8494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14a84bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14a84fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14a84abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14a847250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14a851910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14a846110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14a854710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14a851ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14a84d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14a856990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14a844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14a8563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14a844450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14a854cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14a84eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14a850d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14a853b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14a852490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14a84a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14a841f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14a804880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14a85dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14a807c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14a85f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14a85f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14a85f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14a85f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14a85fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14a85ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14a8601e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14a8604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14a860760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14a860a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14a860ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14a860fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14a861260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14a861520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14a8617e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14a861aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14a861d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14a862020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14a8622e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14a8625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14a862860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14a862b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14a862de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14a8630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14a863360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14a863620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14a8638e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14a863ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14a863e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14a864120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14a8643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14a8646a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14a864960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14a864c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14a864ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14a8651a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14a865460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14a865720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14a8659e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14a865ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14a865f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14a866220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14a8664e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14a8667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14a866a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14a866d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14a866fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14a8672a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14a867560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14a867820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14a867ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14a867da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14a868060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14a868320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14a8685e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14a8688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14a868b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14a868e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14a8690e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14a8693a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14a869660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14a869920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14a869be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14a869ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14a86a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14a86a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14a86a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14a86a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14a86ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14a86af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14a86b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14a86b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14a86b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14a86ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14a86bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14a86bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14a86c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14a86c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14a86c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14a86caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14a86cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14a86d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14a86d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14a86d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14a86d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14a86db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14a86dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14a86e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14a86e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14a86e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14a86e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14a86eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14a86ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14a86f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14a86f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14a86f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14a86f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14a86fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14a86fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14a8701a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14a870460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14a870720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14a8709e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14a870ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14a870f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14a871220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14a8714e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14a8717a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14a871a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14a871d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14a871fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14a8722a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14a872560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14a872820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14a872ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14a872da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14a873060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14a873320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14a8735e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14a8738a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14a873b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14a873e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14a8740e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14a8743a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14a874660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14a874920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14a874be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14a874ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14a875160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14a875420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14a8756e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14a8759a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14a875c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14a875f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14a8761e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14a8764a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14a876760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14a876a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14a876ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14a876fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14a877260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14a877520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14a8777e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14a877aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14a877d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14a878020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14a8782e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14a8785a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14a878860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14a878b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14a878de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14a8790a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14a879360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14a879620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14a8798e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14a879ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14a879e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14a87a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14a87a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14a87a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14a87ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14a87b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14a87b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14a87bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14a87c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14a87c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14a87cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14a87d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14a87d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14a87dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14a87e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14a87e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14a87ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14a87f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14a87f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14a87fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14a880170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14a8806c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14a880c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14a881160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14a8816b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14a881c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14a882150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14a8826a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14a882bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14a883140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14a883690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14a883be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14a884130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14a884680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14a884bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14a885120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14a885670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14a885bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14a886110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14a886660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14a886bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14a887100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14a887650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14a887ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14a8880f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14a888640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14a888b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14a8890e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14a889630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14a889b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14a88a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14a88a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14a88ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14a88b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14a88b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14a88bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14a88c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14a88c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14a88c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14a88cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14a88d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14a88d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14a88da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14ed04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14ed044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14ed04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14ed04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14ed05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14ed056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14ed05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14ed05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14ed06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14ed06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14ed06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14ed07750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14ed07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14ed08590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14ed08cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14ed08f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14ed093e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14ed099e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14ed09ff0 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.994s
user	0m0.241s
sys	0m0.206s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.55 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.72 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   2.27 sec*proc (2 tests)

Total Test time (real) =   2.28 sec
        2.40 real         0.54 user         0.28 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.24 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.56 real         0.13 user         0.08 sys
```
